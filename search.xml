<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[AWS Certified Sysops Administrator - Associate Road Map]]></title>
      <url>/2017/12/04/AWS-Certified-Sysops-Administrator-Associate/</url>
      <content type="html"><![CDATA[<h2 id="Official-AWS-Certification-Page"><a href="#Official-AWS-Certification-Page" class="headerlink" title="Official AWS Certification Page"></a>Official AWS Certification Page</h2><p>访问官网<a href="https://amazonaws-china.com/certification/certification-prep/" target="_blank" rel="external">AWS Certification</a></p>
<ul>
<li>参加 AWS 培训课程</li>
<li>查看考试指南和样题<ul>
<li>了解考试涉及的概念并整体了解需要学习哪些内容, <a href="http://awstrainingandcertification.s3.amazonaws.com/production/AWS_certified_sysops_associate_blueprint.pdf" target="_blank" rel="external">AWS Certified SysOps Administrator - Associate 考试指南</a> 相当于考试大纲, 必看,而且需要反复的看。因为学习过一阵后再来看Guide，会有更深的体会。</li>
<li><a href="http://awstrainingandcertification.s3.amazonaws.com/production/AWS_certified_sysops_associate_examsample.pdf" target="_blank" rel="external">考试样题</a>用于熟悉题目题型</li>
</ul>
</li>
<li>完成自主进度动手实验和备考任务<ul>
<li>官方<a href="https://www.qwiklabs.com/learning_paths/20/lab_catalogue?locale=en" target="_blank" rel="external">qwikLABS 任务</a>提供了一系列动手实验, 提供部分免费实验，但大部分实验所需的积分都需要购买。高性价比的做法是， 注册一个AWS全球账号，使用一年的免费额度来对照着实验手册来进行试验。</li>
</ul>
</li>
<li>学习 AWS 白皮书<ul>
<li>白皮书是纯英文的，而且每个白皮书篇幅都很长，读起来既费时又枯燥。但是有时间还是建议把推荐的几个都看一下。</li>
</ul>
</li>
<li>查看 AWS 常见问题</li>
<li>参加模拟考试<ul>
<li>20美刀一次，主要目的是为了让人熟悉考试时上机的流程。是否需要因人而异, 特别想先熟悉下考试流程的可以考虑参加一次。我个人觉得没有必要, 因为真实考试时，操作界面一目了然，没有磕磕绊绊的机关，省下20美刀可以去买一份课程。</li>
</ul>
</li>
<li>报名考试并获得认证<ul>
<li>登陆<a href="https://www.aws.training/certification" target="_blank" rel="external">https://www.aws.training/certification</a>注册进行考试<a id="more"></a>
</li>
</ul>
</li>
</ul>
<h2 id="考试指南"><a href="#考试指南" class="headerlink" title="考试指南"></a>考试指南</h2><p><a href="http://awstrainingandcertification.s3.amazonaws.com/production/AWS_certified_sysops_associate_blueprint.pdf" target="_blank" rel="external">AWS Certified SysOps Administrator - Associate 考试指南</a> 读三遍，读三遍，读三遍</p>
<p>各个Domain的分数比例如下:<br><img src="/images/AWS/Sysops/Domain_on_sysops_admin_associate_blueprint.jpg" alt="Domain_on_sysops_admin_associate_blueprint"></p>
<h2 id="视频学习"><a href="#视频学习" class="headerlink" title="视频学习"></a>视频学习</h2><p><a href="https://acloud.guru" target="_blank" rel="external">Acloudguru</a> 中<a href="https://acloud.guru/course/aws-certified-sysops-administrator-associate/dashboard" target="_blank" rel="external">aws-certified-sysops-administrator-associate</a>视频的学习</p>
<h3 id="要点摘录"><a href="#要点摘录" class="headerlink" title="要点摘录"></a>要点摘录</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><ul>
<li>Introduction</li>
<li>Monitoring, Metrics &amp; Analysis</li>
<li>High Availability</li>
<li>Deployment &amp; Provisioning</li>
<li>Data Management</li>
<li>OpsWorks</li>
<li>Security</li>
<li>Networking</li>
<li>VPCs</li>
</ul>
<h4 id="Monitoring-Metrics-amp-Analysis"><a href="#Monitoring-Metrics-amp-Analysis" class="headerlink" title="Monitoring, Metrics &amp; Analysis"></a>Monitoring, Metrics &amp; Analysis</h4><ul>
<li><p>CloudWatch</p>
<ul>
<li><p>Amazon CloudWatch is a monitoring service to monitor your AWS resources, as well as the applications that you run on AWS.</p>
</li>
<li><p>CloudWatch can monitor things like :</p>
<ul>
<li>Compute<ul>
<li>Autoscaling Groups</li>
<li>Elastic Load Balancers</li>
<li>Route53 Health Checks</li>
</ul>
</li>
<li>Storage &amp; Content Delivery<ul>
<li>EBS Volumes</li>
<li>Storage Gateways</li>
<li>CloudFront</li>
</ul>
</li>
<li>Database &amp; Analytics<ul>
<li>DynamoDB</li>
<li>Elasticache Nodes</li>
<li>RDS Instances</li>
<li>Elastic MapReduce Job Flows</li>
<li>Redshift</li>
</ul>
</li>
<li>Other<ul>
<li>SNS Topics</li>
<li>SQS Queues</li>
<li>Opsworks</li>
<li>CloudWatch Logs</li>
<li>Estimated Charges on your AWS Bill</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>CloudWatch and EC2</p>
<ul>
<li>Host Level Metrics Consist of:<ul>
<li>CPU</li>
<li>Network</li>
<li>Disk</li>
<li>Status Check</li>
</ul>
</li>
<li>RAM Utilization is a custom metric! By default EC2 monitoring is 5 minute intervals, unless you enable detailed monitoring which will then make it 1 minute intervals.</li>
<li><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ec2-metricscollected.html" target="_blank" rel="external">EC2 Metric</a><ul>
<li>CPUCreditUsage - Units: Count</li>
<li>CPUCreditBalance - Units: Count</li>
<li>CPUUtilization - Units: Percent</li>
<li>DiskReadOps - Units: Count</li>
<li>DiskWriteOps - Units: Count</li>
<li>DiskReadBytes - Units: Bytes</li>
<li>DiskWriteBytes - Units: Bytes</li>
<li>NetworkIn - Units: Bytes</li>
<li>NetworkOut - Units: Bytes</li>
<li>NetworkPacketsIn - Units: Count</li>
<li>NetworkPacketsOut - Units: Count</li>
<li>StatusCheckFailed - Units: Count</li>
<li>StatusCheckFailed_Instance - Units: Count</li>
<li>StatusCheckFailed_System - Units: Count</li>
</ul>
</li>
<li><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ec2-metricscollected.html" target="_blank" rel="external">EC2 Dimension</a><br>  If you’re using Detailed Monitoring, you can filter the EC2 instance data using any of the dimensions in the following table.<ul>
<li>AutoScalingGroupName</li>
<li>ImageId</li>
<li>InstanceId</li>
<li>InstanceType</li>
</ul>
</li>
<li>By Default - CloudWatch Metric stored 2 Weeks. You can retrieve data that is longer than 2 weeks using the GetMetricStatistics API or by using third party tools offered by AWS partners.</li>
<li>You can retrieve data from any terminated EC2 or ELB instance for up to 2 weeks after it’s termination.</li>
</ul>
</li>
<li><p>Metric Granularity</p>
<ul>
<li>It depends on the AWS service. Many default metrics for many default services are 1 minute, but it can be 3 or 5 minutes depending on the service.</li>
<li>Exam Tips: For custom metric the minimum granularity that you can have is 1 minute.</li>
</ul>
</li>
<li><p>CloudWatch Alarms</p>
<ul>
<li>You can create an alarm to monitor any Amazon CloudWatch metric in your account. This can include EC2 CPU Utilization, Elastic Loa Balancer Latency of even the charges on your AWS bill. You can set the appropriate thresholds in which to trigger the alarms and also set what actions should be taken if an alarm state is reached. This will be covered in a subsequent lecture.</li>
</ul>
</li>
<li><p>EC2 Status check</p>
<ul>
<li>System Status Check (Checks Host, underlying physical Host)<ul>
<li>Loss of network connectivity</li>
<li>Loss of system power</li>
<li>Software issues on the physical host</li>
<li>Hardware issues on the physical host</li>
<li>Best way to resolve issues is to stop and then start the VM again.</li>
</ul>
</li>
<li>Instance Status Check (Checks VM)<ul>
<li>Failed system status checks</li>
<li>Misconfigured networking or startup configuration</li>
<li>Exhausted memory</li>
<li>Corrupted file system</li>
<li>Incompatible kernel</li>
<li>Best way to trouble shoot is by rebooting the instance or by making modifications in your operating system.</li>
</ul>
</li>
</ul>
</li>
<li><p>Custom Metrics</p>
<ul>
<li>AWS Namespaces</li>
<li>Custom Namespaces (the custom metric is selected in this namespace)</li>
</ul>
</li>
<li><p>Monitoring EBS</p>
<ul>
<li>4 Different Types of EBS Storage<ul>
<li>General Purpose (SSD) - gp2</li>
<li>Provisioned IOPS(SSD) - io1</li>
<li>Throughput Optimized(HDD) - st1</li>
<li>Cold (HDD) - sc1</li>
</ul>
</li>
<li><p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html" target="_blank" rel="external">Compare with each volumes type</a><br>  <img src="/images/AWS/Sysops/volumes_type_of_EBS.png" alt="volumes_type_of_EBS"></p>
</li>
<li><p>IOPS &amp; Volumes</p>
<ul>
<li>General Purpose SSD volumes have a base of 3 IOPS per/GiB of volume size.<ul>
<li>Maximum volume size of 16,384 GiB</li>
<li>Maximum IOPS Size of 10,000 IOPS Total (after that you need to move to provisioned IOPS)</li>
</ul>
</li>
</ul>
</li>
<li>IOPS &amp; Volumes Examples<ul>
<li>Say we have a 1 GiB Volume. We get 3 IOPS per Gb so we have 3*1=3 IOPS</li>
<li>We can burst performance on this volume up to 3000 IOPS if we want</li>
<li>Using I/O Credits</li>
<li>The burst would be 2997 IOPS( 3000 - 3)</li>
</ul>
</li>
<li>I/O Credits<ul>
<li>When your volume requires more than the baseline performance I/O level, it simply uses I/O credits in the credit balance to burst to the required performance level, up to a maximum of 3,000 IOPS.<ul>
<li>Each volume receives an initial I/O credit balance of 5,400,000 I/O credits.</li>
<li>This is enough to sustain the maximum burst performance of 3,000 IOPS for 30 minutes.</li>
<li>When you are not going over your provisioned IO level(ie bursting) you will be earning credits.</li>
</ul>
</li>
</ul>
</li>
<li>I/O Credits - it is beyond the scope of the SysOps Associate Exam to be able to calculate this.<br><img src="/images/AWS/Sysops/IO_credits_of_EBS.jpg" alt="IO_credits_of_EBS"></li>
<li><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-initialize.html" target="_blank" rel="external">Pre-Warming EBS Volumes</a><ul>
<li>New EBS volumes receive their maximum performance the moment that they are available and do not require initialization (formerly known as pre-warming). However, storage blocks on volumes that were restored from snapshots must be initialized (pulled down from Amazon S3 and written to the volume) before you can access the block. This preliminary action takes time and can cause a significant increase in the latency of an I/O operation the first time each block is accessed. For most applications, amortizing this cost over the lifetime of the volume is acceptable. Performance is restored after the data is accessed once.</li>
<li>You can avoid this performance hit in a production environment by reading from all of the blocks on your volume before you use it; this process is called initialization. For a new volume created from a snapshot, you should read all the blocks that have data before using the volume.</li>
</ul>
</li>
<li><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html" target="_blank" rel="external">EBS CloudWatch Metrics</a><br>  <img src="/images/AWS/Sysops/ebs_cloudwatch_metric.png" alt="ebs_cloudwatch_metric"></li>
<li><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html" target="_blank" rel="external">Volume Status Checks</a><br>  <img src="/images/AWS/Sysops/volume_status_check_EBS.jpg" alt="volume_status_check_EBS"><ul>
<li>Exam Tips<ul>
<li>Degraded or Severely Degraded = Warning</li>
<li>Stalled or Not Available = Impaired</li>
</ul>
</li>
</ul>
</li>
<li><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modify-volume.html" target="_blank" rel="external">Modifying EBS Volumes</a><ul>
<li>If your Amazon EBS volume is attached to a current generation EC2 instance type, you can increase its size, change its volume type, or (for an io1 volume) adjust its IOPS performance, all without detaching it. You can apply these changes to detached volumes as well.<ul>
<li>Issue the modification command (console or command line)</li>
<li>Monitor the progress of the modification</li>
<li>If the size of the volume was modified, extend the volume’s file system to take advantage of the increased storage capacity.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Monitoring RDS</p>
<ul>
<li>Two Type of monitoring available for RDS<ul>
<li>CloudWatch - In CloudWatch you can monitor RDS by Metrics.</li>
<li>RDS itself - In RDS itself, you can monitor RDS by Events.</li>
</ul>
</li>
<li><a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Monitoring.html" target="_blank" rel="external">RDS Metrics</a><ul>
<li>BinLogDiskUsage - Units: Bytes</li>
<li>CPUUtilization - Units: Percent</li>
<li><strong>DatabaseConnections</strong> - Units: Count</li>
<li><strong>DiskQueueDepth</strong> - Units: Count</li>
<li>FreeableMemory - Units: Bytes</li>
<li><strong>FreeStorageSpace</strong> - Units: Bytes</li>
<li><strong>ReplicaLag</strong> - Units: Seconds</li>
<li>SwapUsage - Units: Bytes</li>
<li><strong>ReadIOPS</strong> - Units: Count/Second</li>
<li><strong>WriteIOPS</strong> - Units: Count/Second</li>
<li><strong>ReadLatency</strong> - Units: Seconds</li>
<li><strong>WriteLatency</strong> - Units: Seconds</li>
<li>ReadThroughput - Units: Bytes/Second</li>
<li>WriteThroughput - Units: Bytes/Second</li>
<li>NetworkReceiveThroughput - Units: Bytes/second</li>
<li>NetworkTransmitThroughput - Units: Bytes/second</li>
</ul>
</li>
<li>Have a general idea of what each metric does. You do not need to know the name of each metric, but pay attention to the ones in bold.</li>
</ul>
</li>
<li><p>Monitoring ELB</p>
<ul>
<li>ELB - Every 60 seconds (Provided there is traffic)<br>Elastic Load Balancing only reports when requests are flowing through the load balancer. If there are no requests or data for a given metric, the metric will not be reported to CloudWatch. If there are requests flowing through the load balancer, Elastic Load Balancing will measure and send metrics for that load balancer in 60 second intervals</li>
<li><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/elb-metricscollected.html" target="_blank" rel="external">ELB Metrics</a><br>  <img src="/images/AWS/Sysops/ELB_Metric.png" alt="ELB_Metric"><ul>
<li>Have a general idea of what each metric does. You do not need to know the name of each metric, but pay attention to <strong>SurgeQueueLength</strong> &amp; <strong>SpilloverCount</strong>.</li>
</ul>
</li>
</ul>
</li>
<li><p>Monitoring Elasticache</p>
<ul>
<li>Two engines<ul>
<li>Memcached</li>
<li>Redis</li>
</ul>
</li>
<li><a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/CacheMetrics.WhichShouldIMonitor.html" target="_blank" rel="external">Four important things</a><ul>
<li>CPU Utilization</li>
<li>Swap Usage</li>
<li>Evictions</li>
<li>Concurrent Connections</li>
</ul>
</li>
<li>CPU Utilization<ul>
<li>Memcached<ul>
<li>Multi-threaded</li>
<li>Can handle loads of up to 90%. If it exceeds 90% add more nodes to the cluster</li>
</ul>
</li>
<li>Redis<ul>
<li>Not multi-threaded. To determine the point in which to scale, take 90 and divide by the number of cores</li>
<li>For example, suppose you are using a cache.m1.xlarge node, which has four cores. In this case, the threshold for CPU Utilization would be (90/4), or 22.5%</li>
</ul>
</li>
<li>You will not have to calculate Redis CPU Utilization in the exam.</li>
</ul>
</li>
<li>Swap Usage<ul>
<li>Put simply, swap usage is simply the amount of the Swap file that is used. The Swap File (or Paging File) is the amount of disk storage space reserved on disk if your computer runs out of ram. Typically the size of the swap file = the size of the RAM. So if you have 4Gb of RAM, you will have a 4 GB Swap File.</li>
<li>Memcached<ul>
<li>Should be around 0 most of the time and should not exceed 50Mb.</li>
<li>If this exceeds 50Mb you should increase the memcached_connections_overhead parameter.</li>
<li>The memcached_connections_overhead defines the amount of memory to be reserved for memcached connections and other miscellaneous overhead.</li>
<li>Learn More: <a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/ParameterGroups.Memcached.html" target="_blank" rel="external">https://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/ParameterGroups.Memcached.html</a></li>
</ul>
</li>
<li>Redis<ul>
<li>No SwapUsage metric, instead use reserved-memory</li>
</ul>
</li>
</ul>
</li>
<li>Evictions<ul>
<li>Think of evictions like tenants in an apartment building. There are a number of empty apartments that slowly fill up with tenants. Eventually the apartment block is full, however more tenants need to be added.</li>
<li>An Eviction occurs when a new item is added and an old item must be removed due to lack of free space in the system.</li>
<li>Memcached<ul>
<li>There is no recommended setting. Choose a threshold based off your application</li>
<li>Either Scale up (ie increase the memory of existing nodes) OR</li>
<li>Scale Out (add more nodes)</li>
</ul>
</li>
<li>Redis<ul>
<li>There is no recommended setting. Choose a threshold based off your application.</li>
<li>scale your cluster up by using a larger node type.</li>
</ul>
</li>
<li>This can be an exam question. Remember the different approaches between Memcached &amp; Redis</li>
</ul>
</li>
<li>Concurrent Connections<ul>
<li>Memcached &amp; Redis<ul>
<li>There is no recommended setting. Choose a threshold based off your application</li>
<li>If there is a large and sustained spike in the number of concurrent connections this can either mean a large traffic spike OR your application is not releasing connections as it should be</li>
</ul>
</li>
<li>This can be an exam question. Remember to set an alarm on the number of concurrent connections for elasticache.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Centralized Monitoring<br>  Most enterprises have monitoring solutions such as Zennos, Nimsoft,Splunk, IBM Tivoli, HP Operations Manager etc. These usually involve installing agents on the servers to be monitored and then allowing these agents to report metrics back to the centralized monitoring server.</p>
<ul>
<li>Protocol<ul>
<li>Depends on what is being monitored. Most basic monitoring is going to use ICMP</li>
<li>Could be SQL (1433) or MySQL (3306)</li>
<li>Exam Tip: This can come up in several areas of the exam. This could include either inside your own VPC or a VPC that is connected to your on premise data center. Remember to allow ICMP/Specific Ports to either a specific IP address or a specific range of IP Addresses.</li>
</ul>
</li>
</ul>
</li>
<li><p>AWS Organizations &amp; Consolidated Billing</p>
<ul>
<li>AWS Organizations<br>  AWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage.<ul>
<li>Available in two feature sets:<ul>
<li>Consolidated Billing</li>
<li>ALL Features</li>
</ul>
</li>
<li>consist of<ul>
<li>Root</li>
<li>Organization Unit (OU)</li>
<li>AWS Account</li>
</ul>
</li>
</ul>
</li>
<li>Consolidated Billing<ul>
<li>Accounts<ul>
<li>Paying Account (Paying account is independent. Cannot access resources of the other accounts)</li>
<li>Linked Accounts (All linked accounts are independent)</li>
</ul>
</li>
<li>Advantages<ul>
<li>One bill per AWS account</li>
<li>Very easy to track charges and allocate costs</li>
<li>Volume pricing discount</li>
</ul>
</li>
<li>S3 pricing</li>
<li>Reserved EC2 Instances</li>
<li>Best Practices<ul>
<li>Always enable multi-factor authentication on root account.</li>
<li>Always use a strong and complex password on root account.</li>
<li>Paying account should be used for billing purposes only. Do not deploy resources in to paying account.</li>
</ul>
</li>
<li>Notes<ul>
<li>Linked Accounts<ul>
<li>20 linked accounts only</li>
<li>To add more visit <a href="https://aws-portal.amazon.com/gp/aws/html-forms-controller/contactus/aws-account-and-billing" target="_blank" rel="external">https://aws-portal.amazon.com/gp/aws/html-forms-controller/contactus/aws-account-and-billing</a></li>
</ul>
</li>
<li>Billing Alerts<ul>
<li>When monitoring is enabled on the paying account the billing data for all linked accounts is included</li>
<li>You can still create billing alerts per individual account</li>
</ul>
</li>
<li>CloudTrail<ul>
<li>Per AWS Account and is enabled per region.</li>
<li>Can consolidate logs using an S3 bucket.<ol>
<li>Turn on CloudTrail in the paying account</li>
<li>Create a bucket policy that allows cross account access</li>
<li>Turn on CloudTrail in the other accounts and use the bucket in the paying account</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips:<ul>
<li>Consolidated billing allows you to get volume discounts on all your accounts.</li>
<li>Unused reserved instances for EC2 are applied across the group.</li>
<li>CloudTrail is on a per account and per region basis but can be aggregate in to a single bucket in the paying account.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>EC2 Cost Optimization<ul>
<li>EC2 Instance Types<ul>
<li>On Demand</li>
<li>Reserved</li>
<li>Spot Price</li>
</ul>
</li>
<li>Spot Instances<br>  Spot Instances allow you to name your own price for Amazon EC2 computing capacity. You simply bid on spare Amazon EC2 instances and these will run automatically whenever your bid exceeds the current Spot Price, which varies in real-time based on supply and demand. If your bid goes below the spot price after these instances are provisioned, your instances will automatically be terminated.</li>
<li>On Demand<ul>
<li>On-Demand Instances let you pay for compute capacity by the hour with no long-term commitments or upfront payments. You can increase or decrease your compute capacity depending on the demands of your application and only pay the specified hourly rate for the instances you use. Amazon EC2 always strives to have enough On-Demand capacity available to meet your needs, but during periods of very high demand, it is possible that you might not be able to launch specific On-Demand instance types in specific Availability Zones for short periods of time.</li>
</ul>
</li>
<li>Reserved<ul>
<li>Reserved Instances provide you with a significant discount (up to 75%) compared to On-Demand Instance pricing. You are assured that your Reserved Instance will always be available for the operating system (e.g. Linux/UNIX or Windows) and Availability Zone in which you purchased it. For applications that have steady state needs, Reserved Instances can provide significant savings compared to using On-Demand Instances. Functionally, Reserved Instances and On-Demand Instances perform identically.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="High-Availability"><a href="#High-Availability" class="headerlink" title="High Availability"></a>High Availability</h4><ul>
<li>Elasticity and Scalability<ul>
<li>What is Elasticity<ul>
<li>Think of elasticity as a rubber band. Elasticity allows you to stretch out and retract back your infrastructure, based on your demand.</li>
<li>Under this model you only pay for what you need.</li>
<li>Elasticity is used during a short time period, such as hours or days.</li>
</ul>
</li>
<li>What is Scalability<ul>
<li>Scalability is used to talk about building out the infrastructure to meet your demands long term.</li>
<li>Scalability is used over longer time periods, such as weeks, days, months and years.</li>
</ul>
</li>
<li>AWS Services - Scalability vs Elasticity<ul>
<li>EC2<ul>
<li>Scalability - Increase Instance Sizes as required, using reserved instances</li>
<li>Elasticity - Increase the number of EC2 instances, based on autoscaling</li>
</ul>
</li>
<li>DynamoDB<ul>
<li>Scalability - Unlimited amount of storage</li>
<li>Elasticity - Increase additional IOPS for additional spikes in traffic. Decrease that IOPS after the spike.</li>
</ul>
</li>
<li>RDS<ul>
<li>Scalability - Increase instance size, eg from small to medium</li>
<li>Elasticity - not very elastic, can’t scale RDS based on demand</li>
</ul>
</li>
<li>Automated Elasticity + Scalability<br>  <img src="/images/AWS/Sysops/aws_automated_elasticity_scalability.jpg" alt="aws_automated_elasticity_scalability"></li>
</ul>
</li>
</ul>
</li>
<li>Scale Up or Scale Up<ul>
<li>Scaling Up - 向上升级<ul>
<li>Traditional IT - increases the number of processors, the number of RAM or the amount of storage.</li>
<li>EC2 - increase th instance type from say T1.micro to T2.small, T2.medium etc</li>
</ul>
</li>
<li>Scaling Out - 横向扩展<ul>
<li>Traditionally - adding more resources (such as webservers)</li>
<li>EC2 - adding additional EC2 Instances and using autoscaling.</li>
</ul>
</li>
<li>In the exam<ul>
<li>Eliminate the obviously incorrect answers</li>
<li>ask yourself where the bottle neck is? Is it network related? If so it’s probably a scale up answer.</li>
<li>Is the problem in relation to not having enough resources (ie you can’t increase the instance size further)? If so, it’s probably a scale out answer.</li>
<li>Remember elasticity. Scaling out, you can scale back. Scaling up is easy, scaling down is not so easy.</li>
</ul>
</li>
</ul>
</li>
<li>RDS Multi-AZ Failover<ul>
<li>Introduction<ul>
<li>Multi-AZ deployments for the MySQL, Oracle and PostgreSQL engines utilize synchronous physical replication to keep data on the standby up-to-date with the primary.</li>
<li>Mylti-AZ deployments for the SQLServer engine use synchronous logical replication to achieve the same result, employing SQL Server-native Mirroring technology.</li>
<li>Both approaches safeguard your data in the event of a DB Instance failure or loss of an Availability Zone.</li>
</ul>
</li>
<li>RDS Multi-AZ Failover Advantages<ul>
<li>High availability</li>
<li>Backups are taken from secondary which avoids I/O suspension to the primary</li>
<li>Restore’s are taken from secondary which avoids I/O suspension to the primary</li>
<li>Exam Tip: You can <strong>force</strong> a failover from one AZ to another by rebooting your instance. This can be done through the AWS Management console or by using RebootDbInstance API call.</li>
</ul>
</li>
<li>RDS Multi-AZ Failover is not a scaling solution</li>
<li>Read Replica’s are used to scale</li>
</ul>
</li>
<li>RDS Read Replicas<ul>
<li>What are Read Replica<ul>
<li>Read Replicas make it easy to take advantage of supported engine’s built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB Instance for read-heave database workloads.</li>
<li>Read only copies of your database.</li>
<li>You can create a Read Replica with a few clicks in the AWS Management Console or using the CreateDBInstanceReadReplica API. Once the Read Replica is created, database updates on the source DB Instance will be replicated using a supported engine’s native, asynchronous replication. You can create multiple Read Replicas for a given source DB Instance and distribute your application’s read traffic amongst them.</li>
</ul>
</li>
<li>When would you use read replica’s<ul>
<li>Scaling beyond the compute or I/O capacity of a single DB Instance for read-heavy database workloads. This excess read traffic can be directed to one or more Read Replicas</li>
<li>Serving read traffic while the source DB Instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your Read Replica</li>
<li>Business reporting or data warehousing scenarios; you may want business reporting queries to run against a Read Replica, rather than your primary, production DB Instance.</li>
</ul>
</li>
<li>Supported Versions<ul>
<li>MySQL</li>
<li>PostgreSQL</li>
<li>MariaDB<ul>
<li>For all 3 Amazon uses these engines native asynchronous replication to update the read replica</li>
</ul>
</li>
<li>Aurora<ul>
<li>Aurora employees an SSD-backed virtualized storage layer purpose-built for database workloads. Amazon Aurora replica share the same underlying storage as the source instance, lowering costs and avoiding the need to copy data to the replica nodes.</li>
</ul>
</li>
</ul>
</li>
<li>Creating Read Replicas<ul>
<li>When creating a new Read Replica, AWS will take a snapshot of your database.</li>
<li>If Multi-AZ is not enabled:<ul>
<li>This snapshot will be of your primary database and can cause brief I/O suspension for around 1 minute.</li>
</ul>
</li>
<li>If Multi-AZ is enabled:<ul>
<li>The snapshot will be of your secondary database and you will not experience any performance hits on your primary database.</li>
</ul>
</li>
</ul>
</li>
<li>Connecting to Read Replica<ul>
<li>When a new read replica is created you will be able to connect to it using a new end point DNS address.</li>
</ul>
</li>
<li>Read Replica’s Can Be Promoted<ul>
<li>You can promote a read replica to it’s own standalone database. Doing this will break the replication link between the primary and the secondary.</li>
</ul>
</li>
<li>Exam Tips - 1<ul>
<li>You can have up to 5 read replicas for MySQL, PostgreSQL &amp; MariaDB</li>
<li>You can have read replicas in different Regions for all engines</li>
<li>Replication is Asynchronous</li>
<li>Read Replica’s can be built off Multi-AZ’s databases</li>
<li>But Read Replica’s themselves cannot be Multi-AZ currently</li>
<li>You can have Read Replica’s of Read Replica’s beware of latency</li>
<li>DB Snapshots and Automated backups cannot be taken of read replicas</li>
<li>Key Metric to look for is Replica Lag</li>
<li><strong>Know the difference between read replicas and multi-AZ</strong></li>
</ul>
</li>
<li>Exam Tips - 2<ul>
<li>If you can’t create a Read Replica, you most likely have disabled Database backups. Modify the database and turn them on.</li>
<li>You can create read replicas of read replicas in multiple Regions</li>
<li>You can either modify the database itself or create a new database from a snapshot</li>
<li>Endpoints will NOT change if you modify a database, they will change if you create a new database from a snap or if you create a read replica</li>
<li>You can manually fail over a Multi-AZ database from one AZ to another by rebooting it.</li>
</ul>
</li>
</ul>
</li>
<li><p>Bastion Hosts &amp; High Availability</p>
<ul>
<li>What is a Bastion Host<ul>
<li>A bastion host is a security measure that you can implement which acts as a gateway between you and your EC2 instances. The bastion host helps to reduce attack vectors on your infrastructure and means that you only have to harden 1 or 2 EC2 instances, rather than your entire fleet.</li>
</ul>
</li>
</ul>
</li>
<li><p>Troubleshooting Autoscaling</p>
<ul>
<li>Instances not launching in to Autoscaling Groups<br>  Below is a list of things to look for if your instances are not launching in to an autoscaling group:<ul>
<li>Associate Key Pair does not exist</li>
<li>Security group does not exist</li>
<li>Autoscaling config is not working correctly</li>
<li>Autoscaling group not found</li>
<li>Instance type specified is not supported in the AZ</li>
<li>AZ is no longer supported</li>
<li>Invalid EBS device mapping</li>
<li>Autoscaling service is not enabled on your account</li>
<li>Attempting to attach and EBS block device to an instance-store AMI</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Deployment-and-Provisioning"><a href="#Deployment-and-Provisioning" class="headerlink" title="Deployment and Provisioning"></a>Deployment and Provisioning</h4><ul>
<li><p>Services with root/admin access to Operating System</p>
<ul>
<li>This is a very popular question in both the Solutions Architect Associate Exam &amp; the SysOps Administrator Associate Exam<ul>
<li>Elastic Beanstalk</li>
<li>Elastic MapReduce</li>
<li>OpsWork</li>
<li>EC2</li>
</ul>
</li>
<li>Exam Tips: There’s a good change this will be worth 1 point in your exam, so remember these services. Remember that you do not have access to RDS, DynamoDB, S3 or Glacier</li>
</ul>
</li>
<li><p>Elastic Load Balancer Configurations</p>
<ul>
<li>Exam Tips<ul>
<li>You can use Elastic Load Balancers to load balance across different availability zones within the same region, but not to different regions (or different VPC’s) themselves.</li>
<li>An ELB and a NAT are different things entirely.</li>
</ul>
</li>
<li>Two types of ELB<ul>
<li>External Elastic Load Balancers (with external DNS names)</li>
<li>Internal Elastic Load Balancers (with internal DNS names)</li>
</ul>
</li>
<li>Health Check - Recap<br>  <img src="/images/AWS/Sysops/EB_healthy_check_recap.jpg" alt="pic_need_to_place"></li>
<li>Sticky Sessions<br>  By default, a load balancer routes each request independently to the application instance with the smallest load.<br>  However, you can use the sticky session feature (also known as session affinity), which enables the load balancer to lock a user down to a specific web server (EC2 instance).<br>  This ensures that all requests from the user during the session are always sent to the same server.<br>  The key to managing sticky sessions is to determine how long your load balancer should consistently route the user’s request to the same application instance.<ul>
<li>Sticky Session Types<ul>
<li>Duration Based Session Stickiness</li>
<li>Application-Controlled Session Stickiness</li>
</ul>
</li>
<li>Edit in the [Port Configuration] setting in the [Description] tab.</li>
<li>Duration Based<br>  Most commonly used. The load balancer itself creates the session cookie.<br>  When the load balancer receives a request, it first checks to see if this cookie is present in the request. If so, the request is sent to the application instance specified in the cookie. If there is no cookie, the load balancer chooses an application instance based on the existing load balancing algorithm and adds a new cookie in to the response.<br>  The stickiness policy configuration defines a cookie expiration, which established the duration of validity for each cookie. The cookie is automatically updated after its duration expires.<br>  If an application instance fails or becomes unhealthy, the load balancer stops routing request to that instance, instead chooses a new instance based on the existing load balancing algorithm. The request is routed to the new instance as if there is no cookie and the session is no longer sticky.</li>
<li>Application Controlled<br>  The load balancer uses a special cookie to associate the session with the instance that handled the initial request, but follows the lifetime of the application cookie specified in the policy configuration. The load balancer only inserts a new stickiness cookie if the application response includes a new application cookie. The load balancer stickiness cookie does not update with each request. If the application cookie is explicitly removed or expires, the session stops being sticky until a new application cookie is issued.<br>  If an instance fails or becomes unhealthy, the load balancer stops routing requests to that instance, and chooses a new healthy instance based on the existing load balancing algorithm. The load balancer treats the session as now “stuck” to the new healthy instance, and continues routing requests to that instance even if the failed instance comes back. However, it is up to the new application instance whether and how to respond to a session which it has not previously seen.</li>
</ul>
</li>
</ul>
</li>
<li><p>Pre-warming the Elastic Load Balancer</p>
<ul>
<li>What is ‘Pre-warming’ for ELB<ul>
<li>AWS Staff can pre-configure the load balancer to have the appropriate level of capacity based on expected traffic.</li>
<li>This is used in certain scenarios, such as when flash traffic is expected, or in the case where a load test cannot be configured to gradually increase traffic.</li>
<li>This can be done by contacting AWS staff prior to the expected event. You will need to know the start and end dates of your expected flash traffic or test, the expected request rate per second and the total size of the typical request/response that you will be experiencing.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Data-Management"><a href="#Data-Management" class="headerlink" title="Data Management"></a>Data Management</h4><ul>
<li><p>Disaster Recovery</p>
<ul>
<li>Back Up &amp; Disaster Recovery is quite a key component of the SysOps Exam. It’s important to understand the key concepts well. You can also want to read the following white paper: <a href="https://media.amazonwebservices.com/AWS_Disaster_Recovery.pdf" target="_blank" rel="external">https://media.amazonwebservices.com/AWS_Disaster_Recovery.pdf</a></li>
<li>What is Disaster Recovery<ul>
<li>Disaster recovery (DR) is about preparing for and recovering from a disaster. Any event that has a negative impact on a company’s business continuity or finances could be termed a disaster. This includes hardware or software failure, a network outage, a power outage, physical damage to a building like fire or flooding, human error, or some other significant event.</li>
</ul>
</li>
<li>Traditional Approaches to DR<ul>
<li>A traditional approach to DR usually involves an N+1 approach and has different levels of off-site duplication of data and infrastructure.<ul>
<li>Facilities to house the infrastructure, including power and cooling</li>
<li>Security to ensure the physical protection of assets</li>
<li>Suitable capacity to scale the environment</li>
<li>Support for repairing, replacing, and refreshing the infrastructure</li>
<li>Contractual agreements with an Internet service provider (ISP) to provide Internet connectivity that can sustain bandwidth utilization for the environment under a full load</li>
<li>Network infrastructure such as firewalls, routers, switches, and load balancers</li>
<li>Enough server capacity to run all mission-critical services, including storage appliances for the supporting data, and servers to run applications and backend services such as user authentication, Domain Name System (DNS)</li>
<li>Dynamic Host Configuration Protocol (DHCP), monitoring, and alerting</li>
</ul>
</li>
</ul>
</li>
<li>Why use aws for DR<ul>
<li>Only minimum hardware is required for ‘data replication’</li>
<li>Allows you to be flexible depending on what your disaster is and how to recover from it</li>
<li>Open cost model (pay as you use) rather than heavy investment upfront. Scaling is quick and easy</li>
<li>Automate disaster recovery deployment</li>
</ul>
</li>
<li>What Services<ul>
<li>Regions</li>
<li>Storage</li>
<li>S3 - 99.999999999% durability and Cross Region Replication</li>
<li>Glacier</li>
<li>Elastic Block Store (EBS)</li>
<li>Direct Connect</li>
<li>AWS Storage Gateway</li>
<li>Gateway-cached volumes - store primary data and cache most recently used data locally.</li>
<li>Gateway-stored volumes - store entire dataset on site and asynchronously replicate data back to S3</li>
<li>Gateway-virtual tape library - Store your virtual tapes in either S3 or Glacier</li>
<li>Compute<ul>
<li>EC2</li>
<li>EC2 VM Import Connector - Virtual appliance which allows you to import virtual machine images from your existing environment to Amazon EC2 instances.</li>
</ul>
</li>
<li>Networking<ul>
<li>Route53</li>
<li>Elastic Load Balancing</li>
<li>Amazon Virtual Private Cloud (VPC)</li>
<li>Amazon Direct Connect</li>
</ul>
</li>
<li>Database<ul>
<li>RDS</li>
<li>DynamoDB</li>
<li>Redshift</li>
</ul>
</li>
<li>Orchestration<ul>
<li>CloudFormation</li>
<li>ElasticBeanstalk</li>
<li>OpsWork</li>
</ul>
</li>
<li>Lambda</li>
</ul>
</li>
<li>RTO vs RPO<ul>
<li>Recovery Time Objective (RTO)<ul>
<li>RTO is the length of time from which you can recover from a disaster. It is measured from when the disaster first occurred as to when you have fully recovered from it.</li>
</ul>
</li>
<li>Recovery Point Objective (RPO)<ul>
<li>RPO is the amount of data your organization is prepared to lose in the event of a disaster (1 days worth of emails, 5 hours of online transaction records etc, 24 hours of backup etc)</li>
</ul>
</li>
<li>Typically the lower RTO &amp; RPO threshold, the more costly it solution will be.</li>
</ul>
</li>
<li>DR Scenarios<ul>
<li>Four Scenarios<ul>
<li>Backup &amp; Restore</li>
<li>Pilot Light</li>
<li>Warm Standby</li>
<li>Multi Site</li>
</ul>
</li>
<li>Backup &amp; Restore<ul>
<li>In most traditional environments, data is backed up to tape and sent off-site regularly. If you use this method, it can take a long time to restore your system in the event of a disruption or disaster. Amazon S3 is an ideal destination for backup data that might be needed quickly to perform a restore. Transferring data to and from Amazon S3 is typically done through the network, and is therefore accessible from any location.</li>
<li>You can use AWS Import/Export to transfer very large data sets by shipping storage devices directly to AWS. For longer-term data storage where retrieval times of several hours are adequate, there is Amazon Glacier, which has the same durability model as Amazon S3. . Amazon Glacier and Amazon S3 can be used in conjunction to produce a tiered backup solution.</li>
<li>Data Backup Options to Amazon S3 from On-Site Infrastructure or from AWS.<br>  <img src="/images/AWS/Sysops/data_backup_options_to_S3_from_on_site_infrastructure_from_aws.jpg" alt="data_backup_options_to_S3_from_on_site_infrastructure_from_aws"></li>
<li>Restoring a System from Amazon S3 Backups to Amazon EC2<br>  <img src="/images/AWS/Sysops/restoring_a_system_from_amazon_s3_backups_to_amazon_ec2.jpg" alt="restoring_a_system_from_amazon_s3_backups_to_amazon_ec2"></li>
<li>Key steps for backup &amp; restore<ul>
<li>Select an appropriate tool or method to back up your data into AWS.</li>
<li>Ensure that you have an appropriate retention policy for this data.</li>
<li>Ensure that appropriate security measures are in place for this data, including encryption and access policies.</li>
<li>Regularly test the recovery of this data and the restoration of your system.</li>
</ul>
</li>
</ul>
</li>
<li>Pilot Light<ul>
<li>The term pilot light is often used to describe a DR scenario in which a minimal version of an environment is always running in the cloud. The idea of the pilot light is an analogy that comes from the gas heater. In a gas heater, a small flame that’s always on can quickly ignite the entire furnace to heat up a house.</li>
<li>This scenario is similar to a backup-and-restore scenario. For example, with AWS you can maintain a pilot light by configuring and running the most critical core elements of your system in AWS. When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core.</li>
<li>Infrastructure elements for the pilot light itself typically include your database servers, which would replicate data to Amazon EC2 or Amazon RDS. Depending on the system, there might be other critical data outside of the database that needs to be replicated to AWS. This is the critical core of the system (the pilot light) around which all other infrastructure pieces in AWS (the rest of the furnace) can quickly be provisioned to restore the complete system.</li>
<li>To provision the remainder of the infrastructure to restore business-critical services, you would typically have some preconfigured servers bundled as Amazon Machine Images (AMIs), which are ready to be started up at a moment’s notice.  When starting recovery, instances from these AMIs come up quickly with their pre-defined role (for example, Web or App Server) within the deployment around the pilot light.</li>
<li>From a networking point of view, you have two main options for provisioning:<ul>
<li>Use pre-allocated elastic IP address and associate them with your instances when invoking DR. You can also use pre-allocated elastic network interfaces (ENIs) with pre-allocated Mac Addresses for applications with special licensing requirements</li>
<li>Use Elastic Load Balancing (ELB) to distribute traffic to multiple instances. You would then update your DNS records to point at your Amazon EC2 instance or point to your load balancer using a CNAME</li>
</ul>
</li>
<li>Preparation phase<ul>
<li>The Preparation Phase of the Pilot Light Scenario<br>  <img src="/images/AWS/Sysops/the_preparation_phase_of_the_pilot_light_scenario.jpg" alt="the_preparation_phase_of_the_pilot_light_scenario"></li>
<li>Key steps for preparation:<ul>
<li>Set up Amazon EC2 instances to replicate or mirror data.</li>
<li>Ensure that you have all supporting custom software packages available in AWS.</li>
<li>Create and maintain AMIs of key servers where fast recovery is required.</li>
<li>Regularly run these servers, test them, and apply any software updates and configuration changes.</li>
<li>Consider automating the provisioning of AWS resources</li>
</ul>
</li>
</ul>
</li>
<li>Recovery phase<ul>
<li>The Recovery Phase of the Pilot Light Scenario<br>  <img src="/images/AWS/Sysops/the_recovery_phase_of_the_pilot_light_scenario.jpg" alt="the_recovery_phase_of_the_pilot_light_scenario"></li>
<li>Key steps for recovery:<ul>
<li>Start your application Amazon EC2 instances from your custom AMIs.</li>
<li>Resize existing database/data store instances to process the increased traffic.</li>
<li>Add additional database/data store instances to give the DR site resilience in the data tier; if you are using Amazon RDS, turn on Multi-AZ to improve resilience.</li>
<li>Change DNS to point at the Amazon EC2 servers.</li>
<li>Install and configure any non-AMI based systems, ideally in an automated way.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Warm Standby<ul>
<li>The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud. A warm standby solution extends the pilot light elements and preparation. It further decreases the recovery time because some services are always running. By identifying your business-critical systems, you can fully duplicate these systems on AWS and have them always on.</li>
<li>These servers can be running on a minimum-sized fleet of Amazon EC2 instances on the smallest sizes possible. This solution is not scaled to take a full-production load, but it is fully functional. It can be used for non-production work, such as testing, quality assurance, and internal use.</li>
<li>In a disaster, the system is scaled up quickly to handle the production load. In AWS, this can be done by adding more instances to the load balancer and by resizing the small capacity servers to run on larger Amazon EC2 instance types.</li>
<li>Horizontal scaling is preferred over vertical scaling</li>
<li>Preparation phase<ul>
<li>The Preparation Phase of the Warm Standby Scenario<br>  <img src="/images/AWS/Sysops/the_preparation_phase_of_the_warm_standby_scenario.jpg" alt="the_preparation_phase_of_the_warm_standby_scenario"></li>
<li>Key steps for preparation:<ul>
<li>Set up Amazon EC2 instances to replicate or mirror data.</li>
<li>Create and maintain AMIs.</li>
<li>Run your application using a minimal footprint of Amazon EC2 instances or AWS infrastructure.</li>
<li>Patch and update software and configuration files in line with your live environment.</li>
</ul>
</li>
</ul>
</li>
<li>Recovery phase<ul>
<li>The Recovery Phase of the Warm Standby Scenario<br>  <img src="/images/AWS/Sysops/the_recovery_phase_of_the_warm_standby_scnario.jpg" alt="the_recovery_phase_of_the_warm_standby_scnario"></li>
<li>Key steps for recovery:<ul>
<li>Increase the size of the Amazon EC2 fleets in service with the load balancer (horizontal scaling).</li>
<li>Start applications on larger Amazon EC2 instance types as needed (vertical scaling).</li>
<li>Either manually change the DNS records, or use Amazon Route 53 automated health checks so that all traffic is routed to the AWS environment.</li>
<li>Consider using Auto Scaling to right-size the fleet or accommodate the increased load.</li>
<li>Add resilience or scale up your database.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Multi Site<ul>
<li>A multi-site solution runs in AWS as well as on your existing on-site infrastructure, in an active-active configuration. The data replication method that you employ will be determined by the recovery point that you choose.</li>
<li>You can use Route53 to root traffic to both sites either symmetrically or asymmetrically.</li>
<li>In an on-site disaster situation, you can adjust the DNS weighting and send all traffic to the AWS servers. The capacity of the AWS service can be rapidly increased to handle the full production load. You can use Amazon EC2 Auto Scaling to automate this process. You might need some application logic to detect the failure of the primary database services and cut over to the parallel database services running in AWS.</li>
<li>Preparation phase<ul>
<li>The Preparation Phase of the Multi-Site Scenario<br>  <img src="/images/AWS/Sysops/the_preparation_phase_of_the_multi_site_scenario.jpg" alt="the_preparation_phase_of_the_multi_site_scenario"></li>
<li>Key steps for preparation:<ul>
<li>Set up your AWS environment to duplicate your production environment.</li>
<li>Set up DNS weighting, or similar traffic routing technology, to distribute incoming requests to both sites.  Configure automated failover to re-route traffic away from the affected site.</li>
</ul>
</li>
</ul>
</li>
<li>Recovery phase<ul>
<li>The Recovery Phase of the Multi-Site Scenario Involving On-Site and AWS Infrastructure.<br>  <img src="/images/AWS/Sysops/the_recovery_phase_of_the_multi_site_scenario_involving_on_site_and_aws_infrastructure.jpg" alt="the_recovery_phase_of_the_multi_site_scenario_involving_on_site_and_aws_infrastructure"></li>
<li>Key steps for recovery:<ul>
<li>Either manually or by using DNS failover, change the DNS weighting so that all requests are sent to the AWS site.</li>
<li>Have application logic for failover to use the local AWS database servers for all queries.</li>
<li>Consider using Auto Scaling to automatically right-size the AWS fleet.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Failing Back<ul>
<li>Backup and restore<ol>
<li>Freeze data changes to the DR site</li>
<li>Take a backup</li>
<li>Restore the backup to the primary site</li>
<li>Re-point users to the primary site</li>
<li>Unfreeze the changes</li>
</ol>
</li>
<li>Pilot light, warm standby, and multi-site:<ol>
<li>Establish reverse mirroring/replication from the DR site back to the primary site, once the primary site has caught up with the changes.</li>
<li>Freeze data changes to the DR site</li>
<li>Re-point users to the primary site</li>
<li>Unfreeze the changes</li>
</ol>
</li>
</ul>
</li>
<li>Exam Tips<br>  Pay particular attention to the Pilot Light Scenario and specifically the fact that you can have Elastic Network Interfaces (ENI’s) with preconfigured MAC addresses.</li>
</ul>
</li>
</ul>
</li>
<li><p>AWS Services and Automated Backups</p>
<ul>
<li>Services that have Automated Backup<ul>
<li>RDS</li>
<li>Elasticache (Redis only)</li>
<li>Redshift</li>
</ul>
</li>
<li>Services that do not have Automated Backup<ul>
<li>EC2</li>
</ul>
</li>
<li>RDS Automatd Backups<ul>
<li>For MySQL you need innoDB (transactional engine)</li>
<li>There is a performance hit if Multi-AZ is not enabled</li>
<li>If you delete an instance, then ALL automated backups are deleted</li>
<li>However, manual DB snapshots will NOT be deleted</li>
<li>All stored on S3</li>
<li>When you do a restore, you can change the engine type (SQL Standard to SQL Enterprise for example). Provided you have enough storage space</li>
</ul>
</li>
<li>Elasticache Backups<ul>
<li>Available for Redis Cache Cluster only</li>
<li>The entire cluster is snapshotted</li>
<li>Snapshot will degrade performance</li>
<li>Therefore only set your snapshot window during the least busy part of the day</li>
<li>Stored on S3</li>
</ul>
</li>
<li>Redshift Backups<ul>
<li>Stored on S3</li>
<li>By default, Amazon Redshift enables automated backups of your data warehouse cluster with a 1-day retention period</li>
<li>Amazon Redshift only backs up data that has changed so most snapshots only use up a small amount of your free backup storage</li>
</ul>
</li>
<li>EC2<ul>
<li>No automated backups</li>
<li>Backups degrade you performance, schedule these times wisely</li>
<li>Snapshots are stored in S3</li>
<li>Can create automated backups using either the command line interface or Python</li>
<li>They are incremental:<ul>
<li>Snapshots only store incremental changes since last snapshot</li>
<li>Only charged for incremental storage</li>
<li>Each snapshot still contains the base snapshot data</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>EC2 Type - EBS vs Instance Store</p>
<ul>
<li>History<ul>
<li>When EC2 was first launched, all AMI’s were backed by Amazon’s Instance Store. Instance store is known as ephemeral storage, which simply means non-persistence or temporary storage.</li>
<li>Later on AWS launched EBS, Elastic Block Storage, which allows users to have data persistence and to save their data permanently.</li>
</ul>
</li>
<li>Confusion<ul>
<li>There is a lot of confusion between instance store volumes and EBS volumes in the AWS community and you need to have a good understanding of the differences between the two. Let’s start with volumes. There are two types of volumes:<ul>
<li>Root Volume (this is where your operating system is installed)</li>
<li>Additional Volumes (this can be your D:\ E:\ F:\ or /dev/sdb, /dev/sdc, /dev/sdd etc)</li>
</ul>
</li>
</ul>
</li>
<li>Route Volume Sizes<ul>
<li>Root device volumes can either be EBS volumes or Instance Store volumes</li>
<li>An Instance store root device volume’s maximum size is 10Gb</li>
<li>EBS root device volume can be up to 1 or 2Tb depending on the OS</li>
</ul>
</li>
<li>Terminating an Instance - EBS<ul>
<li>EC2 Instances can be terminated:<ul>
<li>EBS root device volumes are terminated by DEFAULT when the EC2 instance is terminated. You can stop this b unselecting the “Delete on Termination” option when creating the instance or by setting the deleteontermination flag to false using the command line</li>
<li>Other EBS volumes attached to the instance are preserved however, if you delete the instance</li>
<li>Instance store device root volumes are terminated by DEFAULT when the EC2 instance is terminated. You cannot stop this</li>
<li>Other instance store volumes will be deleted on termination automatically</li>
<li>Other EBS volumes attached to the EC2 instance will persist automatically</li>
</ul>
</li>
</ul>
</li>
<li>Stopping an Instance<ul>
<li>EBS backed instances can be stopped</li>
<li>Instance Store backed instances CANNOT be stopped. Only rebooted or terminated</li>
</ul>
</li>
<li>Instance Store Data<ul>
<li>The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists. However, data on instance store volumes is lost under the following circumstances:<ul>
<li>Failure of an underlying drive</li>
<li>Stopping an Amazon EBS-backed instance</li>
<li>Terminating an instance</li>
</ul>
</li>
</ul>
</li>
<li>Instance Store<ul>
<li>Therefore, do not rely on instance store volumes for valuable, long-term data. Instead, keep your data safe by using a replication strategy across multiple instances, storing data in Amazon S3, or using Amazon EBS volumes.</li>
</ul>
</li>
<li>Comparison EBS vs Instance Store<br>  <img src="/images/AWS/Sysops/comparison_EBS_InstanceStore.jpg" alt="comparison_EBS_InstanceStore"></li>
<li>Exam Tips<ul>
<li>‘Delete on Termination’ is the default for all EBS root device volumes. You can set this to false however but only at instance creation time</li>
<li>Additional volumes will persist automatically. You need to delete these manually when you delete an instance</li>
<li>Instance Store is known as ephemeral storage, meaning that data will not persist after an instance is deleted. You cannot set this to false, data will always be deleted when that instance disappears</li>
</ul>
</li>
</ul>
</li>
<li><p>Upgrading EBS volume types - Exam tips</p>
<ul>
<li>EBS volumes can be changed on the fly (except for magnetic standard).</li>
<li>Best practice to stop the EC2 instance and then change the volume</li>
<li>You can change volume types by taking a snapshot and then using the snapshot to create a new volume</li>
<li>If you change a volume on the fly you must wait for 6 hours before making another change</li>
<li>You can scale EBS Volumes up only</li>
<li>Volumes must be in the same AZ as the EC2 instances.</li>
</ul>
</li>
<li><p>Storing Log Files &amp; Backups</p>
<ul>
<li>Centralized Monitoring &amp; Logging<ul>
<li>You can monitor your environment in a number of ways:<ul>
<li>Using a third party, centralized monitoring platform, such as Zennos, Splunk, RSyslog, Kiwi etc. These logs can then be stored on S3.</li>
<li>Using CloudWatch Logs (relatively new service)</li>
<li>Utilizing Access Logging on S3 (S3 only)</li>
</ul>
</li>
</ul>
</li>
<li>Suggested to use S3<ul>
<li>The best place to store your logs is probably S3:<ul>
<li>99.999999999% durability</li>
<li>Life cycle management</li>
<li>Archive off to Glacier</li>
<li>Effectively allows you to Tier your back up solution</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="OpsWorks"><a href="#OpsWorks" class="headerlink" title="OpsWorks"></a>OpsWorks</h4><ul>
<li><p>What is OpsWorks</p>
<ul>
<li>Cloud-based applications usually require a group of related resources—application servers, database servers, and so on—that must be created and managed collectively. This collection of instances is called a stack. A simple application stack might look something like the following.<br><img src="/images/AWS/Sysops/what_is_opswork.png" alt="what_is_opswork"></li>
<li>AWS OpsWorks Stacks provides a simple and straightforward way to create and manage stacks and their associated applications and resources</li>
<li>Amazon Definition:<ul>
<li>AWS OpsWorks is an application management service that helps you automate operational tasks like code deployment, software configurations, package installations, database setups, and server scaling using Chef. OpsWorks gives you the flexibility to define your application architecture and resource configuration and handles the provisioning and management of your AWS resources for you. OpsWorks includes automation to scale your application based on time or load, monitoring to help you troubleshoot and take automated action based on the state of your resources, and permissions and policy management to make management of multi-user environments easier.</li>
</ul>
</li>
</ul>
</li>
<li><p>What is Chef</p>
<ul>
<li>Chef turns infrastructure into code. With Chef, you can automate how you build, deploy, and manage your infrastructure. Your infrastructure becomes as versionable, testable, and repeatable as application code.</li>
<li>Chef server stores your recipes as well as other configuration data. The Chef client is installed on each server, virtual machine, container, or networking device you manage - we’ll call these nodes. The client periodically polls Chef server latest policy and state of your network. If anything on the node is out of date, the client brings it up to date.</li>
</ul>
</li>
<li><p>What is OpsWorks</p>
<ul>
<li>A GUI to deploy and configure your infrastructure quickly. OpsWorks consists of two elements, Stacks and Layers.</li>
<li>A stack is a container (or group) of resources such as ELBS, EC2 instances, RDS instances etc.</li>
<li>A layer exists within a stack and consists of things like a web application layer. An application processing layer or a Database layer.</li>
<li>When you create a layer, rather than going and configuring everything manually (like installing Apache, PHP etc) OpsWorks takes care of this for you.</li>
</ul>
</li>
<li><p>Layers</p>
<ul>
<li>1 or more layers in the stack</li>
<li>An instance must be assigned to at least 1 layer</li>
<li>Which chef layers run, are determined by the layer the instance belongs to</li>
<li>Preconfigured Layers include:<ul>
<li>Applications</li>
<li>Databases</li>
<li>Load Balancers</li>
<li>Caching</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h4><ul>
<li>Security Token Service (STS)<ul>
<li>what is Security Token Service (STS)<ul>
<li>Grants users limited and temporary access to AWS resources. Users can come from three sources:<ul>
<li>Federation (typically Active Directory)<ul>
<li>Uses Security Assertion Markup Language (SAML)</li>
<li>Grants temporary access based off the users Active Directory credentials. Does not need to be a user in IAM</li>
<li>Single sign on allows users to log in to AWS console without assigning IAM credentials</li>
</ul>
</li>
<li>Federation with Mobile Apps<ul>
<li>Use Facebook/Amazon/Google or other OpenID providers to log in.</li>
</ul>
</li>
<li>Cross Account Access<ul>
<li>Let’s users from one AWS account access resources in another</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Understanding Key Terms<ul>
<li>Federation: combining or joining a list of users in one domain (such as IAM) with a list of users in another domain (such as Active Directory, Facebook etc)</li>
<li>Identity Broker: a service that allows you to take an identity from point A and join it (federate it) to point B</li>
<li>Identity Store - Services like Active Directory, Facebook, Google etc</li>
<li>Identities - a user of a service like Facebook etc.</li>
</ul>
</li>
<li>Scenario<ul>
<li>AWS blog [<a href="https://aws.amazon.com/cn/blogs/aws/aws-identity-and-access-management-now-with-identity-federation/" target="_blank" rel="external">AWS Identity and Access Management – Now With Identity Federation</a>]</li>
<li>You are hosting a company website on some EC2 web servers in your VPC. Users of the website must log in to the site which then authenticates against the companies active directory servers which are based on site at the companies head quarters. Your VPC is connected to your company HQ via a secure IPSEC VPN. Once logged in the user can only have access to their own S3 bucket. How do you set this up?<br>  <img src="/images/AWS/Sysops/sts_scenario.jpg" alt="sts_scenario"></li>
<li>Steps of scenario<ul>
<li>Employee enters their username and password</li>
<li>The application calls an Identity Broker. The broker captures the username and password.</li>
<li>The Identity Broker uses the organization’s LDAP directory to validate the employee’s identity.</li>
<li>The Identity Broker calls the new GetFederationToken function using IAM credentials. The call must include an IAM policy and a duration (1 to 36 hours), along with a policy that  specifies the permissions to be granted to the temporary security credentials.</li>
<li>The Security Token Service confirms that the policy of the IAM user making the call to GetFederationToken gives permission to create new tokens and then returns four values to the application: An access key, a secret access key, a token, and a duration (the token’s lifetime).</li>
<li>The Identity Broker returns the temporary security credentials to the reporting application.</li>
<li>The data storage application uses the temporary security credentials (including the token) to make requests to Amazon S3.</li>
<li>Amazon S3 uses IAM to verify that the credentials allow the requested operation on the given S3 bucket and key</li>
<li>IAM provides S3 with the go-ahead to perform the requested operation.</li>
</ul>
</li>
<li>In the Exam<ul>
<li>Develop an Identity Broker to communicate with LDAP and AWS STS</li>
<li>Identity Broker always authenticates with LDAP first, Then with AWS STS</li>
<li>Application then gets temporary access to AWS resources</li>
</ul>
</li>
</ul>
</li>
<li>Scenario 2<ul>
<li>steps of scenario<ul>
<li>Develop an Identity Broker to communicate with LDAP and AWS STS</li>
<li>Identity Broker always authenticates with LDAP first, gets an IAM Role associate with a user</li>
<li>Application then authenticates with STS and assumes that IAM Role</li>
<li>Application uses that IAM role to interact with S3</li>
</ul>
</li>
<li>In the Exam<ul>
<li>Develop an Identity Broker to communicate with LDAP and AWS STS</li>
<li>Identity Broker always authenticates with LDAP first, Then with AWS STS</li>
<li>Application then gets temporary access to AWS resources</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Overview of Security Processes</p>
<ul>
<li><a href="https://d0.awsstatic.com/whitepapers/aws-security-whitepaper.pdf" target="_blank" rel="external">aws-security-whitepaper</a></li>
<li><p>Shared Security Model</p>
<ul>
<li>AWS is responsible for securing the underlying infrastructure that supports the cloud. You’re responsible for anything you put on the cloud or connect to the cloud.<br>  <img src="/images/AWS/Sysops/aws_shared_security_responsibility_model.jpg" alt="aws_shared_security_responsibility_model"></li>
</ul>
</li>
<li><p>AWS Security Responsibilities</p>
<ul>
<li>Amazon Web Services is responsible for protecting the global infrastructure that runs all of the services offered in the AWS cloud. This infrastructure is comprised of the hardware, software, networking, and facilities that run AWS services.</li>
<li>AWS is responsible for the security configuration of its products that are considered managed services. Examples of these types of services include Amazon DynamoDB, Amazon RDS, Amazon Redshift, Amazon Elastic MapReduce, Amazon WorkSpaces</li>
</ul>
</li>
<li>Customer Security Responsibilities<ul>
<li>IAAS - —such as Amazon EC2, Amazon VPC, and Amazon S3—are completely under your control and require you to perform all of the necessary security configuration and management tasks.</li>
<li>Managed Services at AWS is responsible for patching, antivirus etc, however you are responsible for account management and user access. Its recommended that MFA be implemented, communicate to these services using SSL/TLS and that API/user activity logging be setup with CloudTrail.</li>
</ul>
</li>
<li>Storage Decommissioning<ul>
<li>When a storage device has reached the end of its useful life, AWS procedures include a decommissioning process that is designed to prevent customer data from being exposed to unauthorized individuals. AWS uses the techniques detailed in DoD 5220.22-M (“National Industrial Security Program Operating Manual “) or NIST 800-88 (“Guidelines for Media Sanitization”) to destroy data as part of the decommissioning process. All decommissioned magnetic storage devices are degaussed and physically destroyed in accordance with industry-standard practices.</li>
</ul>
</li>
<li>Network Security<ul>
<li>Transmission Protection<ul>
<li>You can connect to an AWS access point via HTTP or HTTPS using Secure Sockets Layer (SSL), a cryptographic protocol that is designed to protect against eavesdropping, tampering, and message forgery</li>
<li>For customers who require additional layers of network security, AWS offers the Amazon Virtual Private Cloud (VPC), which provides a private subnet within the AWS cloud, and the ability to use an IPsec Virtual Private Network (VPN) device to provide an encrypted tunnel between the Amazon VPC and your data center.</li>
</ul>
</li>
<li>Amazon Corporate Segregation<ul>
<li>Logically, the AWS Production network is segregated from the Amazon Corporate network by means of a complex set of network security / segregation devices.</li>
</ul>
</li>
</ul>
</li>
<li>Network Monitoring &amp; Protection<ul>
<li>Types<ul>
<li>DDos</li>
<li>Man in the middle attacks (MITM)</li>
<li>Ip Spoofing</li>
<li>Port Scanning</li>
<li>Packet Sniffing by other tenants</li>
</ul>
</li>
<li>Ip Spoofing<ul>
<li>The AWS-controlled, host-based firewall infrastructure will not permit an instance to send traffic with a source IP or MAC address other than its own.</li>
</ul>
</li>
<li>Port Scanning<ul>
<li>Unauthorized port scans by Amazon EC2 customers are a violation of the AWS Acceptable Use Policy. . You may request permission to conduct vulnerability scans as required to meet your specific compliance requirements. These scans must be limited to your own instances and must not violate the AWS Acceptable Use Policy. <strong>You must request a vulnerability scan in advance</strong>.</li>
</ul>
</li>
</ul>
</li>
<li>AWS Credentials<br>  <img src="/images/AWS/Sysops/aws_security_credentials.jpg" alt="aws_security_credentials"></li>
<li>AWS Trusted Advisor<ul>
<li>Trusted Advisor inspects your AWS environment and makes recommendations when opportunities may exist to save money, improve system performance, or close security gaps. It provides alerts on several of the most common security misconfigurations that can occur, including leaving certain ports open that make you vulnerable to hacking and unauthorized access, neglecting to create IAM accounts for your internal users, allowing public access to Amazon S3 buckets, not turning on user activity logging (AWS CloudTrail), or not using MFA on your root AWS Account.</li>
</ul>
</li>
<li>Instance Isolation<ul>
<li>Different instances running on the same physical machine are isolated from each other via the Xen hypervisor. Amazon is active in the Xen community, which provides awareness of the latest developments. In addition, the AWS firewall resides within the hypervisor layer, between the physical network interface and the instance’s virtual interface. All packets must pass through this layer, thus an instance’s neighbors have no more access to that instance than any other host on the Internet and can be treated as if they are on separate physical hosts. The physical RAM is separated using similar mechanisms.</li>
<li>Customer instances have no access to raw disk devices, but instead are presented with virtualized disks. The AWS proprietary disk virtualization layer automatically resets every block of storage used by the customer, so that one customer’s data is never unintentionally exposed to another. In addition, memory allocated to guests is scrubbed (set to zero) by the hypervisor when it is unallocated to a guest. The memory is not returned to the pool of free memory available for new allocations until the memory scrubbing is complete.<br><img src="/images/AWS/Sysops/Amazon_EC2_Multiple_Layers_of_Security.jpg" alt="Amazon_EC2_Multiple_Layers_of_Security"></li>
</ul>
</li>
<li>Other Considerations<ul>
<li><strong>Guest Operating System</strong> -  Virtual instances are completely controlled by you, the customer. You have full root access or administrative control over accounts, services, and applications. AWS does not have any access rights to your instances or the guest OS.</li>
<li><strong>Firewall</strong> -  Amazon EC2 provides a complete firewall solution; this mandatory inbound firewall is configured in a default deny-all mode and Amazon EC2 customers must explicitly open the ports needed to allow inbound traffic.</li>
<li><strong>Elastic Block Storage(EBS) Security</strong> - Encryption of sensitive data is generally a good security practice, and AWS provides the ability to encrypt EBS volumes and their snapshots with AES-256. The encryption occurs on the servers that host the EC2 instances, providing encryption of data as it moves between EC2 instances and EBS storage. In order to be able to do this efficiently and with low latency, the EBS encryption feature is only available on EC2’s more powerful instance types (e.g., M3, C3, R3, G2).</li>
<li><strong>Elastic Load Balancing</strong> - SSL Termination on the load balancer is supported.</li>
<li>Direct Connect<ul>
<li>Bypass Internet service providers in your network path. You can procure rack space within the facility housing the AWS Direct Connect location and deploy your equipment nearby. Once deployed, you can connect this equipment to AWS Direct Connect using a cross-connect.</li>
<li>Using industry standard 802.1q VLANs, the dedicated connection can be partitioned into multiple virtual interfaces. This allows you to use the same connection to access public resources such as objects stored in Amazon S3 using public IP address space, and private resources such as Amazon EC2 instances running within an Amazon VPC using private IP space, while maintaining network separation between the public and private environments.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>AWS &amp; IT Audits</p>
<ul>
<li>Compliance<ul>
<li>SOC 1/SSAE 16/ISAE 3402 (formerly SAS 70 Type II)</li>
<li>SOC2</li>
<li>SOC3</li>
<li>FISMA, DIACAP, and FedRAMP</li>
<li>PCI DSS Level 1</li>
<li>ISO 27001</li>
<li>ISO 9001</li>
<li>ITAR</li>
<li>FIPS 140-2</li>
<li>Several industry-specific standards:<ul>
<li>HIPAA</li>
<li>Cloud Security Alliance (CSA)</li>
<li>Motion Picture Association of America (MPAA)</li>
</ul>
</li>
</ul>
</li>
<li>Auditing on AWS<ul>
<li>your organization may undergo an audit. This cloud be for PCI Compliance, ISO 27001, SOC etc. There is a level of shared responsibility in regards to audits:<ul>
<li>AWS Provides - their annual certifications and reports (ISO 27001, PCI-DSS certificates etc). Amazon are responsible the global infrastructure including all hardware, data centers, physical security etc</li>
<li>Customer provides - everything they have put on AWS, such as EC2 instances, RDS instances, Applications, Assets in S3 etc. Essentially the organizations AWS assets (this can include the data itself)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h4><ul>
<li>Route53<ul>
<li>ELB’s do not have pre-defined IPv4 addresses, you resolve to them using a DNS name.</li>
<li>Understand the difference between an Alias Record and a CNAME</li>
<li>Given the choice, always choose an Alias Record over a CNAME.</li>
<li>Remember the different routing policies and their use cases.<ul>
<li>Simple</li>
<li>Weighted</li>
<li>Latency</li>
<li>Failover</li>
<li>Geolocation</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="VPC’s"><a href="#VPC’s" class="headerlink" title="VPC’s"></a>VPC’s</h4><ul>
<li>Basic Info<ul>
<li>Think of a VPC as a logical datacenter in AWS</li>
<li>Consists of IGW’s (Or Virtual Private Gateways), Route Tables, Network Access Control Lists, Subnets, Security Groups</li>
<li>1 Subnet = 1 Availability Zone</li>
<li>Security Groups are Stateful, Network Access Control Lists are Stateless.</li>
<li>Can Peer VPCs both in the same account and with other AWS accounts.</li>
<li>No Transitive Peering</li>
<li>Custom VPC network block size has to be between a /16 netmask and /28 netmask.</li>
</ul>
</li>
</ul>
<ul>
<li>What can you do with a VPC<ul>
<li>Launch instances into a subnet of your choosing</li>
<li>Assign custom IP address ranges in each subnet</li>
<li>Configure route tables between subnets</li>
<li>Create internet gateway and attach it to our VPC</li>
<li>Much better security control over your AWS resources</li>
<li>Instance security groups</li>
<li>Subnet network access control lists (ACLS)</li>
</ul>
</li>
</ul>
<ul>
<li><p>Default VPC vs Custom VPC</p>
<ul>
<li>Default VPC is user friendly, allowing you to immediately deploy instances</li>
<li>All Subnets in default VPC have a route out to the internet.</li>
<li>Each EC2 instance has both a public and private IP address</li>
<li>If you delete the default VPC the only way to get it back is to contact AWS.</li>
</ul>
</li>
<li><p>VPC peering</p>
<ul>
<li>Allows you to connect one VPC with another via a direct network route using private IP addresses.</li>
<li>Instances behave as if they were on the same private network</li>
<li>You can peer VPC’s with other AWS accounts as well as with other VPCs in the same account.</li>
<li>Peering is in a star configuration, ie 1 central VPC peers with 4 others, <strong>NO TRANSITIVE PEERING!!!</strong></li>
</ul>
</li>
<li><p>Create VPC</p>
<ul>
<li>things automatically created<ul>
<li>Route tables</li>
<li>Network ACLs</li>
<li>Security Groups</li>
<li>DHCP options set</li>
</ul>
</li>
<li>things are not automatically created<ul>
<li>Internet Gateways</li>
<li>Subnets</li>
</ul>
</li>
</ul>
</li>
<li><p>VPC Subnet</p>
<ul>
<li>There are 5 IP address reserved in each subnet by AWS, take CIDR block 10.0.0.0/24 as example<ul>
<li>10.0.0.0 Network address</li>
<li>10.0.0.1 Reserved by AWS for the VPC router</li>
<li>10.0.0.2 Reserved by AWS for DNS</li>
<li>10.0.0.3 Reserved by AWS for future use.</li>
<li>10.0.0.255 Network broadcast address, we do not support broadcast in a VPC, therefore we reserve this address.</li>
</ul>
</li>
</ul>
</li>
<li><p>NAT instances</p>
<ul>
<li>When creating a NAT instance, Disable Source/Destination Check on the Instance</li>
<li>NAT instance must be in a public subnet</li>
<li>Must have an elastic IP address to work</li>
<li>There must be a route out of the private subnet to the NAT instance, in order for this to work</li>
<li>The amount of traffic that NAT instances supports, depends on the instance size. If you are bottlenecking, increase the instance size</li>
<li>You can create high availability using Autoscaling Groups, multiple subnets in different AZ’s and a script to automate failover</li>
<li>Behind a Security Group.</li>
</ul>
</li>
<li><p>NAT Gateways</p>
<ul>
<li>Very new</li>
<li>Preferred by the enterprise</li>
<li>Scale automatically up to 10Gbps</li>
<li>No need to patch</li>
<li>Not associated with security groups</li>
<li>Automatically assigned a public ip address</li>
<li>Remember to update your route tables.</li>
<li>No need to disable Source/Destination Checks.</li>
</ul>
</li>
<li><p>NAT instances vs NAT Gateways</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Attribute</th>
<th style="text-align:left">NAT gateway</th>
<th style="text-align:left">NAT instance</th>
</tr>
</thead>
<tbody>
<tr>
<td>Availability</td>
<td style="text-align:left">Highly available. NAT gateways in each Availability Zone are implemented with redundancy. Create a NAT gateway in each Availability Zone to ensure zone-independent architecture.</td>
<td style="text-align:left">Use a script to manage failover between instances.</td>
</tr>
<tr>
<td>Bandwidth</td>
<td style="text-align:left">Supports bursts of up to 10Gbps.</td>
<td style="text-align:left">Depends on the bandwidth of the instance type.</td>
</tr>
<tr>
<td>Maintenance</td>
<td style="text-align:left">Managed by AWS.You do not need to perform any maintenance.</td>
<td style="text-align:left">Managed by you, for example, by installing software updates or operating system patches on the instance.</td>
</tr>
<tr>
<td>Performance</td>
<td style="text-align:left">Software is optimized for handling NAT traffic.</td>
<td style="text-align:left">A generic Amazon Linux AMI that’s configured to perform NAT.</td>
</tr>
<tr>
<td>Cost</td>
<td style="text-align:left">Charged depending on the number of NAT gateways you use, duration of usage, and amount of data that you send through the NAT gateways.</td>
<td style="text-align:left">Charged depending on the number of NAT instances that you use, duration of usage, and instance type and size.</td>
</tr>
<tr>
<td>Type and size</td>
<td style="text-align:left">Uniform offering; you don’t need to decide on the type or size.</td>
<td style="text-align:left">Choose a suitable instance type and size, according to your predicted workload.</td>
</tr>
<tr>
<td>Public IP addresses</td>
<td style="text-align:left">Choose the Elastic IP address to associate with a NAT gateway at creation.</td>
<td style="text-align:left">Use an Elastic IP address or a public IP address with a NAT instance. You can change the public IP address at any time by associating a new Elastic IP address with the instance.</td>
</tr>
<tr>
<td>Private IP addresses</td>
<td style="text-align:left">Automatically selected from the subnet’s IP address range when you create the gateway.</td>
<td style="text-align:left">Assign a specific private IP address from the subnet’s IP address range when you launch the instance.</td>
</tr>
<tr>
<td>Security groups</td>
<td style="text-align:left">Cannot be associated with a NAT gateway. You can associate security groups with your resources behind the NAT gateway to control inbound and outbound traffic.</td>
<td style="text-align:left">Associate with your NAT instance and the resources behind your NAT instance to control inbound and outbound traffic.</td>
</tr>
<tr>
<td>Network ACLs</td>
<td style="text-align:left">Use a network ACL to control the traffic to and from the subnet in which your NAT gateway resides.</td>
<td style="text-align:left">Use a network ACL to control the traffic to and from the subnet in which your NAT instance resides.</td>
</tr>
<tr>
<td>Flow logs</td>
<td style="text-align:left">Use flow logs to capture the traffic.</td>
<td style="text-align:left">Use flow logs to capture the traffic.</td>
</tr>
<tr>
<td>Port forwarding</td>
<td style="text-align:left">Not supported.</td>
<td style="text-align:left">Manually customize the configuration to support port forwarding.</td>
</tr>
<tr>
<td>Bastion servers</td>
<td style="text-align:left">Not supported.</td>
<td style="text-align:left">Use as a bastion server.</td>
</tr>
<tr>
<td>Traffic metrics</td>
<td style="text-align:left">Not supported.</td>
<td style="text-align:left">View CloudWatch metrics.</td>
</tr>
<tr>
<td>Timeout behavior</td>
<td style="text-align:left">When a connection times out, a NAT gateway returns an RST packet to any resources behind the NAT gateway that attempt to continue the connection (it does not send a FIN packet).</td>
<td style="text-align:left">When a connection times out, a NAT instance sends a FIN packet to resources behind the NAT instance to close the connection.</td>
</tr>
<tr>
<td>IP fragmentation</td>
<td style="text-align:left">Supports forwarding of IP fragmented packets for the UDP protocol. Does not support fragmentation for the TCP and ICMP protocols. Fragmented packets for these protocols will get dropped.</td>
<td style="text-align:left">Supports reassembly of IP fragmented packets for the UDP, TCP, and ICMP protocols.</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Network ACL’s</p>
<ul>
<li>Your VPC automatically comes a default network ACL and by default it allows all outbound and inbound traffic.</li>
<li>You can create a custom network ACL. By default, each custom network ACL denies all inbound and outbound traffic until you add rules.</li>
<li>Each subnet in your VPC must be associated with a network ACL. If you don’t explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL.</li>
<li>You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed.</li>
<li>A network ACl contains a numbered list of rules that is evaluated in order, starting with the lowest numbered rule.</li>
<li>A network ACl has separate inbound and outbound rules, and each rule can either allow or deny traffic.</li>
<li>Network ACLs are stateless responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa)</li>
<li>Block IP Addresses using network ACL’s not Security Groups</li>
</ul>
</li>
<li><p>Security Group vs Network ACL</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Security Group</th>
<th style="text-align:left">Network ACL</th>
</tr>
</thead>
<tbody>
<tr>
<td>operates at the instance level (first layer of defense)</td>
<td style="text-align:left">Operates at the subnet level (second layer of defense)</td>
</tr>
<tr>
<td>Supports allow rules only</td>
<td style="text-align:left">Supports allow rules and deny rules</td>
</tr>
<tr>
<td>Is stateful: Return traffic is automatically allowed, regardless of any rules</td>
<td style="text-align:left">Is stateless: Return traffic must be explicitly allowed by rules</td>
</tr>
<tr>
<td>We evaluate all rules before deciding whether to allow traffic</td>
<td style="text-align:left">We process rules in number order when deciding whether to allow traffic</td>
</tr>
<tr>
<td>Applies to an instance only if someone specifies the security group when launching the instance, or associates the security group with the instance later on</td>
<td style="text-align:left">Automatically applies to all instances in the subnets it’s associated with (backup layer of defense, so you don’t have to rely on someone specifying the security group)</td>
</tr>
</tbody>
</table>
<ul>
<li><p>NAT vs Bastions</p>
<ul>
<li>A NAT is used to provide internet traffic to EC2 instances in private subnets</li>
<li>A Bastion is used to securely administer EC2 instance (using SSH or RDP) in private subnets. In Australia we call them jump boxes.</li>
</ul>
</li>
<li><p>Resilient Architecture</p>
<ul>
<li>If you want resiliency, always have 2 public subnets and 2 private subnets. Make sure each subnet is in different availability zones.</li>
<li>With ELB’s make sure they are in 2 public subnets in 2 different availability zones.</li>
<li>With Bastion hosts, put them behind an autoscaling group with a minimum size of 2. Use Route53 (either round robin or using a health check) to automatically fail over.</li>
<li>NAT instances are tricky to make resilient. You need 1 in each public subnet, each with their own public IP address, and you need to write a script to fail between the two. Instead where possible, use NAT gateways.</li>
</ul>
</li>
<li><p>VPC Flow Logs</p>
<ul>
<li>You can monitor network traffic within your custom VPC’s using VPC Flow Logs.</li>
</ul>
</li>
<li><p>VPC limit</p>
<ul>
<li>Currently you can create 200 subnets per VPC by default</li>
</ul>
</li>
<li><p>Direct Connect</p>
<ul>
<li>What is Direct Connect <a href="https://aws.amazon.com/directconnect/?nc1=h_ls" target="_blank" rel="external">https://aws.amazon.com/directconnect/?nc1=h_ls</a><ul>
<li>AWS Direct Connect makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.</li>
<li>AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. This allows you to use the same connection to access public resources such as objects stored in Amazon S3 using public IP address space, and private resources such as Amazon EC2 instances running within an Amazon Virtual Private Cloud (VPC) using private IP space, while maintaining network separation between the public and private environments. Virtual interfaces can be reconfigured at any time to meet your changing needs.</li>
</ul>
</li>
<li>Advantage of Direct Connect over VPN<ul>
<li><strong>Bandwidth &amp; a more consistent network experience!</strong></li>
<li>A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC.</li>
</ul>
</li>
</ul>
</li>
<li><p>Network Bottlenecks</p>
<ul>
<li>Each Instance type have the ma bandwidth and throughput.</li>
</ul>
</li>
</ul>
<h2 id="专项问题点"><a href="#专项问题点" class="headerlink" title="专项问题点"></a>专项问题点</h2><ul>
<li><p>ELB Connection draining</p>
<ul>
<li>Min: 1s, Max: 3600, default: 300s</li>
<li>helps the user to stop sending new requests traffic from the load balancer to the EC2 instance when the instance is being deregistered while continuing in-flight requests?</li>
</ul>
</li>
<li><p>AutoScaling Group Lifecycle</p>
<ul>
<li><a href="https://docs.aws.amazon.com/autoscaling/latest/userguide/AutoScalingGroupLifecycle.html" target="_blank" rel="external">Auto Scaling Lifecycle</a><br><img src="/images/AWS/Sysops/autoscaling_group_lifecycle.jpg" alt="autoscaling_group_lifecycle"></li>
<li><a href="https://docs.aws.amazon.com/autoscaling/latest/userguide/as-maintain-instance-levels.html" target="_blank" rel="external">Maintaining the Number of Instances in Your Auto Scaling Group</a></li>
</ul>
</li>
<li>CloudWatch 能监控的Metrics <a href="https://aws.amazon.com/cloudwatch/faqs/" target="_blank" rel="external">https://aws.amazon.com/cloudwatch/faqs/</a><ul>
<li>Amazon EC2 instances</li>
<li>EBS volumes</li>
<li>Elastic Load Balancers</li>
<li>Auto Scaling groups</li>
<li>EMR job flows</li>
<li>RDS DB instances</li>
<li>DynamoDB tables</li>
<li>ElastiCache clusters</li>
<li>RedShift clusters</li>
<li>OpsWorks stacks</li>
<li>Route 53 health checks</li>
<li>SNS topics</li>
<li>SQS queues</li>
<li>SWF workflows</li>
<li>and Storage Gateways</li>
</ul>
</li>
<li><p>不提供Detail monitoring的service, 最细颗粒是5 mins的service</p>
<ul>
<li>EMR</li>
<li>SNS</li>
</ul>
</li>
<li><p>CloudWatch 免费detail monitoring的几个service <a href="https://aws.amazon.com/cloudwatch/details/?nc1=h_ls" target="_blank" rel="external">https://aws.amazon.com/cloudwatch/details/?nc1=h_ls</a></p>
<ul>
<li><strong>Auto Scaling groups</strong>: seven pre-selected metrics at one-minute frequency, optional and for no additional charge.</li>
<li><strong>Elastic Load Balancers</strong>: thirteen pre-selected metrics at one-minute frequency, for no additional charge.</li>
<li><strong>Amazon Route 53 health checks</strong>: One pre-selected metric at one-minute frequency, for no additional charge</li>
<li><strong>Amazon EBS PIOPS (SSD) volumes</strong>: ten pre-selected metrics at one-minute frequency, for no additional charge</li>
<li><strong>Amazon CloudFront</strong>: six pre-selected metrics at one-minute frequency, for no additional charge</li>
<li><strong>Amazon ElastiCache nodes</strong>: thirty-nine pre-selected metrics at one-minute frequency, for no additional charge.</li>
<li><strong>Amazon RDS DB instances</strong>: fourteen pre-selected metrics at one-minute frequency, for no additional charge.</li>
<li><strong>Amazon Redshift</strong>: Sixteen pre-selected metrics at one-minute frequency, for no additional charge</li>
<li><strong>AWS Opsworks</strong>: fifteen pre-selected metrics at one-minute frequency, for no additional charge.</li>
<li><strong>Amazon CloudWatch Logs</strong>: six pre-selected metrics at one-minute frequency, for no additional charge</li>
</ul>
</li>
<li><p>CloudWatch 提供的几种统计值</p>
<ul>
<li>Average</li>
<li>Minimum</li>
<li>Maximum</li>
<li>Sum</li>
<li>Data Samples</li>
<li>p99</li>
<li>p95</li>
<li>p90</li>
<li>p50</li>
<li>p10</li>
</ul>
</li>
<li><p>CloudWatch AWS Namespaces</p>
<ul>
<li>There is no CloudTrail</li>
<li><a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/aws-namespaces.html" target="_blank" rel="external">All Cloudwatch Namespaces</a></li>
</ul>
</li>
<li><p>CloudWatch PutMetricData</p>
<ul>
<li>Each put-metric-data request is limited to 8KB in size for HTTP GET requests and is limited to 40 KB in size for HTTP POST requests.</li>
</ul>
</li>
<li><p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-ssl-security-policy.html" target="_blank" rel="external">SSL Negotiation Configurations for Classic Load Balancers</a></p>
<ul>
<li>Security Policies<ul>
<li>Predefined Security Policies</li>
<li>Custom Security Policies</li>
</ul>
</li>
<li>SSL Protocols (no TLS 1.3)<ul>
<li>supported<ul>
<li>TLS 1.2</li>
<li>TLS 1.1</li>
<li>TLS 1.0</li>
<li>SSL 3.0</li>
</ul>
</li>
<li>deprecated SSL Protocol<ul>
<li>SSL 2.0</li>
</ul>
</li>
</ul>
</li>
<li>The Server Order Preference is not supported in the Security Policy ELBSecurity Policy-2011-08 security policy.</li>
</ul>
</li>
<li><p>ELB sticky session algorithm</p>
<ul>
<li>This is how the ELB algorithm works in general when Cookie is not present<ul>
<li>First it checks the cookie is present in the service request</li>
<li>Since the cookie is not found in the request it will then decide which instance the service request should be routed to.</li>
<li>Finally the cookie is inserted in the response</li>
</ul>
</li>
<li>This is how the ELB algorithm works in general when Cookie is present<ul>
<li>First it checks the cookie is present in the service request</li>
<li>Since the cookie is found in the request it will then decide which instance the service request should be routed to based on the already present cookie.</li>
<li>Finally the cookie is inserted in the response</li>
</ul>
</li>
</ul>
</li>
<li><p>ELB access log file format</p>
<ul>
<li>format: bucket[/prefix]/AWSLogs/aws-account-id/elasticloadbalancing/region/yyyy/mm/dd/aws-account-id_elasticloadbalancing_region_load-balancer-id_end-time_ip-address_random-string.log.gz</li>
<li>sample: /AWSLogs/921187888888/elasticloadbalancing/us-west-2/2017/07/16/921187888888_elasticloadbalancing_us-west-2_awseb-e-u-AWSEBLoa-1KH856XXXXXXX_20170716T0000Z_35.161.113.177_1rhwm1ze.log</li>
</ul>
</li>
<li><p>IAM user name rule</p>
<ul>
<li>User names can be a combination of up to 64 letters, digits, and these characters: plus (+), equal (=), comma (,), period (.), at sign (@), and hyphen (-). Names must be unique within an account. They are not distinguished by case</li>
</ul>
</li>
<li><p>上传到CloudWatch的数据的时间戳</p>
<ul>
<li>past two week ~ two hours in the future : Each metric data point must be marked with a time stamp. The time stamp can be up to two weeks in the past and up to two hours into the future</li>
</ul>
</li>
<li><p>根据Wizard创建 VPC with Public an Private Subnets时</p>
<ul>
<li>默认会创建一个NAT Gateway，可以选择创建Nat Instance，并选择对应的Instance type和Key pair</li>
<li>创建两个Route Table<ul>
<li>面向private子网，0.0.0.0/0指向Nat Instance的是Main Route Table</li>
<li>面向public，0.0.0.0/0指向Internet Gateway的是Custom Route Table</li>
</ul>
</li>
<li>创建后不能直接删除，需要删除Public子网中的Nat Instance后才能删除VPC</li>
</ul>
</li>
<li><p>ACL rule priority</p>
<ul>
<li>Rule number. Rules are evaluated starting with the lowest numbered rule. As soon as a rule matches traffic, it’s applied regardless of any higher-numbered rule that may contradict it.</li>
</ul>
</li>
<li><p>VPC Dedicated Tenancy</p>
<ul>
<li>VPC 是非Dedicated的，建立EC2的时候是可以选Shared, Dedicated, Dedicated Host的</li>
<li>VPC建立的时候就是建立的Dedicated VPC时，建立在此VPC中的EC2，是不能选Shared的，只有Dedicated和Dedicated Host</li>
<li>在启动实例后，要想更改其租赁属性，有一定限制。<ul>
<li>在启动实例后，不能将其租赁属性从 default 改为 dedicated 或 host。</li>
<li>在启动实例后，不能将其租赁属性从 dedicated 或 host 改为 default。</li>
<li>在启动实例后，可以将其租赁属性从 dedicated 改为 host，或从 host 改为 dedicated</li>
</ul>
</li>
<li>You cannot change the tenancy of a default instance after you’ve launched it.</li>
</ul>
</li>
<li><p>VPC Peering Limitations</p>
<ul>
<li>You cannot create a VPC peering connection between VPCs that have matching or overlapping IPv4 or IPv6 CIDR blocks. Amazon always assigns your VPC a unique IPv6 CIDR block. If your IPv6 CIDR blocks are unique but your IPv4 blocks are not, you cannot create the peering connection.</li>
<li>You cannot create a VPC peering connection between VPCs in different regions.</li>
<li>You have a limit on the number active and pending VPC peering connections that you can have per VPC.</li>
<li>VPC peering does not support transitive peering relationships; in a VPC peering connection, your VPC does not have access to any other VPCs that the peer VPC may be peered with. This includes VPC peering connections that are established entirely within your own AWS account.</li>
<li>You cannot have more than one VPC peering connection between the same two VPCs at the same time.</li>
<li>A placement group can span peered VPCs; however, you do not get full-bisection bandwidth between instances in peered VPCs.</li>
<li>Unicast reverse path forwarding in VPC peering connections is not supported.</li>
<li>You can enable resources on either side of a VPC peering connection to communicate with each other over IPv6; however, IPv6 communication is not automatic. You must associate an IPv6 CIDR block with each VPC, enable the instances in the VPCs for IPv6 communication, and add routes to your route tables that route IPv6 traffic intended for the peer VPC to the VPC peering connection.</li>
</ul>
</li>
<li><p>IAM Limitation</p>
<ul>
<li>Groups in an AWS account - 300</li>
<li>Roles in an AWS account - 1000</li>
<li>Customer managed policies in an AWS account - 1500</li>
<li>Users in an AWS account - 5000</li>
<li>Groups an IAM user can be a member of - 10</li>
</ul>
</li>
<li><p>EC2 Termination Protection 不会保护来自instance内部的shutdown命令导致的terminal - <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#Using_ChangingDisableAPITermination" target="_blank" rel="external">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#Using_ChangingDisableAPITermination</a></p>
<ul>
<li>The DisableApiTermination attribute does not prevent you from terminating an instance by initiating shutdown from the instance (using an operating system command for system shutdown) when the InstanceInitiatedShutdownBehavior attribute is set.</li>
</ul>
</li>
<li><p>CloudWatch can monitoring AutoScaling Metric</p>
<ul>
<li>可以直接在CloudWatch中查看AutoScaling中EC2的整体指标，包括CPU，Network In/Out等</li>
</ul>
</li>
<li><p>EBS limitation</p>
<ul>
<li>Number of EBS Volumes - 5000</li>
<li>Number of EBS snapshots - 10000</li>
</ul>
</li>
<li><p>VPC Limitation</p>
<ul>
<li>VPCs per region - 5</li>
<li>Subnets per VPC - 100</li>
</ul>
</li>
<li><p>Bucket Policy 优先级</p>
<ul>
<li>The explicited deny permission will override the allow</li>
</ul>
</li>
<li><p>关于Redis中出现Eviction时的对应方法:</p>
<ul>
<li>官网说是直接scale up by using a larger node type</li>
<li>AcloudGuru上有讨论说处理方法是Only Scale out</li>
<li>各个<ul>
<li><a href="http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/CacheMetrics.WhichShouldIMonitor.html" target="_blank" rel="external">Which Metrics Should I Monitor</a></li>
<li><a href="https://acloud.guru/forums/aws-certified-sysops-administrator-associate/discussion/-Kc_XDW2XzOR32IFCY9f/redis-_scale_up_or_scale_out_-" target="_blank" rel="external">Redis- Scale up or scale out - question about lecture</a></li>
<li><a href="https://acloud.guru/forums/aws-certified-sysops-administrator-associate/discussion/-KDdkFs7QIlvu1npl2kR/redis-evictions-correction" target="_blank" rel="external">Redis Evictions Correction</a></li>
<li><a href="https://acloud.guru/forums/aws-certified-sysops-administrator-associate/discussion/-KNcfMRPnKdJinvljh8W/redis-evictions" target="_blank" rel="external">Redis Evictions</a></li>
<li><a href="https://acloud.guru/forums/aws-certified-sysops-administrator-associate/discussion/-K_D1AlOj9xH30fa0tl0/why-are-the-quizzes-so-messed-up-it-is-very-confusing" target="_blank" rel="external">Why are the quizzes so messed up? It is very confusing</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Doc"><a href="#Doc" class="headerlink" title="Doc"></a>Doc</h2><h3 id="User-Guide"><a href="#User-Guide" class="headerlink" title="User Guide"></a>User Guide</h3><ul>
<li><a href="http://docs.aws.amazon.com/opsworks/latest/userguide/welcome.html" target="_blank" rel="external">AWS Opsworks User Guide</a></li>
</ul>
<h3 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h3><ul>
<li><a href="https://aws.amazon.com/cn/elasticache/faqs/?nc1=h_ls" target="_blank" rel="external">Elasticache FAQ</a></li>
<li><a href="https://amazonaws-china.com/lambda/faqs/" target="_blank" rel="external">Lambda FAQ</a></li>
<li><a href="https://amazonaws-china.com/api-gateway/faqs/" target="_blank" rel="external">API Gateway FAQ</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS Certified </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Sysops </tag>
            
            <tag> CloudWatch </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[在AWS RDS中为Postgresql开启慢查询日志]]></title>
      <url>/2017/11/29/how-to-enable-slow-query-on-aws-rds-postgresql/</url>
      <content type="html"><![CDATA[<p>sql遇到性能问题，就需要开启慢查询日志，将执行时间超过某个限定值的sql输出到日志，给后续开发人员分析。下面是AWS RDS中Postgresql开启慢查询日志的方法。包含</p>
<ul>
<li>RDS设置</li>
<li>下载RDS log的方法</li>
<li>一些注意点</li>
</ul>
<h3 id="RDS设置步骤"><a href="#RDS设置步骤" class="headerlink" title="RDS设置步骤"></a>RDS设置步骤</h3><ol>
<li>登陆aws console，切换到RDS<br><img src="/images/AWS/RDSSlowQuery/switch_to_rds.png" alt="switch_to_rds.png"><a id="more"></a></li>
<li>在左边菜单栏中，找到”Parameter Groups”<br><img src="/images/AWS/RDSSlowQuery/parameter_groups.png" alt="parameter_groups.png"></li>
<li>选中要开启慢查询日志的PostgreSQL使用的参数组, 此处是myparametergroup, 点击”Edit Parameters”修改配置<br><img src="/images/AWS/RDSSlowQuery/select_parameter_group.png" alt="select_parameter_group.png"></li>
<li>修改<code>log_min_duration_statement</code>为超限的毫秒数，超过这个数值，PostgreSQL就会记录下相关的log。日志格式参见后续说明。<br><img src="/images/AWS/RDSSlowQuery/set_log_min_duration_statement.png" alt="set_log_min_duration_statement.png"></li>
<li>如果原来开启了<code>log_statement</code>和<code>log_duration</code>的，需要将<code>log_statement</code>设为默认的none，将<code>log_duration</code>设为0, 否则输出的慢查询日志的sql和执行时间就不在同一行，不便于观察。</li>
<li>因为<code>log_min_duration_statement</code>是动态参数的(修改页面上的Apply Type属性是Dynamic)，因此修改设置后不需要重启，RDS会自动load新的设置。<br><img src="/images/AWS/RDSSlowQuery/load_parameter_group.png" alt="load_parameter_group.png"></li>
<li><strong>注意:</strong> 如果RDS原先使用的是default的参数组，那么RDS换为自己定义的参数组的时候，RDS会重启</li>
</ol>
<h3 id="下载日志的方法"><a href="#下载日志的方法" class="headerlink" title="下载日志的方法"></a>下载日志的方法</h3><h4 id="设置IAM权限来允许下载RDS-logs"><a href="#设置IAM权限来允许下载RDS-logs" class="headerlink" title="设置IAM权限来允许下载RDS logs"></a>设置IAM权限来允许下载RDS logs</h4><p>将如下Policy 添加到IAM User或者Role中，就可以使用API或者CLI来下载RDS的log了</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"Version"</span>: <span class="string">"2012-10-17"</span>,</div><div class="line">    <span class="attr">"Statement"</span>: [</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"Action"</span>: [</div><div class="line">                <span class="string">"rds:Describe*"</span>,</div><div class="line">                <span class="string">"rds:DownloadDBLogFilePortion"</span></div><div class="line">            ],</div><div class="line">            <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</div><div class="line">            <span class="attr">"Resource"</span>: <span class="string">"*"</span></div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="aws-cli下载文件的命令"><a href="#aws-cli下载文件的命令" class="headerlink" title="aws cli下载文件的命令"></a>aws cli下载文件的命令</h4><p><code>aws rds download-db-log-file-portion help</code>中有下载完整log的用法</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">aws rds download-db-log-file-portion --db-instance-identifier myinstance --<span class="built_in">log</span>-file-name log.txt --starting-token 0 --output text &gt; full.txt</div></pre></td></tr></table></figure>
<p>如下是下载名为mydb的Instance中某个指定日志的示例:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">[ec2-user@ip-172-31-13-115 rds_log]$ aws rds describe-db-log-files --db-instance-identifier=mydb</div><div class="line">&#123;</div><div class="line">    <span class="string">"DescribeDBLogFiles"</span>: [</div><div class="line">        &#123;</div><div class="line">            <span class="string">"LastWritten"</span>: 1511938537000,</div><div class="line">            <span class="string">"LogFileName"</span>: <span class="string">"error/postgres.log"</span>,</div><div class="line">            <span class="string">"Size"</span>: 307</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="string">"LastWritten"</span>: 1511938540000,</div><div class="line">            <span class="string">"LogFileName"</span>: <span class="string">"error/postgresql.log.2017-11-29-06"</span>,</div><div class="line">            <span class="string">"Size"</span>: 1051</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="string">"LastWritten"</span>: 1511942140000,</div><div class="line">            <span class="string">"LogFileName"</span>: <span class="string">"error/postgresql.log.2017-11-29-07"</span>,</div><div class="line">            <span class="string">"Size"</span>: 4032</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="string">"LastWritten"</span>: 1511942441000,</div><div class="line">            <span class="string">"LogFileName"</span>: <span class="string">"error/postgresql.log.2017-11-29-08"</span>,</div><div class="line">            <span class="string">"Size"</span>: 336</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div><div class="line">[ec2-user@ip-172-31-13-115 rds_log]$ aws rds download-db-log-file-portion --db-instance-identifier mydb --<span class="built_in">log</span>-file-name error/postgresql.log.2017-11-29-07 --starting-token 0 --output text &gt; postgresql.log.2017-11-29-07</div><div class="line">[ec2-user@ip-172-31-13-115 rds_log]$ ll</div><div class="line">总用量 4</div><div class="line">-rw-rw-r-- 1 ec2-user ec2-user 4033 11月 29 08:02 postgresql.log.2017-11-29-07</div><div class="line">[ec2-user@ip-172-31-13-115 rds_log]$</div></pre></td></tr></table></figure>
<p>注意下载下来文件的大小，会比<code>describe-db-log-files</code>显示的文件大小大1 Byte, 因为下载下来的文件末尾会额外多一个空行</p>
<h4 id="Ruby-SDK-下载log的一个例子"><a href="#Ruby-SDK-下载log的一个例子" class="headerlink" title="Ruby SDK 下载log的一个例子"></a>Ruby SDK 下载log的一个例子</h4><p>参见<a href="https://gist.github.com/ruckus/d30531c543d677eb3acb" target="_blank" rel="external">GithubGist</a></p>
<p>摘录关键点如下:</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">additional_data_pending = <span class="literal">true</span></div><div class="line">File.open(out_log_file, <span class="string">"wb+"</span>) <span class="keyword">do</span> <span class="params">|file|</span></div><div class="line">  <span class="keyword">while</span> additional_data_pending <span class="keyword">do</span></div><div class="line">    out = rds.download_db_log_file_portion(opts)</div><div class="line">    file.write(out[<span class="symbol">:log_file_data</span>])</div><div class="line">    <span class="comment">#puts out[:marker]</span></div><div class="line">    opts[<span class="symbol">:marker</span>] = out[<span class="symbol">:marker</span>]</div><div class="line">    additional_data_pending = out[<span class="symbol">:additional_data_pending</span>]</div><div class="line">  <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><h4 id="log-min-duration-statement和log-statement同时使用的格式"><a href="#log-min-duration-statement和log-statement同时使用的格式" class="headerlink" title="log_min_duration_statement和log_statement同时使用的格式"></a>log_min_duration_statement和log_statement同时使用的格式</h4><p>按照<a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.PostgreSQL.html" target="_blank" rel="external">官方文档</a>说明,开启了<code>log_statement</code>和<code>log_duration</code>后，error log中输出的日志格式如下.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">2013-11-05 16:51:10 UTC:[local]:master@postgres:[9193]:LOG:  statement: SELECT c2.relname, i.indisprimary, i.indisunique, i.indisclustered, i.indisvalid, pg_catalog.pg_get_indexdef(i.indexrelid, 0, true),</div><div class="line">	  pg_catalog.pg_get_constraintdef(con.oid, true), contype, condeferrable, condeferred, c2.reltablespace</div><div class="line">	FROM pg_catalog.pg_class c, pg_catalog.pg_class c2, pg_catalog.pg_index i</div><div class="line">	  LEFT JOIN pg_catalog.pg_constraint con ON (conrelid = i.indrelid AND conindid = i.indexrelid AND contype IN (&apos;p&apos;,&apos;u&apos;,&apos;x&apos;))</div><div class="line">	WHERE c.oid = &apos;1255&apos; AND c.oid = i.indrelid AND i.indexrelid = c2.oid</div><div class="line">	ORDER BY i.indisprimary DESC, i.indisunique DESC, c2.relname;</div><div class="line">2013-11-05 16:51:10 UTC:[local]:master@postgres:[9193]:LOG:  duration: 3.367 ms</div><div class="line">2013-11-05 16:51:10 UTC:[local]:master@postgres:[9193]:LOG:  statement: SELECT c.oid::pg_catalog.regclass FROM pg_catalog.pg_class c, pg_catalog.pg_inherits i WHERE c.oid=i.inhparent AND i.inhrelid = &apos;1255&apos; ORDER BY inhseqno;</div><div class="line">2013-11-05 16:51:10 UTC:[local]:master@postgres:[9193]:LOG:  duration: 1.002 ms</div><div class="line">2013-11-05 16:51:10 UTC:[local]:master@postgres:[9193]:LOG:  statement: SELECT c.oid::pg_catalog.regclass FROM pg_catalog.pg_class c, pg_catalog.pg_inherits i WHERE c.oid=i.inhrelid AND i.inhparent = &apos;1255&apos; ORDER BY c.oid::pg_catalog.regclass::pg_catalog.text;</div><div class="line">2013-11-05 16:51:18 UTC:[local]:master@postgres:[9193]:LOG:  statement: select proname from pg_proc;</div><div class="line">2013-11-05 16:51:18 UTC:[local]:master@postgres:[9193]:LOG:  duration: 3.469 ms</div></pre></td></tr></table></figure></p>
<p>如果关闭了<code>log_statement</code>和<code>log_duration</code>, 只开启了<code>log_min_duration_statement</code>时，输出的日志格式中，duration和sql在同一行，比较便于阅读。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2017-11-26 08:01:32 UTC:172.31.13.115(40782):user@mydb:[4234]:LOG: duration: 1326.449 ms execute &lt;unnamed&gt;: SELECT COUNT(&quot;location_properties&quot;.&quot;id&quot;) FROM &quot;location_properties&quot; WHERE (location_record_id = 175034)</div></pre></td></tr></table></figure>
<h4 id="RDS-Parameter-Groups动态和静态参数的描述"><a href="#RDS-Parameter-Groups动态和静态参数的描述" class="headerlink" title="RDS Parameter Groups动态和静态参数的描述"></a>RDS Parameter Groups动态和静态参数的描述</h4><p>官方文档<a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html" target="_blank" rel="external">Working with DB Parameter Groups</a>中对修改RDS动态参数和静态参数的描述</p>
<blockquote>
<p>When you change a dynamic parameter and save the DB parameter group, the change is applied immediately regardless of the Apply Immediately setting. When you change a static parameter and save the DB parameter group, the parameter change will take effect after you manually reboot the DB instance.</p>
</blockquote>
<p>总结起来就是:</p>
<ul>
<li>修改动态参数, RDS会立即apply</li>
<li>修改静态参数, 只能reboot后才能生效</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="http://www.getfareye.com/blog/amazon-rds-postgres-activate-slow-query-logs" target="_blank" rel="external">Amazon RDS + Postgres – Activate slow query logs</a></li>
<li><a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.PostgreSQL.html" target="_blank" rel="external">PostgreSQL Database Log Files</a></li>
<li><a href="https://gist.github.com/ruckus/d30531c543d677eb3acb" target="_blank" rel="external">rds_download_logfiles.rb</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AWS SDK </tag>
            
            <tag> PostgreSQL </tag>
            
            <tag> Ruby </tag>
            
            <tag> AWS CLI </tag>
            
            <tag> RDS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[创建AWS Account后的基本账号设置]]></title>
      <url>/2017/10/31/account-setting-by-aws/</url>
      <content type="html"><![CDATA[<p>记录创建好AWS账号后的几个基本设置, 大概介绍如下几点:</p>
<ul>
<li>查看是否满足Free Tier Usage</li>
<li>创建IAM账号</li>
<li>为IAM账号启用Billing</li>
<li>如何提交Case</li>
<li>切换Console语言</li>
</ul>
<a id="more"></a>
<h3 id="查看是否满足Free-Tier-Usage"><a href="#查看是否满足Free-Tier-Usage" class="headerlink" title="查看是否满足Free Tier Usage"></a>查看是否满足Free Tier Usage</h3><p>根据官方文档 <a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/free-tier-eligibility.html" target="_blank" rel="external">Eligibility for the Free Tier</a> 所描述，在 <a href="https://console.aws.amazon.com/billing/home#/" target="_blank" rel="external">Billing and Cost Management console</a> 页面中的<strong>Alerts &amp; Notifications</strong>中看到 <strong>You are eligible for the AWS Free Usage Tier. See the Getting Started Guide AWS Free Usage Tier to learn how to get started with the free usage tier</strong> 的描述，就说明该账号是符合Free Tier条件的。</p>
<p><img src="/images/AWS/Startup/free-tier-usage-prompt.jpg" alt="Free Tier Usage Prompt"></p>
<h3 id="创建IAM用户"><a href="#创建IAM用户" class="headerlink" title="创建IAM用户"></a>创建IAM用户</h3><p>AWS 的root账号就是注册时使用的email账号，因为该账号拥有全部的操作权限。经常登录相对来说更容易泄露账号和密码，根据AWS的最佳实践的要求，不建议日常登陆来进行操作，强烈推荐创建IAM用户来进行日常的操作。</p>
<p>如果是为了练习AWS各个Feature用的，建议就直接创建拥有Admin权限的IAM用户。 进入<a href="https://console.aws.amazon.com/iam/home?#/home" target="_blank" rel="external">IAM Consoe</a>页面</p>
<h4 id="创建Groups"><a href="#创建Groups" class="headerlink" title="创建Groups"></a>创建Groups</h4><p>依次点击侧边栏[Groups]-&gt;[Create New Group], 跳出[Create New Group Wizard]页面, 在后续页面上依次操作如下:</p>
<ol>
<li>[Step 1 : Group Name] – 输入Group名字，可以输入 Admin</li>
<li>[Step 2 : Attach Policy] – 在Filter输入框中输入admin, 然后选择AdministratorAccess</li>
<li>[Step 3 : Review] – 点击”Create Group”</li>
</ol>
<h4 id="创建Users"><a href="#创建Users" class="headerlink" title="创建Users"></a>创建Users</h4><p>依次点击侧边栏[Users]-&gt;[Add user],  跳出[Add User]页面，后续页面依次操作如下:</p>
<ol>
<li><p>[Details] 页面</p>
<ul>
<li>输入User name</li>
<li>Access type勾选 AWS Management Console access</li>
<li>Console password 中设置密码</li>
<li>因为是练习使用， 不勾选 Require password reset 前面的checkbox</li>
</ul>
</li>
<li><p>[Permissions] 页面 – 直接勾选前面创建好的Admin组</p>
</li>
<li>[Review] 页面 – 确认信息是否都正确， 然后点击Create User</li>
<li>[Complete] – 记录类似 <code>https://xxxxxxxxxxxx.signin.aws.amazon.com/console</code>的登陆链接, 其中xxxxxxxxxxxx应该是一个12位的数字, 每个AWS账号都有自己独立的一串12位的数字</li>
</ol>
<p>至此，一个有Admin操作权限的用户就生成好了。</p>
<h3 id="为IAM账号启用Billing"><a href="#为IAM账号启用Billing" class="headerlink" title="为IAM账号启用Billing"></a>为IAM账号启用Billing</h3><p>为了安全起见，AWS IAM用户默认是没有Billing操作功能的，但作为使用Free Tier的练习者，每天使用后免不了都会去Billing页面瞅瞅是否有超额使用导致了费用。每次切换root account会很麻烦，因此为IAM用户启用Billing权限，是很有必要的。</p>
<p>操作步骤:</p>
<ol>
<li>使用root account访问 <a href="https://console.aws.amazon.com/billing/home?#/account" target="_blank" rel="external">Billing页面</a></li>
<li>找到<code>IAM User and Role Access to Billing Information</code>一栏</li>
<li>点击右边的<code>Edit</code>按钮，在展开的页面中，勾选<code>Activate IAM Access</code>前面的勾选框，然后点击<code>Update</code>按钮</li>
</ol>
<p>此时，退出root account或者换个浏览器，使用创建用户时候获取的<code>https://xxxxxxxxxxxx.signin.aws.amazon.com/console</code>登陆页面，输入IAM账号密码，然后访问<a href="https://console.aws.amazon.com/billing/home?#/" target="_blank" rel="external">Billing页面</a>, 就可以看到账单信息了。</p>
<p>如果没有开启IAM用户的Billing权限，访问<a href="https://console.aws.amazon.com/billing/home?#/" target="_blank" rel="external">Billing页面</a>, 就会报告<code>You Need Permissions</code>的错误，提示没有权限</p>
<p><img src="/images/AWS/Startup/No_Permission_To_Access_Billing.jpg" alt="No Permission To Access Billing"></p>
<h3 id="如何提交Case"><a href="#如何提交Case" class="headerlink" title="如何提交Case"></a>如何提交Case</h3><p>AWS的使用，碰到问题，第一要诀就是Google，第二要诀就是查官方Doc，最后一个途径就是向AWS Support提交Case。</p>
<p>提交Case的几个通用的场景:</p>
<ul>
<li>要提高AWS服务的各个Limit，比如EC2 Instance的Limit，Elastic IP的Limit等</li>
<li>个人账号和账单有关的一些问题</li>
<li>技术问题, 但技术支持需要Developer及以上Level才可用，免费的Basic Support Plan不可用</li>
</ul>
<p>登陆Console后，依次点击右上角的[Support]-&gt;[Support Center], 进入<code>Support Center</code>页面。</p>
<p>提交一个Case的要点:</p>
<ol>
<li>点击左侧[Create Case], 进入<code>Create Case</code>页面</li>
<li>Regarding中选择是<code>Account and Billing Support</code>还是<code>Service Limit Increase</code></li>
<li>填写相关的<code>Category</code>, <code>Description</code>等信息</li>
<li>AWS全球账号中<code>Support Language</code>目前可选English或者日语</li>
<li>最后选择是Web中Comment的方式回复还是Phone直接打电话沟通</li>
</ol>
<p>提交Case后，每一个Case都会列在Case History中。</p>
<p>另外，因为AWS的Suport资源会首先满足付费的Support Plan，因此Basic Support Plan的提问，经常是要过一两天甚至更久后才会有回复。</p>
<h3 id="切换Console语言"><a href="#切换Console语言" class="headerlink" title="切换Console语言"></a>切换Console语言</h3><p>AW Console是可以切换显示语言的，点击左下角<code>Feedback</code>右侧的带一个地球小图标的语言选择栏就可以切换显示语言了。</p>
<p>但注意的是，不同的Service可能会有不同的语言选项， 比如<a href="https://console.aws.amazon.com/billing/home?#/" target="_blank" rel="external">Billing页面</a>支持包括中文在内的8种语言，<a href="https://console.aws.amazon.com/iam/home" target="_blank" rel="external">IAM</a>只支持中文，英语，法语，日语这四种语言，而<a href="https://console.aws.amazon.com/support/home" target="_blank" rel="external">Support Center</a>就只支持日语和英语两种语言了。</p>
<p>不过建议还是保持默认的English，有利于尽快的熟悉各种AWS的专用名词。毕竟到时候出了问题上网找答案时，能找到的大多数有用的回答，都是English。</p>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> IAM </tag>
            
            <tag> AWS Free Tier </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[绑定万网域名到github pages]]></title>
      <url>/2017/10/31/hexo-bind-domain-to-github-pages/</url>
      <content type="html"><![CDATA[<blockquote>
<p>此处介绍如何将万网的域名绑定到github pages</p>
</blockquote>
<h3 id="万网设置"><a href="#万网设置" class="headerlink" title="万网设置"></a>万网设置</h3><p>进入aliyun控制的云解析DNS控制台，选中所要解析的域名，此处是jibing57.com, 点击右侧解析按钮。</p>
<ul>
<li>记录类型选择 CNAME</li>
<li>主机记录填写 www</li>
<li>记录值填入github pages的域名，此处是jibing57.github.io<a id="more"></a>
</li>
</ul>
<p><img src="/images/Hexo/add_dns_record_on_wanwang.jpg" alt="add_dns_record_on_wanwang"></p>
<h3 id="Hexo-设置"><a href="#Hexo-设置" class="headerlink" title="Hexo 设置"></a>Hexo 设置</h3><p>在source目录下添加CNAME文件，输入所要绑定的域名, 此处是<code>www.jibing57.com</code>, 注意不需要http。提交CNAME到git。</p>
<p>使用<code>hexo deploy</code>发布到github pages，访问<a href="http://www.jibing57.com">www.jibing57.com</a>就可以访问到github pages的内容了。</p>
]]></content>
      
        <categories>
            
            <category> Blog </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Github </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hexo引用站内文章]]></title>
      <url>/2017/10/30/how-to-use-post-link-on-hexo/</url>
      <content type="html"><![CDATA[<p>写文章的时候，经常需要引用站内的其他文章，此时可以使用Hexo内置的<a href="https://hexo.io/zh-cn/docs/tag-plugins.html" target="_blank" rel="external">标签插件</a>（Tag Plugins）中的<code>post_link</code>来实现。</p>
<a id="more"></a>
<p>用法:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;% post_link slug [title] %&#125;</div></pre></td></tr></table></figure>
<p>其中slug就是_posts文件夹下需要引用的文章的markdown文件的名字，title可以指定引用的文章需要显示的名字。</p>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>例如，<a href="/2017/07/17/hexo-github-blog/" title="使用Hexo搭建Blog">使用Hexo搭建Blog</a>这篇文章的markdown名字为hexo-github-blog.md</p>
<p>那么在需要引用的markown源文件中，输入如下标记的时候</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">- &#123;% post_link hexo-github-blog %&#125;</div><div class="line">- &#123;% post_link hexo-github-blog 显示的名字 %&#125;</div></pre></td></tr></table></figure>
<p>页面显示效果如下:</p>
<ul>
<li><a href="/2017/07/17/hexo-github-blog/" title="使用Hexo搭建Blog">使用Hexo搭建Blog</a></li>
<li><a href="/2017/07/17/hexo-github-blog/" title="显示的名字">显示的名字</a>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Blog </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Paperclip destroy的callback中attachment的各个attribute为nil的问题调查]]></title>
      <url>/2017/10/29/ruby-paperclip-destroy-callback-nil/</url>
      <content type="html"><![CDATA[<h3 id="问题点"><a href="#问题点" class="headerlink" title="问题点"></a>问题点</h3><p>Rails的Image的models中，使用了paperclip这个Gem来处理图片。近期需要添加一个功能，删除image后，需要向某个email地址发送一封邮件，告之某个图片已经被删除了。</p>
<p>实际操作中，发现在无论函数是定义在before_destroy或after_destroy的callback中，attachment_file_name,attachment_file_size, attachment_content_type, attachment_updated_at的属性，取出来都是nil。</p>
<p>调查了一下，现将结果汇总如下:<br><a id="more"></a></p>
<h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>有两个办法可以解决这个问题:</p>
<h4 id="方法一"><a href="#方法一" class="headerlink" title="方法一:"></a>方法一:</h4><p>将自己的before_destroy置于 has_attached_file 之前，这样就能够在执行Paperclip的 before_destroy之前执行你的before_destroy的方法，此时相关字段还没有设置为nil，还能访问到.</p>
<h4 id="方法二"><a href="#方法二" class="headerlink" title="方法二:"></a>方法二:</h4><p>将自己的before_destory置于 has_attached_file 之后，此时Paperclip的before_destroy会先于自己写的before_destroy之前调用，会将相关字段设置为nil。<br>此时，只能依靠activerecord的_was方法来获取修改前的值, 比如获取attachment_file_name的话，就调用attachment_file_name_was</p>
<h4 id="适用情况"><a href="#适用情况" class="headerlink" title="适用情况"></a>适用情况</h4><p>如果要before_destroy中callback中获取以上字段的值，上述两个方法都可行</p>
<p>如果是after_destroy的话，就只能依靠上述的方法二来获取attachment_file_name之类的值</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="http://www.tikalk.com/use-beforedestroy-model-paperclip/" target="_blank" rel="external">use-beforedestroy-model-paperclip</a></li>
<li><a href="https://github.com/thoughtbot/paperclip/issues/2088" target="_blank" rel="external">Github - paperclip_issue_2088</a></li>
<li><a href="https://stackoverflow.com/questions/6578302/unable-to-access-attached-file-data-in-before-destroy-while-using-paperclip" target="_blank" rel="external">Stackoverflow - unable-to-access-attached-file-data-in-before-destroy-while-using-paperclip</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Code </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ruby </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何在aws cli中使用多个配置文件]]></title>
      <url>/2017/10/24/how-to-use-aws-cli-with-multi-user/</url>
      <content type="html"><![CDATA[<p><code>aws cli</code>使用中，可能会有在多个IAM账户中进行切换的需求，手动切换<code>~/.aws/</code>目录下的<code>config</code>和<code>credentials</code>是十分费力的事情。还好<code>aws cli</code>本身就可以支持多个aws credentials</p>
<h3 id="配置多个profile"><a href="#配置多个profile" class="headerlink" title="配置多个profile"></a>配置多个profile</h3><p><code>aws configure</code>时，加上<code>--profile</code>参数来命名不同的账户, 依次输入access id, access key, region和output format。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ aws configure --profile user1</div><div class="line">$ aws configure --profile user2</div></pre></td></tr></table></figure>
<p>此时生成的<code>config</code>和<code>credentials</code>文件中，会使用账户名来分割不同的配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[carlshen@carl-macpro-lan ~]$ cat ~/.aws/config</div><div class="line">[profile user1]</div><div class="line">output = json</div><div class="line">region = us-west-2</div><div class="line">[profile user2]</div><div class="line">output = json</div><div class="line">region = ap-northeast-2</div><div class="line">[carlshen@carl-macpro-lan ~]$</div><div class="line">[carlshen@carl-macpro-lan ~]$ cat ~/.aws/credentials</div><div class="line">[user1]</div><div class="line">aws_access_key_id = AKIAXXXXXXXXXXXXXXXX</div><div class="line">aws_secret_access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</div><div class="line">[user2]</div><div class="line">aws_access_key_id = AKIAXXXXXXXXXXXXXXXX</div><div class="line">aws_secret_access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</div><div class="line">[carlshen@carl-macpro-lan ~]$</div></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="使用多个profile"><a href="#使用多个profile" class="headerlink" title="使用多个profile"></a>使用多个profile</h3><h4 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h4><p>使用的时候，在命令后面加上参数<code>--profile user_name</code>即可使用user_name对应的profile</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ aws s3 ls --profile user_name</div></pre></td></tr></table></figure>
<p>如下命令使用user2的profile来查看S3下的bucket list</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[carlshen@carl-macpro-lan ~]$ aws s3 ls --profile user2</div><div class="line">2017-10-24 11:18:38 carl-test-at-seoul</div><div class="line">[carlshen@carl-macpro-lan ~]$</div></pre></td></tr></table></figure>
<h4 id="简化"><a href="#简化" class="headerlink" title="简化"></a>简化</h4><p>每次输入<code>--profile user_name</code>是很繁琐的事情，在Mac或者Linux下，可以使用alias来简化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ alias aws_user_name=&apos;aws --profile user_name&apos;</div></pre></td></tr></table></figure>
<p>这样，每次使用的时候，直接使用<code>aws_user_name</code>来使用user_name的profile来运行aws命令</p>
<p>以下命令设置aws_user2为使用user2的profile来运行aws命名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[carlshen@carl-macpro-lan ~]$ alias aws_user2=&apos;aws --profile user2&apos;</div><div class="line">[carlshen@carl-macpro-lan ~]$ aws_user2 s3 ls</div><div class="line">2017-10-24 11:18:38 carl-test-at-seoul</div><div class="line">[carlshen@carl-macpro-lan ~]$</div></pre></td></tr></table></figure>
<p>添加到~/.bashrc中使得alias永久生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ echo &quot;alias aws_user2=&apos;aws --profile user2&apos;&quot; &gt;&gt; ~/.bashrc</div></pre></td></tr></table></figure>
<h4 id="设置默认profile"><a href="#设置默认profile" class="headerlink" title="设置默认profile"></a>设置默认profile</h4><p>如果有一个账号是使用的比较频繁的，而不想每次都使用alias的方式来运行aws，那么也可以设置环境变量<code>AWS_DEFAULT_PROFILE</code>为频繁使用的账号名，此时输入aws时候，会自动使用指定的账号配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ export AWS_DEFAULT_PROFILE=user2</div></pre></td></tr></table></figure>
<p>运行结果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">## 没有设置AWS_DEFAULT_PROFILE时</div><div class="line">[carlshen@carl-macpro-lan ~]$ echo $AWS_DEFAULT_PROFILE</div><div class="line"></div><div class="line">[carlshen@carl-macpro-lan ~]$ aws s3 ls</div><div class="line">Unable to locate credentials. You can configure credentials by running &quot;aws configure&quot;.</div><div class="line">[carlshen@carl-macpro-lan ~]$</div><div class="line"></div><div class="line">## 设置了AWS_DEFAULT_PROFILE为user2后，aws默认就会使用user2的profile</div><div class="line">[carlshen@carl-macpro-lan ~]$ export AWS_DEFAULT_PROFILE=user2</div><div class="line">[carlshen@carl-macpro-lan ~]$ aws s3 ls</div><div class="line">2017-10-24 11:18:38 carl-test-at-seoul</div><div class="line">[carlshen@carl-macpro-lan ~]$</div></pre></td></tr></table></figure>
<p>添加到~/.bashrc中来使AWS_DEFAULT_PROFILE永久生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ echo &quot;export AWS_DEFAULT_PROFILE=user2&quot; &gt;&gt; ~/.bashrc</div></pre></td></tr></table></figure>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>配置 AWS CLI - <a href="http://docs.aws.amazon.com/zh_cn/cli/latest/userguide/cli-chap-getting-started.html" target="_blank" rel="external">http://docs.aws.amazon.com/zh_cn/cli/latest/userguide/cli-chap-getting-started.html</a></li>
</ol>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AWS CLI </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用aws codecommit作为私有的git远程服务器]]></title>
      <url>/2017/10/24/how-to-use-aws-codecommit/</url>
      <content type="html"><![CDATA[<p>博客的hexo的代码，一直是保存在本地的。上次电脑花了一次屏后，感觉到保存在本地实在是不够安全。因此考虑寻觅一个远端的私密git库，存起来。</p>
<p>开始寻寻觅觅合适的仓库.</p>
<ul>
<li>Github?,私有库是收费的。但为了这些哪条就不更新的markdown，每月7刀开个Developer的Plan，感觉不划算。</li>
<li>自建Gitlab？嫌麻烦麻烦。</li>
<li>oschina的私有库？不想用。</li>
<li>。。。。。。</li>
</ul>
<p>好吧，我承认我就是想尝试用一下aws的codecommit。</p>
<p>关于CodeCommit的免费额度，<a href="https://aws.amazon.com/cn/codecommit/pricing/" target="_blank" rel="external">官网</a>介绍:</p>
<ul>
<li>最初5位活动用户<ul>
<li>无限存储库</li>
<li>50GB的月存储量</li>
<li>每月10000个git请求</li>
</ul>
</li>
</ul>
<p>托管我一个小博客，妥妥的够了。毕竟除了我，没人还会来关心这点markdown文件, 5位用户免费足够了。至于50GB的月存储量么，除非把看过的电影都commit进git来，要不然应该是足够了。</p>
<a id="more"></a>
<h3 id="创建Repository"><a href="#创建Repository" class="headerlink" title="创建Repository"></a>创建Repository</h3><ol>
<li><p>在aws console的Services中，找到CodeCommit<br><img src="/images/AWS/CodeCommit/find_codecommit.jpg" alt="find_codecommit"></p>
</li>
<li><p>在CodeCommit页面中点击Create, 打开新建Repository的页面，在<strong>Repository Name</strong>中填入仓库的名字，<strong>Description</strong>中填写仓库的描述, 然后点击<strong>Create repository</strong>创建仓库。<br><img src="/images/AWS/CodeCommit/codecommit_create_repository.jpg" alt="codecommit_create_repository"></p>
</li>
</ol>
<h3 id="配置ssh-key"><a href="#配置ssh-key" class="headerlink" title="配置ssh key"></a>配置ssh key</h3><ol>
<li><p>CodeCommit有两种访问方式，分别是ssh和https模式。我习惯使用ssh方式。</p>
</li>
<li><p>首先，需要在IAM User中添加SSH keys, 用来访问CodeCommit。</p>
<ul>
<li>打开IAM，切换到User界面，</li>
<li>在<code>Security credentials</code>的tab下，找到<code>SSH keys for AWS CodeCommit</code>一栏</li>
<li>点击下面<code>Upload SSH public key</code>按钮</li>
<li>在打开的上传key的页面中输入常用的key pair的public key, 然后点击<code>Upload SSH public Key</code>的按钮<br><img src="/images/AWS/CodeCommit/upload_ssh_for_codecommit.jpg" alt="upload_ssh_for_codecommit.jpg"></li>
</ul>
</li>
<li><p>上传完毕后，就会生成一个新的Entry，复制或保存此处SSH key ID的值。<br><img src="/images/AWS/CodeCommit/publickey_of_IAM_used_for_codecommit.jpg" alt="publickey_of_IAM_used_for_codecommit.jpg"></p>
</li>
<li><p>配置本地~/.ssh/config, 添加有关CodeCommit的Host条目, <code>IdentityFile</code>设置为private key, 并保存。如果本地还没有~/.ssh/config文件，则创建，并在保存后使用命令<code>chmod 600 ~/.ssh/config</code>将访问属性修改为600.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Host git-codecommit.*.amazonaws.com</div><div class="line">  User APKAIHTDFHHOYTHJYI3Q</div><div class="line">  IdentityFile ~/.ssh/id_rsa</div></pre></td></tr></table></figure>
</li>
<li><p>打开codeCommit的repository, 点击<code>Clone URL</code>，选择SSH来获取仓库的ssh地址, 我此处的地址是 ssh://git-codecommit.ap-northeast-2.amazonaws.com/v1/repos/my_blog<br><img src="/images/AWS/CodeCommit/get_ssh_url_of_codecommit.jpg" alt="get_ssh_url_of_codecommit"></p>
</li>
<li><p>找个临时目录，使用git clone命令来测试是否可以正常访问新建的仓库</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$ git clone ssh://git-codecommit.ap-northeast-2.amazonaws.com/v1/repos/my_blog</div><div class="line">Cloning into &apos;my_blog&apos;...</div><div class="line">The authenticity of host &apos;git-codecommit.ap-northeast-2.amazonaws.com (52.95.194.93)&apos; can&apos;t be established.</div><div class="line">RSA key fingerprint is 9f:68:48:9b:5f:fc:96:69:39:45:58:87:95:b3:69:ed.</div><div class="line">Are you sure you want to continue connecting (yes/no)? yes</div><div class="line">Warning: Permanently added &apos;git-codecommit.ap-northeast-2.amazonaws.com,52.95.194.93&apos; (RSA) to the list of known hosts.</div><div class="line">warning: You appear to have cloned an empty repository.</div><div class="line">Checking connectivity... done.</div><div class="line">$ ll</div><div class="line">total 0</div><div class="line">drwxr-xr-x  3 carlshen  staff  102 10 24 21:32 my_blog</div><div class="line">$</div></pre></td></tr></table></figure>
<h3 id="push已有的repository到CodeCommit"><a href="#push已有的repository到CodeCommit" class="headerlink" title="push已有的repository到CodeCommit"></a>push已有的repository到CodeCommit</h3><p>刚建立的repository是空的，我们可以clone下来，然后逐次添加文件，也可以将已经存在的git reposigory push到CodeCommit上的空仓库中。</p>
<p>切换到已有的git 仓库中，然后使用如下命令将git仓库push到CodeCommit中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git push https://git-codecommit.us-east-2.amazonaws.com/v1/repos/MyFirstRepo --all</div></pre></td></tr></table></figure></p>
<p>如下是将本地的my_blog推送到远端的步骤:</p>
<ol>
<li><p>本地的remote为空</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ git remote -v</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
<li><p>推送本地的所有代码到CodeCommit上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">$ git push ssh://git-codecommit.ap-northeast-2.amazonaws.com/v1/repos/my_blog --all</div><div class="line">Warning: Permanently added the RSA host key for IP address &apos;52.95.194.107&apos; to the list of known hosts.</div><div class="line">Counting objects: 490, done.</div><div class="line">Delta compression using up to 8 threads.</div><div class="line">Compressing objects: 100% (463/463), done.</div><div class="line">Writing objects: 100% (490/490), 6.71 MiB | 441.00 KiB/s, done.</div><div class="line">Total 490 (delta 214), reused 0 (delta 0)</div><div class="line">remote: processing To ssh://git-codecommit.ap-northeast-2.amazonaws.com/v1/repos/my_blog</div><div class="line"> * [new branch]      master -&gt; master</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
<li><p>将CodeCommit上的仓库设置为远端的origin</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ git remote add origin ssh://git-codecommit.ap-northeast-2.amazonaws.com/v1/repos/my_blog</div><div class="line">$ git remote -v</div><div class="line">origin	ssh://git-codecommit.ap-northeast-2.amazonaws.com/v1/repos/my_blog (fetch)</div><div class="line">origin	ssh://git-codecommit.ap-northeast-2.amazonaws.com/v1/repos/my_blog (push)</div></pre></td></tr></table></figure>
</li>
<li><p>关联本地和远端的master分支</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ git branch --set-upstream-to=origin/master master</div><div class="line">Branch master set up to track remote branch master from origin.</div><div class="line">$ git pull</div><div class="line">Already up-to-date.</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>将 Git 存储库迁移到 AWS CodeCommit - <a href="https://docs.aws.amazon.com/zh_cn/codecommit/latest/userguide/how-to-migrate-repository-existing.html" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/codecommit/latest/userguide/how-to-migrate-repository-existing.html</a></li>
<li>在 Linux, macOS, or Unix 上设置到 AWS CodeCommit 存储库的 SSH 连接的步骤 - <a href="https://docs.aws.amazon.com/zh_cn/codecommit/latest/userguide/setting-up-ssh-unixes.html#setting-up-ssh-unixes-keys-unixes" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/codecommit/latest/userguide/setting-up-ssh-unixes.html#setting-up-ssh-unixes-keys-unixes</a></li>
</ol>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Git </tag>
            
            <tag> CodeCommit </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Linux 和 Mac下date命令的基本用法]]></title>
      <url>/2017/08/03/date-command-on-Linux-and-Mac/</url>
      <content type="html"><![CDATA[<p>Mac下的date命令是BDS系的, Linux下date命令是GNU系的，两者的用法有一些区别，罗列如下:</p>
<h3 id="共同点"><a href="#共同点" class="headerlink" title="共同点"></a>共同点</h3><p>基本的时间格式的缩写是相同的，规则如下:</p>
<ul>
<li>%Y 表示四位数形式的年份, 比如2017</li>
<li>%m 表示带前导0的月份，比如02,12</li>
<li>%d 表示带前导0的日子， 比如 02，28</li>
<li>%H 表示带前导0的24小时， 比如 01, 23</li>
<li>%M 表示带前导0的分钟数， 比如 05, 22</li>
<li>%S 表示带前导0的秒数， 比如 06，45</li>
<li>%s 表示距离格林威治时间(1970年1月1日0点)的秒数</li>
</ul>
<a id="more"></a>
<p>运行结果:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># Macos</div><div class="line">$ date</div><div class="line">2017年 8月 4日 星期五 14时31分00秒 CST</div><div class="line">$ date +%Y%m%d%H%M%S</div><div class="line">20170804143107</div><div class="line">$ date +%Y%m%d%H%M%S</div><div class="line">20170804143112</div><div class="line">$</div><div class="line"></div><div class="line"># Linux(Centos)</div><div class="line">$ date</div><div class="line">2017年 08月 04日 星期五 06:31:33 UTC</div><div class="line">$ date +%Y%m%d%H%M%S</div><div class="line">20170804063135</div><div class="line">$ date +%s</div><div class="line">1501828299</div><div class="line">$</div></pre></td></tr></table></figure>
<h3 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h3><p>获取前一天和获取后一天的写法不同</p>
<p>Macos 下通过使用<code>-v</code>参数时间，<code>-v-1d</code>代表前一天， <code>-v-1y</code>代表上一年<br>Linux 下通过<code>--date</code>参数实现, <code>--date=&#39;-1 day&#39;</code>代表前一天， <code>--date=&#39;-1 year&#39;</code>代表上一年</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># Macos</div><div class="line"></div><div class="line">$ date -v-1d -v-1y +%Y-%m-%d</div><div class="line">2016-08-03</div><div class="line">$</div><div class="line"></div><div class="line"># Linux</div><div class="line"></div><div class="line">$ date +%Y-%m-%d --date=&apos;-1 day -1 year&apos;</div><div class="line">2016-08-03</div><div class="line">$</div></pre></td></tr></table></figure>
<h3 id="检查平台来决定如何使用date"><a href="#检查平台来决定如何使用date" class="headerlink" title="检查平台来决定如何使用date"></a>检查平台来决定如何使用date</h3><p>可以使用<code>uname -s</code>的输出来判定是哪个平台，Linux下命令输出是<code>Linux</code>, Macos下命令输出是<code>Darwin</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line"></div><div class="line">os=$(uname -s)</div><div class="line">if [[ &quot;$os&quot; == &quot;Linux&quot; ]]; then</div><div class="line">    date +%Y-%m-%d --date=&apos;-1 day -1 year&apos;</div><div class="line">elif [[ &quot;$os&quot; == &quot;Darwin&quot; ]]; then</div><div class="line">    date -v-1d -v-1y +%Y-%m-%d</div><div class="line">else</div><div class="line">    echo &quot;unknown OS&quot;</div><div class="line">    exit 1</div><div class="line">fi</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Shell </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> MacOS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Macos中如何语音朗读文字]]></title>
      <url>/2017/07/25/how-to-read-text-on-macos/</url>
      <content type="html"><![CDATA[<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>Macos上看到大段大段的英文，有时候除了看以外，还想边听边看</p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="Macos-自带speech"><a href="#Macos-自带speech" class="headerlink" title="Macos 自带speech"></a>Macos 自带speech</h3><p>Macos 自带了文本至语音的功能, 开启方法如下:</p>
<ol>
<li>打开[系统偏好设置] -&gt; [听写与语音] -&gt; 切换至[文本至语音]</li>
<li>可以选择系统嗓音和朗读速率</li>
<li>可以设置快捷键，默认为Option + Esc, 选择文字后按快捷键开启，再次按快捷键关闭</li>
</ol>
<p>或选中文字，右键菜单选择[语音]-&gt;[开始讲话]</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://support.apple.com/kb/PH14230?locale=en_US" target="_blank" rel="external">OS X Mavericks: Hear your Mac speak text</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> MacOS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS相关的一些有用的网址]]></title>
      <url>/2017/07/25/useful-website-of-aws/</url>
      <content type="html"><![CDATA[<h3 id="cloudping-info"><a href="#cloudping-info" class="headerlink" title="cloudping.info"></a>cloudping.info</h3><p><a href="http://www.cloudping.info/" target="_blank" rel="external">clougping.info</a>是一个可以检测当前浏览器到AWS各个Region的延迟的网站, 可以用来评估访问哪个Region更快一点。在建立测试服务的时候十分有用。</p>
<p>单次的测试结果不一定准确，建议多试几次后再选取平均延迟低的结果。</p>
<ul>
<li>国内测试的结果:<br><img src="/images/AWS/Tools/cloudping_on_cn.jpg" alt="cloudping_on_cn"></li>
</ul>
<p>在国内使用AWS全球账号时，在韩国首尔Region建测试服务延迟会小一点。</p>
<a id="more"></a>
<h3 id="open-guides-og-aws"><a href="#open-guides-og-aws" class="headerlink" title="open-guides og-aws"></a>open-guides og-aws</h3><p>AWS拥有非常多的Service，每个Service都有着厚厚的UserGuide, <a href="https://github.com/open-guides/og-aws" target="_blank" rel="external">og-aws</a> 所做的，就是从AWS文档中提炼出各个Service的一些要点，再结合实际使用中各个工程师的经验, 汇总成了一份Guide。通读一遍就可以对整个AWS的服务有个整体的了解，可以结合实际操作经验隔段时间就来读一遍。</p>
<p>内容包含如下:</p>
<ul>
<li>AWS以及其拥有的所有Service的简介</li>
<li>和其他云服务的比较</li>
<li>各个AWS Service的Basics, Tips, 还有Gotchas and Limitations</li>
</ul>
<h3 id="AWS-词汇表"><a href="#AWS-词汇表" class="headerlink" title="AWS 词汇表"></a>AWS 词汇表</h3><ul>
<li>中文版 <a href="https://docs.aws.amazon.com/zh_cn/general/latest/gr/glos-chap.html" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/general/latest/gr/glos-chap.html</a></li>
<li>英文版 <a href="https://docs.aws.amazon.com/general/latest/gr/glos-chap.html" target="_blank" rel="external">https://docs.aws.amazon.com/general/latest/gr/glos-chap.html</a></li>
</ul>
<h2 id="还没调查的"><a href="#还没调查的" class="headerlink" title="还没调查的"></a>还没调查的</h2><p><a href="https://gist.github.com/leonardofed/bbf6459ad154ad5215d354f3825435dc" target="_blank" rel="external">https://gist.github.com/leonardofed/bbf6459ad154ad5215d354f3825435dc</a></p>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AWS Tools </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS Certified Developer - Associate Road Map]]></title>
      <url>/2017/07/24/AWS-Certified-Developer/</url>
      <content type="html"><![CDATA[<h2 id="Official-AWS-Centification-Page"><a href="#Official-AWS-Centification-Page" class="headerlink" title="Official AWS Centification Page"></a>Official AWS Centification Page</h2><p>访问官网<a href="https://amazonaws-china.com/certification/certification-prep/" target="_blank" rel="external">AWS Centification</a></p>
<ul>
<li>参加 AWS 培训课程</li>
<li>查看考试指南和样题<ul>
<li>了解考试涉及的概念并整体了解需要学习哪些内容, <a href="http://awstrainingandcertification.s3.amazonaws.com/production/AWS_certified_developer_associate_blueprint.pdf" target="_blank" rel="external">AWS Certified Developer – Associate 考试指南</a> 相当于考试大纲, 必看,而且需要反复的看。因为学习过一阵后再来看Guide，会有更深的体会。</li>
<li><a href="https://d0.awsstatic-china.com/training-and-certification/docs/AWS_certified_developer_associate_examsample.pdf" target="_blank" rel="external">考试样题</a>用于熟悉题目题型</li>
</ul>
</li>
<li>完成自主进度动手实验和备考任务<ul>
<li>官方<a href="https://www.qwiklabs.com/learning_paths/20/lab_catalogue?locale=en" target="_blank" rel="external">qwikLABS 任务</a>提供了一系列动手实验, 提供部分免费实验，但大部分实验所需的积分都需要购买。高性价比的做法是， 注册一个AWS全球账号，使用一年的免费额度来对照着实验手册来进行试验。</li>
</ul>
</li>
<li>学习 AWS 白皮书<ul>
<li>白皮书是纯英文的，而且每个白皮书篇幅都很长，读起来既费时又枯燥。但是有时间还是建议把推荐的几个都看一下。</li>
</ul>
</li>
<li>查看 AWS 常见问题<ul>
<li>官网推荐的FAQ都建议看完，另外<a href="https://amazonaws-china.com/cn/dynamodb/faqs/" target="_blank" rel="external">DynamoDB FAQ</a>这个必须要看。</li>
</ul>
</li>
<li>参加模拟考试<ul>
<li>20美刀一次，主要目的是为了让人熟悉考试时上机的流程。是否需要因人而异, 特别想先熟悉下考试流程的可以考虑参加一次。我个人觉得没有必要, 因为真实考试时，操作界面一目了然，没有磕磕绊绊的机关，省下20美刀可以去买一份课程。</li>
</ul>
</li>
<li>报名考试并获得认证<ul>
<li>登陆<a href="https://www.aws.training/certification" target="_blank" rel="external">https://www.aws.training/certification</a>注册进行考试</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="考试指南"><a href="#考试指南" class="headerlink" title="考试指南"></a>考试指南</h2><p><a href="http://awstrainingandcertification.s3.amazonaws.com/production/AWS_certified_developer_associate_blueprint.pdf" target="_blank" rel="external">AWS Certified Developer – Associate 考试指南</a> 读三遍，读三遍，读三遍</p>
<p>各个Domain的分数比例如下:<br><img src="/images/AWS/Developer/Domain_on_developer_associate_blueprint.jpg" alt="Domain_on_developer_associate_blueprint"></p>
<p>Domain 3 Deployment and Security 中3.2 中最后一句[CIA and AAA models, ingress vs. egress filtering, and which AWS services and features fit]中的CIA和AAA的解释如下:</p>
<p>CIA are the fundamentals of Information Security</p>
<ul>
<li>Confidentiality 机密性 (generally encryption)</li>
<li>Integrity 完整性 (the accuracy of a message or server…i.e. hash value)</li>
<li>Availability 可用性 (availability of a service)</li>
</ul>
<p>AAA</p>
<ul>
<li>Authentication</li>
<li>Authorization</li>
<li>Accounting</li>
</ul>
<p>参考自: <a href="https://acloud.guru/forums/aws-certified-developer-associate/discussion/-KTdRPtz4PF2rLHO1_tD/what-is-cia-and-aaa-models-ingress-vs-egress-filtering-and-which-aws-services-an" target="_blank" rel="external">Acloudguru discussion</a></p>
<h2 id="视频学习"><a href="#视频学习" class="headerlink" title="视频学习"></a>视频学习</h2><p><a href="https://acloud.guru" target="_blank" rel="external">Acloudguru</a> 中<a href="https://acloud.guru/course/aws-certified-developer-associate/dashboard" target="_blank" rel="external">aws-centified-developer-associate</a>视频的学习，大体内容和Centified Solutions Architect-Associate的大同小异，多了DynamoDB的部分</p>
<h3 id="要点摘录"><a href="#要点摘录" class="headerlink" title="要点摘录"></a>要点摘录</h3><h4 id="IAM"><a href="#IAM" class="headerlink" title="IAM"></a>IAM</h4><ul>
<li><p>IAM give</p>
<ul>
<li>Centralised control of your AWS account</li>
<li>Shared Access to your AWS account</li>
<li>Granular Permissions</li>
<li>Identity Federation (including Active Directory, Facebook, Linkedin etc)</li>
<li>Multifactor Authentication</li>
<li>Provide temporary access for users/devices and services where necessary</li>
<li>Allows you to set up your own password rotation policy</li>
<li>Integrates with many different AWS services</li>
<li>Supports PCI DSS Compliance</li>
</ul>
</li>
<li><p>IAM consists of the following:</p>
<ul>
<li>Users - End Users (think people)</li>
<li>Groups (A collection of users under one set of permissions. A way to group our users and apply polices to them collectively)</li>
<li>Roles - You create roles and can then assign them to AWS resources</li>
<li>Policy Documents - A document that defines one (or more permissions) - <a href="https://awspolicygen.s3.amazonaws.com/policygen.html" target="_blank" rel="external">IAM Online Policy Generator</a><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</div><div class="line">    &quot;Statement&quot;: [</div><div class="line">        &#123;</div><div class="line">            &quot;Effect&quot;: &quot;Allow&quot;,</div><div class="line">            &quot;Action&quot;: &quot;*&quot;,</div><div class="line">            &quot;Resource&quot;: &quot;*&quot;</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>IAM is universal, It does not apply to regions at this time.</p>
</li>
<li>The “root account” is simply the account created when first setup your AWS account. It has complete Admin access.</li>
<li>New Users have NO permissions when first created.</li>
<li>New Users are assigned <strong>Access Key ID &amp; Secret Access Keys</strong> when first created.</li>
<li>There are not the same as a password, and you cannot use the <strong>Access key ID &amp; Secret Access Key</strong> to Login in to the console. You can use this to access AWS via the APIs and Command Line however.</li>
<li>You only get to view these once. If you lose them, you have to regenerate them. So save them in a secure location.</li>
<li>Always setup Multifactor Authentication on your root account.</li>
<li>You can create and customise your own password rotation policies.</li>
<li>Determining whether a request is allowed or denied – <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html" target="_blank" rel="external">http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a><ul>
<li>Start with default deny, and evaluate all applicable policies</li>
<li>If there is explicit deny, result is deny</li>
<li>If there is explicit allow, result is allow<br><img src="/images/AWS/Developer/reference_policies_evaluation_logic.jpg" alt="reference policies evaluation logic"></li>
</ul>
</li>
</ul>
<h4 id="STS"><a href="#STS" class="headerlink" title="STS"></a>STS</h4><ul>
<li>In The Exam<ul>
<li>Develop and Identity Broker to communicate with LDAP and AWS STS</li>
<li>Identity Broker always authenticates with LDAP first, THEN with AWS STS</li>
<li>Application then gets temporary access to AWS resource</li>
</ul>
</li>
</ul>
<h4 id="EC2"><a href="#EC2" class="headerlink" title="EC2"></a>EC2</h4><ul>
<li><p>know the differences between:</p>
<ul>
<li>On Demand - allow you to pay a fixed rate by the hour with no commitment</li>
<li>Spot - enable you to bi whatever price you want for instance capacity, providing for even greater savings if your applications have flexible start and end times.<ul>
<li>If you terminate the instance, you pay for the hour</li>
<li>If AWS terminates the spot instance, you get the hour it was terminated in for free.</li>
</ul>
</li>
<li>Reserved - provide you with a capacity reservation, and offer a significant discount on the hourly charge for an instance. 1 Year or 3 Year Terms</li>
<li>Dedicated Hosts - Physical EC2 server dedicated for your use. Dedicated Hosts can help you reduce costs by allowing you to use your existing server-bound software licenses.</li>
</ul>
</li>
<li><p>EC2 Instance Types</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Family</th>
<th style="text-align:center">Speciality</th>
<th style="text-align:right">Use case</th>
</tr>
</thead>
<tbody>
<tr>
<td>D2</td>
<td style="text-align:center">Dense Storage</td>
<td style="text-align:right">Fileservers/Data Wareshousing/Hadoop</td>
</tr>
<tr>
<td>R4</td>
<td style="text-align:center">Memory Optimized</td>
<td style="text-align:right">Memory Intensive Apps/DBs</td>
</tr>
<tr>
<td>M4</td>
<td style="text-align:center">General Purpose</td>
<td style="text-align:right">Application Servers</td>
</tr>
<tr>
<td>C4</td>
<td style="text-align:center">Compute Optimized</td>
<td style="text-align:right">CPU Intensive Apps/DBs</td>
</tr>
<tr>
<td>G2</td>
<td style="text-align:center">Graphics Intensive</td>
<td style="text-align:right">Video Encoding/3D Application Streaming</td>
</tr>
<tr>
<td>I2</td>
<td style="text-align:center">High Speed Storage</td>
<td style="text-align:right">NoSQL DBs, Data Warehousing etc</td>
</tr>
<tr>
<td>F1</td>
<td style="text-align:center">Field Programmable Gate Array</td>
<td style="text-align:right">Hardware acceleration for your code</td>
</tr>
<tr>
<td>T2</td>
<td style="text-align:center">Lowest Cost, General Purpose</td>
<td style="text-align:right">Web Servers/Small DBs</td>
</tr>
<tr>
<td>P2</td>
<td style="text-align:center">Graphics/General Purpose GPU</td>
<td style="text-align:right">Machine Learning, Bit Coin Mining etc</td>
</tr>
<tr>
<td>X1</td>
<td style="text-align:center">Memory Optimize</td>
<td style="text-align:right">SAP HANA/Apache Spark etc</td>
</tr>
</tbody>
</table>
<ul>
<li><p>How to remember Instance type</p>
<ul>
<li>D for Density</li>
<li>R for RAM</li>
<li>M - main choice for general purpose apps</li>
<li>C for Compute</li>
<li>G - Graphics</li>
<li>I for IOPS</li>
<li>F for FPGA</li>
<li>T cheap general purpose (think T2 micro)</li>
<li>P - Graphics (think Pics)</li>
<li>X - Extreme Memory</li>
<li><strong>DR Mc GIFT PX</strong></li>
</ul>
</li>
<li><p>EBS Consists of:</p>
<ul>
<li>SSD, General Purpose - GP2 - (Up to 10,000 IOPS)<ul>
<li>General purpose, balances both price and performance.</li>
<li>Ratio of 3 IOPS per GB with up to 10000 IOPS and the ability to burst up to 3000 IOPS for extended periods of time for volumes under 1Gib.</li>
</ul>
</li>
<li>SSD, Provisioned IOPS - IO1 - (More than 10,000 IOPS)<ul>
<li>Designed for I/O intensive applications such as large relational or NoSQL databases.</li>
<li>Use if you need more than 10000 IOPS</li>
<li>Can provision up to 20000 IOPS per volume.</li>
</ul>
</li>
<li>HDD, Throughput Optimized - ST1 - frequently accessed workloads<ul>
<li>Big data</li>
<li>Data warehouse</li>
<li>Log processing</li>
<li>Cannot be a boot volume</li>
</ul>
</li>
<li>HDD, Cold - SC1 - less frequently accessed data.<ul>
<li>Lowest Cost Storage for infrequently accessed workloads</li>
<li>File Server</li>
<li>Cannot be a boot volume</li>
</ul>
</li>
<li>HDD, Magnetic - Standard - cheap, infrequently accessed storage<ul>
<li>Lowest cost per gigabyte of all EBS volume types that is <strong>bootable</strong>. Magnetic volumes are ideal for workloads where data is accessed infrequently, and applications where the lowest storage cost is important.</li>
</ul>
</li>
<li>You cannot mount 1 EBS volume to multiple EC2 instances, instead use EFS.</li>
</ul>
</li>
<li><p>EC2 Lab Exam Tips</p>
<ul>
<li>Termination Protection is turned off by default, you must turn it on.</li>
<li>On an EBS-backed instance, the default action is for the root EBS volume to be deleted when the instance is terminated.</li>
<li>EBS Root Volumes of your DEFAULT AMI’s cannot be encrypted. you need a third party tool(such as bit locker etc) to encrypt the root volume or this can be done when creating AMI’s in the AWS console or using the API.</li>
<li>Additional volumes can be encrypted.</li>
</ul>
</li>
<li><p>Upgrading EBS Volume Types</p>
<ul>
<li>EBS Volumes can be changed on the fly (except for magnetic standard)</li>
<li>Best practice to stop the EC2 instance and then change the volume</li>
<li>You can change volume types by taking a snapshot and then using the snapshot to create a new volume</li>
<li>If you change a volume on the fly you must wait for 6 hours before making another change</li>
<li>You can scale EBS Volumes up only</li>
<li>Volumes must be in the same AZ as the EC2 instances</li>
</ul>
</li>
<li><p>Security Group</p>
<ul>
<li>All Inbound Traffic is Blocked By Default</li>
<li>All Outbound Traffic is Allowed</li>
<li>Changes to Security Groups take effect immediately</li>
<li>You can have any number of EC2 instances within a security group.</li>
<li>You can have multiple security groups attached to EC2 Instances</li>
<li>Security Groups are <strong>STATEFUL</strong><ul>
<li>If you create an inbound rule allowing traffic in, that traffic is automatically allowed back out again.</li>
</ul>
</li>
<li>You cannot block specific IP addresses using Security Groups, instead use Network Access Control Lists.</li>
<li>You can specify allow rules, but not deny rules.</li>
</ul>
</li>
<li><p>Volumes vs Snapshots</p>
<ul>
<li>Volumes exist on EBS<ul>
<li>Virtual Hard Disk</li>
</ul>
</li>
<li>Snapshots exist on S3</li>
<li>You can take a snapshot of a volume, this will store that volume on S3.</li>
<li>Snapshots are point in time copies of Volumes.</li>
<li>Snapshots are incremental, this means that only the blocks that have changed since your last snapshot are moved to S3.</li>
<li>If this is your first snapshot, it may take some time to create.</li>
</ul>
</li>
<li><p>Volumes vs Snapshots - Security</p>
<ul>
<li>Snapshots of encrypted volumes are encrypted automatically.</li>
<li>Volumes restored from encrypted snapshots are encrypted automatically.</li>
<li>You can share snapshots, but only if they are unencrypted.<ul>
<li>These snapshots can be shared with other AWS accounts of made public.</li>
</ul>
</li>
</ul>
</li>
<li><p>Snapshots of Root Device Volumes</p>
<ul>
<li>To Create a snapshot for Amazon EBS volumes that serve as root devices, you should stop the instance before taking the snapshot.</li>
</ul>
</li>
<li><p>EBS vs Instance Store</p>
<ul>
<li>Instance Store Volumes are sometimes called Ephemeral Storage.</li>
<li>Instance store volumes cannot be stopped. If the underlying host fails, you will lose your data.</li>
<li>EBS backed instances can be stopped. You will not lose the data on this instance if it is stopped.</li>
<li>You can reboot both, you will not lose your data.</li>
<li>By default, both ROOT volumes will be deleted on termination, however with EBS volumes, you can tell AWS to keep the root device volume.</li>
</ul>
</li>
<li><p>How can I take a Snapshot of a RAID Array?</p>
<ul>
<li>Problem - Take a snapshot, the snapshot excludes data held in the cache by applications and the OS. This tends not to matter on a single volume, however using multiple volumes in a RAID array, this can be a problem due to interdependencies of the array.</li>
<li>Solution - Take an application consistent snapshot.<ul>
<li>Stop the application from writing to disk.</li>
<li>Flush all caches to the disk.</li>
<li>How can we do this?<ul>
<li>Freeze the file system</li>
<li>Unmount the RAID Array</li>
<li>Shutting down the associated EC2 instance.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Amazon Machine Images</p>
<ul>
<li>AMI’s are regional. You can only launch an AMI from the region in which it is stored. However you can copy AMI’s to other regions using the console, command line or the Amazon EC2 API.</li>
</ul>
</li>
<li><p>CloudWatch and CloudTrail</p>
<ul>
<li>Standard Monitoring = 5 Minutes</li>
<li>Detailed Monitoring = 1 Minute</li>
<li>CloudWatch is for performance monitoring</li>
<li>CloudTrail is for auditing</li>
<li>CloudWatch<ul>
<li>Dashboards - Creates awesome dashboards to see what is happening with your AWS environment.</li>
<li>Alarms - Allows you to set Alarms that notify you when particular thresholds are hit.</li>
<li>Events - CloudWatch Events helps you to respond to state changes in your AWS resources.</li>
<li>Logs - CloudWatch Logs helps you to aggregate, monitor, and store logs.</li>
</ul>
</li>
</ul>
</li>
<li><p>EC2 Role</p>
<ul>
<li>Roles are more secure than storing your access key and secret access key on individual EC2 instances.</li>
<li>Roles are easier to manage</li>
<li>Roles can only be assigned when that EC2 instance is being provisioned.</li>
<li>Roles are universal, you can use them in any region.</li>
</ul>
</li>
<li><p>Instance Meta-data</p>
<ul>
<li>Used to get information about an instance(such as public ip)</li>
<li>curl <a href="http://169.254.169.254/latest/meta-data/" target="_blank" rel="external">http://169.254.169.254/latest/meta-data/</a></li>
<li>No such thing as user-data for an instance</li>
</ul>
</li>
<li><p>EFS Features</p>
<ul>
<li>Supports the Network File System version 4(NFSv4) protocol</li>
<li>You only pay for the storage you use (no pre-provisioning required)</li>
<li>Can scale up to the petabytes</li>
<li>Can support thousands of concurrent NFS connections</li>
<li>Data is stored across multiple AZ’s within a region</li>
<li>Read After Write Consistency</li>
<li><strong>Great use cases for a file server</strong>. You can apply both file level and directory level permissions within EFS.</li>
</ul>
</li>
<li><p>EC2 CLI Command</p>
<ul>
<li>aws ec2 describe-instances</li>
<li>aws ec2 describe-images</li>
<li>aws ec2 run-instances</li>
<li>Do not confuse <strong>start-instances</strong> with <strong>run-instances</strong><ul>
<li><strong>start-instances</strong> starts an stopped instance</li>
<li><strong>run-instances</strong> is used to create a new instance</li>
</ul>
</li>
</ul>
</li>
<li><p>Lambda</p>
<ul>
<li>AWS Lambda is a compute service where you can upload your code and create a Lambda function. AWS Lambda takes care of provisioning and managing the servers that you use to run the code. You don’t have to worry about operating systems, patching, scaling, etc. You can use Lambda in the following ways.<ul>
<li>As an event-driven compute service where AWS Lambda runs your code in response to events. These events could be changes to data in an Amazon S3 bucket or an Amazon DynamoDB table.</li>
<li>As a compute service to run your code in response to HTTP requests using Amazon API Gateway or API calls made using AWS SDKs.</li>
</ul>
</li>
<li>Language supported<ul>
<li>Node.js</li>
<li>Java</li>
<li>Python</li>
<li>C#</li>
</ul>
</li>
<li>Lambda Priced<ul>
<li>Number of requests<ul>
<li>First 1 million requests are free. $0.20 per 1 million requests thereafter.</li>
</ul>
</li>
<li>Duration<ul>
<li>Duration is calculated from the time you code begins executing until it returns or otherwise terminates, rounded up to the nearest 100ms. The price depends on the amount of memory you allocate to your function. You are charged $0.00001667 for every GB-second used.</li>
</ul>
</li>
</ul>
</li>
<li>Why is Lambda Cool<ul>
<li>No Servers!</li>
<li>Continuous Scaling</li>
<li>Super super super cheap!</li>
</ul>
</li>
<li>The default timeout of Lambda Function is 3 second, maximum time is 300 second (5 mins), minimum time is 1 second.</li>
<li>Lambda code(and any dependent libraries) as a Zip and upload to console, Uploads must be no larger than 50MB(compressed).</li>
<li>Compute resource : You can set your memory in 64MB increments from 128MB to 1.5GB</li>
</ul>
</li>
<li><p>Elastic Load Balancers</p>
<ul>
<li>Instances monitored by ELB are reported as:<ul>
<li>InService</li>
<li>OutOfService</li>
</ul>
</li>
<li>Health Checks check the instance health by talking to it</li>
<li>Have their own DNS name. You are never given an IP address.</li>
<li>Classic Load Balancer FAQ</li>
</ul>
</li>
<li><p>SDK Tips</p>
<ul>
<li>Available SDK<ul>
<li><a href="https://aws.amazon.com/tools/" target="_blank" rel="external">https://aws.amazon.com/tools/</a></li>
<li>Android, iOS, JavaScript(Browser)</li>
<li>Java</li>
<li>.Net</li>
<li>Node.js</li>
<li>PHP</li>
<li>Python</li>
<li>Ruby</li>
<li>Go</li>
<li>C++</li>
</ul>
</li>
<li>Default Region - US-EAST-1<ul>
<li>Some have default regions(Java)</li>
<li>Some do not (Node.js)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="S3"><a href="#S3" class="headerlink" title="S3"></a>S3</h4><ul>
<li><p>Exam Tips</p>
<ul>
<li>Remember that S3 is Object based i.e. allows you to upload files.</li>
<li>File can be from 0 Bytes to 5TB.</li>
<li>There is unlimited storage.</li>
<li>Files are stored in Buckets.</li>
<li>S3 is a universal namespace, that is, names must be unique globally.</li>
<li><a href="https://s3-eu-west-1.amazonaws.com/acloudguru" target="_blank" rel="external">https://s3-eu-west-1.amazonaws.com/acloudguru</a></li>
<li>Read after Write consistency for PUTS of new Objects</li>
<li>Eventual Consistency for overwrite PUTS and DELETS (can take some time to propagate)</li>
<li>Object based storage only ( for files)</li>
<li><strong>Not suitable to install an operating system on.</strong></li>
<li>You can insert a presigned url into a webpage to download private data directly from S3.</li>
</ul>
</li>
<li><p>S3 object consist of:</p>
<ul>
<li>Key (This is simply the name of the object)</li>
<li>Value (This is simply the data and is made up of a sequence of bytes)</li>
<li>Version ID (Important for versioning)</li>
<li>Metadata (Data about the data you are storing)</li>
<li>Subresources<ul>
<li>Access control lists</li>
<li>Torrent</li>
</ul>
</li>
</ul>
</li>
<li><p>S3 The Basics</p>
<ul>
<li>Built for 99.99 availability for the S3 platform.</li>
<li>Amazon Guarantee 99.9% availability</li>
<li>Amazon Guarantees 99.999999999% durability for S3 information. ( Remember 11x9’s)</li>
<li>Tiered Storage Available</li>
<li>Lifecycle Management</li>
<li>Versioning</li>
<li>Encryption</li>
<li>Secure your data using Access Control Lists and Bucket Policies</li>
</ul>
</li>
<li><p>S3 Storage Classes/Tiers</p>
<ul>
<li>S3 (durable, immediately available, frequently accessed)<ul>
<li>99.99% availability, 99.999999999% durability, stored redundantly across multiple devices in multiple facilities and is designed to sustain the loss of 2 facilities concurrently.</li>
</ul>
</li>
<li>S3 - IA (durable, immediately available, infrequently accessed)<ul>
<li>For data that is accessed less frequently, but requires rapid access when needed. Lower fee than S3, but you are charged a retrieval fee.</li>
</ul>
</li>
<li>S3 - Reduced Redundancy Storage (data that is easily reproducible, such as thumb nails etc).<ul>
<li>Designed to provide 99.99% durability and 99.99% availability of objects over a given year.</li>
</ul>
</li>
<li>Glacier - Archived data, where you can wait 3 -5 hours before accessing.</li>
<li>S3 Storage Tier<br><img src="/images/AWS/Developer/s3_storage_tier.jpg" alt="s3_storage_tier"></li>
<li>S3 vs Glacier<br><img src="/images/AWS/Developer/s3_vs_glacier.jpg" alt="s3_vs_glacier"></li>
</ul>
</li>
<li><p>S3 Charges</p>
<ul>
<li>Storage</li>
<li>Requests</li>
<li>Storage Management Pricing</li>
<li>Data Transfer Pricing</li>
<li>Transfer Acceleration</li>
</ul>
</li>
<li><p>S3 - Versioning</p>
<ul>
<li>Stores all versions of an object (including all writes and even if you delete an object)</li>
<li>Great backup tool.</li>
<li>Once enabled, Versioning cannot be disabled, only suspended.</li>
<li>Integrates with Lifecycle rules.</li>
<li>Versioning’s MFA Delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security.</li>
<li>Cross Region Replication, requires versioning enabled on the source bucket.</li>
<li>Any objects uploaded prior to versioning will have the version ID as NULL</li>
</ul>
</li>
<li><p>Cross Region Replication</p>
<ul>
<li>Versioning must be enabled on both the source and destination buckets.</li>
<li>Regions must be unique.</li>
<li>Files in an existing bucket are not replicated automatically. All subsequent updated files will be replicated automatically.</li>
<li>You cannot replicate to multiple buckets of use daisy chaining (at this time).</li>
<li>When delete object, the delete markers are replicated.</li>
<li>Deleting individual versions or delete markers will not be replicated.</li>
</ul>
</li>
<li><p>Lifecycle Management</p>
<ul>
<li>Can be used in conjunction with versioning.</li>
<li>Can be applied to current versions and previous versions.</li>
<li>Following actions can now be done:<ul>
<li>Transition to the Standard - Infrequent Access Storage Class(128Kb and 30 days after the creation date).</li>
<li>Archive to the Glacier Storage Class (30 days after IA, if relevant)</li>
<li>Permanently Delete</li>
</ul>
</li>
</ul>
</li>
<li><p>CloudFront</p>
<ul>
<li>Edge Location - This is the location where content will be cached. This is separate to an AWS Region/AZ</li>
<li>Origin - This is the origin of all the files that the CDN will distribute. This can be either an S3 Bucket, an EC2 Instance, an Elastic Load Balancer or Route53.</li>
<li>Distribution - This is the name given the CDN which consists of a collection of Edge Locations.<ul>
<li>Web Distribution - Typically used for Websites.</li>
<li>RTMP - Used for Media Streaming.</li>
</ul>
</li>
<li>Edge locations are not just READ only, you can write to them too. (ie put an object on to them)</li>
<li>Objects are cached for the life of the TTL (Time To Live)</li>
<li>You can clear cached objects, but you will be charged.</li>
<li>Restrict viewer access by signed URL or Signed Cookies</li>
<li>Restrict content based on geo location(whitelist and blacklist)</li>
</ul>
</li>
<li><p>Securing your buckets</p>
<ul>
<li>By default, all newly created buckets are PRIVATE</li>
<li>You can setup access control to your buckets using:<ul>
<li>Bucket Policies</li>
<li>Access Control Lists</li>
</ul>
</li>
<li>S3 buckets can be configured to create access logs which log all requests made to the S3 bucket. This can be done to another bucket.</li>
</ul>
</li>
<li><p>Encryption</p>
<ul>
<li>In Transit:<ul>
<li>SSL/TLS</li>
</ul>
</li>
<li>At Rest<ul>
<li>Server Side Encryption<ul>
<li>S3 Managed Keys - SSE-S3</li>
<li>AWS Key Management Service, Managed Keys - SSE-KMS</li>
<li>Server Side Encryption With Customer Provided Keys - SSE-C</li>
</ul>
</li>
<li>Client Side Encryption</li>
</ul>
</li>
</ul>
</li>
<li><p>Storage Gateway</p>
<ul>
<li>File Gateway - For flat files, stored directly on S3.<ul>
<li>NFS</li>
<li>Unlimited amount of storage. However maximal file size is 5TB.</li>
</ul>
</li>
<li>Volume Gateway<ul>
<li>Stored Volumes - Entire Dataset is stored on site and is asynchronously backed up to S3.<ul>
<li>iSCSI based block storage</li>
<li>Each Volume can store up to 16TB in Size.</li>
<li>32 Volumes supported. 512TB of data can be stored (32*16)</li>
</ul>
</li>
<li>Cached Volumes - Entire Dataset is stored on S3 and the most frequently accessed data is cache on site.<ul>
<li>iSCSI based block storage</li>
<li>Each Volume can store up to 32TB in Size.</li>
<li>32 Volumes supported. 1PB of data can be stored(32*32)</li>
</ul>
</li>
</ul>
</li>
<li>Gateway Virtual Tape Library (VTL)<ul>
<li>Used for backup and uses popular backup applications like NetBackup, Backup Exec, Veam etc<ul>
<li>iSCSI based virtual tape solution</li>
<li>Virtual Tape Library (S3) 1500 virtual tapes (1PB)</li>
<li>Virtual Tape Shelf (Glacier) unlimited tapes.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Storage Gateway - General Facts</p>
<ul>
<li>Can be deployed on-premise, or as an EC2 instance.</li>
<li>Can schedule snapshots.</li>
<li>You can use Storage Gateway with Direct Connect.</li>
<li>You can implement bandwidth throttling.</li>
<li>On-Premise needs with either Vmware’s ESXi or Hyper-V.</li>
<li>Hardware Requirements:<ul>
<li>4 or 8vCPUs</li>
<li>7.5 GB of RAM</li>
<li>75 GB for installation of VM image and system data</li>
</ul>
</li>
</ul>
</li>
<li><p>Storage Gateway - Storage Requirements</p>
<ul>
<li>For gateway-cached volume configuration, you will need storage for the local cache and an upload buffer.</li>
<li>For gateway-stored volume configuration, you will need storage for your entire dataset and an upload buffer. Gateway-stored volumes can range from 1GiB to 1 TB. Each gateway configured for gateway-stored volumes can support up to 12 volumes and a total volume storage of 16TB.</li>
<li>For gateway-VTL configuration, you will need storage for the local cache and an upload buffer.</li>
</ul>
</li>
<li><p>Storage Gateway - Networking Requirements</p>
<ul>
<li>Open port 443 on your firewalls.</li>
<li>Internally, you will need to allow port 80 (activation only), port 3260 (by local systems to connect to iSCSI targets exposed by the gateway) and port UDP 53 (DNS)</li>
</ul>
</li>
<li><p>Storage Gateway - Encryption</p>
<ul>
<li>Data in transit is secured using SSL</li>
<li>Data at rest can be encrypted using AES-256</li>
</ul>
</li>
<li><p>Gateway-Cached and Gateway-Stored Volumes</p>
<ul>
<li>You can take point-in-time, incremental snapshots of your volume and store them in Amazon S3 in the form of Amazon EBS snapshots.</li>
<li>Snapshots can be initiated on a scheduled or ad-hoc basis.</li>
<li>Gateway Stored Snapshots<ul>
<li>If your volume data is stored on-premises, snapshots provide durable, off-site backups in Amazon S3.</li>
<li>You can create a new Gateway-Stored volume from a snapshot in the event you need to recover a backup.</li>
<li>You can also use a snapshot of your Gateway-Stored volume as the starting point for a new Amazon EBS volume which you can then attach to an Amazon EC2 instance.</li>
</ul>
</li>
<li>Gateway Cached Snapshots<ul>
<li>Snapshots can be used to preserve versions of your data, allowing you to revert to a prior version when required or to repurpose a point-in-time version as a new Gateway-Cached volume.</li>
</ul>
</li>
</ul>
</li>
<li><p>Gateway-Virtual Tape Library Retrieval<br>  The virtual tape containing your data must be stored in a Virtual Tape Library before it can be accessed. Access to virtual tapes in your Virtual Tape Library is <strong>instantaneous</strong>.</p>
<p>  If the virtual tape containing your data is in your Virtual Tape Shelf, you must first retrieve the virtual tape from your Virtual Tape Shelf. It takes about <strong>24 Hours</strong> for the retrieved virtual tape to be available in the selected Virtual Tape Library.</p>
</li>
<li><p>Gateway-Virtual Tape Library Supports</p>
<ul>
<li>Symantec NetBackup version 7.x</li>
<li>Symantec Backup Exec 2012</li>
<li>Symantec Backup Exec 2014</li>
<li>Symantec Backup Exec 15</li>
<li>Microsoft System Center 2012 R2 Data Protection Manager</li>
<li>Veeam Backup &amp; Replication V7</li>
<li>Veeam Backup &amp; Replication V8</li>
<li>Dell NetVault Backup 10.0</li>
</ul>
</li>
<li><p>Storage Gateway Exam Tips</p>
<ul>
<li>Know the four different Storage Gateway Types:<ul>
<li>File Gateway</li>
<li>Volume Gateway<ul>
<li>Cached - OLD NAME (Gateway-Cached Volumes)</li>
<li>Stored - OLD NAME (Gateway-Stored Volumes)</li>
</ul>
</li>
<li>Tape Gateway - OLD NAME (Gateway-Virtual Tape Library)</li>
</ul>
</li>
<li>Remember that access to virtual tapes in your virtual tape library are instantaneous. If your tape is in the virtual tape shelf(glacier) it can take 24 hours to get back to your virtual tape library.</li>
<li>Encrypted using SSL for transit and is encrypted at rest in Amazon S3 using AES-256.</li>
<li>Gateway-Stored Volumes - stores data as Amazon EBS Snapshots in S3.</li>
<li>Snapshot can be scheduled.</li>
<li>Bandwidth can be throttled (good for remote sites)</li>
<li>You need a storage gateway in each site if using multiple locations.</li>
</ul>
</li>
<li><p>Snowball</p>
<ul>
<li>Types<ul>
<li>Snowball</li>
<li>Snowball Edge</li>
<li>Snowmobile</li>
</ul>
</li>
<li>Understand what Snowball is</li>
<li>Understand what Import Export is</li>
<li>Snowball Can<ul>
<li>Import to S3</li>
<li>Export from S3</li>
</ul>
</li>
</ul>
</li>
<li><p>Import/Export</p>
<ul>
<li>Import/Export Disk<ul>
<li>Import to S3, EBS, Glacier</li>
<li>export from S3</li>
</ul>
</li>
<li>Import/Export Snowball<ul>
<li>Import to S3</li>
<li>Export to S3</li>
</ul>
</li>
</ul>
</li>
<li><p>S3 Transfer Acceleration</p>
<ul>
<li>You can speed up transfers to S3 using S3 transfer acceleration. This costs extra, and has the greatest impact on people who are in for away location.</li>
</ul>
</li>
</ul>
<ul>
<li><p>S3 static Websites</p>
<ul>
<li>You can use S3 to host static websites</li>
<li>Serverless</li>
<li>Very cheap, scales automatically.</li>
<li>STATIC only, cannot host dynamic sites.</li>
<li>Website url example: <a href="http://examplebucket.s3-website-us-west-2.amazonaws.com/" target="_blank" rel="external">http://examplebucket.s3-website-us-west-2.amazonaws.com/</a></li>
</ul>
</li>
<li><p>S3 CORS</p>
<ul>
<li>Cross Origin Resource Sharing</li>
<li>Need to enable it on the resources bucket and state the URL for the origin that will be calling the bucket.</li>
<li><a href="http://mybucketname.s3-website.en-west-2.amazonaws.com" target="_blank" rel="external">http://mybucketname.s3-website.en-west-2.amazonaws.com</a></li>
<li><a href="https://s3.eu-west-2.amazonaws.com/mybucketname" target="_blank" rel="external">https://s3.eu-west-2.amazonaws.com/mybucketname</a></li>
</ul>
</li>
<li><p>S3 multipart upload advantages</p>
<ul>
<li>Improved throughput - You can upload parts in parallel to improve throughput.</li>
<li>Quick recovery from any network issues - Smaller part size minimizes the impact of restarting a failed upload due to a network error.</li>
<li>Pause and resume object uploads - You can upload object parts over time. Once you initiate a multipart upload there is no expiry; you must explicitly complete or abort the multipart upload.</li>
<li>Begin an upload before you know the final object size - You can upload an object as you are creating it.</li>
</ul>
</li>
<li><p>Last Few Tips</p>
<ul>
<li>Write to S3 - HTTP 200 code for a successful write.</li>
<li>You can load files to S3 much faster by enabling multipart upload.</li>
<li>Read the S3 FAQ before taking the exam. It comes up A LOT!</li>
<li>S3 bucket name rules<ul>
<li>Bucket names must be at least 3 and no more than 63 characters long</li>
<li>Bucket names must be a series of one or more labels. Adjacent labels are separated by a single period (.). Bucket names can contain lowercase letters, numbers, and hyphens. Each label must start and end with a lowercase letter or a number (可以有多个lable，每个lable使用.分割，每个lable中只能包含小写字母，数字和连字符-， lable首尾必须要是小写字母或者数字)</li>
<li>Bucket names must not be formatted as an IP address (e.g., 192.168.5.4).</li>
<li>When using virtual hosted–style buckets with SSL, the SSL wildcard certificate only matches buckets that do not contain periods. To work around this, use HTTP or write your own certificate verification logic. We recommend that you do not use periods (“.”) in bucket names.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Database"><a href="#Database" class="headerlink" title="Database"></a>Database</h4><ul>
<li><p>Database component</p>
<ul>
<li>Database</li>
<li>Tables</li>
<li>Row</li>
<li>Fields(Columns)</li>
</ul>
</li>
<li><p>RDS Types</p>
<ul>
<li>SQL Server</li>
<li>Oracle</li>
<li>MySQL Server</li>
<li>PostgreSQL</li>
<li>Aurora</li>
<li>MariaDB</li>
</ul>
</li>
<li><p>Non Relational Databases</p>
<ul>
<li>Database<ul>
<li>Collection        = Table</li>
<li>Document          = Row</li>
<li>Key Value Pairs   = Fields</li>
</ul>
</li>
</ul>
</li>
<li><p>Data Warehousing</p>
<ul>
<li>Used for business intelligence. Tools like Cognos, Jaspersoft, SQL Server Reporting Services, Oracle Hyperion, SAP NetWeaver.</li>
<li>Used to pull in very large and complex data sets. Usually used by management to do queries on data ( such as current performance vs targets etc)</li>
</ul>
</li>
<li><p>OLTP vs OLAP</p>
<ul>
<li>Online Transaction Processing (OLTP) differs from Online Analytics Processing (OLAP) in terms of the types of queries run.</li>
<li>OLTP Example:<br>  Order number 2120212<br>  Pulls up a row of data such as Name, Date, Address to Deliver to, Delivery Status etc.</li>
<li><p>OLAP transaction Example:<br>  Net profit for EMEA and Pacific for the Digital Radio Product.<br>  Pulls in large numbers of records</p>
<p>  Sum of Radios Sold in EMEA<br>  Sum of Radios Sold in Pacific<br>  Unit Cost of Radio in each region<br>  Sales price of each radio<br>  Sales price - unit cost.</p>
<p>  Data Warehousing databases use different type of architecture both from a database perspective and infrastructure layer.</p>
</li>
</ul>
</li>
<li><p>Elasticache</p>
<ul>
<li>Elastic Cache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based databases.</li>
<li>ElastiCache supports two open-source in-memory caching engines:<ul>
<li>Memcached</li>
<li>Redis</li>
</ul>
</li>
</ul>
</li>
<li><p>DMS</p>
<ul>
<li>Announced at re:Invent 2015, DMS stands for Database Migration Service. Allows you to migrate your production database to AWS. Once the migration has started, AWS manages all the complexities of the migration process like data type transformation, compression, and parallel transfer ( for faster data transfer) while ensuring that data changes to the source database that occur during the migration process are automatically replicated to the target.</li>
<li>AWS schema conversion tool automatically converts the source database schema and a majority of the custom code, including views, stored procedures, and functions, to a format compatible with the target database.</li>
</ul>
</li>
<li><p>Database Summary</p>
<ul>
<li>RDS -OLTP<ul>
<li>SQL</li>
<li>MySQL</li>
<li>PostgreSQL</li>
<li>Oracle</li>
<li>Aurora</li>
<li>MariaDB</li>
</ul>
</li>
<li>DynamoDB - No SQL</li>
<li>Redshift - OLAP</li>
<li>Elasticache - In Memory Caching.<ul>
<li>Memcached</li>
<li>Redis</li>
</ul>
</li>
<li>DMS</li>
</ul>
</li>
</ul>
<h4 id="DynamoDB"><a href="#DynamoDB" class="headerlink" title="DynamoDB"></a>DynamoDB</h4><ul>
<li>Quick Facts about DynameDB<ul>
<li>Stored on SSD Storage</li>
<li>Spread Across 3 geographically distinct data centers</li>
<li>Eventual Consistent Reads (Default)<ul>
<li>Consistency across all copies of data is usually reached within a second. Repeating a read after a short time should return the updated data. (Best Read Performance)</li>
</ul>
</li>
<li>Strongly Consistent Reads<ul>
<li>A strongly consistent read returns a result that reflects all writes that received a successful response prior to the read.</li>
</ul>
</li>
</ul>
</li>
<li><p>The Basic</p>
<ul>
<li>Tables</li>
<li>Items ( Think a row of data in table)</li>
<li>Attributes (Think of a column of data in a table)</li>
</ul>
</li>
<li><p>Pricing</p>
<ul>
<li>Provisioned Throughput Capacity<ul>
<li>Write Throughput $0.0065 per hour for every 10 units</li>
<li>Read Throughput $0.0065 per hour for every 50 units</li>
</ul>
</li>
<li>First 25 GB stored per month is free</li>
<li>Storage costs of $0.25 GB per month there after.</li>
</ul>
</li>
<li><p>Primary Keys</p>
<ul>
<li>Two Types of Primary Keys Available<ul>
<li>Single Attribute (think unique ID)<ul>
<li>Partition Key (Hash Key) composed of one attribute.</li>
</ul>
</li>
<li>Composite (think unique ID and a data range)<ul>
<li>Partition Key &amp; Sort Key (Hash &amp; Range) composed of two attributes.</li>
</ul>
</li>
</ul>
</li>
<li>Partition Key and Sort Key<ul>
<li>DynamoDB uses the partition key’s value as input to an internal hash function. The output from the hash function determines the partition (this is simply the physical location in which the data is stored)</li>
<li>Two items can have the same partition key, but they <strong>must have a different sort key</strong>.</li>
<li>All items with the same partition key are stored together, in sorted order by sort key value.</li>
</ul>
</li>
</ul>
</li>
<li><p>DynamoDB - Indexes</p>
<ul>
<li>Local Secondary Index<ul>
<li>Has the SAME Partition key, different sort key.</li>
<li>Can ONLY be created when creating a table. They cannot be removed or modified later.</li>
</ul>
</li>
<li>Global Secondary Index<ul>
<li>Has DIFFERENT Partition key and different sort key.</li>
<li>Can be created at table creation or added LATER.</li>
</ul>
</li>
</ul>
</li>
<li><p>DynamoDB Streams<br>Used to capture any kind of modification of the DynamoDB tables.</p>
<ul>
<li>If a new item is added to the table, the stream captures an image of the entire item, including all of its attributes.</li>
<li>If an item is updated, the stream captures the “before” and “after” image of any attributes that were modified in the item.</li>
<li>If an item is deleted from the table, the stream captures an image of the entire item before it was deleted</li>
</ul>
</li>
<li><p>Query &amp; Scans</p>
<ul>
<li>Query<ul>
<li>A Query operation finds items in a table using only primary key attribute values. You must provide a partition key attribute name and a distinct value to search for.</li>
<li>You can optionally provide a sort key attribute name and value, and use a comparison operator to refine the search results.</li>
<li>A Scan operation examines every item in the table. By default, a Scan returns all of the data attributes for every item; however, you can use the <strong>ProjectionExpression</strong> parameter so that the Scan only returns some of the attributes, rather than all of them.</li>
<li>Query results are always sorted by the sort key. If the data type of the sort key is a number, the results are returned in numeric order; otherwise, the results are returned in order of ASCII character code values. By default, the sort order is ascending. To reverse the order, set the <strong>ScanIndexForward</strong> parameter to false.</li>
<li>By Default is eventually consistent but can be changed to be strongly consistent.</li>
</ul>
</li>
<li>Scan<ul>
<li>A Scan operation examines every item in the table. By default, a Scan returns all of the data attributes for every item; however, you can use the <strong>ProjectionExpression</strong> parameter so that the Scan only returns some of the attributes, rather than all of them.</li>
</ul>
</li>
<li>Try to use a query operation over a Scan operation as it is more efficient.</li>
</ul>
</li>
<li><p>DynamoDB Provisioned Throughput</p>
<ul>
<li>One read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size</li>
<li>One write capacity unit represents one write per second for an item up to 1 KB in size</li>
<li><strong>400 HTTP Status Code - ProvisionedThroughputExceededException</strong> indicated You exceeded your maximum allowed provisioned throughput for a table or for one or more global secondary indexes.</li>
<li>Example</li>
</ul>
</li>
<li><p>Step taken to authenticate</p>
<ul>
<li>User Authenticates with ID provider (such as Facebook)</li>
<li>They are passed a Token by their ID provider.</li>
<li>Your code calls <strong>AssumeRoleWithWebIdentity</strong> API and provides the providers token and specifies the ARN for the IAM Role</li>
<li>App can now access Dynamodb from between 15 minutes to 1 hour (default is 1 hour)</li>
</ul>
</li>
<li><p>Conditional Writes<br>IF item = $10 then update to $12</p>
<ul>
<li>The conditional writes are idempotent</li>
<li>You can send the same conditional write request multiple times, but it will have no further effect on the item after the first time DynamoDB performs the specified update.</li>
</ul>
</li>
<li><p>Atomic Counters</p>
<ul>
<li>DynamoDB supports atomic counters</li>
<li>use the <strong>UpdateItem</strong> operation to increment or decrement the value of an existing attribute without interfering with other write requests.</li>
<li>All write requests are applied in the order in which they were received</li>
</ul>
</li>
<li><p>Batch Operations<br>If you application needs to read multiple items, you can use the <strong>BatchGetItem</strong> API. A single <strong>BatchGetItem</strong> request can retrieve up to 16 MB of data, which can contain as many as 100 items. In addition, a single BatchGetItem request can retrieve items from multiple tables.</p>
</li>
<li><p>DynamoDB的操作</p>
<ul>
<li>DynamoDB的插入操作<ul>
<li>PutItem</li>
<li>BatchWriteItem (<strong>记住并没有BatchPutItem这种操作</strong>)</li>
</ul>
</li>
<li>DynamoDB的检索操作<ul>
<li>GetItem</li>
<li>BatchGetItem</li>
<li>Query API</li>
</ul>
</li>
</ul>
</li>
<li><p>Dynamodb limit</p>
<ul>
<li>maximum 5 local secondary index per table</li>
<li>maximum 5 global secondary index per table</li>
<li>default provision throughput (Account can increase them by contacting AWS)<ul>
<li>US East (N. Virginia) Region<ul>
<li>Per table – 40,000 read capacity units and 40,000 write capacity units</li>
<li>Per account – 80,000 read capacity units and 80,000 write capacity units</li>
</ul>
</li>
<li>All Other Regions<ul>
<li>Per table – 10,000 read capacity units and 10,000 write capacity units</li>
<li>Per account – 20,000 read capacity units and 20,000 write capacity units</li>
</ul>
</li>
</ul>
</li>
<li>No table size limit</li>
<li>There is an initial limit of 256 tables per region.</li>
<li>Maximum item size is 400KB(包括属性名称和属性值)</li>
<li>No limit on the number of attributes of a item, but the size of item can’t exceed 400KB</li>
<li>BatchGetItem - A single BatchGetItem operation can retrieve a maximum of 100 items. The total size of all the items retrieved cannot exceed 16 MB</li>
<li>Query - The result set from a Query is limited to 1 MB per call</li>
<li>Scan - The result set from a Scan is limited to 1 MB per call</li>
<li>The smallest Reserved Capacity offering is 100 Capacity units(reads or writes)</li>
</ul>
</li>
<li><p><strong>Read The FAQ!!!</strong></p>
</li>
</ul>
<h4 id="SQS"><a href="#SQS" class="headerlink" title="SQS"></a>SQS</h4><ul>
<li>SQS usage example<ul>
<li>Asynchronously pulls the task messages from the queue</li>
<li>Retrieves the named file</li>
<li>Processes the conversion</li>
<li>Writes the image back to Amazon S3</li>
<li>Writes a “task complete” message to another queue</li>
<li>Deletes the original task message</li>
<li>Checks for more messages in the worker queue</li>
</ul>
</li>
</ul>
<ul>
<li><p>SQS Tips</p>
<ul>
<li>Does not offer FIFO</li>
<li>12 hours visibility time out</li>
<li>Amazon SQS is engineered to provide “at least once” delivery of all messages in its queues. Although most of the time each message will be delivered to your application exactly once, you should design your system so that processing a message more than once does not create any errors or inconsistencies.</li>
<li>256kb message size now available</li>
<li>Billed at 64kb “Chunks”</li>
<li>A 256kb message will be 4*64kb “chunks”</li>
<li>You can create any number of message queues.</li>
</ul>
</li>
<li><p>SQS Pricing</p>
<ul>
<li>First 1 million Amazon SQS requests per month are free</li>
<li>$0.05 per 1 million Amazon SQS Requests per month thereafter ($0.00000050 per SQS Request)</li>
<li>A single request can have from 1 to 10 messages, up to a maximum total payload of 256KB.</li>
<li>Each 64KB ‘chunk’ of payload is billed as 1 request. For example, a single API call with a 256KB payload will be billed as four requests.</li>
</ul>
</li>
<li><p>SQS Delivery</p>
<ul>
<li>SQS messages can be delivered multiple times and in any order.</li>
</ul>
</li>
<li><p>SQS Default Visibility Time Out</p>
<ul>
<li>Default Visibility Time Out is 30 Seconds</li>
<li>Maximum Time Out is 12 Hours</li>
<li>When you receive a message from a queue and begin processing it, you may find the visibility timeout for the queue is insufficient to fully process and delete that message. To give yourself more time to process the message, you can extend its visibility timeout by using the <strong>ChangeMessageVisibility</strong> action to specify a new timeout value. Amazon SQS restarts the timeout period using the new value.</li>
</ul>
</li>
<li><p>SQS Long Polling</p>
<ul>
<li>SQS long polling is a way to retrieve messages from your SQS queues. While the traditional SQS short polling returns immediately, even if the queue being polled is empty, SQS long polling doesn’t return a response until a message arrives in the queue, or the long poll times out. SQS long polling makes it easy and inexpensive to retrieve messages from your SQS queue as soon as they are available.</li>
<li>Maximum Long Poll Time Out = 20 seconds</li>
<li>队列属性<strong>ReceiveMessageWaitTimeSeconds</strong>设置为1~20的数字，在队列中启动长轮询</li>
<li>单个ReveiveMessge的请求中将WaitTimeSeconds设置为1~20的数字</li>
</ul>
</li>
<li><p>SQS Fanning Out</p>
<ul>
<li>Create an SNS topic first using SNS. Then create and subscribe multiple SQS queues to the SNS topic.</li>
<li>Now whenever a message is sent to the SNS topic, the message will be fanned out to the SQS queues, i.e. SNS will deliver the message to all the SQS queues that are subscribed to the topic.</li>
</ul>
</li>
</ul>
<h4 id="SNS"><a href="#SNS" class="headerlink" title="SNS"></a>SNS</h4><ul>
<li><p>SNS Benefits</p>
<ul>
<li>Instantaneous, push-based delivery (no polling)</li>
<li>Simple APIs and easy integration with applications</li>
<li>Flexible message delivery over multiple transport protocols</li>
<li>Inexpensive, pay-as-you-go model with no up-front costs</li>
<li>Web-based AWS Management Console offers the simplicity of a point-and-click interface</li>
</ul>
</li>
<li><p>SNS vs SQS</p>
<ul>
<li>Both Messaging Services in AWS</li>
<li>SNS - Push</li>
<li>SQS - Polls (Pulls)</li>
</ul>
</li>
<li><p>SNS Pricing</p>
<ul>
<li>Users pay $0.50 per 1 million Amazon SNS Requests</li>
<li>$0.06 per 100,000 Notification deliveries over HTTP</li>
<li>$0.75 per 100 Notification deliveries over SMS</li>
<li>$2.00 per 100,000 Notification deliveries over Email</li>
</ul>
</li>
<li><p>SNS Summary</p>
<ul>
<li>Instantaneous, push-bashed delivery (no polling)</li>
<li>Protocols include:<ul>
<li>HTTP</li>
<li>HTTPS</li>
<li>Email</li>
<li>Email-JSON</li>
<li>Amazon SQS</li>
<li>Application</li>
<li>AWS Lambda</li>
<li>SMS</li>
</ul>
</li>
<li>Messages can be customized for each protocol</li>
</ul>
</li>
<li><p>Valid arguments for an SNS Publish request</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">POST / HTTP/1.1</div><div class="line">x-amz-sns-message-type: SubscriptionConfirmation</div><div class="line">x-amz-sns-message-id: 165545c9-2a5c-472c-8df2-7ff2be2b3b1b</div><div class="line">x-amz-sns-topic-arn: arn:aws:sns:us-west-2:123456789012:MyTopic</div><div class="line">Content-Length: 1336</div><div class="line">Content-Type: text/plain; charset=UTF-8</div><div class="line">Host: myhost.example.com</div><div class="line">Connection: Keep-Alive</div><div class="line">User-Agent: Amazon Simple Notification Service Agent</div><div class="line"></div><div class="line">&#123;</div><div class="line">  &quot;Type&quot; : &quot;SubscriptionConfirmation&quot;,</div><div class="line">  &quot;MessageId&quot; : &quot;165545c9-2a5c-472c-8df2-7ff2be2b3b1b&quot;,</div><div class="line">  &quot;Token&quot; : &quot;2336412f37fb687f5d51e6e241d09c805a5a57b30d712f794cc5f6a988666d92768dd60a747ba6f3beb71854e285d6ad02428b09ceece29417f1f02d609c582afbacc99c583a916b9981dd2728f4ae6fdb82efd087cc3b7849e05798d2d2785c03b0879594eeac82c01f235d0e717736&quot;,</div><div class="line">  &quot;TopicArn&quot; : &quot;arn:aws:sns:us-west-2:123456789012:MyTopic&quot;,</div><div class="line">  &quot;Message&quot; : &quot;You have chosen to subscribe to the topic arn:aws:sns:us-west-2:123456789012:MyTopic.\nTo confirm the subscription, visit the SubscribeURL included in this message.&quot;,</div><div class="line">  &quot;SubscribeURL&quot; : &quot;https://sns.us-west-2.amazonaws.com/?Action=ConfirmSubscription&amp;TopicArn=arn:aws:sns:us-west-2:123456789012:MyTopic&amp;Token=2336412f37fb687f5d51e6e241d09c805a5a57b30d712f794cc5f6a988666d92768dd60a747ba6f3beb71854e285d6ad02428b09ceece29417f1f02d609c582afbacc99c583a916b9981dd2728f4ae6fdb82efd087cc3b7849e05798d2d2785c03b0879594eeac82c01f235d0e717736&quot;,</div><div class="line">  &quot;Timestamp&quot; : &quot;2012-04-26T20:45:04.751Z&quot;,</div><div class="line">  &quot;SignatureVersion&quot; : &quot;1&quot;,</div><div class="line">  &quot;Signature&quot; : &quot;EXAMPLEpH+DcEwjAPg8O9mY8dReBSwksfg2S7WKQcikcNKWLQjwu6A4VbeS0QHVCkhRS7fUQvi2egU3N858fiTDN6bkkOxYDVrY0Ad8L10Hs3zH81mtnPk5uvvolIC1CXGu43obcgFxeL3khZl8IKvO61GWB6jI9b5+gLPoBc1Q=&quot;,</div><div class="line">  &quot;SigningCertURL&quot; : &quot;https://sns.us-west-2.amazonaws.com/SimpleNotificationService-f3ecfb7224c7233fe7bb5f59f96de52f.pem&quot;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
</li>
<li><p>SNS limit</p>
<ul>
<li>Topic names are limited to 256 characters.</li>
<li>Token included in the confirmation message sent to end-points on a subscription request are valid for 3 days.</li>
</ul>
</li>
</ul>
<h4 id="SWF"><a href="#SWF" class="headerlink" title="SWF"></a>SWF</h4><ul>
<li>SWF worker<ul>
<li>Workers are programs that interact with Amazon SWF to get tasks, process received tasks, and return the results.</li>
</ul>
</li>
<li>SWF Decider<ul>
<li>The Decider is a program that controls the coordination of tasks, i.e. their ordering, concurrency, and scheduling according to the application logic.</li>
</ul>
</li>
<li>SWF Workers &amp; Deciders<ul>
<li>The workers and the decider can run on cloud infrastructure, such as Amazon EC2, or on machines behind firewalls. Amazon SWF brokers the interactions between workers and the decider. It allows the decider to get consistent views into the progress of tasks and to initiate new tasks in an ongoing manner. At the same time, Amazon SWF stores tasks, assigns them to workers when they are ready, and monitors their progress. It ensures that a task is assigned only once and is never duplicated. Since Amazon SWF maintains the application’s state durably, workers and deciders don’t have to keep track of execution state. They can run independently and scale quickly.</li>
</ul>
</li>
<li>SWF Domains<ul>
<li>Your workflow and activity types and the workflow execution itself are all scoped to a domain. Domains isolate a set of types, executions, and task lists from others within the same account. You can register a domain by using the AWS Management Console or by using the RegisterDomain action in the Amazon SWF API.</li>
<li>The parameter are specified in JavaScript Object Notation(JSON) format.<br><a href="https://swf.us-east-1.amazonaws.com" target="_blank" rel="external">https://swf.us-east-1.amazonaws.com</a><br>RetisterDomain<br>{<br>  “name” : “1234567”,<br>  “description” : “music”,<br>  “workflowExecutionRetentionPeriodInDays” : “60”<br>}</li>
</ul>
</li>
<li>Maximum WorkFlow can be 1 year and the value is always measured in seconds.</li>
<li><p>SWF vs SQS</p>
<ul>
<li>Amazon SWF presents a task-oriented API, whereas Amazon SQS offers a message-oriented API.</li>
<li>Amazon SWF ensures that a task is assigned only once and is never duplicated. With Amazon SQS, you need to handle duplicated messages and may also need to ensure that a message is processed only once.</li>
<li>Amazon SWF keeps track of all the tasks and events in an application. With Amazon SQS, you need to implement your own application-level tracking especially if your application uses multiple queues.</li>
</ul>
</li>
<li><p>SWF limit</p>
<ul>
<li>maximum number of SWF domains is 100 (includes both registered and deprecated domains)</li>
<li>Maximum workflow and activity types - 10,000 each per domain (includes both registered and deprecated types)</li>
<li>Maximum request size is 1 MB per request (including the request header and all other associated request data.)</li>
<li>Maximum open workflow executions - 100,000 per domain (includes child workflow executions)</li>
<li>Maximum workflow execution time - 1 year</li>
<li>You can only have a maximum of 1,000 open activity tasks per workflow execution.</li>
</ul>
</li>
</ul>
<h4 id="Elastic-Beanstalk"><a href="#Elastic-Beanstalk" class="headerlink" title="Elastic Beanstalk"></a>Elastic Beanstalk</h4><ul>
<li>Supported Application<ul>
<li>Java</li>
<li>.NET</li>
<li>PHP</li>
<li>Node.js</li>
<li>Python</li>
<li>Ruby</li>
<li>Go</li>
<li>Docker</li>
</ul>
</li>
<li>Its uses ASG, ELB, EC2, RDS, SNS and S3 to provision things.</li>
<li>Environment Tier - WebServer, Worker</li>
<li>Preconfigured docker:<ul>
<li>Glassfish</li>
<li>Python</li>
<li>Go</li>
</ul>
</li>
<li>Environment URL - has to be unique</li>
<li>Console Item<ul>
<li>Dashboard</li>
<li>Configuration<ul>
<li>Scaling</li>
<li>Instances(DIRTMCG instance types, key pair)</li>
<li>Notifications</li>
<li>Software configuration</li>
<li>Updates and Deployments</li>
<li>Health</li>
<li>Managed Updates</li>
<li>Networking tier(ELB, VPC)</li>
<li>Data tier(RDS)</li>
</ul>
</li>
<li>Logs</li>
<li>Health</li>
<li>Monitoring</li>
<li>Alarms</li>
<li>Managed Updates</li>
<li>Events</li>
<li>Tags</li>
</ul>
</li>
</ul>
<h4 id="Route53-DNS"><a href="#Route53-DNS" class="headerlink" title="Route53 DNS"></a>Route53 DNS</h4><ul>
<li>ELB’s do not have pre-defined IPv4 addresses, you resolve to them using a DNS name.</li>
<li>Understand the difference between an Alias Record and a CNAME</li>
<li>Given the choice, always choose an Alias Record over a CNAME.</li>
<li>Remember the different routing policies and their use cases.<ul>
<li>Simple</li>
<li>Weighted</li>
<li>Latency</li>
<li>Failover</li>
<li>Geolocation</li>
</ul>
</li>
</ul>
<h4 id="VPC"><a href="#VPC" class="headerlink" title="VPC"></a>VPC</h4><ul>
<li>Basic Info<ul>
<li>Think of a VPC as a logical datacenter in AWS</li>
<li>Consists of IGW’s (Or Virtual Private Gateways), Route Tables, Network Access Control Lists, Subnets, Security Groups</li>
<li>1 Subnet = 1 Availability Zone</li>
<li>Security Groups are Stateful, Network Access Control Lists are Stateless.</li>
<li>Can Peer VPCs both in the same account and with other AWS accounts.</li>
<li>No Transitive Peering</li>
<li>Custom VPC network block size has to be between a /16 netmask and /28 netmask.</li>
</ul>
</li>
</ul>
<ul>
<li>What can you do with a VPC<ul>
<li>Launch instances into a subnet of your choosing</li>
<li>Assign custom IP address ranges in each subnet</li>
<li>Configure route tables between subnets</li>
<li>Create internet gateway and attach it to our VPC</li>
<li>Much better security control over your AWS resources</li>
<li>Instance security groups</li>
<li>Subnet network access control lists (ACLS)</li>
</ul>
</li>
</ul>
<ul>
<li><p>Default VPC vs Custom VPC</p>
<ul>
<li>Default VPC is user friendly, allowing you to immediately deploy instances</li>
<li>All Subnets in default VPC have a route out to the internet.</li>
<li>Each EC2 instance has both a public and private IP address</li>
<li>If you delete the default VPC the only way to get it back is to contact AWS.</li>
</ul>
</li>
<li><p>VPC peering</p>
<ul>
<li>Allows you to connect one VPC with another via a direct network route using private IP addresses.</li>
<li>Instances behave as if they were on the same private network</li>
<li>You can peer VPC’s with other AWS accounts as well as with other VPCs in the same account.</li>
<li>Peering is in a star configuration, ie 1 central VPC peers with 4 others, <strong>NO TRANSITIVE PEERING!!!</strong></li>
</ul>
</li>
<li><p>Create VPC</p>
<ul>
<li>things automatically created<ul>
<li>Route tables</li>
<li>Network ACLs</li>
<li>Security Groups</li>
<li>DHCP options set</li>
</ul>
</li>
<li>things are not automatically created<ul>
<li>Internet Gateways</li>
<li>Subnets</li>
</ul>
</li>
</ul>
</li>
<li><p>VPC Subnet</p>
<ul>
<li>There are 5 IP address reserved in each subnet by AWS, take CIDR block 10.0.0.0/24 as example<ul>
<li>10.0.0.0 Network address</li>
<li>10.0.0.1 Reserved by AWS for the VPC router</li>
<li>10.0.0.2 Reserved by AWS for DNS</li>
<li>10.0.0.3 Reserved by AWS for future use.</li>
<li>10.0.0.255 Network broadcast address, we do not support broadcast in a VPC, therefore we reserve this address.</li>
</ul>
</li>
</ul>
</li>
<li><p>NAT instances</p>
<ul>
<li>When creating a NAT instance, Disable Source/Destination Check on the Instance</li>
<li>NAT instance must be in a public subnet</li>
<li>Must have an elastic IP address to work</li>
<li>There must be a route out of the private subnet to the NAT instance, in order for this to work</li>
<li>The amount of traffic that NAT instances supports, depends on the instance size. If you are bottlenecking, increase the instance size</li>
<li>You can create high availability using Autoscaling Groups, multiple subnets in different AZ’s and a script to automate failover</li>
<li>Behind a Security Group.</li>
</ul>
</li>
<li><p>NAT Gateways</p>
<ul>
<li>Very new</li>
<li>Preferred by the enterprise</li>
<li>Scale automatically up to 10Gbps</li>
<li>No need to patch</li>
<li>Not associated with security groups</li>
<li>Automatically assigned a public ip address</li>
<li>Remember to update your route tables.</li>
<li>No need to disable Source/Destination Checks.</li>
</ul>
</li>
<li><p>NAT instances vs NAT Gateways</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Attribute</th>
<th style="text-align:left">NAT gateway</th>
<th style="text-align:left">NAT instance</th>
</tr>
</thead>
<tbody>
<tr>
<td>Availability</td>
<td style="text-align:left">Highly available. NAT gateways in each Availability Zone are implemented with redundancy. Create a NAT gateway in each Availability Zone to ensure zone-independent architecture.</td>
<td style="text-align:left">Use a script to manage failover between instances.</td>
</tr>
<tr>
<td>Bandwidth</td>
<td style="text-align:left">Supports bursts of up to 10Gbps.</td>
<td style="text-align:left">Depends on the bandwidth of the instance type.</td>
</tr>
<tr>
<td>Maintenance</td>
<td style="text-align:left">Managed by AWS.You do not need to perform any maintenance.</td>
<td style="text-align:left">Managed by you, for example, by installing software updates or operating system patches on the instance.</td>
</tr>
<tr>
<td>Performance</td>
<td style="text-align:left">Software is optimized for handling NAT traffic.</td>
<td style="text-align:left">A generic Amazon Linux AMI that’s configured to perform NAT.</td>
</tr>
<tr>
<td>Cost</td>
<td style="text-align:left">Charged depending on the number of NAT gateways you use, duration of usage, and amount of data that you send through the NAT gateways.</td>
<td style="text-align:left">Charged depending on the number of NAT instances that you use, duration of usage, and instance type and size.</td>
</tr>
<tr>
<td>Type and size</td>
<td style="text-align:left">Uniform offering; you don’t need to decide on the type or size.</td>
<td style="text-align:left">Choose a suitable instance type and size, according to your predicted workload.</td>
</tr>
<tr>
<td>Public IP addresses</td>
<td style="text-align:left">Choose the Elastic IP address to associate with a NAT gateway at creation.</td>
<td style="text-align:left">Use an Elastic IP address or a public IP address with a NAT instance. You can change the public IP address at any time by associating a new Elastic IP address with the instance.</td>
</tr>
<tr>
<td>Private IP addresses</td>
<td style="text-align:left">Automatically selected from the subnet’s IP address range when you create the gateway.</td>
<td style="text-align:left">Assign a specific private IP address from the subnet’s IP address range when you launch the instance.</td>
</tr>
<tr>
<td>Security groups</td>
<td style="text-align:left">Cannot be associated with a NAT gateway. You can associate security groups with your resources behind the NAT gateway to control inbound and outbound traffic.</td>
<td style="text-align:left">Associate with your NAT instance and the resources behind your NAT instance to control inbound and outbound traffic.</td>
</tr>
<tr>
<td>Network ACLs</td>
<td style="text-align:left">Use a network ACL to control the traffic to and from the subnet in which your NAT gateway resides.</td>
<td style="text-align:left">Use a network ACL to control the traffic to and from the subnet in which your NAT instance resides.</td>
</tr>
<tr>
<td>Flow logs</td>
<td style="text-align:left">Use flow logs to capture the traffic.</td>
<td style="text-align:left">Use flow logs to capture the traffic.</td>
</tr>
<tr>
<td>Port forwarding</td>
<td style="text-align:left">Not supported.</td>
<td style="text-align:left">Manually customize the configuration to support port forwarding.</td>
</tr>
<tr>
<td>Bastion servers</td>
<td style="text-align:left">Not supported.</td>
<td style="text-align:left">Use as a bastion server.</td>
</tr>
<tr>
<td>Traffic metrics</td>
<td style="text-align:left">Not supported.</td>
<td style="text-align:left">View CloudWatch metrics.</td>
</tr>
<tr>
<td>Timeout behavior</td>
<td style="text-align:left">When a connection times out, a NAT gateway returns an RST packet to any resources behind the NAT gateway that attempt to continue the connection (it does not send a FIN packet).</td>
<td style="text-align:left">When a connection times out, a NAT instance sends a FIN packet to resources behind the NAT instance to close the connection.</td>
</tr>
<tr>
<td>IP fragmentation</td>
<td style="text-align:left">Supports forwarding of IP fragmented packets for the UDP protocol. Does not support fragmentation for the TCP and ICMP protocols. Fragmented packets for these protocols will get dropped.</td>
<td style="text-align:left">Supports reassembly of IP fragmented packets for the UDP, TCP, and ICMP protocols.</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Network ACL’s</p>
<ul>
<li>Your VPC automatically comes a default network ACL and by default it allows all outbound and inbound traffic.</li>
<li>You can create a custom network ACL. By default, each custom network ACL denies all inbound and outbound traffic until you add rules.</li>
<li>Each subnet in your VPC must be associated with a network ACL. If you don’t explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL.</li>
<li>You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed.</li>
<li>A network ACl contains a numbered list of rules that is evaluated in order, starting with the lowest numbered rule.</li>
<li>A network ACl has separate inbound and outbound rules, and each rule can either allow or deny traffic.</li>
<li>Network ACLs are stateless responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa)</li>
<li>Block IP Addresses using network ACL’s not Security Groups</li>
</ul>
</li>
<li><p>Security Group vs Network ACL</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Security Group</th>
<th style="text-align:left">Network ACL</th>
</tr>
</thead>
<tbody>
<tr>
<td>operates at the instance level (first layer of defense)</td>
<td style="text-align:left">Operates at the subnet level (second layer of defense)</td>
</tr>
<tr>
<td>Supports allow rules only</td>
<td style="text-align:left">Supports allow rules and deny rules</td>
</tr>
<tr>
<td>Is stateful: Return traffic is automatically allowed, regardless of any rules</td>
<td style="text-align:left">Is stateless: Return traffic must be explicitly allowed by rules</td>
</tr>
<tr>
<td>We evaluate all rules before deciding whether to allow traffic</td>
<td style="text-align:left">We process rules in number order when deciding whether to allow traffic</td>
</tr>
<tr>
<td>Applies to an instance only if someone specifies the security group when launching the instance, or associates the security group with the instance later on</td>
<td style="text-align:left">Automatically applies to all instances in the subnets it’s associated with (backup layer of defense, so you don’t have to rely on someone specifying the security group)</td>
</tr>
</tbody>
</table>
<ul>
<li><p>NAT vs Bastions</p>
<ul>
<li>A NAT is used to provide internet traffic to EC2 instances in private subnets</li>
<li>A Bastion is used to securely administer EC2 instance (using SSH or RDP) in private subnets. In Australia we call them jump boxes.</li>
</ul>
</li>
<li><p>Resilient Architecture</p>
<ul>
<li>If you want resiliency, always have 2 public subnets and 2 private subnets. Make sure each subnet is in different availability zones.</li>
<li>With ELB’s make sure they are in 2 public subnets in 2 different availability zones.</li>
<li>With Bastion hosts, put them behind an autoscaling group with a minimum size of 2. Use Route53 (either round robin or using a health check) to automatically fail over.</li>
<li>NAT instances are tricky to make resilient. You need 1 in each public subnet, each with their own public IP address, and you need to write a script to fail between the two. Instead where possible, use NAT gateways.</li>
</ul>
</li>
<li><p>VPC Flow Logs</p>
<ul>
<li>You can monitor network traffic within your custom VPC’s using VPC Flow Logs.</li>
</ul>
</li>
<li><p>VPC limit</p>
<ul>
<li>Currently you can create 200 subnets per VPC by default</li>
</ul>
</li>
</ul>
<h4 id="CloudFormation"><a href="#CloudFormation" class="headerlink" title="CloudFormation"></a>CloudFormation</h4><ul>
<li>Use of CFT(CloudFormation Templates), Beanstalk and AutoScaling are free but you pay for the AWS resources that these services create.</li>
<li>Fn::GetAtt - values that you can use to return result for an AWS created resource or used to display in output</li>
<li>By Default - rollback everything on error</li>
<li>Infrastructure as a code, Version controlled, declarative and flexible</li>
<li>API ListStackResources is used to list all resources that belong to a CloudFormation Stack</li>
<li><p>You can use intrinsic functions only in specific parts of a template. Currently, you can use intrinsic functions in resource properties, metadata attributes, and update policy attributes.</p>
</li>
<li><p>CloudFormation Basic</p>
<ul>
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/gettingstarted.templatebasics.html" target="_blank" rel="external">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/gettingstarted.templatebasics.html</a></li>
<li>Learn about the following about templates:<ul>
<li>Declaring resources and their properties</li>
<li><strong>Ref</strong>erencing(提取) other resources with the <strong>Ref</strong> function and resource attributes using the Fn::GetAtt function</li>
<li>Using parameters to enable users to specify values at stack creation time and using constraints to validate parameter input</li>
<li>Using mappings to determine conditional values</li>
<li>Using the Fn::Join function to construct values based on parameters, resource attributes, and other things</li>
<li>Using output values based to capture information about the stack’s resources.</li>
</ul>
</li>
</ul>
</li>
<li><p>CloudFormation intrinsic Function</p>
<ul>
<li>Fn::Base64<ul>
<li>returns the Base64 representation of the input string. this function is typically used to pass encoded data to Amazon EC2 instances by way of the UserData property.</li>
<li>JSON Format { “Fn::Base64” : valueToEncode }</li>
</ul>
</li>
<li>Fn::FindInMap<ul>
<li>returns the value corresponding to keys in a two-level map that is declared in the Mapping section.</li>
<li>JSON Format { “Fn::FindInMap” : [ “MapName”, “TopLevelKey”, “SecondLevelKey”] }</li>
</ul>
</li>
<li>Fn::GetAtt<ul>
<li>returns the value of an attribute from a resource in the template</li>
<li>JSON Format { “Fn::GetAtt” : [ “logicalNameOfResource”, “attributeName” ] }</li>
</ul>
</li>
<li>Fn::Join<ul>
<li>Fn::Join appends a set of values into a single value, separated by the specified delimiter. If a delimiter is the empty string, the set of values are concatenated with no delimiter.</li>
<li>JSON Format { “Fn::Join” : [ “delimiter”, [ comma-delimited list of values ] ] }</li>
<li>JSON example {“Fn::Join” : [ “:”, [ “a”, “b”, “c” ] ]} will returns “a:b:c”</li>
</ul>
</li>
<li>Fn::Select<ul>
<li>returns a single object from a list of objects by index</li>
<li>JSON Format { “Fn::Select” : [ index, listOfObjects ] }</li>
<li>JSON example { “Fn::Select” : [ “1”, [ “apples”, “grapes”, “oranges”, “mangoes” ] ] } will returns “grapes”</li>
</ul>
</li>
<li>Fn::Split<ul>
<li>To split a string into a list of string values so that you can select an element from the resulting string list.</li>
<li>JSON Format { “Fn::Split” : [ “delimiter”, “source string” ] }</li>
<li>JSON example { “Fn::Split” : [ “|” , “a|b|c” ] } will return [“a”, “b”, “c”]</li>
</ul>
</li>
<li><p>Fn::Sub</p>
<ul>
<li>将输入字符串中的变量替换为您指定的值</li>
<li>JSON Format  { “Fn::Sub” : [ String, { Var1Name: Var1Value, Var2Name: Var2Value } ] }</li>
<li>JSON example - 将AWS::Region和AWS::StackName替换为实际的值<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&quot;UserData&quot;: &#123; &quot;Fn::Base64&quot;: &#123; &quot;Fn::Join&quot;: [&quot;\n&quot;, [</div><div class="line">&quot;#!/bin/bash -xe&quot;,</div><div class="line">&quot;yum update -y aws-cfn-bootstrap&quot;,</div><div class="line">&#123; &quot;Fn::Sub&quot;: &quot;/opt/aws/bin/cfn-init -v --stack $&#123;AWS::StackName&#125; --resource LaunchConfig --configsets wordpress_install --region $&#123;AWS::Region&#125;&quot; &#125;,</div><div class="line">&#123; &quot;Fn::Sub&quot;: &quot;/opt/aws/bin/cfn-signal -e $? --stack $&#123;AWS::StackName&#125; --stack $&#123;AWS::StackName&#125; --resource WebServer --region $&#123;AWS::Region&#125;&quot; &#125;]]</div><div class="line">&#125;&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Ref</p>
<ul>
<li>returns the value of the specified parameter or resource.<ul>
<li>When you specify a parameter’s logical name, it returns the value of the parameter.</li>
<li>When you specify a resource’s logical name, it returns a value that you can typically use to refer to that resource, such as a physical ID.</li>
</ul>
</li>
<li>JSON Format { “Ref” : “logicalName” }</li>
</ul>
</li>
</ul>
</li>
<li><p>CloudFormation limit</p>
<ul>
<li>You can include up to 60 parameters and 60 outputs in a template.</li>
<li>There are no limit to the number of templates.</li>
<li>Each AWS CloudFormation account is limited to a maximum of 200 stacks.</li>
</ul>
</li>
<li><p>CloudFormation – Ref, Fn::Join, GetAtt, Fn::split, Fn::select and etc function</p>
</li>
</ul>
<h2 id="Doc"><a href="#Doc" class="headerlink" title="Doc"></a>Doc</h2><h3 id="DynamoDB-Doc"><a href="#DynamoDB-Doc" class="headerlink" title="DynamoDB Doc"></a>DynamoDB Doc</h3><ul>
<li><a href="https://docs.aws.amazon.com/zh_cn/amazondynamodb/latest/developerguide/Introduction.html" target="_blank" rel="external">DynamoDB Developer Guide (Html)</a></li>
<li><a href="https://docs.aws.amazon.com/zh_cn/amazondynamodb/latest/developerguide/dynamodb-dg-zh_cn.pdf" target="_blank" rel="external">DynamoDB Developer Guide (PDF)</a></li>
<li><a href="https://amazonaws-china.com/dynamodb/faqs/" target="_blank" rel="external">DynamoDB FAQ</a></li>
</ul>
<h3 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h3><ul>
<li><a href="https://amazonaws-china.com/cn/dynamodb/faqs/?nc1=h_ls" target="_blank" rel="external">https://amazonaws-china.com/cn/dynamodb/faqs/?nc1=h_ls</a></li>
<li><a href="https://amazonaws-china.com/lambda/faqs/" target="_blank" rel="external">https://amazonaws-china.com/lambda/faqs/</a></li>
<li><a href="https://amazonaws-china.com/api-gateway/faqs/" target="_blank" rel="external">https://amazonaws-china.com/api-gateway/faqs/</a></li>
</ul>
<h3 id="考点"><a href="#考点" class="headerlink" title="考点"></a>考点</h3><ul>
<li><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers.html" target="_blank" rel="external">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers.html</a></li>
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html" target="_blank" rel="external">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a><ul>
<li>Template Sections</li>
</ul>
</li>
<li><a href="https://aws.amazon.com/security/penetration-testing/" target="_blank" rel="external">https://aws.amazon.com/security/penetration-testing/</a></li>
</ul>
<h3 id="注意点摘录"><a href="#注意点摘录" class="headerlink" title="注意点摘录:"></a>注意点摘录:</h3><ul>
<li>筛选表达式(–filter-expression)在 Scan 已完成但结果尚未返回时应用。因此，无论是否存在筛选表达式，Scan 都将占用同等数量的读取容量</li>
</ul>
<h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><ul>
<li>scalar data types –&gt; 标量数据类型</li>
<li>collection data types –&gt; 集合数据类型</li>
<li>projection –&gt; 投影</li>
<li>Fine Grained Access Control (FGAC)  –&gt; 精细访问控制</li>
<li>write-through –&gt; 直写</li>
<li>optimistic – 乐观</li>
</ul>
<h2 id="外部资料"><a href="#外部资料" class="headerlink" title="外部资料"></a>外部资料</h2><ul>
<li>老外心得 <a href="https://acloud.guru/forums/aws-certified-developer-associate/discussion/-KUdI5f2LNbi4wvK7v4I/how-to-pass-aws-certified-developer-exam" target="_blank" rel="external">https://acloud.guru/forums/aws-certified-developer-associate/discussion/-KUdI5f2LNbi4wvK7v4I/how-to-pass-aws-certified-developer-exam</a></li>
<li>老外心得 <a href="https://acloud.guru/forums/aws-certified-developer-associate/discussion/-KPuWHwfTCiCJNsGzAwu/passed" target="_blank" rel="external">https://acloud.guru/forums/aws-certified-developer-associate/discussion/-KPuWHwfTCiCJNsGzAwu/passed</a></li>
<li>re:Invent videos. “Deep Dive on Amazon DynamoDB” <a href="https://www.youtube.com/watch?v=bCW3lhsJKfw" target="_blank" rel="external">https://www.youtube.com/watch?v=bCW3lhsJKfw</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS Certified </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DynamoDB </tag>
            
            <tag> AWS SDK </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用Hexo搭建Blog]]></title>
      <url>/2017/07/17/hexo-github-blog/</url>
      <content type="html"><![CDATA[<h2 id="Hexo-安装"><a href="#Hexo-安装" class="headerlink" title="Hexo 安装"></a>Hexo 安装</h2><h3 id="安装前提"><a href="#安装前提" class="headerlink" title="安装前提"></a>安装前提</h3><p>安装Hexo需要依赖如下两个程序, 需要提前安装</p>
<ul>
<li>Node.js</li>
<li>git</li>
</ul>
<h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><p>Node.js和git都安装完毕后，执行如下命令安装Hexo</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install -g hexo-cli</div></pre></td></tr></table></figure>
<h3 id="初始化Blog"><a href="#初始化Blog" class="headerlink" title="初始化Blog"></a>初始化Blog</h3><p>cd到存放博客的目标目录，执行<code>hexo init</code>命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo init my_blog</div></pre></td></tr></table></figure>
<p>其中<code>my_blog</code>就是博客所在的文件夹名字。</p>
<p>注意: 最新版的<code>hexo</code>不需要切换到文件夹下敲击<code>npm install</code>了，<code>init</code>的时候会一并安装所需的npm packet。</p>
<p>进入目录，目录结构类似如下.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">$ cd my_blog/</div><div class="line">$ tree -I &quot;node_modules&quot; ./</div><div class="line">./</div><div class="line">├── _config.yml</div><div class="line">├── db.json</div><div class="line">├── package.json</div><div class="line">├── scaffolds</div><div class="line">│   ├── draft.md</div><div class="line">│   ├── page.md</div><div class="line">│   └── post.md</div><div class="line">├── source</div><div class="line">│   └── _posts</div><div class="line">│       └── hello-world.md</div><div class="line">└── themes</div></pre></td></tr></table></figure>
<p>执行下面的命令开启<code>hexo</code>服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo -s --debug</div></pre></td></tr></table></figure>
<p>访问 <code>http://0.0.0.0:4000</code>应该就能看到默认的页面了。</p>
<a id="more"></a>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="基本信息配置"><a href="#基本信息配置" class="headerlink" title="基本信息配置"></a>基本信息配置</h3><p>打开博客根目录下的<code>_config.yml</code>文件，修改<code>title</code>, <code>subtitle</code>, <code>description</code>, <code>author</code>, <code>url</code>等个人信息</p>
<p>将<code>language</code>设置为<code>default</code>.</p>
<p>配置文件中默认参数的描述可以参见官网说明 <a href="https://hexo.io/zh-cn/docs/configuration.html" target="_blank" rel="external">https://hexo.io/zh-cn/docs/configuration.html</a></p>
<h2 id="更换Theme"><a href="#更换Theme" class="headerlink" title="更换Theme"></a>更换Theme</h2><p>以下以更换<a href="(https://github.com/wzpan/hexo-theme-freemind/)">hexo-theme-freemind</a>主题为例:</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>使用如下命令安装<a href="(https://github.com/wzpan/hexo-theme-freemind/)">hexo-theme-freemind</a>主题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git clone https://github.com/wzpan/hexo-theme-freemind.git themes/freemind</div></pre></td></tr></table></figure>
<p>安装可选插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-tag-bootstrap --save</div><div class="line">$ npm install hexo-tag-bootstrap --save</div></pre></td></tr></table></figure>
<h3 id="启用freemind预定义的几个pages"><a href="#启用freemind预定义的几个pages" class="headerlink" title="启用freemind预定义的几个pages"></a>启用freemind预定义的几个pages</h3><p>Freemind 预先定义了 Categories（分类）、Tags（标签） 和 About（关于）页面，要使用它们，你需要先在博客的<code>source</code>目录中添加相应页面。</p>
<p>使用如下命令来生成几个Pages页面</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ hexo new page &quot;tags&quot;</div><div class="line">$ hexo new page &quot;categories&quot;</div><div class="line">$ hexo new page &quot;about&quot;</div></pre></td></tr></table></figure>
<p>修改生成的目录下的<code>index.md</code>文件为如下内容:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">$ cat about/index.md</div><div class="line">---</div><div class="line">title: About</div><div class="line">layout: about</div><div class="line">---</div><div class="line">$ cat categories/index.md</div><div class="line">---</div><div class="line">title: Categories</div><div class="line">layout: categories</div><div class="line">---</div><div class="line">$ cat tags/index.md</div><div class="line">---</div><div class="line">title: Tags</div><div class="line">layout: tags</div><div class="line">---</div><div class="line">$</div></pre></td></tr></table></figure>
<h3 id="启用-freemind"><a href="#启用-freemind" class="headerlink" title="启用 freemind"></a>启用 freemind</h3><p>在根目录_config.yml中，替换<code>theme</code>选项为<code>freemind</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># Extensions</div><div class="line">## Plugins: https://hexo.io/plugins/</div><div class="line">## Themes: https://hexo.io/themes/</div><div class="line"># theme: landscape</div><div class="line">theme: freemind</div></pre></td></tr></table></figure>
<h3 id="修改freemind的配置文件"><a href="#修改freemind的配置文件" class="headerlink" title="修改freemind的配置文件"></a>修改freemind的配置文件</h3><ul>
<li>修改<code>slogan</code></li>
<li>修改<code>links</code>为自己想要链接的网址</li>
<li>暂时不想开启评论，因此注释掉了<code>comment_js</code></li>
<li>修改<code>theme</code>来调整color theme, <code>freemind</code>所支持的<a href="http://www.hahack.com/hexo-theme-freemind/2016/01/30/color-themes/" target="_blank" rel="external">color theme</a></li>
</ul>
<p>freemind配置文件的详细解释参见<a href="(https://github.com/wzpan/hexo-theme-freemind/)">freemind github</a>中的Configuration章节。</p>
<h3 id="freemind-front-matter-选项"><a href="#freemind-front-matter-选项" class="headerlink" title="freemind front-matter 选项"></a>freemind front-matter 选项</h3><p>根据<a href="(https://github.com/wzpan/hexo-theme-freemind/)">Github</a>中的描述，<code>freemind</code>共提供了如下5个设置:</p>
<ul>
<li>description - a short description about the articles that will be display at the top of the post</li>
<li>feature - sets a feature image that will be show at the index page</li>
<li>toc - renders a table of contents</li>
<li>top - pin the article to top if it is set to true</li>
<li>issue_id - comment.js issue_id for explicitly point out which Github issue should be connect to your post. For most situations you don’t need it unless the post doesn’t link to the issue you want.</li>
</ul>
<p>以Hexo默认生成的<code>_posts/hello-world.md</code>的为例来展示<code>description</code>,<code>feature</code>,<code>toc</code>,<code>top</code>四个设置的显示效果。</p>
<p>首先修改后的<code>hello-world.md</code>文件头部如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">title: Hello World</div><div class="line">description: Add description of freemind to title &quot;Hello World&quot;</div><div class="line">feature: images/7217667e206c9bec45dbddb7b608bcb4.jpg</div><div class="line">toc: true</div><div class="line">top: true</div></pre></td></tr></table></figure>
<p>设置<code>description</code>和<code>toc</code>后的效果如下:</p>
<p><img src="/images//Hexo/Starting/freemind_description_toc.jpg" alt="info"></p>
<p>设置<code>feature</code>和<code>top</code>后的显示效果如下:</p>
<p><img src="/images//Hexo/Starting/freemind_top_feature.jpg" alt="info"></p>
<h3 id="添加统计"><a href="#添加统计" class="headerlink" title="添加统计"></a>添加统计</h3><p>开启<a href="https://tongji.baidu.com" target="_blank" rel="external">百度统计</a></p>
<p>freemind自带百度统计功能，在主题的_config.yml中找到Analytics, 设置baidu_tongji一栏下面的enable为true，再添加上siteid即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># Analytics</div><div class="line">google_analytics:</div><div class="line">  enable: false</div><div class="line">  siteid:</div><div class="line">baidu_tongji:</div><div class="line">  enable: true</div><div class="line">  siteid: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</div></pre></td></tr></table></figure>
<h2 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h2><h3 id="安装豆瓣插件"><a href="#安装豆瓣插件" class="headerlink" title="安装豆瓣插件"></a>安装豆瓣插件</h3><p>安装豆瓣插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm install hexo-generator-douban --save</div></pre></td></tr></table></figure>
<p>在hexo的<code>_config.yml</code>的<code>Extensions</code>设置下添加如下配置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">douban:</div><div class="line">    user: douban_id</div></pre></td></tr></table></figure>
<p>在<code>freemind</code>的<code>_config.yml</code>的<code>menu</code>项下添加豆瓣的page页面</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">- title: Douban</div><div class="line">  url: douban</div><div class="line">  intro: &quot;Douban&quot;</div><div class="line">  icon: &quot;fa fa-book&quot;</div></pre></td></tr></table></figure>
<p>重启一下就能看到主页中Douban的page了。</p>
<h3 id="安装rss插件"><a href="#安装rss插件" class="headerlink" title="安装rss插件"></a>安装rss插件</h3><p>添加<code>hexo-generator-feed</code>插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-generator-feed</div></pre></td></tr></table></figure>
<p>修改_config.yml，添加Extensions</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># Feed Atom</div><div class="line">feed:</div><div class="line">  type: atom</div><div class="line">  path: atom.xml</div><div class="line">  limit: 20</div><div class="line">  hub:</div><div class="line">  content:</div></pre></td></tr></table></figure>
<p>设置完成后，可以访问<code>http://jibing57.github.io/atom.xml</code>来检验是否成功生成。</p>
<h3 id="安装sitemap"><a href="#安装sitemap" class="headerlink" title="安装sitemap"></a>安装sitemap</h3><p>添加<code>hexo-generator-sitemap</code>插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-generator-sitemap --save</div></pre></td></tr></table></figure>
<p>修改_config.yml, 添加Extensions</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sitemap:</div><div class="line">  path: sitemap.xml</div></pre></td></tr></table></figure>
<p>设置完成后，访问<code>http://jibing57.github.io/sitemap.xml</code>来检验是否成功生成了sitemap</p>
<h3 id="安装搜索插件"><a href="#安装搜索插件" class="headerlink" title="安装搜索插件"></a>安装搜索插件</h3><p>添加<code>hexo-generator-search</code>插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-generator-search --save</div></pre></td></tr></table></figure>
<h2 id="post相关"><a href="#post相关" class="headerlink" title="post相关"></a>post相关</h2><h3 id="修改默认的post脚手架"><a href="#修改默认的post脚手架" class="headerlink" title="修改默认的post脚手架"></a>修改默认的post脚手架</h3><p>修改<code>scaffolds/post.md</code>，添加如下freemind支持的<code>Front-matter</code>, 每次<code>hexo new post xx</code>的时候，就自动会生成到新的post文件中，不用每次手动生成了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">title: &#123;&#123; title &#125;&#125;</div><div class="line">date: &#123;&#123; date &#125;&#125;</div><div class="line">categories:</div><div class="line">tags:</div><div class="line">description:</div><div class="line">feature:</div><div class="line">toc: true</div><div class="line">---</div></pre></td></tr></table></figure>
<h3 id="调整post的侧边栏"><a href="#调整post的侧边栏" class="headerlink" title="调整post的侧边栏"></a>调整post的侧边栏</h3><p>将Toc调整为显示时间之下，categories和tags之上，并且调整显示时间为精确到秒</p>
<p>修改调整 <code>themes/freemind/layout/_partial/post/meta.ejs</code>的内容如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&lt;!-- date --&gt;</div><div class="line">&lt;% if (item.date) &#123; %&gt;</div><div class="line">&lt;div class=&quot;meta-widget&quot;&gt;</div><div class="line">&lt;i class=&quot;fa fa-clock-o&quot;&gt;&lt;/i&gt;</div><div class="line">&lt;%= item.date.format(config.date_format)+&apos; &apos;+item.date.format(config.time_format) %&gt;</div><div class="line">&lt;/div&gt;</div><div class="line">&lt;% &#125; %&gt;</div><div class="line"></div><div class="line">&lt;!-- toc --&gt;</div><div class="line">&lt;div class=&quot;meta-widget&quot;&gt;</div><div class="line">&lt;% if(item.toc)&#123; %&gt;</div><div class="line">   &lt;a data-toggle=&quot;collapse&quot; data-target=&quot;#toc&quot;&gt;&lt;i class=&quot;fa fa-bars&quot;&gt;&lt;/i&gt;&lt;/a&gt;</div><div class="line">   &lt;div id=&quot;toc&quot; class=&quot;toc collapse in&quot;&gt;</div><div class="line">		&lt;%- toc(item.content, &#123;class: &quot;toc-article&quot;, list_number:false&#125;) %&gt;</div><div class="line">	&lt;/div&gt;</div><div class="line">&lt;% &#125; %&gt;</div><div class="line">&lt;/div&gt;</div></pre></td></tr></table></figure>
<h3 id="Post文章在列表中的预览"><a href="#Post文章在列表中的预览" class="headerlink" title="Post文章在列表中的预览"></a>Post文章在列表中的预览</h3><p>默认情况下，在列表预览中会将所有的文章内容都显示出来，会显得比较冗余，可以在Post文章中，添加<code>&lt;!-- more --&gt;</code>预览标签，这样列表预览中只会显示文章开头到<code>&lt;!-- more --&gt;</code>预览标签之间的文字。</p>
<h3 id="多tag"><a href="#多tag" class="headerlink" title="多tag"></a>多tag</h3><p>给文章设置多个tags的方法:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 方法1</div><div class="line">tags: [tag1,tag2,tag3]</div><div class="line"></div><div class="line">#方法2</div><div class="line">tags:</div><div class="line">  - tag1</div><div class="line">  - tag2</div><div class="line">  - tag3</div></pre></td></tr></table></figure>
<h2 id="deploy部署至Github-Pages"><a href="#deploy部署至Github-Pages" class="headerlink" title="deploy部署至Github Pages"></a>deploy部署至Github Pages</h2><h3 id="deploy至Github-Pages"><a href="#deploy至Github-Pages" class="headerlink" title="deploy至Github Pages"></a>deploy至Github Pages</h3><p>首先需要在<code>github</code>上创建<code>jibing57.github.io</code>的repository, 其中<code>jibing57</code>需要替换为自己的<code>github</code>的用户名, 还需要在<code>~/.ssh/config</code>中设置好访问<code>github</code>的私钥。</p>
<p>安装Hexo的扩展</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-deployer-git --save</div></pre></td></tr></table></figure>
<p>修改<code>_config.yml</code>中<code>deploy</code>一栏的设置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">deploy:</div><div class="line">  type: git</div><div class="line">  repo: https://github.com/jibing57/jibing57.github.io.git</div><div class="line">  branch: master</div></pre></td></tr></table></figure>
<p>使用以下命令发布到Github Pages</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo d</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Blog </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Github </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何给本地视频生成字幕]]></title>
      <url>/2017/07/17/How-to-get-subtitle-of-local-video/</url>
      <content type="html"><![CDATA[<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>适合如下场景:</p>
<ul>
<li>视频本身不带字幕</li>
<li>直接看英文视频比较吃力</li>
</ul>
<p>此时可以使用<code>autosub</code>这个工具来根据视频中的声音来生成对应的字幕，虽然生成的字幕并不总是那么正确，但当做参考还是不错的。</p>
<h2 id="autosub简介"><a href="#autosub简介" class="headerlink" title="autosub简介"></a>autosub简介</h2><p><a href="https://github.com/agermanidis/autosub" target="_blank" rel="external">autosub</a>是一款由python 2编写的通过Google Web Speech API和FFmpeg来获取视频subtitle的软件</p>
<p>Github地址: <a href="https://github.com/agermanidis/autosub" target="_blank" rel="external">https://github.com/agermanidis/autosub</a></p>
<p>Github上的Usage:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">$ autosub -h</div><div class="line">usage: autosub [-h] [-C CONCURRENCY] [-o OUTPUT] [-F FORMAT] [-S SRC_LANGUAGE]</div><div class="line">               [-D DST_LANGUAGE] [-K API_KEY] [--list-formats]</div><div class="line">               [--list-languages]</div><div class="line">               [source_path]</div><div class="line"></div><div class="line">positional arguments:</div><div class="line">  source_path           Path to the video or audio file to subtitle</div><div class="line"></div><div class="line">optional arguments:</div><div class="line">  -h, --help            show this help message and exit</div><div class="line">  -C CONCURRENCY, --concurrency CONCURRENCY</div><div class="line">                        Number of concurrent API requests to make</div><div class="line">  -o OUTPUT, --output OUTPUT</div><div class="line">                        Output path for subtitles (by default, subtitles are</div><div class="line">                        saved in the same directory and name as the source</div><div class="line">                        path)</div><div class="line">  -F FORMAT, --format FORMAT</div><div class="line">                        Destination subtitle format</div><div class="line">  -S SRC_LANGUAGE, --src-language SRC_LANGUAGE</div><div class="line">                        Language spoken in source file</div><div class="line">  -D DST_LANGUAGE, --dst-language DST_LANGUAGE</div><div class="line">                        Desired language for the subtitles</div><div class="line">  -K API_KEY, --api-key API_KEY</div><div class="line">                        The Google Translate API key to be used. (Required for</div><div class="line">                        subtitle translation)</div><div class="line">  --list-formats        List all available subtitle formats</div><div class="line">  --list-languages      List all available source/destination languages</div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="Macos-下安装方法"><a href="#Macos-下安装方法" class="headerlink" title="Macos 下安装方法"></a>Macos 下安装方法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">brew install ffmpeg</div><div class="line">pip install autosub</div></pre></td></tr></table></figure>
<h2 id="批量生成"><a href="#批量生成" class="headerlink" title="批量生成"></a>批量生成</h2><p>原程序参数貌似不支持多文件，基于自身需要，写了个粗糙的小脚本来批量转换整个目录下的视频文件。目前够用，后续如果有子目录以及除mp4的其他格式的需求，再改进</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">USAGE=&quot;USAGE: $0 dir|file&quot;</div><div class="line"></div><div class="line">if [ &quot;$1&quot; == &quot;&quot; ]; then</div><div class="line">    echo $USAGE</div><div class="line">    exit 1</div><div class="line">fi</div><div class="line"></div><div class="line">if [ -f $1 ]; then</div><div class="line">    autosub $1</div><div class="line">    exit 1</div><div class="line">fi</div><div class="line"></div><div class="line">if [ ! -d $1 ]; then</div><div class="line">    echo $USAGE</div><div class="line">    exit 1</div><div class="line">fi</div><div class="line"></div><div class="line">base_dir=$1</div><div class="line">echo &quot;Start to use autosub to generate subtitle for video in $base_dir&quot;</div><div class="line">echo &quot;cd to $base_dir&quot;</div><div class="line">echo &quot;**************************************************&quot;</div><div class="line">echo &quot;&quot;</div><div class="line">cd $base_dir</div><div class="line">file_list=`ls *.mp4`</div><div class="line">for file in $file_list</div><div class="line">do</div><div class="line">    echo &quot;oooooooooooooooooooooooooo&quot;</div><div class="line">    echo &quot; ==== Start to process file - $&#123;file&#125;&quot;</div><div class="line">    file_basename=`basename $file`</div><div class="line">    subtitle_file_name=&quot;$&#123;file_basename%.*&#125;.srt&quot;</div><div class="line">    if [ -e $subtitle_file_name ]; then</div><div class="line">        echo &quot;Subtitle of $file has already existed. don&apos;t need to process again&quot;</div><div class="line">    else</div><div class="line">        autosub $file</div><div class="line">    fi</div><div class="line">    echo &quot; ==== Finish to process file - $&#123;file&#125;&quot;</div><div class="line">    echo &quot;oooooooooooooooooooooooooo&quot;</div><div class="line">    echo &quot;&quot;</div><div class="line">done</div><div class="line"></div><div class="line">echo &quot;&quot;</div><div class="line">echo &quot;**************************************************&quot;</div><div class="line">echo &quot;Start to use autosub to generate subtitle for video in $base_dir&quot;</div><div class="line">echo &quot;**************************************************&quot;</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Video </tag>
            
            <tag> Tips </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
