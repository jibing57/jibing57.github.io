<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[PostgreSQL Tips]]></title>
      <url>/2019/10/27/postgresql-tips/</url>
      <content type="html"><![CDATA[<p>收录一些PostgreSQL日常使用的小命令和小配置，备查。</p>
<hr>
<h3 id="将sql结果导入csv文件"><a href="#将sql结果导入csv文件" class="headerlink" title="将sql结果导入csv文件"></a>将sql结果导入csv文件</h3><p>使用\copy命令将sql结果导入csv文件, postgresql命令行中输入以下命令:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">\copy (Select * From foo) To &apos;/tmp/test.csv&apos; With CSV HEADER</div></pre></td></tr></table></figure></p>
<p>Reference :<a href="http://stackoverflow.com/questions/1517635/save-pl-pgsql-output-from-postgresql-to-a-csv-file" target="_blank" rel="external">http://stackoverflow.com/questions/1517635/save-pl-pgsql-output-from-postgresql-to-a-csv-file</a></p>
<a id="more"></a>
<hr>
<h3 id="将shapefile导入postgresql的步骤"><a href="#将shapefile导入postgresql的步骤" class="headerlink" title="将shapefile导入postgresql的步骤"></a>将shapefile导入postgresql的步骤</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">$ createdb -U postgres shape_test</div><div class="line">$ psql -U postgres shape_test -c &quot;CREATE EXTENSION postgis;&quot;;</div><div class="line">CREATE EXTENSION</div><div class="line">$ shp2pgsql -s 4326 SAMPLE_mt_boundary_201510.shp &gt; shape_file.sql</div><div class="line">Shapefile type: Polygon</div><div class="line">Postgis type: MULTIPOLYGON[2]</div><div class="line">$ psql -U postgres shape_test &lt; shape_file.sql</div><div class="line">SET</div><div class="line">SET</div><div class="line">BEGIN</div><div class="line">CREATE TABLE</div><div class="line">ALTER TABLE</div><div class="line">                             addgeometrycolumn</div><div class="line">---------------------------------------------------------------------------</div><div class="line"> public.sample_mt_boundary_201510.geom SRID:4326 TYPE:MULTIPOLYGON DIMS:2</div><div class="line">(1 row)</div><div class="line"></div><div class="line">INSERT 0 1</div><div class="line">INSERT 0 1</div><div class="line">COMMIT</div><div class="line">$</div></pre></td></tr></table></figure>
<p>Reference :</p>
<ul>
<li><a href="http://gis.stackexchange.com/a/41802" target="_blank" rel="external">http://gis.stackexchange.com/a/41802</a></li>
<li><a href="http://www.bostongis.com/pgsql2shp_shp2pgsql_quickguide_20.bqg" target="_blank" rel="external">http://www.bostongis.com/pgsql2shp_shp2pgsql_quickguide_20.bqg</a></li>
</ul>
<hr>
<h3 id="pg-dump多表到一个sql文件中"><a href="#pg-dump多表到一个sql文件中" class="headerlink" title="pg_dump多表到一个sql文件中"></a>pg_dump多表到一个sql文件中</h3><p>可以使用多个-t 参数来dump多张表<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ pg_dump -U postgres db_name -t table1 -t table 2 &gt; output_sql.sql</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="如何不dump表结构，只dump数据"><a href="#如何不dump表结构，只dump数据" class="headerlink" title="如何不dump表结构，只dump数据"></a>如何不dump表结构，只dump数据</h3><p>可以使用<code>--data-only</code>来实现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ pg_dump --data-only -U postgres db_name -t table1 &gt; output_sql.sql</div></pre></td></tr></table></figure></p>
<p>如果结合<code>--column-inserts</code>参数使用的话，那么就会为每条记录都生成一个insert语句</p>
<p>使用<code>--column-inserts</code>的缺点:</p>
<ol>
<li>sql文件会比较大</li>
<li>从sql恢复(restore)的时候也会很慢.</li>
<li>恢复的时候是单条记录恢复，一条记录错误不会导致整个table的恢复失败, 容易造成数据不完整。</li>
</ol>
<p>但是适合于导出的sql会用于其他non-PostgreSQL的场景<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ pg_dump --column-inserts --data-only -U postgres db_name -t table1 &gt; output_sql.sql</div></pre></td></tr></table></figure></p>
<p>Reference : <a href="https://www.postgresql.org/docs/9.3/static/app-pgdump.html" target="_blank" rel="external">https://www.postgresql.org/docs/9.3/static/app-pgdump.html</a></p>
<hr>
<h3 id="在postgresql中建立一个只读用户"><a href="#在postgresql中建立一个只读用户" class="headerlink" title="在postgresql中建立一个只读用户"></a>在postgresql中建立一个只读用户</h3><p>建立用户的命令如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CREATE USER intern2019 WITH LOGIN ENCRYPTED PASSWORD &apos;x!&amp;LosA&amp;@4&apos; VALID UNTIL &apos;infinity&apos;;</div></pre></td></tr></table></figure></p>
<p>对用户授权只读权限的命令如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">GRANT USAGE ON SCHEMA public TO intern2019;</div><div class="line">GRANT SELECT ON ALL TABLES IN SCHEMA public To intern2019;  -- only effect by current table</div><div class="line">ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO intern2019; -- effect by all new table</div></pre></td></tr></table></figure>
<p>删除某个用户</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ALTER DEFAULT PRIVILEGES IN SCHEMA public revoke SELECT ON TABLES FROM intern2019</div><div class="line">REVOKE ALL PRIVILEGES ON ALL TABLES IN SCHEMA public FROM intern2019;</div><div class="line">REVOKE USAGE ON SCHEMA public FROM intern2019</div><div class="line"></div><div class="line">DROP USER intern2019;</div></pre></td></tr></table></figure>
<hr>
<h3 id="如何在group-by中统计基于group-by字段和其他条件的记录的总数"><a href="#如何在group-by中统计基于group-by字段和其他条件的记录的总数" class="headerlink" title="如何在group by中统计基于group by字段和其他条件的记录的总数"></a>如何在group by中统计基于group by字段和其他条件的记录的总数</h3><p>sum函数中，可以结合case when来统计符合某些条件的总数，在group by的时候，就可以顺带让Postgresql来检查计算符合条件的记录总数，比简单粗暴的子查询来的快速。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">select mls_id, mls_name, SUM( CASE WHEN (trs_type=&apos;long term rentals&apos; or trs_type=&apos;&apos; or trs_type is null) THEN 1 ELSE 0 END) as rental_num, SUM( CASE WHEN (trs=&apos;for sale&apos;) THEN 1 ELSE 0 END) as for_sale_num  from listings  where active=true and validity=true group by mls_id, mls_name order by mls_id</div></pre></td></tr></table></figure>
<p>Reference:</p>
<ul>
<li><a href="http://stackoverflow.com/questions/24030674/postgres-aggregating-conditional-sum" target="_blank" rel="external">http://stackoverflow.com/questions/24030674/postgres-aggregating-conditional-sum</a></li>
<li><a href="http://stackoverflow.com/questions/2045463/sql-query-to-conditionally-sum-based-on-moving-date-window" target="_blank" rel="external">http://stackoverflow.com/questions/2045463/sql-query-to-conditionally-sum-based-on-moving-date-window</a></li>
</ul>
<hr>
<h3 id="PG的时间函数"><a href="#PG的时间函数" class="headerlink" title="PG的时间函数"></a>PG的时间函数</h3><p>原字段加上<code>interval &#39;1 second&#39;</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">postgres=# select NOW(), NOW()+interval &apos;1 second&apos;;</div><div class="line">             now              |           ?column?</div><div class="line">------------------------------+------------------------------</div><div class="line"> 2017-01-09 14:49:38.52691+08 | 2017-01-09 14:49:39.52691+08</div><div class="line">(1 row)</div><div class="line"></div><div class="line">postgres=#</div></pre></td></tr></table></figure>
<hr>
<h3 id="查看数据库中各个表上次vacuum和autovacuum时间的sql"><a href="#查看数据库中各个表上次vacuum和autovacuum时间的sql" class="headerlink" title="查看数据库中各个表上次vacuum和autovacuum时间的sql"></a>查看数据库中各个表上次vacuum和autovacuum时间的sql</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">partition_test=# select relname,last_vacuum, last_autovacuum, last_analyze, last_autoanalyze from pg_stat_user_tables;</div><div class="line"> relname  |          last_vacuum          | last_autovacuum |         last_analyze          | last_autoanalyze</div><div class="line">----------+-------------------------------+-----------------+-------------------------------+------------------</div><div class="line"> user_new | 2017-05-10 09:30:06.156982+08 |                 | 2017-05-10 09:30:06.181667+08 |</div><div class="line"> users    | 2017-05-10 09:29:51.52591+08  |                 |                               |</div><div class="line">(2 rows)</div><div class="line"></div><div class="line">partition_test=#</div></pre></td></tr></table></figure>
<hr>
<h3 id="Postgis中比较geometry类型的值的方法"><a href="#Postgis中比较geometry类型的值的方法" class="headerlink" title="Postgis中比较geometry类型的值的方法"></a>Postgis中比较geometry类型的值的方法</h3><p>使用ST_Equals<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">select id, source, lat, lon, geom4326, geom, created_at, updated_at from properties where not ST_Equals(geom4326, ST_SetSRID(ST_MakePoint(lon, lat),  4326)) or not ST_Equals(geom, ST_SetSRID(ST_MakePoint(lon, lat), 4269)) limit 1;</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="PG中查看sql执行时间的方法"><a href="#PG中查看sql执行时间的方法" class="headerlink" title="PG中查看sql执行时间的方法"></a>PG中查看sql执行时间的方法</h3><p>使用<code>\timing</code>来显示sql执行时间，再次输入<code>\timing</code>关闭显示时间<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">postgres=# \timing</div><div class="line">Timing is on.</div><div class="line">postgres=# select 1;</div><div class="line"> ?column?</div><div class="line">----------</div><div class="line">        1</div><div class="line">(1 row)</div><div class="line"></div><div class="line">Time: 0.214 ms</div><div class="line">postgres=# \timing</div><div class="line">Timing is off.</div><div class="line">postgres=#</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="切割第一个字符串是不是数字的sql"><a href="#切割第一个字符串是不是数字的sql" class="headerlink" title="切割第一个字符串是不是数字的sql"></a>切割第一个字符串是不是数字的sql</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">select split_part(&apos;10059 Pine Glade Dr&apos;, &apos; &apos; ,1)~&apos;^([0-9]+\.?[0-9]*|\.[0-9]+)$&apos;;</div></pre></td></tr></table></figure>
<hr>
<h3 id="如何建索引时不锁表"><a href="#如何建索引时不锁表" class="headerlink" title="如何建索引时不锁表"></a>如何建索引时不锁表</h3><p>Postgresql创建index时，默认是要锁表的。可以通过添加<code>CONCURRENTLY</code>参数，在不锁表的情况下建立索引。<br>但需要多次扫描，因此建索引时间会比较久<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">create index CONCURRENTLY index_feeds_on_take_id on feeds(take_id);</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="一些常用的psql命令"><a href="#一些常用的psql命令" class="headerlink" title="一些常用的psql命令"></a>一些常用的psql命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">\l  -- 列出所有数据库,数据库字符集等</div><div class="line">\l+ -- 列出所有数据库, 也包含数据库size</div><div class="line">\c DATABASE_NAME  -- 使用某个数据库</div><div class="line">\d  -- 查看表列表,包含sequence</div><div class="line">\dt -- 查看数据库中的表，不包含sequence</div><div class="line">\d+ -- 查看表列表(带Size和Description)</div><div class="line">\d TABLE_NAME -- 查看表结构</div><div class="line">\d+ TABLE_NAME -- 查看表结构，更多信息，比如可显示子表等</div><div class="line">\df -- 列出所有的function</div><div class="line">\df+ FUNCTION_NAME -- 列出某个function的详细信息</div><div class="line">\x  -- 打开扩展显示</div><div class="line">\q  -- 退出</div></pre></td></tr></table></figure>
<hr>
<h3 id="数据库磁盘容量命令"><a href="#数据库磁盘容量命令" class="headerlink" title="数据库磁盘容量命令"></a>数据库磁盘容量命令</h3><p>数据库中所有表磁盘空间的命令,包含各个表的大小，索引的大小<br><a href="http://stackoverflow.com/questions/2596624/how-do-you-find-the-disk-size-of-a-postgres-postgresql-table-and-its-indexes" target="_blank" rel="external">http://stackoverflow.com/questions/2596624/how-do-you-find-the-disk-size-of-a-postgres-postgresql-table-and-its-indexes</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">SELECT</div><div class="line">    table_name,</div><div class="line">    pg_size_pretty(table_size) AS table_size,</div><div class="line">    pg_size_pretty(indexes_size) AS indexes_size,</div><div class="line">    pg_size_pretty(total_size) AS total_size</div><div class="line">FROM (</div><div class="line">    SELECT</div><div class="line">        table_name,</div><div class="line">        pg_table_size(table_name) AS table_size,</div><div class="line">        pg_indexes_size(table_name) AS indexes_size,</div><div class="line">        pg_total_relation_size(table_name) AS total_size</div><div class="line">    FROM (</div><div class="line">        SELECT (&apos;&quot;&apos; || table_schema || &apos;&quot;.&quot;&apos; || table_name || &apos;&quot;&apos;) AS table_name</div><div class="line">        FROM information_schema.tables</div><div class="line">    ) AS all_tables</div><div class="line">    ORDER BY total_size DESC</div><div class="line">) AS pretty_sizes</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="PG中重建索引"><a href="#PG中重建索引" class="headerlink" title="PG中重建索引"></a>PG中重建索引</h3><p>Postgresql中的索引是不停增长的，当数据被反复删除，更新后，索引文件会变得异常的巨大,称之为”bloated”。<br>使用<code>reindex</code>来重建索引，可以减少磁盘空间，提升索引效率。<br>但直接<code>reindex</code>重建索引会存在锁表现象。在PostgreSQL 12中才支持<code>reindex</code>的CONCURRENTLY参数。<br>12以下的版本中，只能通过手动新建和删除索引的方法来实现。方法如下:</p>
<ol>
<li><p>以schema是public,tablename是’feed_records’为例，列出该表下所有索引以及其大小的sql如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">SELECT</div><div class="line">    t.tablename,</div><div class="line">    indexname,</div><div class="line">    c.reltuples AS num_rows,</div><div class="line">    pg_size_pretty(pg_relation_size(quote_ident(t.tablename)::text)) AS table_size,</div><div class="line">    pg_size_pretty(pg_relation_size(quote_ident(indexrelname)::text)) AS index_size,</div><div class="line">    CASE WHEN indisunique THEN &apos;Y&apos;</div><div class="line">       ELSE &apos;N&apos;</div><div class="line">    END AS UNIQUE,</div><div class="line">    idx_scan AS number_of_scans,</div><div class="line">    idx_tup_read AS tuples_read,</div><div class="line">    idx_tup_fetch AS tuples_fetched</div><div class="line">FROM pg_tables t</div><div class="line">LEFT OUTER JOIN pg_class c ON t.tablename=c.relname</div><div class="line">LEFT OUTER JOIN</div><div class="line">    ( SELECT c.relname AS ctablename, ipg.relname AS indexname, x.indnatts AS number_of_columns, idx_scan, idx_tup_read, idx_tup_fetch, indexrelname, indisunique FROM pg_index x</div><div class="line">           JOIN pg_class c ON c.oid = x.indrelid</div><div class="line">           JOIN pg_class ipg ON ipg.oid = x.indexrelid</div><div class="line">           JOIN pg_stat_all_indexes psai ON x.indexrelid = psai.indexrelid )</div><div class="line">    AS foo</div><div class="line">    ON t.tablename = foo.ctablename</div><div class="line">WHERE t.schemaname=&apos;public&apos; and t.tablename=&apos;feed_records&apos;</div><div class="line">ORDER BY 1,2;</div></pre></td></tr></table></figure>
</li>
<li><p>选一个负载小的时间，重建索引。步骤:</p>
<ul>
<li>新增一个同原索引内容相同的临时索引A’</li>
<li>删除原来的索引A</li>
<li>重新建原来的索引A</li>
<li>删除新建的临时索引A’<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">以重建feed_records中last_update_time字段的索引为例</div><div class="line">\d feed_records;</div><div class="line">create index CONCURRENTLY xxx_index_feed_records_on_last_update_time on feed_records(last_update_time);</div><div class="line">drop index index_feed_records_on_last_update_time;</div><div class="line">create index CONCURRENTLY index_feed_records_on_last_update_time on feed_records(last_update_time);</div><div class="line">drop index xxx_index_feed_records_on_last_update_time;</div><div class="line">\d feed_records;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<hr>
<h3 id="数据库导出和恢复"><a href="#数据库导出和恢复" class="headerlink" title="数据库导出和恢复"></a>数据库导出和恢复</h3><p>数据库导出和恢复命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># 导出数据和表，不包含drop语句 </div><div class="line">pg_dump --host localhost --username postgres --dbname mydatabase &gt; db.sql</div><div class="line"># 导出数据和表，添加-c参数后，会包含drop语句 </div><div class="line">pg_dump --host localhost --username postgres --dbname mydatabase -c &gt; db.sql</div><div class="line"># 恢复表</div><div class="line">psql --host localhost --username postgres --dbname mydatabase &lt; db.sql</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="pg-dump和pg-restore加速"><a href="#pg-dump和pg-restore加速" class="headerlink" title="pg_dump和pg_restore加速"></a>pg_dump和pg_restore加速</h3><p>pg_dump在大数据量下dump和restore速度很慢，可以使用-j参数来起多个job并行操作。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ pg_dump --host localhost -j8 -Fd -v -Uintern -d web_v2_staging -f web_v2_staging_dump</div><div class="line">$ pg_restore --verbose --clean --no-acl --no-owner -h localhost -d web_v3_staging -Uintern -j8 --format=d web_v2_staging_dump</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="输出结果中显示NULL字串"><a href="#输出结果中显示NULL字串" class="headerlink" title="输出结果中显示NULL字串"></a>输出结果中显示NULL字串</h3><p>psql的默认输出格式中，没法区分NULL和空字串，使用<code>\pset null (null)</code>来显示NULL<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">postgres=# \pset null (null);</div><div class="line">Null display is &quot;(null);&quot;.</div><div class="line">postgres=# select NULL;</div><div class="line"> ?column?</div><div class="line">----------</div><div class="line"> (null);</div><div class="line">(1 row)</div><div class="line"></div><div class="line">postgres=#</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="explain-analyze用来定位慢查询"><a href="#explain-analyze用来定位慢查询" class="headerlink" title="explain analyze用来定位慢查询"></a>explain analyze用来定位慢查询</h3><p>针对慢查询，使用<code>explain</code>和<code>explain analyze</code>来定位问题。<br><code>explain</code>只做评估，并不真正执行sql。<br><code>explain analyze</code>会直接运行sql。<br>一般还需开启<code>\timing</code>来使用</p>
<hr>
<h3 id="修改用户密码"><a href="#修改用户密码" class="headerlink" title="修改用户密码"></a>修改用户密码</h3><p>修改密码的命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ALTER ROLE username WITH PASSWORD &apos;password&apos;;</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="查看所有的参数"><a href="#查看所有的参数" class="headerlink" title="查看所有的参数"></a>查看所有的参数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">show all</div></pre></td></tr></table></figure>
<hr>
<h3 id="AWS-RDS-Postgresql-master-replicate要修改的几个配置"><a href="#AWS-RDS-Postgresql-master-replicate要修改的几个配置" class="headerlink" title="AWS RDS Postgresql master/replicate要修改的几个配置"></a>AWS RDS Postgresql master/replicate要修改的几个配置</h3><p>Master参数修改<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wal_keep_segments: 需要需要设大一点，否则写过多的话，会导致replica暂停流复制，从S3下载archived WAL来恢复数据</div><div class="line">random_page_cost: 设为seq_page_cost一样的值，默认random_page_cost是4，seq_page_cost是1。SSD磁盘下可以认为顺序查找和随机查找开销是一样的。</div></pre></td></tr></table></figure></p>
<p>Replica参数修改<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hot_standby_feedback: 设为1，防止replica查询还在执行时，Master已经将受影响的数据清理掉并同步给replica，导致replica上的查询失败。</div><div class="line">random_page_cost: 设为seq_page_cost一样的值，默认random_page_cost是4，seq_page_cost是1。SSD磁盘下可以认为顺序查找和随机查找开销是一样的。</div></pre></td></tr></table></figure></p>
<p><strong>Reference:</strong> <a href="https://amazonaws-china.com/blogs/database/best-practices-for-amazon-rds-postgresql-replication/" target="_blank" rel="external">Best practices for Amazon RDS PostgreSQL replication</a></p>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> PostgreSQL </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[设置AirPort Express时获取不到设备信息的解决方案]]></title>
      <url>/2019/09/30/apple-airport-express-access-unexpected-error-occurred/</url>
      <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>设置新airport express，使用AirPort Utility获取设备信息时，提示”An unexpected error occurred try again”。</p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>解决办法如下:</p>
<ol>
<li>打开系统设置”System Preferences”，选择网络”Network”</li>
<li>左侧选择连接设备的网络，”Wi-Fi”或者有线网络, 点击右下角的”Advanced..”</li>
<li>选择”TCP/IP”的tab</li>
<li>在”Configure IPv6”页面，选择”Link-local only”</li>
<li>点击”OK”和”Apply”</li>
</ol>
<a id="more"></a>
<p><img src="/images/Apple/network_setting.jpg" alt="network_setting"></p>
<p>在AirPort Utility中，就可以获取airport express的信息，从而进行设置了。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://www.howtoisolve.com/airport-express-unexpected-error-occurred-solved/" target="_blank" rel="external">Airport express an unexpected error has occurred Mojave: Here’s Solved</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Network </tag>
            
            <tag> MacOS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ssh设置私钥登录后还是提示需要输入密码]]></title>
      <url>/2019/09/29/still-password-when-setting-public-key-ssh-authorization/</url>
      <content type="html"><![CDATA[<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>在客户机器上配置好使用key免密登录后，尝试使用私钥登录，却被提示还是需要密码。<br>检查了一圈服务器上<code>~/.ssh/</code>, <code>~/.ssh/authorized_keys</code>的权限, 权限设置都没有问题。<br>可登录时还是提示需要输入密码。只能一步一步来检查调试</p>
<h3 id="解决步骤"><a href="#解决步骤" class="headerlink" title="解决步骤"></a>解决步骤</h3><ul>
<li>首先尝试在客户端ssh时加上-vv参数来输出debug信息, 看是否有明显的错误信息。<br>debug输出如下<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">debug1: SSH2_MSG_EXT_INFO received</div><div class="line">debug1: kex_input_ext_info: server-sig-algs=&lt;ssh-ed25519,ssh-rsa,rsa-sha2-256,rsa-sha2-512,ssh-dss,ecdsa-sha2-nistp256,ecdsa-sha2-nistp384,ecdsa-sha2-nistp521&gt;</div><div class="line">debug2: service_accept: ssh-userauth</div><div class="line">debug1: SSH2_MSG_SERVICE_ACCEPT received</div><div class="line">debug1: Authentications that can continue: publickey,password,keyboard-interactive</div><div class="line">debug1: Next authentication method: publickey</div><div class="line">debug1: Trying private key: /home/deployer/.ssh/id_rsa_sync</div><div class="line">debug2: we sent a publickey packet, wait for reply</div><div class="line">debug1: Authentications that can continue: publickey,password,keyboard-interactive</div></pre></td></tr></table></figure>
</li>
</ul>
<p>在正常情况下，<code>Trying private key: /home/deployer/.ssh/id_rsa_sync</code>之后，应该是提示密钥验证成功<code>Authentication succeeded (publickey).</code>并登录到服务器上。<br>这边是直接提示<code>Authentications that can continue: publickey,password,keyboard-interactive</code>，表示没有成功。<br><a id="more"></a></p>
<ul>
<li>client侧没有得到有效线索，转而到服务器侧查看日志<br>查看了<code>/var/log/secure</code>和<code>/var/log/messages</code>后，发现<code>/var/log/messages</code>中有一处关键信息<code>User deployer not allowed because account is locked</code>。<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">... sshd[14230]: User deployer not allowed because account is locked</div><div class="line">... sshd[14230]: Connection closed by invalid user deployer 192.168.101.192 port 44832 [preauth]</div></pre></td></tr></table></figure>
</li>
</ul>
<p>发现客户服务器的sshd_config中配置了<code>UsePAM no</code>, 而建立deployer用户时也没有设置密码，导致了拒绝ssh链接。</p>
<ul>
<li>两个解决方法</li>
</ul>
<ol>
<li>修改sshd_config中UsePAM为yes</li>
<li>为账号deployer加一个密码<code>passwd deployer</code></li>
</ol>
<p>限于不能修改客户机器的配置，采用了方法2.为deployer设置密码后，就可以使用私钥无密码登录了。</p>
<h3 id="总结几个免密登录失败的可能情况"><a href="#总结几个免密登录失败的可能情况" class="headerlink" title="总结几个免密登录失败的可能情况"></a>总结几个免密登录失败的可能情况</h3><ol>
<li>权限问题: <code>~/.ssh</code>需要设为700，<code>.ssh/authorized_keys</code>需要设为600</li>
<li>用户被锁定了。查看sshd_config的PAM配置和/etc/shadow中用户密码设置</li>
<li>SELINUX权限限制 </li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://github.com/camptocamp/puppet-accounts/issues/35" target="_blank" rel="external">User username not allowed because account is locked</a></li>
<li><a href="https://unix.stackexchange.com/questions/193066/how-to-unlock-account-for-public-key-ssh-authorization-but-not-for-password-aut" target="_blank" rel="external">How to unlock account for public key ssh authorization, but not for password authorization?</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> CentOS </tag>
            
            <tag> SSH </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[BorgBackup -- 基于命令行的增量备份工具]]></title>
      <url>/2019/09/24/backup-tools-borgbackup/</url>
      <content type="html"><![CDATA[<p><a href="https://www.borgbackup.org/" target="_blank" rel="external">BorgBackup</a>(简称 Borg)是一款基于命令行的增量备份工具。Github地址:<a href="https://github.com/borgbackup/borg" target="_blank" rel="external">https://github.com/borgbackup/borg</a><br>官网上所述的优势如下:</p>
<ul>
<li>Space efficient storage of backups. - 高效存储</li>
<li>Secure, authenticated encryption. - 可加密</li>
<li>Compression: LZ4, zlib, LZMA, zstd (since borg 1.1.4). - 支持多种压缩算法</li>
<li>Mountable backups with FUSE. - 可使用FUSE异地本非</li>
<li>Easy installation on multiple platforms: Linux, macOS, BSD, … - 支持多个平台</li>
<li>Free software (BSD license). - 开源</li>
<li>Backed by a large and active open source community.</li>
</ul>
<h3 id="CentOS下安装"><a href="#CentOS下安装" class="headerlink" title="CentOS下安装"></a>CentOS下安装</h3><p>borgbackup在EPEL中已有提供，CentOS 7下使用如下命令安装:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</div><div class="line">yum install -y borgbackup</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<h3 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h3><p>BorgBackup备份有两层概念,分为repository和archive, 顾名思义，repository就是一个备份仓库，archive就是每次备份。每次备份时，工具会检查各个archive，只备份有变动的数据。</p>
<p>BorgBackup几个基本的命令如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">borg init -- 初始化一个仓库</div><div class="line">borg create -- 创建一个archive到仓库中</div><div class="line">borg list -- 列出所有的仓库或者某个仓库中某个archive的内容</div><div class="line">borg extract -- 还原某个archive</div><div class="line">borg delete -- 手动删除某个archive</div><div class="line">borg config -- 获取或者设置某个配置</div></pre></td></tr></table></figure></p>
<p>一个简单的备份和恢复的流程如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div></pre></td><td class="code"><pre><div class="line"># 创建仓库存储目录</div><div class="line">$ mkdir -p /opt/backup/</div><div class="line"></div><div class="line"># 新建仓库</div><div class="line">$ borg init --encryption=repokey /opt/backup/borg_sample</div><div class="line"></div><div class="line"></div><div class="line"># 创建测试备份内容</div><div class="line"># 会被修改的文件file_change.txt，不会被修改的文件file_static.txt，二进制大文件random.dump</div><div class="line">$ mkdir -p /opt/data</div><div class="line">$ cd /opt/data</div><div class="line">$ echo &quot;here is the first line of file 1&quot; &gt;&gt; file_change.txt</div><div class="line">$ echo &quot;here is the static file&quot; &gt;&gt; file_static.txt</div><div class="line">$ dd if=/dev/urandom of=random.dump bs=10M count=10</div><div class="line">10+0 records in</div><div class="line">10+0 records out</div><div class="line">104857600 bytes (105 MB) copied, 0.950948 s, 110 MB/s</div><div class="line">$</div><div class="line"></div><div class="line"># 第一次备份</div><div class="line">$ borg create --stats /opt/backup/borg_sample::first /opt/data/</div><div class="line">------------------------------------------------------------------------------</div><div class="line">Archive name: first</div><div class="line">Archive fingerprint: 92bf20bca7a1d620d92f831e763601ca63ff951944de81146332ad12e93bb787</div><div class="line">Time (start): Tue, 2019-09-24 04:06:51</div><div class="line">Time (end):   Tue, 2019-09-24 04:06:52</div><div class="line">Duration: 0.82 seconds</div><div class="line">Number of files: 3</div><div class="line">Utilization of max. archive size: 0%</div><div class="line">------------------------------------------------------------------------------</div><div class="line">                       Original size      Compressed size    Deduplicated size</div><div class="line">This archive:              104.86 MB            105.27 MB            105.27 MB</div><div class="line">All archives:              104.86 MB            105.27 MB            105.27 MB</div><div class="line"></div><div class="line">                       Unique chunks         Total chunks</div><div class="line">Chunk index:                      48                   48</div><div class="line">------------------------------------------------------------------------------</div><div class="line">$</div><div class="line"></div><div class="line"># 修改备份内容,修改file_change.txt， 新增file_new.txt，新增random_2.dump</div><div class="line">$ cd /opt/data</div><div class="line">$ echo &quot;here is the second line of file 1&quot; &gt;&gt; file_change.txt</div><div class="line">$ echo &quot;here is new file for second backup&quot; &gt;&gt; file_new.txt</div><div class="line">$ dd if=/dev/urandom of=random_2.dump bs=10M count=10</div><div class="line"></div><div class="line">10+0 records in</div><div class="line">10+0 records out</div><div class="line">104857600 bytes (105 MB) copied, 0.599837 s, 175 MB/s</div><div class="line">$</div><div class="line"></div><div class="line"># 第二次备份</div><div class="line">$ borg create --stats /opt/backup/borg_sample::second /opt/data/</div><div class="line">------------------------------------------------------------------------------</div><div class="line">Archive name: second</div><div class="line">Archive fingerprint: a423a94e8a8f4352e72c0951e6a408f4f4f6d5f362518dcbcba77b9005dafa12</div><div class="line">Time (start): Tue, 2019-09-24 04:10:55</div><div class="line">Time (end):   Tue, 2019-09-24 04:10:56</div><div class="line">Duration: 1.26 seconds</div><div class="line">Number of files: 5</div><div class="line">Utilization of max. archive size: 0%</div><div class="line">------------------------------------------------------------------------------</div><div class="line">                       Original size      Compressed size    Deduplicated size</div><div class="line">This archive:              209.72 MB            210.55 MB            105.28 MB</div><div class="line">All archives:              314.58 MB            315.82 MB            210.55 MB</div><div class="line"></div><div class="line">                       Unique chunks         Total chunks</div><div class="line">Chunk index:                      92                  137</div><div class="line">------------------------------------------------------------------------------</div><div class="line">$</div><div class="line"></div><div class="line"># 列出所有的备份</div><div class="line">$ borg list /opt/backup/borg_sample/</div><div class="line"></div><div class="line">first                                Tue, 2019-09-24 04:06:51 [92bf20bca7a1d620d92f831e763601ca63ff951944de81146332ad12e93bb787]</div><div class="line">second                               Tue, 2019-09-24 04:10:55 [a423a94e8a8f4352e72c0951e6a408f4f4f6d5f362518dcbcba77b9005dafa12]</div><div class="line">$</div><div class="line"></div><div class="line"># 列出第一次备份的内容</div><div class="line">$ borg list /opt/backup/borg_sample::first</div><div class="line">drwxr-xr-x root   root          0 Tue, 2019-09-24 04:03:45 opt/data</div><div class="line">-rw-r--r-- root   root         33 Tue, 2019-09-24 04:03:45 opt/data/file_change.txt</div><div class="line">-rw-r--r-- root   root         24 Tue, 2019-09-24 04:03:45 opt/data/file_static.txt</div><div class="line">-rw-r--r-- root   root   104857600 Tue, 2019-09-24 04:03:46 opt/data/random.dump</div><div class="line">$</div><div class="line"></div><div class="line"># 将第一次备份导出, 查看file_change.txt内容，只包含第一次的内容</div><div class="line">$ mkdir -p /opt/restore/first</div><div class="line">$ cd /opt/restore/first</div><div class="line">$ borg extract --list /opt/backup/borg_sample::first</div><div class="line">opt/data</div><div class="line">opt/data/file_change.txt</div><div class="line">opt/data/file_static.txt</div><div class="line">opt/data/random.dump</div><div class="line">$</div><div class="line">$ cat opt/data/file_change.txt</div><div class="line">here is the first line of file 1</div><div class="line">$</div><div class="line"></div><div class="line"></div><div class="line"># 将第二次备份导出, 查看file_change.txt内容，其中包含了第二次新增加的内容</div><div class="line"># 也包含了第一次新增的file_new.txt和random_2.dump</div><div class="line">$ mkdir -p /opt/restore/second</div><div class="line">$ cd /opt/restore/second</div><div class="line">$ borg extract --list /opt/backup/borg_sample::second</div><div class="line">opt/data</div><div class="line">opt/data/file_change.txt</div><div class="line">opt/data/file_static.txt</div><div class="line">opt/data/random.dump</div><div class="line">opt/data/file_new.txt</div><div class="line">opt/data/random_2.dump</div><div class="line">$</div><div class="line">$ cat opt/data/file_change.txt</div><div class="line">here is the first line of file 1</div><div class="line">here is the second line of file 1</div><div class="line">$</div><div class="line"></div><div class="line"># 第一次备份是100M,第二次是200M, 但由于是增量备份的，random.dump没有改变，因此仓库的总容量只有200M</div><div class="line">$ du -sh /opt/backup/borg_sample/</div><div class="line">201M	/opt/backup/borg_sample/</div><div class="line">$</div><div class="line"></div><div class="line"># 删除第一次备份，恢复第二次的备份</div><div class="line"># 可以看到再第一次和第二次备份中都有的file_static.txt和random.dump都可以恢复出来。</div><div class="line"># 恢复出来的数据和之前恢复的数据是一样的</div><div class="line">$ borg delete /opt/backup/borg_sample::first</div><div class="line">$ borg list /opt/backup/borg_sample</div><div class="line">second                               Tue, 2019-09-24 04:10:55 [a423a94e8a8f4352e72c0951e6a408f4f4f6d5f362518dcbcba77b9005dafa12]</div><div class="line">$</div><div class="line">$ mkdir -p /opt/restore/second_back</div><div class="line">$ cd /opt/restore/second_back</div><div class="line">$ borg extract --list /opt/backup/borg_sample::second</div><div class="line">opt/data</div><div class="line">opt/data/file_change.txt</div><div class="line">opt/data/file_static.txt</div><div class="line">opt/data/random.dump</div><div class="line">opt/data/file_new.txt</div><div class="line">opt/data/random_2.dump</div><div class="line">$</div><div class="line">$ diff -r /opt/restore/second /opt/restore/second_back</div><div class="line">$</div><div class="line">$ cat opt/data/file_change.txt</div><div class="line">here is the first line of file 1</div><div class="line">here is the second line of file 1</div><div class="line">$</div><div class="line">$ cat opt/data/file_static.txt</div><div class="line">here is the static file</div><div class="line">$</div></pre></td></tr></table></figure></p>
<h3 id="自动备份"><a href="#自动备份" class="headerlink" title="自动备份"></a>自动备份</h3><p>官网上的一个自动备份的shell脚本<a href="https://borgbackup.readthedocs.io/en/stable/quickstart.html#automating-backups" target="_blank" rel="external">automating-backups</a><br>可按照实际使用要求进行修改。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line">#!/bin/sh</div><div class="line"></div><div class="line"># Setting this, so the repo does not need to be given on the commandline:</div><div class="line">export BORG_REPO=ssh://username@example.com:2022/~/backup/main</div><div class="line"></div><div class="line"># Setting this, so you won&apos;t be asked for your repository passphrase:</div><div class="line">export BORG_PASSPHRASE=&apos;XYZl0ngandsecurepa_55_phrasea&amp;&amp;123&apos;</div><div class="line"># or this to ask an external program to supply the passphrase:</div><div class="line">export BORG_PASSCOMMAND=&apos;pass show backup&apos;</div><div class="line"></div><div class="line"># some helpers and error handling:</div><div class="line">info() &#123; printf &quot;\n%s %s\n\n&quot; &quot;$( date )&quot; &quot;$*&quot; &gt;&amp;2; &#125;</div><div class="line">trap &apos;echo $( date ) Backup interrupted &gt;&amp;2; exit 2&apos; INT TERM</div><div class="line"></div><div class="line">info &quot;Starting backup&quot;</div><div class="line"></div><div class="line"># Backup the most important directories into an archive named after</div><div class="line"># the machine this script is currently running on:</div><div class="line"></div><div class="line">borg create                         \</div><div class="line">    --verbose                       \</div><div class="line">    --filter AME                    \</div><div class="line">    --list                          \</div><div class="line">    --stats                         \</div><div class="line">    --show-rc                       \</div><div class="line">    --compression lz4               \</div><div class="line">    --exclude-caches                \</div><div class="line">    --exclude &apos;/home/*/.cache/*&apos;    \</div><div class="line">    --exclude &apos;/var/cache/*&apos;        \</div><div class="line">    --exclude &apos;/var/tmp/*&apos;          \</div><div class="line">                                    \</div><div class="line">    ::&apos;&#123;hostname&#125;-&#123;now&#125;&apos;            \</div><div class="line">    /etc                            \</div><div class="line">    /home                           \</div><div class="line">    /root                           \</div><div class="line">    /var                            \</div><div class="line"></div><div class="line">backup_exit=$?</div><div class="line"></div><div class="line">info &quot;Pruning repository&quot;</div><div class="line"></div><div class="line"># Use the `prune` subcommand to maintain 7 daily, 4 weekly and 6 monthly</div><div class="line"># archives of THIS machine. The &apos;&#123;hostname&#125;-&apos; prefix is very important to</div><div class="line"># limit prune&apos;s operation to this machine&apos;s archives and not apply to</div><div class="line"># other machines&apos; archives also:</div><div class="line"></div><div class="line">borg prune                          \</div><div class="line">    --list                          \</div><div class="line">    --prefix &apos;&#123;hostname&#125;-&apos;          \</div><div class="line">    --show-rc                       \</div><div class="line">    --keep-daily    7               \</div><div class="line">    --keep-weekly   4               \</div><div class="line">    --keep-monthly  6               \</div><div class="line"></div><div class="line">prune_exit=$?</div><div class="line"></div><div class="line"># use highest exit code as global exit code</div><div class="line">global_exit=$(( backup_exit &gt; prune_exit ? backup_exit : prune_exit ))</div><div class="line"></div><div class="line">if [ $&#123;global_exit&#125; -eq 0 ]; then</div><div class="line">    info &quot;Backup and Prune finished successfully&quot;</div><div class="line">elif [ $&#123;global_exit&#125; -eq 1 ]; then</div><div class="line">    info &quot;Backup and/or Prune finished with warnings&quot;</div><div class="line">else</div><div class="line">    info &quot;Backup and/or Prune finished with errors&quot;</div><div class="line">fi</div><div class="line"></div><div class="line">exit $&#123;global_exit&#125;</div></pre></td></tr></table></figure></p>
<h3 id="详细使用"><a href="#详细使用" class="headerlink" title="详细使用"></a>详细使用</h3><p>各个命令更细化的命令，官网文档中有非常详细的描述<a href="https://borgbackup.readthedocs.io/en/stable/usage/general.html" target="_blank" rel="external">https://borgbackup.readthedocs.io/en/stable/usage/general.html</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://www.reddit.com/r/linux/comments/69lm87/is_borg_backup_suitable_for_the_production/" target="_blank" rel="external">Reddit上关于BorgBackup的一些讨论</a></li>
<li><a href="https://www.borgbackup.org/" target="_blank" rel="external">BorgBackup官网</a></li>
<li><a href="https://github.com/borgbackup/borg" target="_blank" rel="external">BorgBackup Github地址</a></li>
<li><a href="https://wzyboy.im/post/1106.html" target="_blank" rel="external">BorgBackup —— 增量备份方案</a></li>
<li><a href="http://lee.kometo.com/archives/1225" target="_blank" rel="external">Linux的备份方案比选及Borg部署的关键技术</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> CentOS </tag>
            
            <tag> Linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[在Mac下远程登录Linux时,提示cannot change locale (UTF-8) No such file or directory]]></title>
      <url>/2019/07/24/setlocale-LC-CTYPE-cannot-change-locale-UTF-8-issue-when-ssh-to-linux-in-Mac/</url>
      <content type="html"><![CDATA[<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><ul>
<li>Mac下设置第一语言为English</li>
<li>在Terminal或者iTerm2上登录远端Linux时，Linux的prompt提示 <code>setlocale: LC_CTYPE: cannot change locale (UTF-8): No such file or directory</code></li>
<li>登录Linux后无法正常显示中文</li>
</ul>
<h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><ol>
<li>Mac下设置为英文后，locale字符集默认是”C”，Terminal或者iTerm2中有选项会自动设置LC_CTYPE或者LC_LANG为UTF-8</li>
<li>Mac下ssh客户端的配置文件<code>/etc/ssh/ssh_config</code>中，会尝试设置本地的LANG到远端服务器中。</li>
<li>远端Linux服务器，没有UTF-8的字符集，就导致了setlocale的警报</li>
</ol>
<h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>为了登录而来，修改每个服务器的字符集，操作上是不可行的。最简单的办法就是修改Mac本地的ssh客户端配置，不要将LANG设置发送到服务器端。<br>打开ssh配置文件，<code>sudo vim /etc/ssh/ssh_config</code>, 注释掉如下几行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Host *</div><div class="line">	SendEnv LANG LC_*</div></pre></td></tr></table></figure></p>
<p>重新ssh到服务器，就不会再有setlocale的告警了。</p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> MacOS </tag>
            
            <tag> SSH </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS EC2各个类型的网络性能的测试方法]]></title>
      <url>/2019/07/06/how-to-check-network-throughput-on-aws-EC2/</url>
      <content type="html"><![CDATA[<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>在AWS中，不同类型的EC2 instance,拥有的网络带宽是不同的。</p>
<p>让人犯糊的是，在<a href="https://amazonaws-china.com/ec2/instance-types/" target="_blank" rel="external">AWS EC2的介绍页面</a>中,并没有清晰的标明每种类型的Instance的带宽具体是多少？一些类型的Instance只是标注了网络性能(Network Performance)为<code>Up to 5</code>,<code>Low</code>,<code>Moderate</code>或者<code>High</code>。</p>
<p>老外的<a href="https://cloudonaut.io/ec2-network-performance-cheat-sheet/" target="_blank" rel="external">EC2 Network Performance Cheat Sheet</a>这篇文章中，记录了作者使用<a href="(https://github.com/esnet/iperf)">iperf3</a>测试出来的各个Instance类型的带宽。如果选取类型的时候比较在意带宽，可以用来参考下。</p>
<p>顺带介绍一下如何在AWS EC2中使用<a href="(https://github.com/esnet/iperf)">iperf3</a>来测试Instance带宽。如果确实有需要，就可以自己来进行测速。<br>测试环境如下:</p>
<ul>
<li>测试Region是美国Oregon</li>
<li>使用两台m4.large的EC2，一台运行服务，一台用作客户端。</li>
<li>介绍两种Amazon Linux的测试方法<ul>
<li>Amazon Linux AMI 2018.03.0 (HVM), SSD Volume Type</li>
<li>Amazon Linux 2 AMI (HVM), SSD Volume Type</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h3 id="Amazon-Linux-测速"><a href="#Amazon-Linux-测速" class="headerlink" title="Amazon Linux 测速"></a>Amazon Linux 测速</h3><h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><ul>
<li>两台m4.large</li>
<li>AMI为Amazon Linux AMI 2018.03.0 (HVM), SSD Volume Type</li>
<li>两台Instance在同一个AZ中</li>
</ul>
<p><strong>试验中，iperf3监听5000端口，EC2对应的SG中需要开启端口5000</strong></p>
<h4 id="安装测试"><a href="#安装测试" class="headerlink" title="安装测试"></a>安装测试</h4><p>登录作为Server的EC2，执行如下命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># 安装系统更新和iperf3</div><div class="line">sudo yum update</div><div class="line">sudo yum-config-manager --enable epel</div><div class="line">sudo yum install iperf3</div><div class="line"></div><div class="line"># 在端口5000上开启iperf3</div><div class="line">sudo iperf3 -s -p 5000</div></pre></td></tr></table></figure></p>
<p>登录作为client的EC2，执行如下命令:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 安装系统更新和iperf3</div><div class="line">sudo yum update</div><div class="line">sudo yum-config-manager --enable epel</div><div class="line">sudo yum install iperf3</div></pre></td></tr></table></figure></p>
<p>Client上执行命令来测速<code>iperf3 -c 172.31.20.28 -i 1 -t 5 --parallel 2 -p 5000</code></p>
<ul>
<li><code>-c 172.31.20.28</code> 代表client模式, 连接的服务器IP是172.31.20.28</li>
<li><code>-i</code> 代表输出间隔为1秒</li>
<li><code>-t</code> 代表持续5秒</li>
<li><code>--parallel</code> 代表并行数为2</li>
<li><code>-p</code> 代表连接服务器的5000端口。</li>
</ul>
<p><code>iperf3 -c 172.31.20.28 -i 1 -t 5 --parallel 2 -p 5000</code>的输出如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">[ec2-user@ip-172-31-31-88 ~]$ iperf3 -c 172.31.20.28 -i 1 -t 5 --parallel 2 -p 5000</div><div class="line">Connecting to host 172.31.20.28, port 5000</div><div class="line">[  4] local 172.31.31.88 port 41798 connected to 172.31.20.28 port 5000</div><div class="line">[  6] local 172.31.31.88 port 41800 connected to 172.31.20.28 port 5000</div><div class="line">[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd</div><div class="line">[  4]   0.00-1.00   sec  72.6 MBytes   609 Mbits/sec    0    288 KBytes</div><div class="line">[  6]   0.00-1.00   sec  71.1 MBytes   596 Mbits/sec    0    288 KBytes</div><div class="line">[SUM]   0.00-1.00   sec   144 MBytes  1.21 Gbits/sec    0</div><div class="line">- - - - - - - - - - - - - - - - - - - - - - - - -</div><div class="line">[  4]   1.00-2.00   sec  54.2 MBytes   455 Mbits/sec    4    446 KBytes</div><div class="line">[  6]   1.00-2.00   sec  54.7 MBytes   459 Mbits/sec    3    454 KBytes</div><div class="line">[SUM]   1.00-2.00   sec   109 MBytes   913 Mbits/sec    7</div><div class="line">- - - - - - - - - - - - - - - - - - - - - - - - -</div><div class="line">[  4]   2.00-3.00   sec  26.2 MBytes   220 Mbits/sec    1    507 KBytes</div><div class="line">[  6]   2.00-3.00   sec  27.2 MBytes   228 Mbits/sec    1    524 KBytes</div><div class="line">[SUM]   2.00-3.00   sec  53.4 MBytes   448 Mbits/sec    2</div><div class="line">- - - - - - - - - - - - - - - - - - - - - - - - -</div><div class="line">[  4]   3.00-4.00   sec  24.0 MBytes   201 Mbits/sec    2    551 KBytes</div><div class="line">[  6]   3.00-4.00   sec  28.9 MBytes   243 Mbits/sec    1    542 KBytes</div><div class="line">[SUM]   3.00-4.00   sec  52.9 MBytes   444 Mbits/sec    3</div><div class="line">- - - - - - - - - - - - - - - - - - - - - - - - -</div><div class="line">[  4]   4.00-5.00   sec  33.1 MBytes   278 Mbits/sec    1    551 KBytes</div><div class="line">[  6]   4.00-5.00   sec  20.9 MBytes   175 Mbits/sec    4    428 KBytes</div><div class="line">[SUM]   4.00-5.00   sec  54.0 MBytes   453 Mbits/sec    5</div><div class="line">- - - - - - - - - - - - - - - - - - - - - - - - -</div><div class="line">[ ID] Interval           Transfer     Bandwidth       Retr</div><div class="line">[  4]   0.00-5.00   sec   210 MBytes   353 Mbits/sec    8             sender</div><div class="line">[  4]   0.00-5.00   sec   208 MBytes   348 Mbits/sec                  receiver</div><div class="line">[  6]   0.00-5.00   sec   203 MBytes   340 Mbits/sec    9             sender</div><div class="line">[  6]   0.00-5.00   sec   201 MBytes   337 Mbits/sec                  receiver</div><div class="line">[SUM]   0.00-5.00   sec   413 MBytes   693 Mbits/sec   17             sender</div><div class="line">[SUM]   0.00-5.00   sec   409 MBytes   686 Mbits/sec                  receiver</div><div class="line"></div><div class="line">iperf Done.</div><div class="line">[ec2-user@ip-172-31-31-88 ~]$</div></pre></td></tr></table></figure></p>
<h3 id="Amazon-Linux-2测速"><a href="#Amazon-Linux-2测速" class="headerlink" title="Amazon Linux 2测速"></a>Amazon Linux 2测速</h3><h4 id="环境说明-1"><a href="#环境说明-1" class="headerlink" title="环境说明"></a>环境说明</h4><ul>
<li>两台m4.large</li>
<li>AMI为Amazon Linux 2 AMI (HVM), SSD Volume Type</li>
<li>两台Instance在同一个AZ中</li>
</ul>
<p><strong>试验中，iperf3监听5000端口，EC2对应的SG中需要开启端口5000</strong></p>
<h4 id="安装测试-1"><a href="#安装测试-1" class="headerlink" title="安装测试"></a>安装测试</h4><p>登录作为Server的EC2，执行如下命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># 安装系统更新和iperf3</div><div class="line">sudo yum update -y</div><div class="line">sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</div><div class="line">sudo yum install -y iperf3</div><div class="line"></div><div class="line"># 在端口5000上开启iperf3</div><div class="line">sudo iperf3 -s -p 5000</div></pre></td></tr></table></figure></p>
<p>登录作为client的EC2，执行如下命令:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 安装系统更新和iperf3</div><div class="line">sudo yum update -y</div><div class="line">sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</div><div class="line">sudo yum install -y iperf3</div></pre></td></tr></table></figure></p>
<p>Client上执行命令来测速<code>iperf3 -c 172.31.44.223 -i 1 -t 5 --parallel 2 -p 5000</code></p>
<ul>
<li><code>-c 172.31.44.223</code> 代表client模式, 连接的服务端IP地址是172.31.44.223</li>
<li><code>-i</code> 代表输出间隔为1秒</li>
<li><code>-t</code> 代表持续5秒</li>
<li><code>--parallel</code> 代表并行数为2</li>
<li><code>-p</code> 代表连接服务器的5000端口。</li>
</ul>
<p><code>iperf3 -c 172.31.44.223 -i 1 -t 5 --parallel 2 -p 5000</code>的输出如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">[ec2-user@ip-172-31-34-102 ~]$ iperf3 -c 172.31.44.223 -i 1 -t 5 --parallel 2 -p 5000</div><div class="line">Connecting to host 172.31.44.223, port 5000</div><div class="line">[  4] local 172.31.34.102 port 36994 connected to 172.31.44.223 port 5000</div><div class="line">[  6] local 172.31.34.102 port 36996 connected to 172.31.44.223 port 5000</div><div class="line">[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd</div><div class="line">[  4]   0.00-1.00   sec  99.4 MBytes   834 Mbits/sec    5    612 KBytes</div><div class="line">[  6]   0.00-1.00   sec  99.3 MBytes   833 Mbits/sec    6    620 KBytes</div><div class="line">[SUM]   0.00-1.00   sec   199 MBytes  1.67 Gbits/sec   11</div><div class="line">- - - - - - - - - - - - - - - - - - - - - - - - -</div><div class="line">[  4]   1.00-2.00   sec  28.1 MBytes   236 Mbits/sec    0    787 KBytes</div><div class="line">[  6]   1.00-2.00   sec  27.3 MBytes   229 Mbits/sec    0    787 KBytes</div><div class="line">[SUM]   1.00-2.00   sec  55.4 MBytes   465 Mbits/sec    0</div><div class="line">- - - - - - - - - - - - - - - - - - - - - - - - -</div><div class="line">[  4]   2.00-3.00   sec  26.9 MBytes   226 Mbits/sec    1    699 KBytes</div><div class="line">[  6]   2.00-3.00   sec  26.2 MBytes   220 Mbits/sec    1    699 KBytes</div><div class="line">[SUM]   2.00-3.00   sec  53.1 MBytes   446 Mbits/sec    2</div><div class="line">- - - - - - - - - - - - - - - - - - - - - - - - -</div><div class="line">[  4]   3.00-4.00   sec  27.1 MBytes   227 Mbits/sec    1    603 KBytes</div><div class="line">[  6]   3.00-4.00   sec  26.1 MBytes   219 Mbits/sec    0    856 KBytes</div><div class="line">[SUM]   3.00-4.00   sec  53.2 MBytes   446 Mbits/sec    1</div><div class="line">- - - - - - - - - - - - - - - - - - - - - - - - -</div><div class="line">[  4]   4.00-5.00   sec  28.0 MBytes   235 Mbits/sec    1    559 KBytes</div><div class="line">[  6]   4.00-5.00   sec  26.5 MBytes   223 Mbits/sec    0    996 KBytes</div><div class="line">[SUM]   4.00-5.00   sec  54.5 MBytes   457 Mbits/sec    1</div><div class="line">- - - - - - - - - - - - - - - - - - - - - - - - -</div><div class="line">[ ID] Interval           Transfer     Bandwidth       Retr</div><div class="line">[  4]   0.00-5.00   sec   209 MBytes   351 Mbits/sec    8             sender</div><div class="line">[  4]   0.00-5.00   sec   207 MBytes   348 Mbits/sec                  receiver</div><div class="line">[  6]   0.00-5.00   sec   205 MBytes   345 Mbits/sec    7             sender</div><div class="line">[  6]   0.00-5.00   sec   203 MBytes   340 Mbits/sec                  receiver</div><div class="line">[SUM]   0.00-5.00   sec   415 MBytes   696 Mbits/sec   15             sender</div><div class="line">[SUM]   0.00-5.00   sec   410 MBytes   688 Mbits/sec                  receiver</div><div class="line"></div><div class="line">iperf Done.</div><div class="line">[ec2-user@ip-172-31-34-102 ~]$</div></pre></td></tr></table></figure></p>
<h3 id="比较总结"><a href="#比较总结" class="headerlink" title="比较总结"></a>比较总结</h3><ul>
<li>Amazon Linux是基于RHEL6的，Amazon Linux 2是基于RHEL 7的，两者的差别可以认为就类似于RHEL 6和RHEL 7。</li>
<li>Amazon Linux 2中默认不带epel的yum源，需要自己安装。而Amazon Linux中默认带epel源，不过默认是disable的，需要用yum-config-manager enable一下。</li>
<li>iperf3前一两秒会有一个burst，测试时间需要长一点，才能得到一个比较真实的测试结果。</li>
<li>关于EC2网络带宽的猜想: 每个机型的带宽限制，应该是AWS在逻辑层面做的限制，而不是在物理硬件层面的限制。</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://www.linode.com/docs/networking/diagnostics/install-iperf-to-diagnose-network-speed-in-linux/" target="_blank" rel="external">Network Throughput Testing with iPerf</a></li>
<li><a href="https://amazonaws-china.com/premiumsupport/knowledge-center/network-throughput-benchmark-linux-ec2/" target="_blank" rel="external">How do I benchmark network throughput between Amazon EC2 Linux instances in the same VPC?</a></li>
<li><a href="https://amazonaws-china.com/premiumsupport/knowledge-center/ec2-enable-epel/" target="_blank" rel="external">How do I enable the EPEL repository for my Amazon EC2 instance running CentOS, RHEL, or Amazon Linux?</a></li>
<li><a href="https://cloudonaut.io/ec2-network-performance-cheat-sheet/" target="_blank" rel="external">EC2 Network Performance Cheat Sheet</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> EC2 </tag>
            
            <tag> Linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[关闭Mac系统中的自动拼写纠正功能]]></title>
      <url>/2019/07/04/how-to-cancel-automatic-spelling-in-macos/</url>
      <content type="html"><![CDATA[<p>Mac系统中输入英文字母的时候,有一些贴心的设置。拼写错误的时候，系统会自动纠正拼写错误。句子的首字母，会自动帮你设为大写。<br>对于写书来说，这些功能很是贴心，对于码农来说，这些功能有时候就显得很烦人。</p>
<h3 id="关闭方法"><a href="#关闭方法" class="headerlink" title="关闭方法"></a>关闭方法</h3><p>在设置中，可以关闭自动纠正功能。[System preference] -&gt; [Keyboard] -&gt; [Text]</p>
<p>下面三个选项前面的勾去掉即可。</p>
<ul>
<li>Correct spelling automatically</li>
<li>Capitalize words automatically</li>
<li>Add period with double-space</li>
</ul>
<a id="more"></a>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://blog.csdn.net/stc_XC/article/details/72814069" target="_blank" rel="external">OS X 自带输入法关闭首字母大写、拼写纠正、双空格变点号</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> MacOS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[SACK Panic]]></title>
      <url>/2019/06/24/SACK-Panic-in-CentOS-7/</url>
      <content type="html"><![CDATA[<p>在2019-06-17，Netfilx爆出了FreeBSD and Linux系统中的内核上存在严重远程DoS漏洞，攻击者可以构造特定的SACK请求到目标服务器，引起服务器内核奔溃。<br>漏洞编号为: </p>
<ul>
<li><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-11477" target="_blank" rel="external">CVE-2019-11477</a></li>
<li><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-11478" target="_blank" rel="external">CVE-2019-11478</a></li>
<li><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-11479" target="_blank" rel="external">CVE-2019-11479</a></li>
</ul>
<p>几家公司的通告如下:</p>
<ul>
<li><a href="https://github.com/Netflix/security-bulletins/blob/master/advisories/third-party/2019-001.md" target="_blank" rel="external">Netflix官方通告</a></li>
<li><a href="https://access.redhat.com/security/vulnerabilities/tcpsack" target="_blank" rel="external">红帽公告</a></li>
</ul>
<a id="more"></a>
<h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>关于问题的原因，红帽官方描述如下:</p>
<ul>
<li><a href="https://access.redhat.com/security/vulnerabilities/tcpsack" target="_blank" rel="external">TCP SACK PANIC - Kernel vulnerabilities - CVE-2019-11477, CVE-2019-11478 &amp; CVE-2019-11479</a></li>
</ul>
<p>看英文吃力的同学，也可以看InfoQ的翻译</p>
<ul>
<li><a href="https://www.infoq.cn/article/gc3*ZGs49Tj5fHkh9frM" target="_blank" rel="external">Linux 内核被曝 TCP “SACK PANIC”漏洞，多家云服务商给出紧急修复建议</a></li>
</ul>
<p>SACK全称是Selective Acknowledgment，在<a href="http://tools.ietf.org/html/rfc2018" target="_blank" rel="external">RFC-2018</a>中被定义，目的是改善ACK的重传机制。<br>看RFC吃力的同学，可以看耗子叔的两篇关于TCP的文章</p>
<ul>
<li><a href="https://coolshell.cn/articles/11564.html" target="_blank" rel="external">TCP 的那些事儿（上）</a></li>
<li><a href="https://coolshell.cn/articles/11609.html" target="_blank" rel="external">TCP 的那些事儿（下）</a></li>
</ul>
<h3 id="红帽系的检测工具"><a href="#红帽系的检测工具" class="headerlink" title="红帽系的检测工具"></a>红帽系的检测工具</h3><p>红帽给了一个脚本，可用于检测系统是否受这些漏洞影响。下载地址:</p>
<ul>
<li><a href="https://access.redhat.com/sites/default/files/cve-2019-11477--2019-06-17-1629.sh" target="_blank" rel="external">Determine if your system is vulnerable</a></li>
</ul>
<p>拷贝脚本到服务器上，sudo运行<br>如果系统是不被影响的，脚本输出为<code>This system is Not affected</code>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"># bash cve-2019-11477--2019-06-17-1629.sh</div><div class="line"></div><div class="line">This script (v1.0) is primarily designed to detect CVE-2019-11477 on supported</div><div class="line">Red Hat Enterprise Linux systems and kernel packages.</div><div class="line">Result may be inaccurate for other RPM based systems.</div><div class="line"></div><div class="line">Running kernel: 3.10.0-957.21.3.el7.x86_64</div><div class="line"></div><div class="line">This system is Not affected</div><div class="line"></div><div class="line"></div><div class="line">For more information about this vulnerability, see:</div><div class="line">https://access.redhat.com/security/vulnerabilities/tcpsack</div><div class="line">#</div></pre></td></tr></table></figure></p>
<p>如果系统是受影响的，脚本输出为<code>This system is Vulnerable</code>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"># bash cve-2019-11477--2019-06-17-1629.sh</div><div class="line"></div><div class="line">This script (v1.0) is primarily designed to detect CVE-2019-11477 on supported</div><div class="line">Red Hat Enterprise Linux systems and kernel packages.</div><div class="line">Result may be inaccurate for other RPM based systems.</div><div class="line"></div><div class="line">Running kernel: 3.10.0-957.el7.x86_64</div><div class="line"></div><div class="line">This system is Vulnerable</div><div class="line"></div><div class="line">* Running kernel is vulnerable</div><div class="line"></div><div class="line">For more information about this vulnerability, see:</div><div class="line">https://access.redhat.com/security/vulnerabilities/tcpsack</div><div class="line">#</div></pre></td></tr></table></figure></p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><h4 id="升级Kernel"><a href="#升级Kernel" class="headerlink" title="升级Kernel"></a>升级Kernel</h4><p>漏洞发出后，红帽第一时间就发布了kernel-3.10.0-957.21.3的内核更新，CentOS在稍后也同步更新了Kernel补丁。升级命令如下:<br>在CentOS 7中，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># 升级kernel</div><div class="line">yum update kernel -y</div><div class="line"># 重启服务器</div><div class="line">reboot</div><div class="line"># 检查内核版本 </div><div class="line">uname -r</div></pre></td></tr></table></figure></p>
<h4 id="禁用内核SACK"><a href="#禁用内核SACK" class="headerlink" title="禁用内核SACK"></a>禁用内核SACK</h4><p>如果服务器不方便直接重启，则可以先临时关闭sack。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># echo 0 &gt; /proc/sys/net/ipv4/tcp_sack</div><div class="line">或者</div><div class="line"># sysctl -w net.ipv4.tcp_sack=0</div></pre></td></tr></table></figure></p>
<p>此时运行检测监本，提示<code>Running kernel is vulnerable</code>,<code>sysctl mitigation is applied</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"># bash cve-2019-11477--2019-06-17-1629.sh</div><div class="line"></div><div class="line">This script (v1.0) is primarily designed to detect CVE-2019-11477 on supported</div><div class="line">Red Hat Enterprise Linux systems and kernel packages.</div><div class="line">Result may be inaccurate for other RPM based systems.</div><div class="line"></div><div class="line">Running kernel: 3.10.0-957.el7.x86_64</div><div class="line"></div><div class="line">This system is Mitigated</div><div class="line"></div><div class="line">* Running kernel is vulnerable</div><div class="line">* sysctl mitigation is applied</div><div class="line"></div><div class="line">For more information about this vulnerability, see:</div><div class="line">https://access.redhat.com/security/vulnerabilities/tcpsack</div><div class="line">#</div></pre></td></tr></table></figure></p>
<p>如果要重启后还是禁用tcp sack。则需要在<code>/etc/sysctl.d/</code>下建立配置文件，比如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># vi /etc/sysctl.d/99-tcpsack.conf</div><div class="line"></div><div class="line">  # CVE-2019-11477 &amp; CVE-2019-11478</div><div class="line">  net.ipv4.tcp_sack=0</div></pre></td></tr></table></figure></p>
<p>保存文件退出，重启后也会保持sack是禁用状态。</p>
<h3 id="AWS侧应对"><a href="#AWS侧应对" class="headerlink" title="AWS侧应对"></a>AWS侧应对</h3><p>AWS的官方对此次SACK PANIC的通告以及各个服务的措施</p>
<ul>
<li><a href="https://amazonaws-china.com/security/security-bulletins/AWS-2019-005/" target="_blank" rel="external">Linux Kernel TCP SACK Denial of Service Issues</a></li>
</ul>
<p>各个受影响的服务以及对应的应对措施都有详细描述。<br>对于自己管控的EC2 Instance来说，如果使用了Amazon Linux and Amazon Linux 2，那么无论前面ELB类型是什么，都建议选择在合适的时间登录机器升级一下Kernel, 然后重启下。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum update kernel</div></pre></td></tr></table></figure></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://github.com/Netflix/security-bulletins/blob/master/advisories/third-party/2019-001.md" target="_blank" rel="external">Netflix官方通告</a></li>
<li><a href="https://access.redhat.com/security/vulnerabilities/tcpsack" target="_blank" rel="external">TCP SACK PANIC - Kernel vulnerabilities - CVE-2019-11477, CVE-2019-11478 &amp; CVE-2019-11479</a></li>
<li><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-11477" target="_blank" rel="external">CVE-2019-11477</a></li>
<li><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-11478" target="_blank" rel="external">CVE-2019-11478</a></li>
<li><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-11479" target="_blank" rel="external">CVE-2019-11479</a></li>
<li><a href="https://community.centminmod.com/threads/kernel-tcp-sack-panic-security-update-cve-2019-11477-cve-2019-11478-cve-2019-11479.17763/" target="_blank" rel="external">Kernel TCP SACK PANIC Security Update CVE-2019-11477, CVE-2019-11478 &amp; CVE-2019-11479</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> CentOS </tag>
            
            <tag> Network </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[GoAccess-一个处理web日志的工具]]></title>
      <url>/2019/06/11/a-tool-goaccess-to-process-web-log/</url>
      <content type="html"><![CDATA[<h3 id="GoAccess介绍"><a href="#GoAccess介绍" class="headerlink" title="GoAccess介绍"></a>GoAccess介绍</h3><p>GoAccess是一个开源的基于终端的快速日志分析器。<br>Github地址:<a href="https://github.com/allinurl/goaccess" target="_blank" rel="external">https://github.com/allinurl/goaccess</a><br>官网地址: <a href="https://goaccess.io/" target="_blank" rel="external">https://goaccess.io/</a><br>说明文档: <a href="https://goaccess.io/man" target="_blank" rel="external">https://goaccess.io/man</a></p>
<p>通过自己配置，可以支持日志格式，默认已经集成的log格式</p>
<ul>
<li>Amazon CloudFront (Download Distribution).</li>
<li>Amazon Simple Storage Service (S3)</li>
<li>AWS Elastic Load Balancing</li>
<li>Combined Log Format (XLF/ELF) Apache | Nginx</li>
<li>Common Log Format (CLF) Apache</li>
<li>Google Cloud Storage.</li>
<li>Apache virtual hosts</li>
<li>Squid Native Format.</li>
<li>W3C format (IIS)</li>
</ul>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><h4 id="MacOS下"><a href="#MacOS下" class="headerlink" title="MacOS下"></a>MacOS下</h4><p>使用brew来安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">brew install goaccess</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<h4 id="CentOS下"><a href="#CentOS下" class="headerlink" title="CentOS下"></a>CentOS下</h4><ol>
<li><p>使用yum安装，goaccess在epel repository下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">yum -y install epel-release</div><div class="line">yum install goaccess</div></pre></td></tr></table></figure>
</li>
<li><p>下载源码编译</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># yum install ncurses-devel geoip-devel  # 安装依赖包</div><div class="line">$ wget https://tar.goaccess.io/goaccess-1.3.tar.gz</div><div class="line">$ tar -xzvf goaccess-1.3.tar.gz</div><div class="line">$ cd goaccess-1.3/</div><div class="line">$ ./configure --enable-utf8 --enable-geoip=legacy</div><div class="line">$ make</div><div class="line"># make install</div></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="Amazon-Linux"><a href="#Amazon-Linux" class="headerlink" title="Amazon Linux"></a>Amazon Linux</h4><p>下载源码编译<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># yum install ncurses-devel geoip-devel  # 安装依赖包</div><div class="line">$ wget https://tar.goaccess.io/goaccess-1.3.tar.gz</div><div class="line">$ tar -xzvf goaccess-1.3.tar.gz</div><div class="line">$ cd goaccess-1.3/</div><div class="line">$ ./configure --enable-utf8 --enable-geoip=legacy</div><div class="line">$ make</div><div class="line"># make install</div></pre></td></tr></table></figure></p>
<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>摘录了部分常用的参数，完整参数参见<a href="https://goaccess.io/man" target="_blank" rel="external">GoAccess Man</a></p>
<h4 id="PARSE-OPTIONS"><a href="#PARSE-OPTIONS" class="headerlink" title="PARSE OPTIONS"></a>PARSE OPTIONS</h4><ul>
<li>-a –agent-list<br>Enable a list of user-agents by host. For faster parsing, do not enable this flag.</li>
<li>-e –exclude-ip <ip|ip-range><br>Exclude an IPv4 or IPv6 from being counted. Ranges can be included as well using a dash in between the IPs (start-end)<ul>
<li>exclude-ip 127.0.0.1</li>
<li>exclude-ip 192.168.0.1-192.168.0.100</li>
<li>exclude-ip ::1</li>
<li>exclude-ip 0:0:0:0:0:ffff:808:804-0:0:0:0:0:ffff:808:808</li>
</ul>
</ip|ip-range></li>
<li>-o –output=<json|csv><br>Write output to stdout given one of the following files and the corresponding extension for the output format:<ul>
<li>/path/file.csv - Comma-separated values (CSV)</li>
<li>/path/file.json - JSON (JavaScript Object Notation)</li>
<li>/path/file.html - HTML</li>
</ul>
</json|csv></li>
<li>-q –no-query-string<br>Ignore request’s query string. i.e., www.google.com/page.htm?query =&gt; www.google.com/page.htm</li>
<li>–444-as-404<br>Treat non-standard status code 444 as 404.</li>
<li>–4xx-to-unique-count<br>Add 4xx client errors to the unique visitors count.</li>
<li>–browsers-file=<path></path><br>Include an additional tab delimited list of browsers/crawlers/feeds etc. See <a href="https://raw.githubusercontent.com/allinurl/goaccess/master/config/browsers.list" target="_blank" rel="external">config/browsers.list</a>.</li>
<li>–ignore-crawlers<br>Ignore crawlers.</li>
</ul>
<h4 id="GEOLOCATION-OPTIONS"><a href="#GEOLOCATION-OPTIONS" class="headerlink" title="GEOLOCATION OPTIONS"></a>GEOLOCATION OPTIONS</h4><ul>
<li>-g –std-geoip<br>Standard GeoIP database for less memory usage.</li>
<li>–geoip-database <geocityfile><br>Specify path to GeoIP database file. i.e., GeoLiteCity.dat. File needs to be downloaded from maxmind.com. IPv4 and IPv6 files are supported as well. Note: <code>--geoip-city-data</code> is an alias of <code>--geoip-database</code>.<br><strong>Note</strong>: If using GeoIP2, you will need to download the City/Country database from <a href="http://dev.maxmind.com/geoip/geoip2/geolite2/" target="_blank" rel="external">MaxMind</a> and use the option <code>--geoip-database</code> to specify the database. Currently cities are only shown in the hosts panel (per host).</geocityfile></li>
</ul>
<h3 id="自定义格式"><a href="#自定义格式" class="headerlink" title="自定义格式"></a>自定义格式</h3><p>参见<a href="https://goaccess.io/man#custom-log" target="_blank" rel="external">GoAccess Man Page</a></p>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>解析nginx默认日志</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">goaccess --log-format=COMBINED access.log -o report.html</div></pre></td></tr></table></figure>
<p>解析AWS ELB日志<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">goaccess --log-format=AWSELB 20161231T2200Z_52.27.188.188_1d0y2vo4.log &gt; report.html</div></pre></td></tr></table></figure></p>
<p>多文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">goaccess access.log access.log.1</div></pre></td></tr></table></figure></p>
<p>实时生成<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tail -f access.log | goaccess --log-format=COMBINED -</div></pre></td></tr></table></figure></p>
<p>报告页显示为中文<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">LANG=zh_CN.UTF-8</div><div class="line">LC_CTYPE=zh_CN.UTF-8</div><div class="line">goaccess --log-format=COMBINED access.log -o report.html</div></pre></td></tr></table></figure></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://github.com/allinurl/goaccess" target="_blank" rel="external">GoAccess</a></li>
<li><a href="https://goaccess.io/" target="_blank" rel="external">https://goaccess.io/</a></li>
<li><a href="https://blog.51cto.com/linuxg/2335007" target="_blank" rel="external">如何让goaccess的dashboard界面显示中文</a></li>
<li><a href="https://ericdraken.com/goaccess-web-log-analyzer-installation-centos/" target="_blank" rel="external">GoAccess Web Log Analyzer Installation on CentOS</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Nginx </tag>
            
            <tag> HTTP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CentOS 7下制作本地yum源]]></title>
      <url>/2019/06/04/how-to-make-local-yum-repository-on-centos-7/</url>
      <content type="html"><![CDATA[<p>CentOS 7 制作本地repository的方法, 适用于没有外网访问权限的CentOS 7的升级。<br>大概步骤:</p>
<ol>
<li>找一台系统版本相同，但是有外网访问权限的CentOS 7的机器</li>
<li>在这台机器上下载yum包，并制作本地repository</li>
<li>拷贝repository到内网机器</li>
<li>配置内网机器，从制作好的本地repository升级yum包<a id="more"></a>
</li>
</ol>
<h3 id="详细步骤"><a href="#详细步骤" class="headerlink" title="详细步骤"></a>详细步骤</h3><ol>
<li><p>在有公网连接的机器上，下载yum包, 并制作本地repository</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"># 建立目录</div><div class="line">mkdir download</div><div class="line"></div><div class="line"># 如果是系统需要升级的包使用yum install --downloadonly 进行下载</div><div class="line">yum install --downloadonly --downloaddir=./download/ openjpeg2.x86_64</div><div class="line"></div><div class="line"># 如果是系统已经安装过的包，则使用yumdownloader命令</div><div class="line">yumdownloader ftp --destdir=./download/</div><div class="line"></div><div class="line"># 制作本地repository</div><div class="line">createrepo -pdo ./download/ ./download/</div><div class="line">createrepo --update ./download/</div><div class="line"></div><div class="line"># 打包成tar.gz文件</div><div class="line">tar czvf download.tar.gz download/</div></pre></td></tr></table></figure>
</li>
<li><p>内网机器上通过制作好的本地repository来安装yum包<br>目标机器上，本地repository目录为<code>/root/tmp/only_for_repository</code>为例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"># 拷贝download.tar.gz到目标机器上 /root/tmp/only_for_repository</div><div class="line"></div><div class="line"># 解压tar包</div><div class="line">tar xzvf download.tar.gz</div><div class="line"></div><div class="line"># 建立本地repository</div><div class="line">cd /etc/yum.repos.d</div><div class="line">cp CentOS-Media.repo local-custom.repo</div><div class="line">vim local-custom.repo</div><div class="line"></div><div class="line">[local-custom]</div><div class="line">name=CentOS-local-custom</div><div class="line">baseurl=file:///root/tmp/only_for_repository/download</div><div class="line">gpgcheck=1</div><div class="line">enabled=1</div><div class="line">gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</div><div class="line"></div><div class="line"># 使用local custom reposiroty进行升级</div><div class="line">yum --disablerepo=\* --enablerepo=local-custom update</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="一些yum命令"><a href="#一些yum命令" class="headerlink" title="一些yum命令"></a>一些yum命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"># 安装yum-config-manager</div><div class="line">yum install yum-utils</div><div class="line"></div><div class="line"># 查看yum repository</div><div class="line">yum repolist [enabled|disabled|all]</div><div class="line"></div><div class="line"># 启用某个yum repository</div><div class="line">1. yum-config-manager --enable repository_name # 或者到</div><div class="line">2. 到/etc/yum.repos.d目录下，找到要启动的repository的文件，修改enabled字段为1</div><div class="line"></div><div class="line"># 禁用某个yum repository</div><div class="line">1. yum-config-manager --disable repository_name</div><div class="line">2. 到/etc/yum.repos.d目录下，找到要启动的repository的文件，修改enabled字段为0</div><div class="line"></div><div class="line"># yum安装</div><div class="line">yum install ftp</div><div class="line"></div><div class="line"># yum更新</div><div class="line">yum update ftp</div><div class="line"></div><div class="line"># yum删除包</div><div class="line">yum remove ftp</div><div class="line"></div><div class="line"># yum 列出包</div><div class="line">yum list ftp # 某个包</div><div class="line">yum list installed # 列出已经安装的包</div><div class="line"></div><div class="line"># 搜索某个包</div><div class="line">yum search ftp</div><div class="line"></div><div class="line"># 查看包的信息</div><div class="line">yum info ftp</div><div class="line"></div><div class="line"># 查看可更新的包</div><div class="line">yum check-update</div><div class="line"></div><div class="line"># yum group 相关</div><div class="line">yum grouplist  # 列出所有组</div><div class="line">yum groupinstall &apos;虚拟化主机&apos;   # 按组安装，支持中文</div><div class="line">yum groupupdate &apos;虚拟化主机&apos;    # 按组更新</div><div class="line">yum groupremove &apos;虚拟化主机&apos;    # 安祖删除</div><div class="line"></div><div class="line"></div><div class="line"># yum 清理cache</div><div class="line">yum clean all</div><div class="line"></div><div class="line"># 查看yum操作记录</div><div class="line">yum history</div><div class="line"></div><div class="line"># 查看某个repository下的可用包, 以pgdg96为例</div><div class="line">yum --disablerepo=&quot;*&quot; --enablerepo=&quot;pgdg96&quot; list available</div><div class="line"></div><div class="line"># 查看某几个repository下的可用包,  repository之间用逗号隔开</div><div class="line">yum --disablerepo=&quot;*&quot; --enablerepo=&quot;pgdg96,epel&quot; list available</div><div class="line"></div><div class="line"># 只更新来自某个repository的package</div><div class="line">yum --disablerepo=&quot;*&quot; --enablerepo=&quot;pgdg96,epel&quot; update</div><div class="line"></div><div class="line"></div><div class="line"># yum security相关</div><div class="line">yum --security check-update  # 检查安全更新</div><div class="line">yum --security update        # 只更新安全补丁</div><div class="line"></div><div class="line"># 查看包依赖</div><div class="line">yum deplist ImageMagick-devel</div></pre></td></tr></table></figure>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://access.redhat.com/solutions/1355683" target="_blank" rel="external">Need to set up yum repository for locally-mounted DVD on Red Hat Enterprise Linux 7 or later</a></li>
<li><a href="https://access.redhat.com/solutions/10021" target="_blank" rel="external">Is it possible to limit yum so that it lists or installs only security updates?</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> CentOS </tag>
            
            <tag> Yum </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[在AWS Lightsail上给hexo blog配置HTTPS]]></title>
      <url>/2019/06/04/configure-https-for-hexo-blog-on-lightsail/</url>
      <content type="html"><![CDATA[<h3 id="申请免费SSL证书"><a href="#申请免费SSL证书" class="headerlink" title="申请免费SSL证书"></a>申请免费SSL证书</h3><p>免费证书渠道:</p>
<ul>
<li>如果是万网的域名，可以使用阿里云的<a href="https://help.aliyun.com/product/28533.html" target="_blank" rel="external">免费SSL证书</a></li>
<li>如果不是万网的域名，可以考虑<a href="https://letsencrypt.org/" target="_blank" rel="external">Let’s Encrypt</a><br>我的域名是万网注册的，因此直接使用阿里云提供的免费SSL证书</li>
</ul>
<h3 id="配置SSL证书"><a href="#配置SSL证书" class="headerlink" title="配置SSL证书"></a>配置SSL证书</h3><ol>
<li><p>lightsail证书配置<br>万网证书下载后，选择其中适用于nginx的版本，将对应的.key和.pem文件传到lightsail instance中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># 建立cert目录</div><div class="line">sudo mkdir -p /etc/nginx/cert</div><div class="line"></div><div class="line"># 上传证书, 可以scp也可以直接vim拷贝写入 (此处命名为www.jibing57.com.pem和www.jibing57.com.key)</div><div class="line">sudo vim /etc/nginx/cert/www.jibing57.com.key</div><div class="line"></div><div class="line">sudo vim /etc/nginx/cert/www.jibing57.com.pem</div></pre></td></tr></table></figure>
</li>
<li><p>修改配置nginx<br>修改nginx配置, 支持SSL，并配置http请求redirect到https。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">$ cat hexo.conf</div><div class="line">server &#123;</div><div class="line">      listen       80;</div><div class="line">      server_name  www.jibing57.com;</div><div class="line">      return 301 https://$server_name$request_uri;</div><div class="line">&#125;</div><div class="line"></div><div class="line">server &#123;</div><div class="line">      listen	   443 ssl;</div><div class="line">      server_name  www.jibing57.com;</div><div class="line">      ssl_session_cache   shared:SSL:10m;</div><div class="line">      ssl_session_timeout 10m;</div><div class="line">      keepalive_timeout   70;</div><div class="line">      ssl_certificate     cert/www.jibing57.com.pem;</div><div class="line">      ssl_certificate_key cert/www.jibing57.com.key;</div><div class="line">      ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;</div><div class="line">      ssl_ciphers         HIGH:!aNULL:!MD5;</div><div class="line"></div><div class="line">      # gzip</div><div class="line">      gzip on;</div><div class="line">      gzip_disable &quot;msie6&quot;;</div><div class="line"></div><div class="line">      gzip_vary on;</div><div class="line">      gzip_proxied any;</div><div class="line">      gzip_comp_level 6;</div><div class="line">      gzip_http_version 1.1;</div><div class="line">      gzip_min_length 256;</div><div class="line">      gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript application/javascript application/vnd.ms-fontobject application/x-font-ttf font/opentype image/svg+xml image/x-icon;</div><div class="line"></div><div class="line">      root         /opt/app/web/hexo;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>重启nginx</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 检查配置是否正确</div><div class="line">sudo service nginx configtest</div><div class="line"></div><div class="line"># 重启nginx</div><div class="line">sudo service nginx restart</div></pre></td></tr></table></figure>
</li>
<li><p>试验<br>访问<code>https://www.jibing57.com</code>成功。</p>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Nginx </tag>
            
            <tag> Hexo </tag>
            
            <tag> Lightsail </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[将hexo部署到AWS的Lightsail]]></title>
      <url>/2019/05/29/how-to-deploy-hexo-to-aws-lightsail/</url>
      <content type="html"><![CDATA[<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>Github Pages的访问最近有些慢，感觉不太稳。最近正好买了台Lightsail，就考虑将blog丢到Lightsail上去, 试试Lightsail的访问速度。<br>整理了一下相关操作步骤，该步骤应该也同样适用于EC2。区别就是Lightsail默认配置了22和80端口是开放的。部署到EC2的话，需要自己设置Security Group开放22和80端口。</p>
<p><strong>步骤前提</strong></p>
<ul>
<li>本地已经有<a href="https://hexo.io/" target="_blank" rel="external">hexo</a>环境</li>
<li>有可用的<a href="https://aws.amazon.com/lightsail/" target="_blank" rel="external">Lightsail Instance</a></li>
</ul>
<p><strong>步骤中特定的信息，部署时可能需要根据自己实际情况进行修改</strong></p>
<ul>
<li>Lightsail Instance相关:<ul>
<li>AMI是原生的</li>
<li>Public IP是<code>54.203.178.195</code></li>
<li>博客目录<code>/opt/app/web/hexo</code></li>
<li>存放git库的账号名字为git, 博客git库代码存放目录:<code>/opt/app/git/hexo.git</code></li>
</ul>
</li>
<li>本地信息<ul>
<li>本地系统是<code>Macos 10.14.4</code></li>
<li>以identity key而不是密码形式登录远端服务器<ul>
<li>此处在~/tmp/tmp/下新生成key，也可以按照实际要求修改为使用平时用的key</li>
</ul>
</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h3 id="aws-lightsail侧步骤"><a href="#aws-lightsail侧步骤" class="headerlink" title="aws lightsail侧步骤"></a>aws lightsail侧步骤</h3><ol>
<li><p>安装Nginx<br>Amazon yum源中的nginx版本是1.14.1, 原生的nginx 1.14.1是带有一些已知安全漏洞的buggy版本, Amazon yum源中的版本不知道是不是bug fixed的版本。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum install nginx</div></pre></td></tr></table></figure>
</li>
<li><p>安装git</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum install git</div></pre></td></tr></table></figure>
</li>
<li><p>创建app和git库目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"># 创建hexo的web目录</div><div class="line">sudo mkdir -p /opt/app/web/hexo</div><div class="line">sudo chown -R nginx:nginx /opt/app/web</div><div class="line">sudo chmod g+w /opt/app/web/hexo</div><div class="line"></div><div class="line"># 创建git账号和代码库路径</div><div class="line">sudo mkdir -p /opt/app/git</div><div class="line">sudo useradd git</div><div class="line">sudo chown -R git:git /opt/app/git</div><div class="line"></div><div class="line"># 添加git到nginx组</div><div class="line">sudo usermod -a -G nginx git</div><div class="line"></div><div class="line"># 建立hexo.git库</div><div class="line">sudo -u git bash</div><div class="line">cd /opt/app/git/</div><div class="line">git init --bare hexo.git</div><div class="line"></div><div class="line"># 创建git钩子,用来自动发布</div><div class="line">vim /opt/app/git/hexo.git/hooks/post-receive</div><div class="line"></div><div class="line">  #!/bin/bash</div><div class="line">  git --work-tree=/opt/app/web/hexo --git-dir=/opt/app/git/hexo.git checkout -f</div><div class="line"></div><div class="line"># 添加可执行</div><div class="line">chmod +x /opt/app/git/hexo.git/hooks/post-receive</div><div class="line"></div><div class="line"># 退出git用户</div><div class="line">exit</div></pre></td></tr></table></figure>
</li>
<li><p>配置nginx</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"># 修改nginx配置文件, 添加hexo的server</div><div class="line">sudo vim /etc/nginx/conf.d/hexo.conf</div><div class="line">  server &#123;</div><div class="line">        listen       80;</div><div class="line">        server_name  54.203.178.195;</div><div class="line">        root         /opt/app/web/hexo;</div><div class="line">  &#125;</div><div class="line"></div><div class="line"># 添加测试页</div><div class="line">sudo -u nginx vim /opt/app/web/hexo/index.html</div><div class="line">&lt;h1&gt;I&apos;m working here.&lt;/h1&gt;</div><div class="line"></div><div class="line"># 启动nginx</div><div class="line">sudo service nginx start</div><div class="line"></div><div class="line"># 测试是否OK</div><div class="line">浏览器访问: http://54.203.178.195/</div><div class="line"></div><div class="line"># 设置开机自启动</div><div class="line">sudo chkconfig nginx on</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="本地hexo设置"><a href="#本地hexo设置" class="headerlink" title="本地hexo设置"></a>本地hexo设置</h3><p>将本地常用的public key拷贝到git用户中, 再设置本地~/.ssh/config</p>
<ol>
<li><p>生成ssh identity key</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 生成key 在目录~/tmp/tmp/下, 会生成私钥id_rsa_for_hexo_git和公钥id_rsa_for_hexo_git.pub</div><div class="line">mkdir -p ~/tmp/tmp/</div><div class="line">cd ~/tmp/tmp/</div><div class="line">ssh-keygen -f ./id_rsa_for_hexo_git</div></pre></td></tr></table></figure>
</li>
<li><p>拷贝公钥到lightsail instance的git账号下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># 登录服务器,切换为git账号</div><div class="line">sudo -u git bash</div><div class="line"></div><div class="line"># 创建.ssh，拷贝id_rsa_for_hexo_git.pub内容到~/.ssh/authorized_keys中</div><div class="line">cd</div><div class="line">mkdir .ssh</div><div class="line">chmod 700 .ssh</div><div class="line">echo &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDm2R5Siv8mig2wyBBRhdCxjGE437zTXslAA0j5XLaeBQRF+m+Zw45ygMYG9DEWUMCSXo1OZWefWeKX+pMGibZ4GxHGgGlVChr9DFwmIWoKR+vOKnL6S48ZqFf3idASFyqxiSH5s+RrB74DQj1n4rGIM5djkniyRWt4hCCp+ZHXuH1F3VLlt38MusO5nctdroYUbuIGPA50rU+NHLxEKeN55Yn0jzq7KaaRcNiRbpb2rKGirhb6YbWSLq2v9OoYP2GSDaM8/JsBO9saeF1nt5PYstJT3Re4SbHxiUODGlif8ExHANIf/DKCyn6Quok9PhMVUUrlaTOX49WEIfmfXiIh&quot; &gt;&gt; ~/.ssh/authorized_keys</div><div class="line">chmod 600 ~/.ssh/authorized_keys</div></pre></td></tr></table></figure>
</li>
<li><p>设定本地ssh config，设置免密登录lightsail的git账号</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 设置本地~/.ssh/config, 添加如下内容</div><div class="line">Host &quot;54.203.178.195&quot;</div><div class="line">  User &quot;git&quot;</div><div class="line">  IdentityFile ~/tmp/tmp/id_rsa_for_hexo_git</div></pre></td></tr></table></figure>
</li>
<li><p>修改hexo deploy 配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 修改hexo deploy config, 修改hexo目录下_config.yml的deploy节点为如下</div><div class="line">deploy:</div><div class="line"> type: git</div><div class="line"> repo: git@54.203.178.195:/opt/app/git/hexo.git/</div><div class="line"> branch: master</div></pre></td></tr></table></figure>
</li>
<li><p>hexo发布</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 发布</div><div class="line">hexo clean</div><div class="line">hexo generate</div><div class="line">hexo deploy</div></pre></td></tr></table></figure>
</li>
<li><p>验证<br>访问<a href="http://54.203.178.195/" target="_blank" rel="external">http://54.203.178.195/</a>, 看是否能正常work</p>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Nginx </tag>
            
            <tag> Hexo </tag>
            
            <tag> Lightsail </tag>
            
            <tag> Git </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Elastic Beanstalk Ruby Passenger平台下设置HTTP重定向到HTTPS]]></title>
      <url>/2019/05/21/how-to-direct-http-to-https-in-elasticbeanstalk-ruby-passenger-platform/</url>
      <content type="html"><![CDATA[<p>Elastic Beanstalk配置HTTPS后，接下来就需要配置HTTP请求redirect到HTTPS。<br>以前都是直接在application层面做redirect的。这次调查了一下在passenger层面来进行redirect的做法,记录在此。</p>
<h3 id="Ruby平台基本信息"><a href="#Ruby平台基本信息" class="headerlink" title="Ruby平台基本信息:"></a>Ruby平台基本信息:</h3><ul>
<li>EB平台: <code>Passenger with Ruby 2.3 running on 64bit Amazon Linux/2.9.3</code><ul>
<li>Ruby 2.3</li>
<li>Passenger模式</li>
</ul>
</li>
<li>前端使用ALB进行负载均衡，和后端EC2之间端口对应关系:<ul>
<li>ALB:443 -&gt; EC2:80 (在ALB侧进行HTTPS terminate)</li>
<li>ALB:80 -&gt; EC2:80</li>
</ul>
</li>
</ul>
<h3 id="修改方法"><a href="#修改方法" class="headerlink" title="修改方法"></a>修改方法</h3><p>使用<code>.ebextension</code>直接修改passenger的配置文件<code>/opt/elasticbeanstalk/support/conf/nginx_config.erb</code>.<br>修改要点:</p>
<ol>
<li>动态请求http_x_forwarded_proto 不是https的，redirect为https</li>
<li>静态文件请求的http_x_forwarded_proto不是https的，redirect为https</li>
<li>来自ELB health check的请求还是直接返回请求，而不用redirect为https</li>
</ol>
<a id="more"></a>
<p>主要修改点如下:</p>
<ul>
<li><p>添加<code>location /</code>节点，判断<code>http_x_forwarded_proto</code>是否为https</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">set $redirect 0;</div><div class="line">location / &#123;</div><div class="line">    if ($http_x_forwarded_proto != &quot;https&quot;) &#123;</div><div class="line">      set $redirect 1;</div><div class="line">    &#125;</div><div class="line">    if ($http_user_agent ~* &quot;ELB-HealthChecker&quot;) &#123;</div><div class="line">      set $redirect 0;</div><div class="line">    &#125;</div><div class="line">    if ($redirect = 1) &#123;</div><div class="line">      return 301 https://$host$request_uri;</div><div class="line">    &#125;</div><div class="line">    passenger_enabled on;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>修改静态文件location，添加<code>http_x_forwarded_proto</code>的判断。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"># Rails asset pipeline support.</div><div class="line"># location ~ &quot;^/assets/.+-[0-9a-f]&#123;32&#125;\..+&quot; &#123;</div><div class="line">location ~ &quot;^/assets/.+-([0-9a-f]&#123;32&#125;|[0-9a-f]&#123;64&#125;)\..+&quot; &#123;</div><div class="line">    if ($http_x_forwarded_proto != &quot;https&quot;) &#123;</div><div class="line">      set $redirect 1;</div><div class="line">    &#125;</div><div class="line">    if ($http_user_agent ~* &quot;ELB-HealthChecker&quot;) &#123;</div><div class="line">      set $redirect 0;</div><div class="line">    &#125;</div><div class="line">    if ($redirect = 1) &#123;</div><div class="line">      return 301 https://$host$request_uri;</div><div class="line">    &#125;</div><div class="line">    error_page 490 = @static_asset;</div><div class="line">    error_page 491 = @dynamic_request;</div><div class="line">    recursive_error_pages on;</div><div class="line"></div><div class="line">    if (-f $request_filename) &#123;</div><div class="line">        return 490;</div><div class="line">    &#125;</div><div class="line">    if (!-f $request_filename) &#123;</div><div class="line">        return 491;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<p>完整配置参见<a href="https://github.com/jibing57/my-snippet/blob/master/AWS/ElasticBeanstalk/02_redirect_http_to_https/20-update-eb-config-for-http-redirect-to-https.config" target="_blank" rel="external">Github</a>。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://github.com/awsdocs/elastic-beanstalk-samples/tree/master/configuration-files/aws-provided/security-configuration/https-redirect/ruby-passenger/https-redirect-load-balanced-ruby-passenger" target="_blank" rel="external">Github awsdocs/elastic-beanstalk-samples</a></li>
<li><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-httpredirect.html" target="_blank" rel="external">Configuring HTTP to HTTPS Redirection</a> </li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Elastic Beanstalk </tag>
            
            <tag> HTTP </tag>
            
            <tag> HTTPS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何在Elastic Load Balancer中添加SSL证书]]></title>
      <url>/2019/05/16/how-to-add-SSL-Certification-to-Elastic-Load-Balancer/</url>
      <content type="html"><![CDATA[<h3 id="上传方式"><a href="#上传方式" class="headerlink" title="上传方式"></a>上传方式</h3><p>AWS侧上传HTTPS证书有两种方式</p>
<ul>
<li>使用 AWS Certificate Manager (ACM)生成，优点是免费，和aws其他服务集成性好，缺点是只能在AWS中使用</li>
<li>上传第三方证书到AWS Identity and Access Management (IAM) 中，优点是证书生成灵活，缺点是操作麻烦。</li>
</ul>
<p>中国区的AWS还没有ACM服务，因此只能使用IAM的方式来上传证书。</p>
<h3 id="证书相关的一系列命令"><a href="#证书相关的一系列命令" class="headerlink" title="证书相关的一系列命令"></a>证书相关的一系列命令</h3><ol>
<li><p>证书上传命令<br>如果没有<code>certificate-chain</code>，可以不传。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ aws iam upload-server-certificate --server-certificate-name ExampleCertificate </div><div class="line">                                    --certificate-body file://Certificate.pem </div><div class="line">                                    --certificate-chain file://CertificateChain.pem </div><div class="line">                                    --private-key file://PrivateKey.pem</div></pre></td></tr></table></figure>
</li>
<li><p>列出证书命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ aws iam list-server-certificates</div></pre></td></tr></table></figure>
</li>
<li><p>获取证书信息命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ aws iam get-server-certificate --server-certificate-name ExampleCertificate</div></pre></td></tr></table></figure>
</li>
<li><p>删除证书命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ aws iam delete-server-certificate --server-certificate-name ExampleCertificate</div></pre></td></tr></table></figure>
</li>
</ol>
<a id="more"></a>
<h3 id="在EB中设置使用SSL证书"><a href="#在EB中设置使用SSL证书" class="headerlink" title="在EB中设置使用SSL证书"></a>在EB中设置使用SSL证书</h3><p>在EB中为ALB添加SSL证书的步骤如下:</p>
<ol>
<li>在EB中，点击[Configuration]-&gt;[Load balancer]打开Load balancer配置页面。</li>
<li><p>点击Application Load Balancer右侧的Add listener，添加listener.<br><img src="/images/AWS/ALB/alb_setting_in_eb.png" alt="alb_setting_in_eb.png"></p>
</li>
<li><p>在Application Load Balancer Listener的添加页面中，设置如下, 再点击Add进行添加Listener.</p>
<ul>
<li>Port设为443</li>
<li>Protocol选择HTTPS</li>
<li>SSL certificate中选择IAM中上传的证书</li>
<li>SSL policy可以自选，也可以留空<br><img src="/images/AWS/ALB/add_ssl_listener_in_eb.png" alt="add_ssl_listener_in_eb.png"></li>
</ul>
</li>
<li><p>回到Load balancer配置页面，可以看到Application Load Balancer部分，多了刚设置的443 HTTPS的Listener。<br>并且EB会自动在Rules部分加上ALB 443端口到默认的target 80的rule, 还会在ALB所在的Security Group中自动添加允许443的inbound rules。<br><img src="/images/AWS/ALB/alb_auto_configure_in_eb.png" alt="alb_auto_configure_in_eb.png"></p>
</li>
<li><p>最后点击右下角的Apply按钮更新配置，或点击Continue后来检查更改。</p>
</li>
</ol>
<h3 id="ALB中手动添加SSL证书"><a href="#ALB中手动添加SSL证书" class="headerlink" title="ALB中手动添加SSL证书"></a>ALB中手动添加SSL证书</h3><ol>
<li><p>在EC2界面，左侧选择Load Balancers, 再选择要设置的Load Balance，下部tab切换至Listeners。<br><img src="/images/AWS/ALB/alb_on_ec2.png" alt="alb_on_ec2.png"></p>
</li>
<li><p>点击Add listener, 在listener添加页面中</p>
<ul>
<li>Protocol : port选择 HTTPS:443</li>
<li>Default action中选择Forward to或者Redirect to一个已有的Target</li>
<li>Default SSL certificate中，选择From IAM，再选择在IAM中upload的证书。<br>点击右上角Save进行保存。<br><img src="/images/AWS/ALB/ssl_listener_setting_on_ec2.png" alt="ssl_listener_setting_on_ec2.png"></li>
</ul>
</li>
</ol>
<p><strong>注意</strong>: <strong>如果443端口不通的话，记住检查一下ALB所在的Security Group中有没有开放443端口。</strong></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://amazonaws-china.com/premiumsupport/knowledge-center/import-ssl-certificate-to-iam/?nc1=h_ls" target="_blank" rel="external">How can I upload and import an SSL certificate to AWS Identity and Access Management (IAM)?</a></li>
<li><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-ssl-upload.html" target="_blank" rel="external">Upload a Certificate to IAM</a></li>
<li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html" target="_blank" rel="external">Working with Server Certificates</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Elastic Load Balancing </tag>
            
            <tag> IAM </tag>
            
            <tag> HTTP </tag>
            
            <tag> Application Load Balancing </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[在Mac下修改Office的界面显示语言]]></title>
      <url>/2019/05/10/how-to-change-office-UI-language-on-macos/</url>
      <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>在Macos中，微软的Office套件的UI，是根据系统默认语言来设置的。系统是中文，Office就是中文UI，系统是英文，Office就是英文UI。微软并没有在Office的偏好设置中，设置一个可以切换语言的设置。<br>这让我这种想保持系统英文，Office中文的人感到很难受。<br>官网上搜了一圈，都没有找到可用的办法。功夫不负有心人，最终在office社区里面的这篇帖子提供了解决办法<a href="https://answers.microsoft.com/en-us/msoffice/forum/msoffice_other-mso_mac-mso_mac2016/how-to-manually-change-the-language-used-in-office/abe2a9c1-f550-45de-9d0e-58b99f206c41?page=2" target="_blank" rel="external">How to manually change the language used in Office for Mac 2016 without changing OS language?</a>, 还是我最喜欢的命令行方式。虽然是15，16年的帖子，但是对应的方法，在最新版的Office(版本16.24-19041401)中仍然可用。</p>
<h3 id="修改方法"><a href="#修改方法" class="headerlink" title="修改方法"></a>修改方法</h3><p>以work为例，记录相关命令如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># 修改word的语言</div><div class="line">$ defaults write com.microsoft.Word AppleLanguages &apos;(&quot;zh-cn&quot;)&apos;</div><div class="line"></div><div class="line"># 读取word的设置, 可以看到多了一个AppleLanguages的选项。</div><div class="line">$ defaults read com.microsoft.Word</div><div class="line">&#123;</div><div class="line">    ...</div><div class="line">    AppleLanguages =     (</div><div class="line">        &quot;zh-cn&quot;</div><div class="line">    );</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>重启word，可以看到word会提示中文的对话框”默认编辑语言已更改”.<br><img src="/images/Microsoft/Office/word_language_change.png" alt="word_language_change"></p>
<p>每个office组件都是独立修改的，Excel和Powerpoint的修改命令如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">defaults write com.microsoft.Excel AppleLanguages &apos;(&quot;zh-cn&quot;)&apos;</div><div class="line">defaults write com.microsoft.Powerpoint AppleLanguages &apos;(&quot;zh-cn&quot;)&apos;</div></pre></td></tr></table></figure></p>
<h3 id="Office字体对照关系"><a href="#Office字体对照关系" class="headerlink" title="Office字体对照关系"></a>Office字体对照关系</h3><p>摘录自简书上的一篇文章<a href="https://www.jianshu.com/p/8cf09c5144e2" target="_blank" rel="external">英文版 Mac 上 office 的中文字体</a>, 记录一下，以备不时之需。</p>
<blockquote>
<p>Mac下英文版Office中常用的中文字体的名字对照关系如下:</p>
<ul>
<li>华文细黑：STHeiti Light [STXihei]</li>
<li>华文黑体：STHeiti</li>
<li>华文楷体：STKaiti</li>
<li>华文宋体：STSong</li>
<li>华文仿宋：STFangsong</li>
<li>俪黑 Pro：LiHei Pro Medium</li>
<li>俪宋 Pro：LiSong Pro Light</li>
<li>标楷体：BiauKai</li>
<li>苹果俪中黑：Apple LiGothic Medium</li>
<li>苹果俪细宋：Apple LiSung Light</li>
</ul>
<p>Windows的</p>
<ul>
<li>新细明体：PMingLiU</li>
<li>细明体：MingLiU</li>
<li>标楷体：DFKai-SB</li>
<li>黑体：SimHei</li>
<li>宋体：SimSun</li>
<li>新宋体：NSimSun</li>
<li>仿宋：FangSong</li>
<li>楷体：KaiTi</li>
<li>仿宋_GB2312：FangSong_GB2312</li>
<li>楷体_GB2312：KaiTi_GB2312</li>
<li>微软正黑体：Microsoft JhengHei</li>
<li>微软雅黑体：Microsoft YaHei</li>
</ul>
</blockquote>
<h3 id="小发现"><a href="#小发现" class="headerlink" title="小发现"></a>小发现</h3><p>查看<code>defaults read com.microsoft.Word</code>的时候，发现有一个名为<code>ChangeInstallLanguageState</code>的值，应该是用来记录总共更换过软件语言的次数的，为啥需要记录这个值呢？呵呵~~</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://answers.microsoft.com/en-us/msoffice/forum/msoffice_other-mso_mac-mso_mac2016/how-to-manually-change-the-language-used-in-office/abe2a9c1-f550-45de-9d0e-58b99f206c41?page=2" target="_blank" rel="external">How to manually change the language used in Office for Mac 2016 without changing OS language?</a></li>
<li><a href="https://www.jianshu.com/p/8cf09c5144e2" target="_blank" rel="external">英文版 Mac 上 office 的中文字体</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Office </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Macos下制作CentOS 7 USB启动盘]]></title>
      <url>/2019/05/01/way-to-make-centos7-bootable-usb-on-macos/</url>
      <content type="html"><![CDATA[<p>记录下在MacOS中制作Linux USB启动盘的操作步骤。</p>
<h3 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h3><h4 id="查看磁盘挂载分区"><a href="#查看磁盘挂载分区" class="headerlink" title="查看磁盘挂载分区"></a>查看磁盘挂载分区</h4><p>使用命令<code>diskutil list</code>查看所在U盘的分区，找到U盘的挂载点，此处挂载点为<code>/dev/disk2</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ diskutil list</div><div class="line">...</div><div class="line"></div><div class="line">/dev/disk2 (external, physical):</div><div class="line">   #:                       TYPE NAME                    SIZE       IDENTIFIER</div><div class="line">   0:     FDisk_partition_scheme                        *31.0 GB    disk2</div><div class="line">   1:                 DOS_FAT_32 UNTITLED                31.0 GB    disk2s1</div><div class="line"></div><div class="line">$</div></pre></td></tr></table></figure></p>
<h4 id="卸载U盘挂载"><a href="#卸载U盘挂载" class="headerlink" title="卸载U盘挂载"></a>卸载U盘挂载</h4><p>使用<code>diskutil unmountDisk</code>命令，卸载U盘的挂载。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ diskutil unmountDisk /dev/disk2</div><div class="line">Unmount of all volumes on disk2 was successful</div><div class="line">$</div></pre></td></tr></table></figure></p>
<p>如果不卸载挂载点就写入启动盘，则会提示<code>dd: /dev/disk2: Resource busy</code>。</p>
<a id="more"></a>
<h4 id="使用dd来来写入iso"><a href="#使用dd来来写入iso" class="headerlink" title="使用dd来来写入iso"></a>使用dd来来写入iso</h4><p>使用dd命令来将CentOS写入启动盘，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo dd if=~/carl_workSpace/software/os/CentOS-7-x86_64-DVD-1810.iso of=/dev/rdisk2 bs=1m</div></pre></td></tr></table></figure>
<p><strong>注意:</strong></p>
<ol>
<li>此处<code>~/carl_workSpace/software/os/CentOS-7-x86_64-DVD-1810.iso</code>是我本地CentOS的路径，需要替换成实际的路径</li>
<li><code>/dev/rdisk2</code>就是上面<code>diskutil list</code>列出的U盘挂载点, 并且注意，此处磁盘前面多了个<strong>r</strong>，是<code>rdisk2</code>，而不是<code>disk2</code>，<code>rdisk2</code>是<code>disk2</code>的原始盘，目的是可以更快速的写入。</li>
</ol>
<p>写入需要花费几分钟时间，期间可以使用CTRL + T来查看写入进度，显示如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">109+0 records in</div><div class="line">108+0 records out</div><div class="line">113246208 bytes transferred in 7.430910 secs (15239884 bytes/sec)</div></pre></td></tr></table></figure></p>
<p>也可以使用<code>iostat</code>来查看磁盘写入进度<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ iostat -w 5</div><div class="line">              disk0               disk2       cpu    load average</div><div class="line">    KB/t  tps  MB/s     KB/t  tps  MB/s  us sy id   1m   5m   15m</div><div class="line">   42.68   14  0.58   849.97    0  0.00   7  4 89  3.84 3.42 2.67</div><div class="line">  450.16   15  6.50  1024.00    7  7.19   3  3 94  3.70 3.39 2.67</div><div class="line">   85.34  124 10.33  1024.00    9  8.80   6  4 90  3.64 3.39 2.67</div><div class="line"></div><div class="line">$</div></pre></td></tr></table></figure></p>
<p>最终完成后，dd命令输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">4376+0 records in</div><div class="line">4376+0 records out</div><div class="line">4588568576 bytes transferred in 539.126637 secs (8511115 bytes/sec)</div></pre></td></tr></table></figure>
<p>写入完成后，Macos会有一个提示框，提示“此电脑不能读取您插入的磁盘。”<br><img src="/images/Linux/disk_was_not_readable.png" alt="disk_was_not_readable.png"></p>
<p>USB启动盘不能被Macos正常读取，但是可以用来被当作启动盘安装CentOS。</p>
<p>使用<code>diskutil list</code>可以查看到此时U盘的分区信息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ diskutil list</div><div class="line">...</div><div class="line">/dev/disk2 (external, physical):</div><div class="line">   #:                       TYPE NAME                    SIZE       IDENTIFIER</div><div class="line">   0:     FDisk_partition_scheme                        *31.0 GB    disk2</div><div class="line">   1:                       0xEF                         8.9 MB     disk2s2</div><div class="line"></div><div class="line">$</div></pre></td></tr></table></figure>
<h4 id="弹出U盘"><a href="#弹出U盘" class="headerlink" title="弹出U盘"></a>弹出U盘</h4><p>使用“磁盘工具”APP或者命令<code>diskutil eject</code>弹出U盘。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">diskutil eject /dev/disk2</div></pre></td></tr></table></figure></p>
<h3 id="延伸"><a href="#延伸" class="headerlink" title="延伸"></a>延伸</h3><h4 id="Macos中-dev-disk和-dev-rdisk的区别"><a href="#Macos中-dev-disk和-dev-rdisk的区别" class="headerlink" title="Macos中/dev/disk和/dev/rdisk的区别"></a>Macos中/dev/disk和/dev/rdisk的区别</h4><p>首先查看<code>man hdiutil</code>的描述:</p>
<blockquote>
<p>Since any /dev entry can be treated as a raw disk image, it is worth noting which devices can be accessed when and how.  /dev/rdisk nodes are character-special devices, but are “raw” in the BSD sense and force block-aligned I/O.  They are closer to the physical disk than the buffer cache.  /dev/disk nodes, on the other hand, are buffered block-special devices and are used primarily by the kernel’s filesystem code.</p>
</blockquote>
<p><code>/dev/rdisk</code>是原始读取模式，没有经过文件系统的文件缓存机制，因此速度比<code>/dev/disk</code>速度更快。</p>
<p>下面以918M大小的<code>CentOS-7-x86_64-Minimal-1810.iso</code>为例来比较<code>/dev/rdisk</code>和<code>/dev/disk</code>的写入速度。两者的命令分别为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 写入/dev/rdisk的速度</div><div class="line">$ sudo dd if=CentOS-7-x86_64-Minimal-1810.iso of=/dev/rdisk2 bs=1m</div><div class="line">918+0 records in</div><div class="line">918+0 records out</div><div class="line">962592768 bytes transferred in 106.192945 secs (9064564 bytes/sec)</div><div class="line"></div><div class="line"># 写入/dev/disk的速度</div><div class="line">sudo dd if=CentOS-7-x86_64-Minimal-1810.iso of=/dev/disk2 bs=1m</div><div class="line">918+0 records in</div><div class="line">918+0 records out</div><div class="line">962592768 bytes transferred in 3016.605565 secs (319098 bytes/sec)</div></pre></td></tr></table></figure>
<p>可以看到写入<code>/dev/rdisk</code>花费了106秒，而写入<code>/dev/disk</code>花费了3016秒, 差别巨大。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://cloudwrk.com/create-centos-7-bootable-usb-on-osx/" target="_blank" rel="external">CREATE CENTOS 7 BOOTABLE USB ON OSX</a></li>
<li><a href="https://superuser.com/questions/631592/why-is-dev-rdisk-about-20-times-faster-than-dev-disk-in-mac-os-x" target="_blank" rel="external">Why is “/dev/rdisk” about 20 times faster than “/dev/disk” in Mac OS X</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> CentOS </tag>
            
            <tag> MacOS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS Autoscaling的Scheduled Scaling(计划扩展)]]></title>
      <url>/2019/05/01/scheduled-action-of-aws-autoscaling/</url>
      <content type="html"><![CDATA[<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>除了用Alarm触发的Scaling Policy，AWS Autoscaling(ASG) 还提供了一个Scheduled Scaling(计划扩展)的功能。该功能主要用作一些流量可预测的情形。比如国外的Black Friday和国内6.18或双11这种活动时，就可以使用Scheduled Scaling来提前规划好机器数量。<br>详细说明和设置参见 AWS官方文档<a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html" target="_blank" rel="external">Scheduled Scaling for Amazon EC2 Auto Scaling</a>。</p>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>举基于如下场景的一个例子</p>
<ol>
<li>正常流量下，2台服务器就可以支撑。</li>
<li>周一到周五的早上8点~10点间是流量高峰期，此时需要4台服务器才能支撑对应的流量。</li>
<li>10点过后恢复正常流量。</li>
</ol>
<p>如上场景分解为2个Schedule Action</p>
<ol>
<li>周一到周五每天早上8点，扩展服务器数量为4.</li>
<li>周一到周五每天早上10点，缩减服务器数量为2.</li>
</ol>
<a id="more"></a>
<h4 id="设置扩展"><a href="#设置扩展" class="headerlink" title="设置扩展"></a>设置扩展</h4><p>设置周一到周五的早上8点进行扩展<br>选中要设置的autoscaling组，点开[Scheduled Actions]的tab，点击Create Scheduled Action按钮。<br>设置如下:</p>
<ul>
<li>Name: 设为WorkdayMorningScalingOut</li>
<li>Min: 设为4</li>
<li>Max和Desired Capacity为空，即Scheduled Actions策略不会修改ASG的对应值</li>
<li>Recurrence:<ul>
<li>下拉菜单选择Cron</li>
<li>时间填写 <code>0 0 * * 1-5</code> (Schedule Action中时间格式是按照UTC来设置的，因此北京时间8点，应该设置为UTC 0时)</li>
</ul>
</li>
<li>Start Time和End Time可以留空。<br><img src="/images/AWS/ASG/workday_morning_scaling_out.png" alt="workday_morning_scaling_out.png"></li>
</ul>
<h4 id="设置缩减"><a href="#设置缩减" class="headerlink" title="设置缩减"></a>设置缩减</h4><p>设置周一到周五的早上10点进行缩减<br>选中要设置的autoscaling组，点开[Scheduled Actions]的tab，点击Create Scheduled Action按钮。<br>设置如下:</p>
<ul>
<li>Name: 起名为WorkdayMorningScalingIn</li>
<li>Min: 设为2</li>
<li>Max 为空</li>
<li>Desired Capacity 设为2。(注意如果ASG有Scaling Policies，那么此处建议留空，从而让ASG使用Scaling Policies判定的Desired Capacity)</li>
<li>Recurrence:<ul>
<li>下拉菜单选择Cron</li>
<li>时间填写 <code>0 2 * * 1-5</code> (Schedule Action中时间格式是按照UTC来设置的，因此北京时间10点，应该设置为UTC 2时)</li>
</ul>
</li>
<li>Start Time和End Time可以留空。</li>
</ul>
<p><img src="/images/AWS/ASG/workday_morning_scaling_in.png" alt="workday_morning_scaling_in.png"></p>
<h4 id="扩展列表"><a href="#扩展列表" class="headerlink" title="扩展列表"></a>扩展列表</h4><p>设置就好后，在列表中可以看到所设置的Scheduled Action。<br><img src="/images/AWS/ASG/schedule_action_list.png" alt="schedule_action_list.png"></p>
<p><strong>注意点</strong></p>
<ol>
<li><strong>Schedule Action中时间格式是按照UTC来设置的，进行设置时，需要将北京时间-8来换算成UTC时间</strong></li>
<li><strong>如果除了Scheduled Action以外，ASG还设置了Scaling Policies，那么设置Scheduled Action的时候，无论是扩展还是缩减，建议都只修改Min和Max，而将Desired Capacity都留空。这样可以让ASG使用Scaling Policies判定的Desired Capacity, 避免Scheduled Action和Scaling Policies之间有冲突。</strong></li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html" target="_blank" rel="external">Scheduled Scaling for Amazon EC2 Auto Scaling</a></li>
<li><a href="https://mohankamaraj.wordpress.com/2016/08/10/auto-scaling-scheduled-scaling-using-cron-expression/" target="_blank" rel="external">Auto Scaling – Scheduled Scaling using CRON expression</a></li>
<li><a href="https://amazonaws-china.com/premiumsupport/knowledge-center/auto-scaling-troubleshooting/" target="_blank" rel="external">How do I troubleshoot scaling issues with my Amazon EC2 Auto Scaling group?</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AutoScaling </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[MacOS上VMWare Fusion中如何给网口设置固定IP]]></title>
      <url>/2019/04/28/how-to-use-static-IP-in-VMWare-Fusion-on-Mac-OS-X/</url>
      <content type="html"><![CDATA[<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>VMWare Fusion中，虚拟机的网口使用NAT模式时，获取到的IP地址是通过DHCP动态分配的，不是固定分配的。每次开机虚拟机网口IP都有可能变动，SSH登录时每次要去查看，太不方便。<br>找了下如何设置固定IP的方法，记录如下。</p>
<p>在如下环境中可以设置成功:</p>
<ul>
<li>MacOS Version: 10.14.4 (18E226)</li>
<li>Vmware Fusion Version: 10.1.5 (10950653)</li>
</ul>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><h4 id="1-获取虚拟主机的MAC地址"><a href="#1-获取虚拟主机的MAC地址" class="headerlink" title="1. 获取虚拟主机的MAC地址"></a>1. 获取虚拟主机的MAC地址</h4><p>默认安装下，所有的虚拟主机的Image都存放在<code>~/Documents/Virtual\ Machines.localized/</code>下，以名为<code>CentOS 7 64bit</code>的虚拟机为例。获取命令为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat ~/Documents/Virtual\ Machines.localized/CentOS\ 7\ 64bit.vmwarevm/CentOS\ 7\ 64bit.vmx | grep generatedAddress</div></pre></td></tr></table></figure></p>
<p>找到对应网口的MAC地址, 本文中要绑定的网口<code>ethernet0</code>的MAC地址为<code>00:0C:29:23:6E:CC</code>。</p>
<h4 id="2-设置静态IP"><a href="#2-设置静态IP" class="headerlink" title="2. 设置静态IP"></a>2. 设置静态IP</h4><p>打开配置文件<code>/Library/Preferences/VMware\ Fusion/vmnet8/dhcpd.conf</code>, <code>vmnet8</code>是VMWare用来设置NAT的虚拟网口，在MacOS下使用命令<code>ifconfig</code>可以看到该虚拟网口。</p>
<p>在配置文件中可以看到DHCP的IP池为192.168.187.128 ~ 192.168.187.254(每个机器可能不同，以实际显示的为准)<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">subnet 192.168.187.0 netmask 255.255.255.0 &#123;</div><div class="line">	range 192.168.187.128 192.168.187.254;</div><div class="line">	option broadcast-address 192.168.187.255;</div><div class="line">	option domain-name-servers 192.168.187.2;</div><div class="line">	option domain-name localdomain;</div><div class="line">	default-lease-time 1800;                # default is 30 minutes</div><div class="line">	max-lease-time 7200;                    # default is 2 hours</div><div class="line">	option netbios-name-servers 192.168.187.2;</div><div class="line">	option routers 192.168.187.2;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>设置静态IP，需要避免使用DHCP的IP池中的IP段，也别使用网段的前几个IP，以防和VM管理IP起冲突，比如<code>192.168.187.1</code>,<code>192.168.187.2</code>就是默认被Fusion占用的。<br>此处以选取192.168.187.100为例。在配置文件末尾添加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">host CentOS7_64bit&#123;</div><div class="line">    hardware ethernet 00:0C:29:23:6E:CC;</div><div class="line">    fixed-address  192.168.187.100;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><strong> 注意: 没有相关知识，不要擅自去修改<code>DO NOT MODIFY SECTION</code>之间的内容, 而是将自定义的配置添加到末尾。</strong></p>
<h4 id="3-退出VMware-Fusion"><a href="#3-退出VMware-Fusion" class="headerlink" title="3. 退出VMware Fusion"></a>3. 退出VMware Fusion</h4><p>修改的配置，需要重启VMware Fusion才能生效。关闭所有虚拟机，Command + Q 退出VM Fusion。</p>
<h4 id="4-检验设置"><a href="#4-检验设置" class="headerlink" title="4. 检验设置"></a>4. 检验设置</h4><p>重启VMware Fusion， 再启动虚拟机<code>CentOS 7 64bit</code>，可以看到MAC地址为<code>00:0c:29:23:6e:cc</code>的网口<code>ens33</code>的IP地址就是设定的<code>192.168.187.100</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># ifconfig</div><div class="line">ens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</div><div class="line">        inet 192.168.187.100  netmask 255.255.255.0  broadcast 192.168.187.255</div><div class="line">        inet6 fe80::1092:5557:4eb5:9694  prefixlen 64  scopeid 0x20&lt;link&gt;</div><div class="line">        ether 00:0c:29:23:6e:cc  txqueuelen 1000  (Ethernet)</div><div class="line">        RX packets 143  bytes 17120 (16.7 KiB)</div><div class="line">        RX errors 0  dropped 0  overruns 0  frame 0</div><div class="line">        TX packets 114  bytes 15575 (15.2 KiB)</div><div class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</div><div class="line"></div><div class="line">        ...</div><div class="line">#</div></pre></td></tr></table></figure>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://willwarren.com/2015/04/02/set-static-ip-address-in-vmware-fusion-7/" target="_blank" rel="external">Set a Static IP Address in VMware Fusion 7</a> </li>
<li><a href="https://gist.github.com/pjkelly/1068716" target="_blank" rel="external">How to setup your VMWare Fusion images to use static IP addresses on Mac OS X</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> VMware </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Vmware Fusion中CentOS 7中安装Vmware Tools]]></title>
      <url>/2019/04/26/manually-install-vmeare-tools-on-centos-7/</url>
      <content type="html"><![CDATA[<h3 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h3><ol>
<li>虚拟机状态栏中选择 <strong>虚拟机 &gt; 安装Vmware Tools</strong><br>期间可能会报CDROM已经被占用等信息，选择还是挂载即可</li>
<li><p>打开Terminal手动挂载CDROM, 如果已经挂载，则不需要挂载</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># 查看cdrom是否已经自动挂载</div><div class="line">mount | grep iso9660</div><div class="line"></div><div class="line"># 手动挂载cdrom</div><div class="line">mkdir /mnt/cdrom</div><div class="line">mount /dev/cdrom /mnt/cdrom</div></pre></td></tr></table></figure>
</li>
<li><p>拷贝cdrom内容到/tmp并安装<br>安装步骤:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"># 如果是CentOS最小化安装，则系统没有perl,gcc和kernel-devel，需要先安装</div><div class="line">yum install -y perl gcc make kernel-devel</div><div class="line"></div><div class="line"># 拷贝vmtools并安装</div><div class="line">cd /tmp/</div><div class="line">cp /mnt/cdrom/VMwareTools-x.x.x-yyyy.tar.gz ./</div><div class="line">tar xzvf VMwareTools-x.x.x-yyyy.tar.gz</div><div class="line">cd vmware-tools-distrib</div><div class="line"># 执行安装程序，没有特殊要求，一路回车即可</div><div class="line">bash ./vmware-install.pl</div><div class="line"></div><div class="line"># 清理文件</div><div class="line">rm -rf /tmp/VMwareTools-x.x.x-yyyy.tar.gz</div><div class="line">rm -rf /tmp/vmware-tools-distrib/</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://docs.vmware.com/en/VMware-Fusion/10.0/com.vmware.fusion.using.doc/GUID-08BB9465-D40A-4E16-9E15-8C016CC8166F.html" target="_blank" rel="external">Manually Installing VMware Tools on a Linux Virtual Machine</a></li>
<li><a href="https://kb.vmware.com/s/article/1018414" target="_blank" rel="external">Installing VMware Tools in a Linux virtual machine using a Compiler (1018414)</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> CentOS </tag>
            
            <tag> Linux </tag>
            
            <tag> VMware </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Command Tips]]></title>
      <url>/2019/04/26/command-tips/</url>
      <content type="html"><![CDATA[<h3 id="git-branch-不要分页"><a href="#git-branch-不要分页" class="headerlink" title="git branch 不要分页"></a>git branch 不要分页</h3><p>设置git branch不分页显示<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git config --global pager.branch false</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="Macos下tree显示中文"><a href="#Macos下tree显示中文" class="headerlink" title="Macos下tree显示中文"></a>Macos下tree显示中文</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 安装tree</div><div class="line">brew install tree`</div><div class="line"></div><div class="line"># 中文不乱码加参数 -N</div><div class="line">tree -N</div></pre></td></tr></table></figure>
<hr>
<h3 id="CentoOS-7-最小化安装后可能需要的一些命令"><a href="#CentoOS-7-最小化安装后可能需要的一些命令" class="headerlink" title="CentoOS 7 最小化安装后可能需要的一些命令"></a>CentoOS 7 最小化安装后可能需要的一些命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">yum install wget curl</div><div class="line">yum install unzip</div><div class="line">yum install tree</div><div class="line">yum install net-tools</div></pre></td></tr></table></figure>
<a id="more"></a>
<hr>
<h3 id="CentOS-7-中命令行UI编辑网口信息的命令"><a href="#CentOS-7-中命令行UI编辑网口信息的命令" class="headerlink" title="CentOS 7 中命令行UI编辑网口信息的命令"></a>CentOS 7 中命令行UI编辑网口信息的命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ yum install NetworkManager-tui</div><div class="line"></div><div class="line">$ nmtui</div></pre></td></tr></table></figure>
<hr>
<h3 id="MacOS-ffplay只有声音没有图像"><a href="#MacOS-ffplay只有声音没有图像" class="headerlink" title="MacOS ffplay只有声音没有图像"></a>MacOS ffplay只有声音没有图像</h3><p>如果rtmp地址加了单引号，ffplay会偶尔抽风只播放声音而没有视频, 去掉单引号或者使用双引号代替<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># 可能会抽风，没有图像,只能解析音频信号</div><div class="line">ffplay &apos;rtmp://192.168.187.128/live/room8878&apos;</div><div class="line"></div><div class="line"># 去掉单引号或换用双引号，没发现过问题 </div><div class="line">ffplay rtmp://192.168.187.128/live/room8878</div><div class="line">ffplay &quot;rtmp://192.168.187.128/live/room8878&quot;</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="grep-中将binary-file当做文本"><a href="#grep-中将binary-file当做文本" class="headerlink" title="grep 中将binary file当做文本"></a>grep 中将binary file当做文本</h3><p>grep加上<code>-a</code>参数，就可以将binary file当做text来对待.<br><code>man grep</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">-a, --text</div><div class="line">       Process a binary file as if it were text; this is equivalent to the --binary-files=text option.</div></pre></td></tr></table></figure>
<hr>
<h3 id="MacOS下Betterzip-quicklook不起作用"><a href="#MacOS下Betterzip-quicklook不起作用" class="headerlink" title="MacOS下Betterzip quicklook不起作用"></a>MacOS下Betterzip quicklook不起作用</h3><p>BetterZipQL被集成到BetterZip中了，因此brew在<a href="https://github.com/Homebrew/homebrew-cask/pull/41954" target="_blank" rel="external">pull request 41954</a>中删除了BetterZipQL。<br>需要使用<code>brew cask install betterzip</code>来安装BetterZip。BetterZip是收费软件，30天免费试用。但官网上说Quick Look功能在试用期结束后，还是可以免费使用。<br><strong>如果安装完成后，发现quicklook功能没起作用，记得需要先在Application中打开一次BetterZip，别问我是怎么知道的。</strong></p>
<hr>
<h3 id="MacOS下QuickLook的路径"><a href="#MacOS下QuickLook的路径" class="headerlink" title="MacOS下QuickLook的路径"></a>MacOS下QuickLook的路径</h3><p>全局目录: <code>/Library/QuickLook/</code><br>用户目录: <code>~/Library/QuickLook/</code></p>
<hr>
<h3 id="CentOS-7下安装lspci"><a href="#CentOS-7下安装lspci" class="headerlink" title="CentOS 7下安装lspci"></a>CentOS 7下安装lspci</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 安装lspci</div><div class="line">sudo yum install pciutils</div><div class="line"></div><div class="line"># 更新PCI ID list</div><div class="line">sudo update-pciids</div></pre></td></tr></table></figure>
<hr>
<h3 id="rsync命令"><a href="#rsync命令" class="headerlink" title="rsync命令"></a>rsync命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 密码方式</div><div class="line">rsync -P -a -z -e ssh /from/dir/ username@hostname:/to/dir/</div><div class="line"></div><div class="line"># identity file免密方式</div><div class="line">rsync -Pav -e &quot;ssh -i $HOME/.ssh/somekey&quot; username@hostname:/from/dir/ /to/dir/</div></pre></td></tr></table></figure>
<hr>
<h3 id="CentOS中启用epel-repository"><a href="#CentOS中启用epel-repository" class="headerlink" title="CentOS中启用epel repository"></a>CentOS中启用epel repository</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum -y install epel-release</div></pre></td></tr></table></figure>
<hr>
<h3 id="CentOS中systemctl常用命令小结"><a href="#CentOS中systemctl常用命令小结" class="headerlink" title="CentOS中systemctl常用命令小结"></a>CentOS中systemctl常用命令小结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"># 启动服务</div><div class="line">systemctl start nginx</div><div class="line"></div><div class="line"># 停止服务</div><div class="line">systemctl stop nginx</div><div class="line"></div><div class="line"># 重启服务</div><div class="line">systemctl restart nginx</div><div class="line"></div><div class="line"># reload服务</div><div class="line">systemctl reload nginx</div><div class="line"></div><div class="line"># 列出服务状态</div><div class="line">systemctl status nginx</div><div class="line"></div><div class="line"># 开机启动服务类似于 chkconfig service on</div><div class="line">systemctl enable nginx</div><div class="line"></div><div class="line"># 开机停止服务，类似于chkconfig service off</div><div class="line">systemctl disable nginx</div><div class="line"></div><div class="line"># 查看服务是否开机启动</div><div class="line">systemctl is-enabled nginx</div><div class="line"></div><div class="line"># 查看所有服务状态</div><div class="line">systemctl list-unit-files --type=service</div><div class="line"></div><div class="line"># 查看当前运行的服务</div><div class="line">systemctl list-units --type service</div></pre></td></tr></table></figure>
<hr>
<h3 id="Split切割文件"><a href="#Split切割文件" class="headerlink" title="Split切割文件"></a>Split切割文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 按行切割</div><div class="line">split -l 300 file.txt new_prefix</div></pre></td></tr></table></figure>
<hr>
<h3 id="Gem切换源"><a href="#Gem切换源" class="headerlink" title="Gem切换源"></a>Gem切换源</h3><p>Gem 切换为RubyChina <a href="https://gems.ruby-china.com/" target="_blank" rel="external">https://gems.ruby-china.com/</a><br>ruby环境<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ gem sources --add https://gems.ruby-china.com/ --remove https://rubygems.org/</div><div class="line">$ gem sources -l</div><div class="line">https://gems.ruby-china.com</div><div class="line"># 确保只有 gems.ruby-china.com</div></pre></td></tr></table></figure></p>
<p>Gemfile和Bundle情况, 修改Gemfile<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">source &apos;https://rubygems.org/&apos;</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="rvm切换ruby安装源"><a href="#rvm切换ruby安装源" class="headerlink" title="rvm切换ruby安装源"></a>rvm切换ruby安装源</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 用户级别</div><div class="line">echo &quot;ruby_url=https://cache.ruby-china.com/pub/ruby&quot; &gt;&gt; ~/.rvm/user/db</div><div class="line"></div><div class="line"># CentOS下系统级别</div><div class="line">echo &quot;ruby_url=https://cache.ruby-china.com/pub/ruby&quot; &gt;&gt; /usr/local/rvm/user/db</div></pre></td></tr></table></figure>
<hr>
<h3 id="gitignore例子"><a href="#gitignore例子" class="headerlink" title="gitignore例子"></a>gitignore例子</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># Ignore Mac DS_Store files</div><div class="line">.DS_Store</div></pre></td></tr></table></figure>
<hr>
<h3 id="测试NTP服务器"><a href="#测试NTP服务器" class="headerlink" title="测试NTP服务器"></a>测试NTP服务器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ntpdate -q 1.cn.pool.ntp.org</div></pre></td></tr></table></figure>
<hr>
<h3 id="CentOS下安装nslookup"><a href="#CentOS下安装nslookup" class="headerlink" title="CentOS下安装nslookup"></a>CentOS下安装nslookup</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">yum install bind-utils</div><div class="line">yum provides &apos;*bin/nslookup&apos;</div></pre></td></tr></table></figure>
<hr>
<h3 id="列出Linux下目录的文件数"><a href="#列出Linux下目录的文件数" class="headerlink" title="列出Linux下目录的文件数"></a>列出Linux下目录的文件数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">find DIR_NAME -type f | wc -l</div></pre></td></tr></table></figure>
<hr>
<h3 id="Linux下几个获取出口公网IP的命令"><a href="#Linux下几个获取出口公网IP的命令" class="headerlink" title="Linux下几个获取出口公网IP的命令"></a>Linux下几个获取出口公网IP的命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">dig TXT +short o-o.myaddr.l.google.com @ns1.google.com | awk -F&apos;&quot;&apos; &apos;&#123; print $2&#125;&apos;</div><div class="line">curl -sSL https://ip.cn</div><div class="line">curl myip.ipip.net</div></pre></td></tr></table></figure>
<hr>
<h3 id="获取磁盘和CPU使用率的命令"><a href="#获取磁盘和CPU使用率的命令" class="headerlink" title="获取磁盘和CPU使用率的命令"></a>获取磁盘和CPU使用率的命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">## 获取CPU使用率</div><div class="line">top -b -n2 | grep &apos;Cpu(s)&apos; | tail -1 | awk &apos;&#123;print $2 + $4&#125;&apos;</div><div class="line">或</div><div class="line">top -bn 2 -d 0.01 | grep &apos;^%Cpu&apos; | tail -n 1 | gawk &apos;&#123;print $2+$4+$6&#125;&apos;</div><div class="line"></div><div class="line">## 获取磁盘使用率</div><div class="line">df -H --output=source,pcent 2&gt;/dev/null | grep -vE &apos;^Filesystem|tmpfs|cdrom&apos; | while read file_system used</div><div class="line">do</div><div class="line">    echo &quot;...&quot;</div><div class="line">done</div></pre></td></tr></table></figure>
<p><strong>top -b参数是有bug的，第一次显示的cpu数据是错误的, 要用第二次的输出来获取CPU的实际使用率。</strong><br>top的bug说明: <a href="https://askubuntu.com/a/399966" target="_blank" rel="external">https://askubuntu.com/a/399966</a>以及<a href="https://bugzilla.redhat.com/show_bug.cgi?id=174619" target="_blank" rel="external">Bug 174619 - top reports wrong values for CPU(s) in batch mode</a></p>
<hr>
<h3 id="解决停止操作后SSH连接断开的办法"><a href="#解决停止操作后SSH连接断开的办法" class="headerlink" title="解决停止操作后SSH连接断开的办法"></a>解决停止操作后SSH连接断开的办法</h3><p>在ssh命令中添加<code>-o ServerAliveInterval=10</code>参数，每隔10s向服务器发送心跳包。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssh -o ServerAliveInterval=10 user@remote-ssh-server-ip</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="本地转义HTML的几个命令"><a href="#本地转义HTML的几个命令" class="headerlink" title="本地转义HTML的几个命令"></a>本地转义HTML的几个命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Perl 版本</div><div class="line">cat foo.html | perl -MHTML::Entities -pe &apos;decode_entities($_);&apos;</div><div class="line"></div><div class="line">Python 3 版本</div><div class="line">cat foo.xml | python3 -c &apos;import html, sys; [print(html.unescape(l), end=&quot;&quot;) for l in sys.stdin]&apos;</div></pre></td></tr></table></figure>
<p>来自stackoverflow <a href="https://stackoverflow.com/questions/5929492/bash-script-to-convert-from-html-entities-to-characters" target="_blank" rel="external">Bash script to convert from HTML entities to characters</a></p>
<hr>
<h3 id="EB手动打包时包含隐藏目录"><a href="#EB手动打包时包含隐藏目录" class="headerlink" title="EB手动打包时包含隐藏目录"></a>EB手动打包时包含隐藏目录</h3><p>进入源文件所在目录中工作<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ zip ../myapp.zip -r * .[^.]*</div></pre></td></tr></table></figure></p>
<p>使用Git打包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 创建当前分支上最新 Git 提交的 ZIP 存档</div><div class="line">$ git archive --format=zip HEAD &gt; &lt;myapp&gt;.zip</div></pre></td></tr></table></figure>
<h3 id="Linux下mail如何添加多个附件"><a href="#Linux下mail如何添加多个附件" class="headerlink" title="Linux下mail如何添加多个附件"></a>Linux下mail如何添加多个附件</h3><p>mailx 命令中使用多个-a选项<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ mailx -s &apos;Few files attached&apos; -a file1.txt -a file2.txt someone@some.com</div></pre></td></tr></table></figure></p>
<p><a href="https://stackoverflow.com/questions/14473732/attaching-more-than-2-files-in-mail-in-unix" target="_blank" rel="external">https://stackoverflow.com/questions/14473732/attaching-more-than-2-files-in-mail-in-unix</a></p>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> MacOS </tag>
            
            <tag> Linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Niu-X resource]]></title>
      <url>/2019/04/20/niu-x-resource/</url>
      <content type="html"><![CDATA[<p>记录一些有用的网站，持续更新中。。</p>
<hr>
<h3 id="Linux-command"><a href="#Linux-command" class="headerlink" title="Linux-command"></a>Linux-command</h3><p>github地址: <a href="https://github.com/jaywcjlove/linux-command" target="_blank" rel="external">linux-command</a></p>
<blockquote>
<p>Linux命令大全搜索工具，内容包含Linux命令手册、详解、学习、搜集 </p>
</blockquote>
<p>比较系统的各个命令，期待有空时每个粗略一遍，需要时再来查询</p>
<a id="more"></a>
<hr>
<h3 id="jaywcjlove-handbook"><a href="#jaywcjlove-handbook" class="headerlink" title="jaywcjlove-handbook"></a>jaywcjlove-handbook</h3><p>github地址: <a href="https://github.com/jaywcjlove/handbook" target="_blank" rel="external">jaywcjlove-handbook</a></p>
<blockquote>
<p>放置我的笔记、搜集、摘录、实践，保持好奇心。看文需谨慎，后果很严重</p>
</blockquote>
<p>保存了jaywcjlove的一些心得笔记, 有空时可以来翻一下</p>
<hr>
<h3 id="jaywcjlove-awesome-mac"><a href="#jaywcjlove-awesome-mac" class="headerlink" title="jaywcjlove-awesome-mac"></a>jaywcjlove-awesome-mac</h3><p>github地址: <a href="https://github.com/jaywcjlove/awesome-mac" target="_blank" rel="external">awesome-mac</a></p>
<blockquote>
<p> Now we have become very big, Different from the original idea. Collect premium software in various categories. <a href="https://git.io/macx" target="_blank" rel="external">https://git.io/macx</a></p>
</blockquote>
<p>Mac下各个APP的清单, <a href="https://github.com/jaywcjlove/awesome-mac/blob/master/README-zh.md" target="_blank" rel="external">中文版的列表</a></p>
<p>已安装列表</p>
<ul>
<li></li>
</ul>
<hr>
<h3 id="macbookpro笔记本维修拆机指南"><a href="#macbookpro笔记本维修拆机指南" class="headerlink" title="macbookpro笔记本维修拆机指南"></a>macbookpro笔记本维修拆机指南</h3><p>网址: <a href="https://zh.ifixit.com/Device/Mac_Laptop" target="_blank" rel="external">Mac 笔记本修理</a></p>
<hr>
<h3 id="AWS-架构图标"><a href="#AWS-架构图标" class="headerlink" title="AWS 架构图标"></a>AWS 架构图标</h3><p>网址: <a href="https://amazonaws-china.com/cn/architecture/icons/?from=groupmessage&amp;isappinstalled=0" target="_blank" rel="external">AWS 架构图标</a><br>包含了Microsoft PowerPoint的工具包和一些推荐的在线工具</p>
<hr>
<h3 id="MacOS下default-write的隐藏设置"><a href="#MacOS下default-write的隐藏设置" class="headerlink" title="MacOS下default write的隐藏设置"></a>MacOS下default write的隐藏设置</h3><p>网址: <a href="https://www.defaults-write.com/" target="_blank" rel="external">defaults-write.com</a><br>MacOS下一些系统设置，可以使用命令行来设置。超级方便，不用一个一个GUI点开来设置了，尤其是迁移到新机器时。</p>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> MacOS </tag>
            
            <tag> Linux </tag>
            
            <tag> Github </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[firewall-cmd基本用法]]></title>
      <url>/2019/04/20/firewall-cmd-basic-usage/</url>
      <content type="html"><![CDATA[<p><strong>还需要装个虚拟机，重新敲命令，拷贝输出内容或者截图</strong></p>
<h3 id="安装和管理firewalld"><a href="#安装和管理firewalld" class="headerlink" title="安装和管理firewalld"></a>安装和管理firewalld</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># 安装firewalld</div><div class="line">yum install firewalld firewall-config</div><div class="line"></div><div class="line">systemctl start  firewalld # 启动</div><div class="line">systemctl status firewalld # 或者 firewall-cmd --state 查看状态</div><div class="line">systemctl stop firewalld  # 停止</div><div class="line">systemctl enable firewalld # 开机自自动</div><div class="line">systemctl disable firewalld # 取消自启动</div><div class="line">systemctl list-unit-files | grep firewalld # 查看是否开机自启动</div></pre></td></tr></table></figure>
<h3 id="基础命令"><a href="#基础命令" class="headerlink" title="基础命令"></a>基础命令</h3><p>一些常用的命令如下:</p>
<ul>
<li>查看防火墙状态<br>命令: <code>firewall-cmd --list-all</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">$ sudo firewall-cmd --list-all</div><div class="line">public (active)</div><div class="line">  target: default</div><div class="line">  icmp-block-inversion: no</div><div class="line">  interfaces: ens33</div><div class="line">  sources:</div><div class="line">  services: ssh dhcpv6-client</div><div class="line">  ports:</div><div class="line">  protocols:</div><div class="line">  masquerade: no</div><div class="line">  forward-ports:</div><div class="line">  source-ports:</div><div class="line">  icmp-blocks:</div><div class="line">  rich rules:</div><div class="line"></div><div class="line">$</div></pre></td></tr></table></figure>
</li>
</ul>
<a id="more"></a>
<ul>
<li><p>列出支持的服务<br>命令: <code>firewall-cmd --get-services</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo firewall-cmd --get-services</div><div class="line">RH-Satellite-6 amanda-client amanda-k5-client bacula bacula-client bgp bitcoin bitcoin-rpc bitcoin-testnet bitcoin-testnet-rpc ceph ceph-mon cfengine condor-collector ctdb dhcp dhcpv6 dhcpv6-client dns docker-registry docker-swarm dropbox-lansync elasticsearch freeipa-ldap freeipa-ldaps freeipa-replication freeipa-trust ftp ganglia-client ganglia-master git gre high-availability http https imap imaps ipp ipp-client ipsec irc ircs iscsi-target jenkins kadmin kerberos kibana klogin kpasswd kprop kshell ldap ldaps libvirt libvirt-tls managesieve mdns minidlna mongodb mosh mountd ms-wbt mssql murmur mysql nfs nfs3 nmea-0183 nrpe ntp openvpn ovirt-imageio ovirt-storageconsole ovirt-vmconsole pmcd pmproxy pmwebapi pmwebapis pop3 pop3s postgresql privoxy proxy-dhcp ptp pulseaudio puppetmaster quassel radius redis rpc-bind rsh rsyncd samba samba-client sane sip sips smtp smtp-submission smtps snmp snmptrap spideroak-lansync squid ssh syncthing syncthing-gui synergy syslog syslog-tls telnet tftp tftp-client tinc tor-socks transmission-client upnp-client vdsm vnc-server wbem-https xmpp-bosh xmpp-client xmpp-local xmpp-server zabbix-agent zabbix-server</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
<li><p>添加某个服务<br>命令: <code>firewall-cmd --add-service=&lt;service&gt; --permanent</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo firewall-cmd --add-service=http --permanent</div></pre></td></tr></table></figure>
</li>
<li><p>删除某个服务<br>命令: <code>firewall-cmd --remove-service=&lt;service&gt;  --permanent</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo firewall-cmd --remove-service=dhcpv6-client  --permanent</div></pre></td></tr></table></figure>
</li>
<li><p>添加某个端口<br>命令: <code>firewall-cmd --add-port=&lt;portid&gt;[-&lt;portid&gt;]/&lt;protocol&gt; --permanent</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"># 添加rtmp端口</div><div class="line">$ sudo firewall-cmd --add-port=1935/tcp --permanent</div><div class="line">$ sudo firewall-cmd --reload</div><div class="line">$ sudo firewall-cmd --list-all</div><div class="line">public (active)</div><div class="line">  target: default</div><div class="line">  icmp-block-inversion: no</div><div class="line">  interfaces: ens33</div><div class="line">  sources:</div><div class="line">  services: ssh dhcpv6-client http</div><div class="line">  ports: 1935/tcp</div><div class="line">  protocols:</div><div class="line">  masquerade: no</div><div class="line">  forward-ports:</div><div class="line">  source-ports:</div><div class="line">  icmp-blocks:</div><div class="line">  rich rules:</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
<li><p>删除某个端口<br>命令: <code>firewall-cmd --remove-port=&lt;portid&gt;[-&lt;portid&gt;]/&lt;protocol&gt; --permanent</code></p>
</li>
<li><p>重启防火墙<br>添加或删除服务和端口后，需要重新load防火墙，才能生效<br>命令: <code>firewall-cmd --reload</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo firewall-cmd --reload</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="自定义service文件"><a href="#自定义service文件" class="headerlink" title="自定义service文件"></a>自定义service文件</h3><h4 id="service文件介绍"><a href="#service文件介绍" class="headerlink" title="service文件介绍"></a>service文件介绍</h4><p>firewalld内置了一些常用的services，对应的配置文件保存在<code>/usr/lib/firewalld/services</code>中，使用命令<code>firewall-cmd --add-service=&lt;service&gt;</code>可以很方便地添加这些服务。</p>
<p>很多情况下，我们要开放的端口不在这些预定义的服务中，此时有两个办法来实现</p>
<ol>
<li>使用<code>--add-port=&lt;portid&gt;[-&lt;portid&gt;]/&lt;protocol&gt;</code>来直接添加端口</li>
<li>自建services配置文件，并添加到firewalld中</li>
</ol>
<p>推荐使用自建services的方式，使用services方式的优点</p>
<ul>
<li>一个配置文件对应一个服务，更清晰</li>
<li>配置文件中,可以添加服务描述</li>
<li>配置文件中,可以添加多个端口和协议，如预置的rsyncd.xml中，就同时添加了873端口的TCP和UDP协议。</li>
</ul>
<h4 id="自建service文件"><a href="#自建service文件" class="headerlink" title="自建service文件"></a>自建service文件</h4><p>firewalld用到的所有service配置文件的存放路径是<code>/usr/lib/firewalld/services</code>，但自建的service文件不要直接写入该目录，而应该在<code>/etc/firewalld/services/</code>中进行设置。</p>
<p>下面是一个root下新建rtmp service的例子。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"># 拷贝/usr/lib/firewalld/services/ssh.xml文件到/etc/firewalld/services/中，命名为rtmp.xml</div><div class="line">cp /usr/lib/firewalld/services/ssh.xml /etc/firewalld/services/rtmp.xml</div><div class="line"></div><div class="line"># 修改/etc/firewalld/services/rtmp.xml为如下内容</div><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;</div><div class="line">&lt;service&gt;</div><div class="line">  &lt;short&gt;RTMP&lt;/short&gt;</div><div class="line">  &lt;description&gt;Allow RTMP Streaming.&lt;/description&gt;</div><div class="line">  &lt;port protocol=&quot;tcp&quot; port=&quot;1935&quot;/&gt;</div><div class="line">&lt;/service&gt;</div><div class="line"></div><div class="line"># 重新reload firewalld</div><div class="line">firewall-cmd --reload</div><div class="line"></div><div class="line"># 重新查看可用的firewall service</div><div class="line">firewall-cmd --get-services</div><div class="line"></div><div class="line"># 添加服务</div><div class="line">firewall-cmd --add-service=rtmp --permanent</div><div class="line"></div><div class="line"># 重新reload firewalld</div><div class="line">firewall-cmd --reload</div><div class="line"></div><div class="line"># 查看防火墙状态，看rtmp是否已成功添加</div><div class="line">firewall-cmd --list-all</div></pre></td></tr></table></figure>
<h3 id="端口转发"><a href="#端口转发" class="headerlink" title="端口转发"></a>端口转发</h3><h4 id="转发到本地"><a href="#转发到本地" class="headerlink" title="转发到本地"></a>转发到本地</h4><p>命令: <code>firewall-cmd --add-forward-port=port=&lt;portid&gt;[-&lt;portid&gt;]:proto=&lt;protocol&gt;[:toport=&lt;portid&gt;[-&lt;portid&gt;]][:toaddr=&lt;address&gt;[/&lt;mask&gt;]]</code><br>例子: 转发80端口流量到12345端口<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo firewall-cmd --zone=&quot;public&quot; --add-forward-port=port=80:proto=tcp:toport=12345</div></pre></td></tr></table></figure></p>
<h4 id="转发到其他机器"><a href="#转发到其他机器" class="headerlink" title="转发到其他机器"></a>转发到其他机器</h4><p>转发到其他机器时，需要开启masquerade（伪装）功能。</p>
<blockquote>
<p>Masquerading will forward packets that are not directed to an IP address associated to the system itself onto the intended destination. The source IP address of the packets that are sent through our system will be changed to the IP address of our system, rather than the IP address of the original traffic source. Responses to these packets will then go through our system and the destination address will be modified so that the traffic will be sent back to the original host that initiated the traffic.</p>
</blockquote>
<p>使用<code>firewall-cmd --add-masquerade</code>开启masquerade。<br>例子: 转发本地80端口到IP 10.0.0.2上的8080端口中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo firewall-cmd --add-forward-port=port=80:proto=tcp:toport=8080:toaddr=10.0.0.2</div></pre></td></tr></table></figure></p>
<h3 id="Rich-rules细粒度控制"><a href="#Rich-rules细粒度控制" class="headerlink" title="Rich rules细粒度控制"></a>Rich rules细粒度控制</h3><p>FirewallD的Rich rules 可以提供更细粒度的控制。可以使用<code>man 5 firewalld.richlanguage</code>来查看rich rules的语法，</p>
<p>例子:</p>
<p>允许来自192.168.0.14的所有IPv4流量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo firewall-cmd --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=192.168.0.14 accept&apos;</div></pre></td></tr></table></figure></p>
<p>拒绝来自192.168.1.10的基于TCP的请求访问22端口, 也就是拒绝192.168.1.10进行ssh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo firewall-cmd --add-rich-rule &apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.1.10&quot; port port=22 protocol=tcp reject&apos;</div></pre></td></tr></table></figure></p>
<p>只允许来自192.168.187.128/25的请求访问22端口，拒绝其余的ssh登录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo firewall-cmd --remove-service=ssh</div><div class="line">sudo firewall-cmd --add-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;192.168.187.128/25&quot; service name=&quot;ssh&quot; accept&quot; --permanent</div></pre></td></tr></table></figure></p>
<p>允许来自10.1.0.3的TCP请求到80端口，并且转发到本地的6532端口。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo firewall-cmd --add-rich-rule &apos;rule family=ipv4 source address=10.1.0.3 forward-port port=80 protocol=tcp to-port=6532&apos;</div></pre></td></tr></table></figure></p>
<p>将来自172.31.4.2的访问80端口的TCP请求转发到主机172.31.4.2的8080端口。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo firewall-cmd --add-masquerade</div><div class="line">sudo firewall-cmd --add-rich-rule &apos;rule family=ipv4 forward-port port=80 protocol=tcp to-port=8080 to-addr=172.31.4.2&apos;</div></pre></td></tr></table></figure></p>
<p>列出所有的rich rules<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo firewall-cmd --list-rich-rules</div></pre></td></tr></table></figure></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://www.linuxhelp.com/firewalld-commands-in-linux-with-examples" target="_blank" rel="external">FirewallD Command in Linux With Examples</a></li>
<li><a href="https://www.linode.com/docs/security/firewalls/introduction-to-firewalld-on-centos/" target="_blank" rel="external">Introduction to FirewallD on CentOS</a></li>
<li><a href="https://www.rootusers.com/how-to-use-firewalld-rich-rules-and-zones-for-filtering-and-nat/" target="_blank" rel="external">How To Use Firewalld Rich Rules And Zones For Filtering And NAT</a></li>
<li><a href="https://wangchujiang.com/linux-command/c/firewall-cmd.html" target="_blank" rel="external">firewall-cmd</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Firewall </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[解析EB logs的一些Linux小命令]]></title>
      <url>/2019/04/08/some-command-to-parse-EB-logs/</url>
      <content type="html"><![CDATA[<p>开启EB rotated log功能后，EB的日志将会被存储在EB对应bucket的<code>resources/environments/logs/logtype/environment-id/instance-id</code>路径下, 供后续查看。官网文档<a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.logging.html" target="_blank" rel="external">Viewing Logs from Amazon EC2 Instances in Your Elastic Beanstalk Environment</a></p>
<p>下面记录几个常用检查日志的小命令</p>
<h3 id="使用exclue和include来获取部分日志"><a href="#使用exclue和include来获取部分日志" class="headerlink" title="使用exclue和include来获取部分日志"></a>使用exclue和include来获取部分日志</h3><p>开启EB的rotated log后，EB中的EC2会每小时将日志上传到S3中。以<code>Passenger with Ruby</code>的Platform为例，有production.log, access.log和passenger.log三种文件会被传到S3中，文件名格式:<code>文件路径</code>+<code>时间戳</code>+<code>gz</code>的格式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ ls *log1554688861.gz*</div><div class="line">_var_app_support_logs_rotated_access.log1554688861.gz  </div><div class="line">_var_app_support_logs_rotated_passenger.log1554688861.gz  </div><div class="line">_var_app_support_logs_rotated_production.log1554688861.gz</div><div class="line">$</div></pre></td></tr></table></figure>
<p>Bucket下的日志，默认不会自动清理，日积月累,instance-id路径下的文件会越来越多，而目前aws cli的<code>s3 cp</code>或<code>s3 sync</code>,并没有提供根据创建时间的filter,有时候获取固定哪段时间的文件就不太方便。<br><a id="more"></a></p>
<p>虽然<code>aws cli</code>没法直接精确地实现，但可以使用–exclude和–include参数，根据文件名字的命名规则来粗略地实现一下。<br>大概的命令如下:<br><code>aws s3 sync s3://${elasticbeanstalk-region-account-id}/resources/environments/logs/publish/${environment-id}/${instance-id}/ logs/ --exclude &quot;*&quot; --include &quot;*log1554[01234567]*&quot;</code></p>
<p>其中</p>
<ul>
<li>elasticbeanstalk-region-account-id，environment-id，instance-id替换成实际环境的值</li>
<li>首先使用–exclude “*”,不包含任何值</li>
<li>再使用–include “*log1554[01234567]*“筛选出要的文件。include的匹配字串，需要根据实际时间和文件前缀进行修改。</li>
</ul>
<p>日期的字符串形式和timestamp形式的转换, Linux下可以使用<code>date</code>命令来进行转换。</p>
<p><code>date -d &quot;@$TIMESTAMP&quot;</code>来将timestamp转换为字符串形式,<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ date -d @1554688861</div><div class="line">2019年 04月 08日 星期一 02:01:01 UTC</div><div class="line">$</div></pre></td></tr></table></figure></p>
<p><code>date -d &#39;YYYY-MM-DDTHH:MM:SS&#39; +%s</code>，将字符形式的日期转为timestamp。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ date -d &apos;2019-04-08T02:01:01&apos; +%s</div><div class="line">1554688861</div><div class="line">$</div></pre></td></tr></table></figure></p>
<h3 id="Linux-小命令"><a href="#Linux-小命令" class="headerlink" title="Linux 小命令"></a>Linux 小命令</h3><p>过滤出access log中4XX的请求<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">zgrep &apos; 4[0-9][0-9] [0-9]&apos; _var_app_support_logs_rotated_access.log1554*.gz &gt; 4xx_in_i-09bb7607be9f0c928.txt</div></pre></td></tr></table></figure></p>
<p>筛选出4XX日志中第一级前缀最多的url</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat 4xx_in_i-09bb7607be9f0c928.txt | awk &apos;&#123;print $6,$7&#125;&apos; | cut -d &apos;/&apos; -f 1,2 | sort | uniq -c | sort -n</div></pre></td></tr></table></figure>
<p>筛选出4XX请求最密集的分钟数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat 4xx_in_i-09bb7607be9f0c928.txt  | awk &apos;&#123;print $4&#125;&apos; | cut -d &apos;:&apos; -f 1,2,3 | sort | uniq -c | sort -n</div></pre></td></tr></table></figure></p>
<p>筛选出某分钟内所有的access 日志<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">zgrep &apos;05/Apr/2019:00:42&apos; _var_app_support_logs_rotated_access.log1554*.gz</div></pre></td></tr></table></figure></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.logging.html" target="_blank" rel="external">Viewing Logs from Amazon EC2 Instances in Your Elastic Beanstalk Environment</a></li>
<li><a href="https://stackoverflow.com/questions/3249827/convert-unix-timestamp-to-a-date-string" target="_blank" rel="external">Convert Unix timestamp to a date string</a></li>
<li><a href="https://stackoverflow.com/questions/26432050/bash-convert-string-to-timestamp" target="_blank" rel="external">Bash convert string to timestamp</a></li>
<li><a href="https://docs.aws.amazon.com/cli/latest/reference/s3/index.html#use-of-exclude-and-include-filters" target="_blank" rel="external">S3</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Elastic Beanstalk </tag>
            
            <tag> S3 </tag>
            
            <tag> AWS CLI </tag>
            
            <tag> Linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS EC2中开启BBR]]></title>
      <url>/2019/04/02/bbr-on-aws-ec2/</url>
      <content type="html"><![CDATA[<p>官网中关于如何在EC2中开启BBR的说明。<a href="https://aws.amazon.com/amazon-linux-ami/2017.09-release-notes/" target="_blank" rel="external">Amazon Linux AMI 2017.09 Release Notes</a></p>
<h3 id="开启方法"><a href="#开启方法" class="headerlink" title="开启方法"></a>开启方法</h3><p>开启方法摘录如下:</p>
<blockquote>
<p>The Kernel has been updated to latest 4.9.y stable tree, new ENA driver 1.3.0 as well as support for TCP Bottleneck Bandwidth and RTT (BBR) which improve network performance.<br>BBR is not enabled by default. You can enable it on your EC2 Instance via:</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">$ sudo modprobe tcp_bbr</div><div class="line"></div><div class="line">$ sudo modprobe sch_fq</div><div class="line"></div><div class="line">$ sudo sysctl -w net.ipv4.tcp_congestion_control=bbr</div><div class="line"></div><div class="line">Persistent configuration should look like:</div><div class="line"></div><div class="line">$ sudo su -</div><div class="line"></div><div class="line"># cat &lt;&lt;EOF&gt;&gt; /etc/sysconfig/modules/tcpcong.modules  </div><div class="line"></div><div class="line">&gt;#!/bin/bash</div><div class="line"></div><div class="line">&gt; exec /sbin/modprobe tcp_bbr &gt;/dev/null 2&gt;&amp;1</div><div class="line"></div><div class="line">&gt; exec /sbin/modprobe sch_fq &gt;/dev/null 2&gt;&amp;1</div><div class="line"></div><div class="line">&gt; EOF</div><div class="line"></div><div class="line"># chmod 755 /etc/sysconfig/modules/tcpcong.modules</div><div class="line"></div><div class="line"># echo &quot;net.ipv4.tcp_congestion_control = bbr&quot; &gt;&gt; /etc/sysctl.d/00-tcpcong.conf</div></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="额外的命令"><a href="#额外的命令" class="headerlink" title="额外的命令"></a>额外的命令</h3><p>Amazon Linux AMI中查看系统中拥塞控制的命令如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># 查看系统支持的拥塞控制方法</div><div class="line">$ sysctl net.ipv4.tcp_available_congestion_control</div><div class="line">net.ipv4.tcp_available_congestion_control = bbr cubic reno</div><div class="line">$</div><div class="line"></div><div class="line"># 查看当前使用的拥塞控制方法</div><div class="line">$ sysctl net.ipv4.tcp_congestion_control</div><div class="line">net.ipv4.tcp_congestion_control = bbr</div><div class="line">$</div></pre></td></tr></table></figure>
<p>其中</p>
<ul>
<li>bbr就是<a href="https://ai.google/research/pubs/pub45646" target="_blank" rel="external">TCP Bottleneck Bandwidth and RTT</a></li>
<li>cubic的介绍页面<a href="https://en.wikipedia.org/wiki/CUBIC_TCP" target="_blank" rel="external">CUBIC TCP</a>以及<a href="https://tools.ietf.org/html/rfc8312" target="_blank" rel="external">RFC8312</a></li>
<li>reno的RFC <a href="https://tools.ietf.org/html/rfc5681" target="_blank" rel="external">TCP Congestion Control</a></li>
</ul>
<p>用命令<code>lsmod | grep bbr</code>查看bbr是否已经启动<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ lsmod | grep bbr</div><div class="line">tcp_bbr                20480  15</div><div class="line">$</div></pre></td></tr></table></figure></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://aws.amazon.com/amazon-linux-ami/2017.09-release-notes/" target="_blank" rel="external">Amazon Linux AMI 2017.09 Release Notes</a></li>
<li><a href="https://ai.google/research/pubs/pub45646" target="_blank" rel="external">TCP Bottleneck Bandwidth and RTT</a></li>
<li><a href="https://tools.ietf.org/html/rfc8312" target="_blank" rel="external">RFC8312</a></li>
<li><a href="https://tools.ietf.org/html/rfc5681" target="_blank" rel="external">TCP Congestion Control</a></li>
<li><a href="https://github.com/google/bbr" target="_blank" rel="external">google bbr</a></li>
<li><a href="https://coolshell.cn/articles/11564.html" target="_blank" rel="external">TCP 的那些事儿（上）</a></li>
<li><a href="https://coolshell.cn/articles/11609.html" target="_blank" rel="external">TCP 的那些事儿（下）</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> EC2 </tag>
            
            <tag> Network </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Mac下共享ShadowsocksX-NG的梯子给iOS]]></title>
      <url>/2019/03/24/share-ShadowsocksX-NG-to-iOS/</url>
      <content type="html"><![CDATA[<p>提供一个Mac下共享<code>ShadowsocksX-NG</code>(小飞机)梯子给iPhone的方法。</p>
<h3 id="Mac端设置"><a href="#Mac端设置" class="headerlink" title="Mac端设置"></a>Mac端设置</h3><p>打开小飞机的“偏好设置”，切换到HTTP一栏，其中的HTTP代理监听地址默认是127.0.0.1，将其改成0.0.0.0。保留默认的1087或改为自己想要设置的端口，并勾选“开启HTTP代理服务器”。<br><img src="/images/ShadowSock/shadowsock_http_proxy.png" alt="shadowsock_http_proxy.png"></p>
<p>Terminal中输入<code>ifconfig</code>查看Mac的IP地址, 供iPhone设置时使用。此处是<code>192.168.3.18</code><br><a id="more"></a></p>
<h3 id="iPhone侧设置"><a href="#iPhone侧设置" class="headerlink" title="iPhone侧设置"></a>iPhone侧设置</h3><p>iPhone连接到和Mac相同的局域网，然后在“无线局域网”中设置HTTP代理。</p>
<ul>
<li>打开“设置”，找到“无线局域网”</li>
<li>点击WIFI名称进入设置详情页</li>
<li>找到“HTTP代理”，点击“配置代理”<br><img src="/images/ShadowSock/setting_of_wifi.jpg" alt="setting_of_wifi.jpg"></li>
<li>选择手动，服务器一栏填入Mac的IP地址，此处是192.168.3.18，端口一栏填入1087<br><img src="/images/ShadowSock/http_proxy_setting.jpg" alt="http_proxy_setting.jpg"></li>
</ul>
<p>打开Chrome，输入<code>www.google.com</code>，就可以畅快的呼吸了。<br><img src="/images/ShadowSock/hello_google.jpg" alt="hello_google.jpg"></p>
<p>Hello,Trump.<br><img src="/images/ShadowSock/hello_trump.jpg" alt="hello_trump.jpg"></p>
<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>网上有些文章讲到了需要下载并配置<code>privoxy</code>来做HTTP代理，可能是针对旧版的<code>ShadowsocksX-NG</code>的教程。我目前使用的1.6.1版本中，已经自带了HTTP代理的功能，而且其实也是使用了<code>privoxy</code>来实现的HTTP代理。只是<code>ShadowsocksX-NG</code>都已经集成了。</p>
<p>打开<code>~/Library/Application Support/ShadowsocksX-NG</code>，可以看到里面自带了一个static版本的<code>privoxy</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$ ll</div><div class="line">total 16</div><div class="line">drwxr-xr-x  3 carlshen  staff   96 11 19 11:58 kcptun_20170322</div><div class="line">lrwxr-xr-x  1 carlshen  staff   89 11 24  2017 kcptun_client -&gt; /Users/carlshen/Library/Application Support/ShadowsocksX-NG/kcptun_20170322/kcptun_client</div><div class="line">lrwxr-xr-x  1 carlshen  staff   89 11 24  2017 privoxy -&gt; /Users/carlshen/Library/Application Support/ShadowsocksX-NG/privoxy-3.0.26.static/privoxy</div><div class="line">drwxr-xr-x  3 carlshen  staff   96 11 24  2017 privoxy-3.0.26.static</div><div class="line">-rw-r--r--  1 carlshen  staff  434  3 26 10:52 privoxy.config</div><div class="line">drwxr-xr-x  9 carlshen  staff  288 11 19 11:58 ss-local-3.0.5</div><div class="line">-rw-r--r--  1 carlshen  staff  190  3 26 10:52 ss-local-config.json</div><div class="line">lrwxr-xr-x  1 carlshen  staff   74 11 24  2017 ss-local-latest -&gt; /Users/carlshen/Library/Application Support/ShadowsocksX-NG/ss-local-3.0.5</div><div class="line">$</div></pre></td></tr></table></figure>
<p>查看进程也可以看到，使用的就是<code>ShadowsocksX-NG</code>下的<code>privoxy</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ ps -ef | grep privoxy</div><div class="line">  505 57716     1   0 日09下午 ??         0:00.38 /Users/carlshen/Library/Application Support/ShadowsocksX-NG/privoxy --no-daemon privoxy.config</div><div class="line">  505 83345 83015   0 10:56上午 ttys046    0:00.00 ggrep privoxy</div><div class="line">$</div></pre></td></tr></table></figure>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://github.com/shadowsocks/ShadowsocksX-NG/issues/236" target="_blank" rel="external">是否考虑增加pac服务的局域网访问</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> MacOS </tag>
            
            <tag> iOS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CentOS 7下Ctrl-C失效问题]]></title>
      <url>/2019/03/21/ctrl-c-invalid-on-Centos-7/</url>
      <content type="html"><![CDATA[<p>rvm 1.29.4在CentOS 7上有一个bug，在Bash下，source了rvm后，执行Crtl+C时，无法终止shell命令。</p>
<p>具体讨论在<a href="https://github.com/rvm/rvm/issues/4422" target="_blank" rel="external">Unable to pass SIGINT signal (control-C) on 1.29.4 </a>中。</p>
<p>解决办法:</p>
<ol>
<li>在用户账号下强制使用trap设置回被改写的信号量INT QUIT</li>
<li>升级rvm到1.29.4以上版本</li>
</ol>
<a id="more"></a>
<h3 id="临时修改trap"><a href="#临时修改trap" class="headerlink" title="临时修改trap"></a>临时修改trap</h3><p>在用户账号下强制使用trap设置回被改写的信号量INT QUIT</p>
<ul>
<li>在~/.bash_profile中source rvm命令之后添加<code>trap - INT QUIT</code></li>
<li>~/.bashrc 末尾加上<code>trap - INT QUIT</code></li>
</ul>
<p>使用trap强行改回的方法不限于rvm，也可用于其他扰乱系统shell的脚本。</p>
<p>关于trap的用法参见这篇文章<a href="https://blog.robotshell.org/2012/necessary-details-about-signal-trap-in-shell/" target="_blank" rel="external">关于Linux Shell的信号trap功能你必须知道的细节</a></p>
<h3 id="升级rvm版本"><a href="#升级rvm版本" class="headerlink" title="升级rvm版本"></a>升级rvm版本</h3><p>使用<code>rvm get stable</code>升级rvm为最新的稳定版</p>
<p>过程中可能会报如下错误</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"># rvm get stable</div><div class="line">Downloading https://get.rvm.io</div><div class="line">Downloading https://raw.githubusercontent.com/rvm/rvm/master/binscripts/rvm-installer.asc</div><div class="line">Verifying /usr/local/rvm/archives/rvm-installer.asc</div><div class="line">gpg: Signature made Sun 30 Dec 2018 10:44:46 AM UTC using RSA key ID 39499BDB</div><div class="line">gpg: Can&apos;t check signature: No public key</div><div class="line">Warning, RVM 1.26.0 introduces signed releases and automated check of signatures when GPG software found. Assuming you trust Michal Papis import the mpapis public key (downloading the signatures).</div><div class="line"></div><div class="line">GPG signature verification failed for &apos;/usr/local/rvm/archives/rvm-installer&apos; - &apos;https://raw.githubusercontent.com/rvm/rvm/master/binscripts/rvm-installer.asc&apos;! Try to install GPG v2 and then fetch the public key:</div><div class="line"></div><div class="line">    sudo gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3</div><div class="line"></div><div class="line">or if it fails:</div><div class="line"></div><div class="line">    command curl -sSL https://rvm.io/mpapis.asc | sudo gpg2 --import -</div><div class="line"></div><div class="line">the key can be compared with:</div><div class="line"></div><div class="line">    https://rvm.io/mpapis.asc</div><div class="line">    https://keybase.io/mpapis</div><div class="line"></div><div class="line">NOTE: GPG version 2.1.17 have a bug which cause failures during fetching keys from remote server. Please downgrade or upgrade to newer version (if available) or use the second method described above.</div><div class="line"></div><div class="line">bash: return: _ret: numeric argument required</div><div class="line">#</div></pre></td></tr></table></figure>
<p>使用如下命令安装新key后再<code>rvm get stable</code>即可正常升级。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gpg --keyserver hkp://keys.gnupg.net --recv-keys 7D2BAF1CF37B13E2069D6956105BD0E739499BDB</div></pre></td></tr></table></figure></p>
<p>rvm 版本升级报错的详细说明参见该issue <a href="https://github.com/rvm/rvm/issues/4520#issuecomment-446688051" target="_blank" rel="external">RVM 1.29.5 stable signed with different key</a></p>
]]></content>
      
        <categories>
            
            <category> Shell </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ruby </tag>
            
            <tag> CentOS </tag>
            
            <tag> Linux </tag>
            
            <tag> RVM </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用nginx处理S3静态资源]]></title>
      <url>/2019/03/11/ways-to-support-public-s3-file-in-nginx/</url>
      <content type="html"><![CDATA[<p>在网页或APP中，使用S3公开的静态资源，比如图片,小视频等。通常有这么几个常用的方式</p>
<ul>
<li>直接在页面中使用S3的public链接</li>
<li>使用自己的域名，再在Web服务器侧redirect到S3的链接</li>
<li>使用自己的域名，再在服务器上经过反向代理服务，获取S3内容后再返回给客户端或浏览器。</li>
</ul>
<p>在APP中，因为不需要考虑SEO，可以在页面或接口中直接返回S3的链接，APP直接访问S3资源，效率最高。<br>如果是网页，有时候为了SEO的需要，可能更多的会采用第二和第三种方式, 将资源经过服务器来中转一下。<br>下面介绍一下第二和第三种方式在Nginx侧的简单实现。</p>
<h3 id="使用Nginx来redirect-S3的链接"><a href="#使用Nginx来redirect-S3的链接" class="headerlink" title="使用Nginx来redirect S3的链接"></a>使用Nginx来redirect S3的链接</h3><p>如下是一个简单的Nginx的配置的例子。基于以下几个前提</p>
<ul>
<li>测试域名为test.jibing57.com</li>
<li>所有放在S3上的静态文件，在网站上都以<code>/s3_redirect</code>路径打头</li>
<li>S3 Region为us-west-2, Bucket 为<code>s3-for-blog</code></li>
<li>将<code>/s3_redirect</code>开头的文件redirect到真实的S3 URL中。</li>
</ul>
<a id="more"></a>
<p>nginx侧配置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">location ~ ^/s3_redirect/(.*) &#123;</div><div class="line">    return 301 https://s3-for-blog.s3-us-west-2.amazonaws.com/$1;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>以访问<code>http://test.jibing57.com/s3_redirect/kulipa.jpg</code>这个测试图片为例。</p>
<p>Chrome侧访问测试链接的截图如下, 可以看到nginx返回了301 redirect，将路径重定向到S3中。<br><img src="/images/Nginx/chrome_inspect_of_S3_file_redirect.png" alt="chrome_inspect_of_S3_file_redirect"></p>
<p>curl 命令的输出<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">$ curl -L -I http://test.jibing57.com/s3_redirect/kulipa.jpg</div><div class="line">HTTP/1.1 301 Moved Permanently</div><div class="line">Server: nginx/1.14.0</div><div class="line">Date: Thu, 21 Mar 2019 09:02:48 GMT</div><div class="line">Content-Type: text/html</div><div class="line">Content-Length: 185</div><div class="line">Connection: keep-alive</div><div class="line">Location: https://s3-for-blog.s3-us-west-2.amazonaws.com/kulipa.jpg</div><div class="line"></div><div class="line">HTTP/1.1 200 OK</div><div class="line">x-amz-id-2: u3CHOBo7dc8TBbI8q9miqTBkiinM0vktcMN6fSeXQOMhIYQyZys6k2vRqQKWiYrBSAiJ1WMzNFg=</div><div class="line">x-amz-request-id: 0069391D5213BD5C</div><div class="line">Date: Thu, 21 Mar 2019 09:02:50 GMT</div><div class="line">Last-Modified: Wed, 20 Mar 2019 05:58:56 GMT</div><div class="line">ETag: &quot;d98f298bce5a81e41ca8f278324c5056&quot;</div><div class="line">Accept-Ranges: bytes</div><div class="line">Content-Type: image/jpeg</div><div class="line">Content-Length: 10857</div><div class="line">Server: AmazonS3</div><div class="line"></div><div class="line">$</div></pre></td></tr></table></figure></p>
<p>几个注意点:</p>
<ol>
<li>return 301，302甚至是307需要按照实际需要进行配置</li>
</ol>
<h3 id="使用Nginx作为反向代理获取S3内容"><a href="#使用Nginx作为反向代理获取S3内容" class="headerlink" title="使用Nginx作为反向代理获取S3内容"></a>使用Nginx作为反向代理获取S3内容</h3><h4 id="不使用Nginx-Cache时"><a href="#不使用Nginx-Cache时" class="headerlink" title="不使用Nginx Cache时"></a>不使用Nginx Cache时</h4><p>如下是一个简单的Nginx反向代理获取S3文件再返回给客户端的例子。基于以下几个前提</p>
<ul>
<li>测试域名为test.jibing57.com</li>
<li>所有放在S3上的静态文件，在网站上都以<code>/s3_proxy</code>路径打头</li>
<li>S3 Region为us-west-2, Bucket 为<code>s3-for-blog</code></li>
<li>Nginx从后端S3中获取文件，再返回给客户端</li>
</ul>
<p>nginx配置:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">location /s3_proxy/ &#123;</div><div class="line">  proxy_http_version     1.1;</div><div class="line">  proxy_set_header       Connection &quot;&quot;;</div><div class="line">  proxy_set_header       Authorization &apos;&apos;;</div><div class="line">  proxy_set_header       Host s3-for-blog.s3-us-west-2.amazonaws.com;</div><div class="line">  proxy_hide_header      x-amz-id-2;</div><div class="line">  proxy_hide_header      x-amz-request-id;</div><div class="line">  proxy_hide_header      x-amz-meta-server-side-encryption;</div><div class="line">  proxy_hide_header      x-amz-server-side-encryption;</div><div class="line">  proxy_hide_header      Set-Cookie;</div><div class="line">  proxy_ignore_headers   Set-Cookie;</div><div class="line">  proxy_intercept_errors on;</div><div class="line">  add_header             Cache-Control max-age=31536000;</div><div class="line">  proxy_pass             https://s3-for-blog.s3-us-west-2.amazonaws.com/;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>测试连接<code>http://test.jibing57.com/s3_proxy/kulipa.jpg</code></p>
<p>Chrome侧访问测试链接的截图如下，整个过程对于客户端来说，只有200的response。<br><img src="/images/Nginx/chrome_inspect_of_S3_file_proxy.png" alt="chrome_inspect_of_S3_file_proxy"></p>
<p>curl 命令输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">$ curl -L -I http://test.jibing57.com/s3_proxy/kulipa.jpg</div><div class="line">HTTP/1.1 200 OK</div><div class="line">Server: nginx/1.14.0</div><div class="line">Date: Thu, 21 Mar 2019 09:18:58 GMT</div><div class="line">Content-Type: image/jpeg</div><div class="line">Content-Length: 10857</div><div class="line">Connection: keep-alive</div><div class="line">Last-Modified: Wed, 20 Mar 2019 05:58:56 GMT</div><div class="line">ETag: &quot;d98f298bce5a81e41ca8f278324c5056&quot;</div><div class="line">Accept-Ranges: bytes</div><div class="line">Cache-Control: max-age=31536000</div><div class="line"></div><div class="line">$</div></pre></td></tr></table></figure>
<p>和上面direct方式相比，由于nginx的配置,少了amazon自定义的以<code>x-amz</code>打头的一些header, 并且header <code>Server</code>变为了<code>nginx/xxxx</code>，而不是<code>AmazonS3</code></p>
<h4 id="使用nginx-cache缓存文件"><a href="#使用nginx-cache缓存文件" class="headerlink" title="使用nginx cache缓存文件"></a>使用nginx cache缓存文件</h4><p>如下是一个简单的Nginx反向代理获取S3文件,缓存后再返回给客户端的例子。基于以下几个前提</p>
<ul>
<li>测试域名为test.jibing57.com</li>
<li>所有放在S3上的静态文件，在网站上都以<code>/s3_proxy_cache</code>路径打头</li>
<li>S3 Region为us-west-2, Bucket 为<code>s3-for-blog</code></li>
<li>Nginx从后端S3中获取文件,返回给客户端,并缓存在本地</li>
</ul>
<p>nginx配置:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">http &#123;</div><div class="line">    ....</div><div class="line">    proxy_cache_path   /tmp/ levels=1:2 keys_zone=s3_proxy_cache:10m max_size=500m</div><div class="line">                     inactive=60m use_temp_path=off;</div><div class="line">    ....</div><div class="line">    server &#123;</div><div class="line">      ......</div><div class="line">      location /s3_proxy_cache/ &#123;</div><div class="line">        proxy_cache            s3_proxy_cache;</div><div class="line">        proxy_http_version     1.1;</div><div class="line">        proxy_set_header       Connection &quot;&quot;;</div><div class="line">        proxy_set_header       Authorization &apos;&apos;;</div><div class="line">        proxy_set_header       Host s3-for-blog.s3-us-west-2.amazonaws.com;</div><div class="line">        proxy_hide_header      x-amz-id-2;</div><div class="line">        proxy_hide_header      x-amz-request-id;</div><div class="line">        proxy_hide_header      x-amz-meta-server-side-encryption;</div><div class="line">        proxy_hide_header      x-amz-server-side-encryption;</div><div class="line">        proxy_hide_header      Set-Cookie;</div><div class="line">        proxy_ignore_headers   Set-Cookie;</div><div class="line">        proxy_cache_revalidate on;</div><div class="line">        proxy_intercept_errors on;</div><div class="line">        proxy_cache_use_stale  error timeout updating http_500 http_502 http_503 http_504;</div><div class="line">        proxy_cache_lock       on;</div><div class="line">        proxy_cache_valid      200 304 60m;</div><div class="line">        add_header             Cache-Control max-age=31536000;</div><div class="line">        add_header             X-Cache-Status $upstream_cache_status;</div><div class="line">        proxy_pass             https://s3-for-blog.s3-us-west-2.amazonaws.com/;</div><div class="line">      &#125;</div><div class="line">      ......</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>测试链接<code>http://test.jibing57.com/s3_proxy_cache/kulipa.jpg</code></p>
<p>Chrom访问测试链接的截图如下，对客户端来说，只有200的response。<br><img src="/images/Nginx/chrome_inspect_of_S3_file_proxy_cache.png" alt="chrome_inspect_of_S3_file_proxy_cache"></p>
<p>curl 命令输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">$ curl -L -I http://test.jibing57.com/s3_proxy_cache/kulipa.jpg</div><div class="line">HTTP/1.1 200 OK</div><div class="line">Server: nginx/1.14.0</div><div class="line">Date: Fri, 22 Mar 2019 05:34:25 GMT</div><div class="line">Content-Type: image/jpeg</div><div class="line">Content-Length: 10857</div><div class="line">Connection: keep-alive</div><div class="line">Last-Modified: Wed, 20 Mar 2019 05:58:56 GMT</div><div class="line">ETag: &quot;d98f298bce5a81e41ca8f278324c5056&quot;</div><div class="line">Cache-Control: max-age=31536000</div><div class="line">X-Cache-Status: MISS</div><div class="line">Accept-Ranges: bytes</div><div class="line"></div><div class="line">$</div><div class="line">$ curl -L -I http://test.jibing57.com/s3_proxy_cache/kulipa.jpg</div><div class="line">HTTP/1.1 200 OK</div><div class="line">Server: nginx/1.14.0</div><div class="line">Date: Fri, 22 Mar 2019 05:34:27 GMT</div><div class="line">Content-Type: image/jpeg</div><div class="line">Content-Length: 10857</div><div class="line">Connection: keep-alive</div><div class="line">Last-Modified: Wed, 20 Mar 2019 05:58:56 GMT</div><div class="line">ETag: &quot;d98f298bce5a81e41ca8f278324c5056&quot;</div><div class="line">Cache-Control: max-age=31536000</div><div class="line">X-Cache-Status: HIT</div><div class="line">Accept-Ranges: bytes</div><div class="line"></div><div class="line">$</div></pre></td></tr></table></figure>
<p>可以看到第一次<code>X-Cache-Status</code>状态是MISS，第二次请求就是HIT了。</p>
<p>注意点:</p>
<ol>
<li>如果重启nginx时，有<code>&quot;proxy_cache_path&quot; directive is not allowed here in</code>的错误提示时，则需要检查一下<code>proxy_cache_path</code>是不是没有放在配置文件的http下。</li>
<li>当proxy_cache_path中设置了缓存目录为<code>/tmp</code>，但在<code>/tmp/</code>目录下，却没有找到缓存的文件时。别慌，记得查看下<code>/lib/systemd/system/nginx.service</code>中的<code>PrivateTmp</code>值有没有设为true，如果是true的话，在类似<code>/tmp/systemd-private-1adb681dede84276b650ea38bfa4dc1d-nginx.service-z9XRZN/tmp</code>这样的目录下找一下。具体原因参考<a href="/2019/03/10/passenger-status-not-work-on-CentOS-7/" title="CentOS 7下passenger-status报错">CentOS 7下passenger-status报错</a>中systemd PrivateTmp一章的说明</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="/2019/03/10/passenger-status-not-work-on-CentOS-7/" title="CentOS 7下passenger-status报错">CentOS 7下passenger-status报错</a></li>
<li><a href="https://stackoverflow.com/a/44749584" target="_blank" rel="external">Nginx proxy Amazon S3 resources</a></li>
<li><a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html" target="_blank" rel="external">ngx_http_proxy_module</a> </li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> S3 </tag>
            
            <tag> Nginx </tag>
            
            <tag> HTTP </tag>
            
            <tag> Linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CentOS 7下passenger-status报错]]></title>
      <url>/2019/03/10/passenger-status-not-work-on-CentOS-7/</url>
      <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>CentOS 7中，执行<code>passenger-status</code>报了如下错误:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ERROR: Phusion Passenger doesn&apos;t seem to be running. If you are sure that it is running, then the causes of this problem could be:</div><div class="line"></div><div class="line">1. You customized the instance registry directory using Apache&apos;s PassengerInstanceRegistryDir option, Nginx&apos;s passenger_instance_registry_dir option, or Phusion Passenger Standalone&apos;s --instance-registry-dir command line argument. If so, please set the environment variable PASSENGER_INSTANCE_REGISTRY_DIR to that directory and run passenger-status again.</div><div class="line">2. The instance directory has been removed by an operating system background service. Please set a different instance registry directory using Apache&apos;s PassengerInstanceRegistryDir option, Nginx&apos;s passenger_instance_registry_dir option, or Phusion Passenger Standalone&apos;s --instance-registry-dir command line argument.</div></pre></td></tr></table></figure>
<p>而命令<code>passenger-memory-stats</code>可以正常工作。</p>
<p>同样的问题，在Amazon Linux 2 AMI中也同样存在。</p>
<a id="more"></a>
<p>机器上Passenger的安装信息如下:</p>
<ul>
<li>没有通过加载yum源来安装passenger，而是通过<code>gem install passenger</code>和<code>passenger-install-nginx-module</code>来安装的passenger-standalone</li>
<li>安装目录使用的默认路径<code>/opt/nginx/</code></li>
<li>参照<a href="https://www.nginx.com/resources/wiki/start/topics/examples/systemd/" target="_blank" rel="external">Nginx官方的说明</a>来设置的nginx启动脚本。</li>
</ul>
<h3 id="原因解析"><a href="#原因解析" class="headerlink" title="原因解析"></a>原因解析</h3><p>RHEL 7中<code>systemd</code>的一些安全特性和<code>passenger-status</code>的一些默认值不相匹配，导致了<code>passenger-status</code>命令的不可用。<br>如下先介绍一下解决过程和其中涉及到的一些知识。</p>
<ul>
<li>systemd PriavteTmp的配置</li>
<li>Nginx推荐的nginx.service的写法</li>
<li>passenger-status命令中passenger_instance_registry_dir的默认路径</li>
</ul>
<h4 id="systemd-PrivateTmp"><a href="#systemd-PrivateTmp" class="headerlink" title="systemd PrivateTmp"></a>systemd PrivateTmp</h4><p>先介绍一些RHEL 7 systemd中名为<code>PrivateTmp</code>的配置，这是在RHEL 7中新添加一个新的安全设置，目的是增强系统的安全性。<br>设立这个设置的原因:</p>
<ul>
<li>有一些系统服务会使用<code>/tmp</code>目录来保存服务的运行时信息，但<code>/tmp</code>目录默认是任何人都有读写权限的，如果某些系统服务使用<code>/tmp</code>，并依赖于其中的某些配置来运行，那么这些配置有可能被普通用户或者服务所修改，导致安全隐患。</li>
<li>设置<code>PrivateTmp</code>为true，使用systemctl启动服务时，系统会将原来设置到<code>/tmp</code>下的内容分配到<code>/tmp/systemd-private-&lt;random string&gt;-&lt;servicename&gt;.service-&lt;random string&gt;</code>这个的一个命令空间中。并且设置这个目录的权限为700。就可以保证对应的配置文件不会被其他用户和进程所更改。</li>
</ul>
<p>例如:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># ll /tmp/ | grep &apos;systemd&apos;</div><div class="line">drwx------ 3 root root 17 3月   1 09:28 systemd-private-1adb681dede84276b650ea38bfa4dc1d-chronyd.service-y66NBd</div><div class="line">drwx------ 3 root root 17 3月   6 09:55 systemd-private-1adb681dede84276b650ea38bfa4dc1d-nginx.service-MgTlBE</div><div class="line">#</div></pre></td></tr></table></figure></p>
<p>详细说明参见红帽官网的这篇blog <a href="https://access.redhat.com/blogs/766093/posts/1976243" target="_blank" rel="external">New Red Hat Enterprise Linux 7 Security Feature: PrivateTmp</a></p>
<h4 id="Nginx推荐的nginx-service的写法"><a href="#Nginx推荐的nginx-service的写法" class="headerlink" title="Nginx推荐的nginx.service的写法"></a>Nginx推荐的nginx.service的写法</h4><p>Nginx官方建议的设置Nginx的文档<a href="https://www.nginx.com/resources/wiki/start/topics/examples/systemd/" target="_blank" rel="external">NGINX systemd service file</a>中推荐的<code>nginx.service</code>的设置如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">Description=The NGINX HTTP and reverse proxy server</div><div class="line">After=syslog.target network.target remote-fs.target nss-lookup.target</div><div class="line"></div><div class="line">[Service]</div><div class="line">Type=forking</div><div class="line">PIDFile=/run/nginx.pid</div><div class="line">ExecStartPre=/usr/sbin/nginx -t</div><div class="line">ExecStart=/usr/sbin/nginx</div><div class="line">ExecReload=/usr/sbin/nginx -s reload</div><div class="line">ExecStop=/bin/kill -s QUIT $MAINPID</div><div class="line">PrivateTmp=true</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div></pre></td></tr></table></figure>
<p>结合<code>gem install passenger</code>的默认路径<code>/opt/nginx/</code>，实际部署的时候，被修改成如下内容来写入<code>/lib/systemd/system/nginx.service</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">Description=The nginx HTTP and reverse proxy server</div><div class="line">After=syslog.target network.target remote-fs.target nss-lookup.target</div><div class="line"></div><div class="line">[Service]</div><div class="line">Type=forking</div><div class="line">PIDFile=/var/run/nginx.pid</div><div class="line">ExecStartPre=/opt/nginx/sbin/nginx -t</div><div class="line">ExecStart=/opt/nginx/sbin/nginx -c /opt/nginx/conf/nginx.conf</div><div class="line">ExecReload=/bin/kill -s HUP $MAINPID</div><div class="line">ExecStop=/bin/kill -s QUIT $MAINPID</div><div class="line">PrivateTmp=true</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div></pre></td></tr></table></figure>
<h4 id="passenger-instance-registry-dir的default值"><a href="#passenger-instance-registry-dir的default值" class="headerlink" title="passenger_instance_registry_dir的default值"></a>passenger_instance_registry_dir的default值</h4><p>根据<a href="https://www.phusionpassenger.com/library/config/nginx/reference/#passenger_instance_registry_dir" target="_blank" rel="external">Passenger官方文档</a>说明，Passenger使用<code>passenger_instance_registry_dir</code>这个参数，来指定存放passenger进程信息的目录。</p>
<blockquote>
<p>Specifies the directory that Passenger should use for registering its current instance.</p>
<p>When Passenger starts up, it creates a temporary directory inside the instance registry directory. This temporary directory is called the instance directory. It contains all sorts of files that are important to that specific running Passenger instance, such as Unix domain socket files so that all the different Passenger processes can communicate with each other. Command line tools such as passenger-status use the files in this directory in order to query Passenger’s status.</p>
</blockquote>
<p>该目录会被<code>passenger-status</code>命令用来获取Passenger的状态。</p>
<p>而根据Passenger官方的介绍，<a href="https://www.phusionpassenger.com/library/config/nginx/reference/#passenger_instance_registry_dir" target="_blank" rel="external"><code>passenger_instance_registry_dir</code></a>参数默认的值是<code>/tmp|/var/run/passenger-instreg</code></p>
<ul>
<li>如果是Red Hat and CentOS，并且是通过Phusion的yum源安装的，那么默认值是<code>/var/run/passenger-instreg</code></li>
<li>其他情况下，默认值是<code>$TMPDIR</code>环境变量的值，如果<code>$TMPDIR</code>没有设置，那么就是<code>/tmp</code></li>
</ul>
<p>如果没有使用默认路径，那么运行<code>passenger-status</code>之前，就需要使用<code>export PASSENGER_INSTANCE_REGISTRY_DIR=/my_temp-dir</code>来声明registry dir。</p>
<h4 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h4><p>了解了相关的原理，就很清晰地知道<code>passenger-status</code>运行不成功的原因了。<br>Nginx推荐的nginx.service脚本中，PrivateTmp是true的，passenger的进程信息就被系统放在了<code>/tmp/systemd-private-xxxx</code>的目录中了。而<code>passenger-status</code>还是去默认的<code>/tmp</code>下去寻找passenger进程状态，导致了命令运行失败。</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>两个解决办法也就一目了然了。</p>
<ul>
<li>修改<code>nginx.service</code>的PrivateTmp值</li>
<li>修改passenger_instance_registry_dir</li>
</ul>
<h4 id="修改PrivateTmp"><a href="#修改PrivateTmp" class="headerlink" title="修改PrivateTmp"></a>修改PrivateTmp</h4><p>第一个简单的办法就是修改<code>nginx.service</code>的PrivateTmp为false。</p>
<p>此时，禁用了systemd的private tmp的隔离机制，passenger的进程信息会被写入<code>/tmp/</code>目录，<code>passenger-status</code>命令就能够获取passenger运行信息并输出了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">Description=The nginx HTTP and reverse proxy server</div><div class="line">After=syslog.target network.target remote-fs.target nss-lookup.target</div><div class="line"></div><div class="line">[Service]</div><div class="line">Type=forking</div><div class="line">PIDFile=/var/run/nginx.pid</div><div class="line">ExecStartPre=/opt/nginx/sbin/nginx -t</div><div class="line">ExecStart=/opt/nginx/sbin/nginx -c /opt/nginx/conf/nginx.conf</div><div class="line">ExecReload=/bin/kill -s HUP $MAINPID</div><div class="line">ExecStop=/bin/kill -s QUIT $MAINPID</div><div class="line">PrivateTmp=false</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div></pre></td></tr></table></figure>
<p>重新load nginx.service并重启nginx.service<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># reload the nginx.service</div><div class="line">systemctl daemon-reload</div><div class="line"></div><div class="line"># restart the nginx</div><div class="line">systemctl restart nginx.service</div></pre></td></tr></table></figure></p>
<p>在<code>/tmp</code>下就可以看到passenger的信息目录，<code>passenger-status</code>命令也就可以输出passenger的状态了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">$ ll /tmp/</div><div class="line">总用量 0</div><div class="line">drwxr-xr-x 5 root root 223 3月  10 09:20 passenger.wRvwBR0</div><div class="line">drwx------ 3 root root  17 3月   1 09:28 systemd-private-1adb681dede84276b650ea38bfa4dc1d-chronyd.service-y66NBd</div><div class="line">$</div><div class="line">$ passenger-status</div><div class="line">Version : 5.3.7</div><div class="line">Date    : 2019-03-10 09:23:15 +0000</div><div class="line">Instance: 9NWwyPOn (nginx/1.14.0 Phusion_Passenger/5.3.7)</div><div class="line"></div><div class="line">----------- General information -----------</div><div class="line">Max pool size : 6</div><div class="line">App groups    : 0</div><div class="line">Processes     : 0</div><div class="line">Requests in top-level queue : 0</div><div class="line"></div><div class="line">----------- Application groups -----------</div><div class="line">$</div></pre></td></tr></table></figure>
<h4 id="指定passenger-instance-registry-dir"><a href="#指定passenger-instance-registry-dir" class="headerlink" title="指定passenger_instance_registry_dir"></a>指定passenger_instance_registry_dir</h4><p>按照上文所述的官方文档的说明，理论上运行<code>passenger-status</code>时，指定正确的passenger_instance_registry_dir，即可成功执行。</p>
<p>但实际执行中，在CentOS 7下，在<code>nginx.service</code>中<code>PrivateTmp=true</code>的情况下，因为路径过长，会报<code>too long unix socket path</code>的错误。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">$ export PASSENGER_INSTANCE_REGISTRY_DIR=/tmp/systemd-private-1adb681dede84276b650ea38bfa4dc1d-nginx.service-xrbYa5/tmp/</div><div class="line">$ passenger-status</div><div class="line">Version : 5.3.7</div><div class="line">Date    : 2019-03-10 09:37:36 +0000</div><div class="line">Instance: wbr6WsVE (nginx/1.14.0 Phusion_Passenger/5.3.7)</div><div class="line"></div><div class="line">/usr/local/rvm/gems/ruby-2.3.7/gems/passenger-5.3.7/src/ruby_supportlib/phusion_passenger/admin_tools/instance.rb:94:in `initialize&apos;: too long unix socket path (115bytes given but 108bytes max) (ArgumentError)</div><div class="line">	from /usr/local/rvm/gems/ruby-2.3.7/gems/passenger-5.3.7/src/ruby_supportlib/phusion_passenger/admin_tools/instance.rb:94:in `new&apos;</div><div class="line">	from /usr/local/rvm/gems/ruby-2.3.7/gems/passenger-5.3.7/src/ruby_supportlib/phusion_passenger/admin_tools/instance.rb:94:in `http_request&apos;</div><div class="line">	from /usr/local/rvm/gems/ruby-2.3.7/gems/passenger-5.3.7/bin/passenger-status:113:in `show_status&apos;</div><div class="line">	from /usr/local/rvm/gems/ruby-2.3.7/gems/passenger-5.3.7/bin/passenger-status:61:in `command_show_status&apos;</div><div class="line">	from /usr/local/rvm/gems/ruby-2.3.7/gems/passenger-5.3.7/bin/passenger-status:332:in `start&apos;</div><div class="line">	from /usr/local/rvm/gems/ruby-2.3.7/gems/passenger-5.3.7/bin/passenger-status:335:in `&lt;top (required)&gt;&apos;</div><div class="line">	from /usr/local/rvm/gems/ruby-2.3.7/bin/passenger-status:23:in `load&apos;</div><div class="line">	from /usr/local/rvm/gems/ruby-2.3.7/bin/passenger-status:23:in `&lt;main&gt;&apos;</div><div class="line">	from /usr/local/rvm/gems/ruby-2.3.7/bin/ruby_executable_hooks:24:in `eval&apos;</div><div class="line">	from /usr/local/rvm/gems/ruby-2.3.7/bin/ruby_executable_hooks:24:in `&lt;main&gt;&apos;</div><div class="line">$</div></pre></td></tr></table></figure>
<p>sock path length是Linux系统的限制。具体可以参考<a href="https://unix.stackexchange.com/questions/367008/why-is-socket-path-length-limited-to-a-hundred-chars" target="_blank" rel="external">Why is socket path length limited to a hundred chars?</a>这篇文章。</p>
<p>既然替换为systemd的privatetmp的路径不可行，那么换个思路，可在Passenger启动的时候, 在配置文件中手动指定passenger注册实例的目录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"># 创建新的注册目录</div><div class="line">$ sudo mkdir /opt/nginx/tmp</div><div class="line"></div><div class="line"># 打开nginx配置文件，添加passenger_instance_registry_dir</div><div class="line">$ sudo vim /opt/nginx/conf/nginx.conf</div><div class="line"></div><div class="line">http &#123;</div><div class="line">    ......</div><div class="line">    passenger_instance_registry_dir /opt/nginx/tmp;</div><div class="line">    ......</div><div class="line">&#125;</div><div class="line"></div><div class="line"># 重启nginx</div><div class="line">sudo systemctl restart nginx.service</div></pre></td></tr></table></figure>
<p>再在执行<code>passenger-status</code>的时候，添加上registry_dir的路径。就可以顺利执行命令了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">$ export PASSENGER_INSTANCE_REGISTRY_DIR=/opt/nginx/tmp/</div><div class="line">$ passenger-status</div><div class="line">Version : 5.3.7</div><div class="line">Date    : 2019-03-10 10:02:15 +0000 </div><div class="line">Instance: inceochh (nginx/1.14.0 Phusion_Passenger/5.3.7)</div><div class="line"></div><div class="line">----------- General information -----------</div><div class="line">Max pool size : 6</div><div class="line">App groups    : 0</div><div class="line">Processes     : 0</div><div class="line">Requests in top-level queue : 0</div><div class="line"></div><div class="line">----------- Application groups -----------</div><div class="line">$</div></pre></td></tr></table></figure>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://access.redhat.com/blogs/766093/posts/1976243" target="_blank" rel="external">New Red Hat Enterprise Linux 7 Security Feature: PrivateTmp</a></li>
<li><a href="https://www.nginx.com/resources/wiki/start/topics/examples/systemd/" target="_blank" rel="external">NGINX systemd service file</a></li>
<li><a href="https://www.phusionpassenger.com/library/config/nginx/reference/#passenger_instance_registry_dir" target="_blank" rel="external">Configuration reference for Passenger + Nginx</a></li>
<li><a href="https://unix.stackexchange.com/questions/367008/why-is-socket-path-length-limited-to-a-hundred-chars" target="_blank" rel="external">Why is socket path length limited to a hundred chars?</a></li>
<li><a href="http://llawlor.github.io/2016/03/05/phusion-passenger-nginx-notes.html" target="_blank" rel="external">Phusion Passenger &amp; Nginx with Systemd Notes</a></li>
<li><a href="https://www.xncoding.com/2016/06/07/linux/systemd.html" target="_blank" rel="external">centos7上systemd详解</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> EC2 </tag>
            
            <tag> Passenger </tag>
            
            <tag> Nginx </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Macos下因为找不到libxml2而导致nokogiri安装失败的解决办法]]></title>
      <url>/2019/03/05/how-resolve-nokogiri-failed-by-xml2-on-macos/</url>
      <content type="html"><![CDATA[<p>今天要跑一个其他的rails项目，为了不污染原先的gemset。就重建了一个gemset, 但是bundle install的时候，安装nokogiri 1.8.4一直失败。<br>系统版本是Macos 10.13 High Sierra中, ruby版本是rvm安装的ruby2.3.0。</p>
<p>令人费解的是，同样在ruby2.3.0下的另一个gemset,nokogiri 1.8.2可以顺利安装。</p>
<p>网上搜了一圈后，找到了解决办法。记录如下:</p>
<p>设置bundle config，编译nokogiri时，使用系统xml2路径即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bundle config build.nokogiri --use-system-libraries --with-xml2-include=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.13.sdk/usr/include/libxml2</div></pre></td></tr></table></figure>
<p>还有一个更简单的方法，自动根据系统版本得到系统libxml2的路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bundle config build.nokogiri --use-system-libraries=true --with-xml2-include=&quot;$(xcrun --show-sdk-path)&quot;/usr/include/libxml2</div></pre></td></tr></table></figure>
<p>参考资料: <a href="https://github.com/sparklemotion/nokogiri/issues/1547" target="_blank" rel="external">Installation of nokogiri fail with bundle on macOS Sierra 10.12 </a></p>
]]></content>
      
        <categories>
            
            <category> Code </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ruby </tag>
            
            <tag> Rails </tag>
            
            <tag> MacOS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Nginx的HTTP Code444和307设置]]></title>
      <url>/2019/02/28/nginx_444_and_307/</url>
      <content type="html"><![CDATA[<h3 id="nginx-444"><a href="#nginx-444" class="headerlink" title="nginx 444"></a>nginx 444</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>当没有显式指定default server的时候，Nginx会使用第一个server来作为默认的响应server，即使请求的Host并没有匹配到server_name。<br>这是为了兼容老旧的一些不带Host的HTTP请求而做的设置。官文描述: <a href="http://nginx.org/en/docs/http/request_processing.html" target="_blank" rel="external">How nginx processes a request</a></p>
<p>此时可能存在一定的风险。如果有非备案的名解析到了你的Public IP上，而你的Web服务对该域名的请求有所响应的话，可能会导致Public IP被运营商封锁。</p>
<h4 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法:"></a>解决办法:</h4><p>在nginx中添加一个default server，并在default server中设置<code>return 444;</code>。此时Nginx就不会响应请求，除非请求的Host是配置的。<br><a id="more"></a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">server &#123;</div><div class="line">    listen      80 default_server;</div><div class="line">    server_name _;</div><div class="line">    return      444;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>此时请求服务器，在Chrome中显示的页面如下:</p>
<p><img src="/images/Nginx/444_no_response.png" alt="444_no_response.png"></p>
<p>和499类似，444不是标准的HTTP status code，而是Nginx自己设立的状态码，参见<a href="https://httpstatuses.com/444" target="_blank" rel="external">444 CONNECTION CLOSED WITHOUT RESPONSE</a><br>设置了444过后，nginx收到不匹配的Host请求的话，不会有任何响应。比起404之类，另外一个好处就是可以节省一点点的带宽。</p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ul>
<li><a href="https://stackoverflow.com/questions/9824328/why-is-nginx-responding-to-any-domain-name" target="_blank" rel="external">Why is nginx responding to any domain name?</a></li>
<li><a href="http://nginx.org/en/docs/http/ngx_http_rewrite_module.html#return" target="_blank" rel="external">Module ngx_http_rewrite_module</a></li>
</ul>
<h3 id="HTTP-307-Temporary-Redirect"><a href="#HTTP-307-Temporary-Redirect" class="headerlink" title="HTTP 307 Temporary Redirect"></a>HTTP 307 Temporary Redirect</h3><h4 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h4><p>随着网络安全越发重要，越来越多的网站都从http迁移到了https。但迁移到https之前分发出去的链接都是http，为了兼容以前的链接，就需要做一个redirect，将http的请求redirect到https中。<br>最常见的redirect方式就是<a href="https://tools.ietf.org/html/rfc7231#section-6.4.2" target="_blank" rel="external">301 Moved Permanently</a>或者<a href="https://tools.ietf.org/html/rfc7231#section-6.4.3" target="_blank" rel="external">302 Found</a>, 将http重定向到https中。</p>
<p>对于普通的GET请求，这种方式是完全可以的。但当请求是POST的时候，由于一些历史原因，一些浏览器和库收到301或302的时候，会将原本的POST请求转为GET请求发送。此时POST的body会丢失，导致请求出问题。<br>如果要redirect POST请求，则需要使用<a href="https://tools.ietf.org/html/rfc7231#section-6.4.7" target="_blank" rel="external">307 Temporary Redirect</a>来进行redirect。RFC 7231中对于307的一个描述”Note: This status code is similar to 302 (Found), except that it does not allow changing the request method from POST to GET. “</p>
<h4 id="nginx-例子"><a href="#nginx-例子" class="headerlink" title="nginx 例子"></a>nginx 例子</h4><p>一个GET是301， POST是307的nginx配置例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">server &#123;</div><div class="line">    listen 80;</div><div class="line">    server_name www.fake.com;</div><div class="line">    if ($request_method = POST) &#123;</div><div class="line">        return 307 https://$host$request_uri;</div><div class="line">    &#125;</div><div class="line">    return 301 https://$host$request_uri;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="国内各大站的处理方式"><a href="#国内各大站的处理方式" class="headerlink" title="国内各大站的处理方式"></a>国内各大站的处理方式</h4><p>查看了国内BAT，国外Google，facebook和amazon是如何从http重定向到https的。发现除了腾讯外，其他几个大站，即使请求是GET,也是直接甩307。</p>
<p>如下是几大巨头网站主页http redirect https的截图:</p>
<ul>
<li><p>百度从http redirect到https时，直接使用的是307。Chrome中访问<a href="http://www.baidu.com的截图如下" target="_blank" rel="external">http://www.baidu.com的截图如下</a>:<br><img src="/images/Nginx/307_redirect_from_http_to_https_in_baidu.png" alt="307_redirect_from_http_to_https_in_baidu.png"></p>
</li>
<li><p>淘宝从http redirect到https时，直接使用的是307。Chrome中访问<a href="http://www.taobao.com的截图如下" target="_blank" rel="external">http://www.taobao.com的截图如下</a>:<br><img src="/images/Nginx/307_redirect_from_http_to_https_in_taobao.png" alt="307_redirect_from_http_to_https_in_taobao.png"></p>
</li>
<li><p>腾讯从http redirect到https时，使用的是302。Chrome中访问<a href="http://www.qq.com的截图如下" target="_blank" rel="external">http://www.qq.com的截图如下</a>:<br><img src="/images/Nginx/302_redirect_from_http_to_https_in_qq.png" alt="302_redirect_from_http_to_https_in_qq.png"></p>
</li>
<li><p>google从http redirect到https时，直接使用的是307。Chrome中访问<a href="http://www.google.com的截图如下" target="_blank" rel="external">http://www.google.com的截图如下</a>:<br><img src="/images/Nginx/307_redirect_from_http_to_https_in_google.png" alt="307_redirect_from_http_to_https_in_google.png"></p>
</li>
<li><p>facebook从http redirect到https时，直接使用的是307。Chrome中访问<a href="http://www.facebook.com的截图如下" target="_blank" rel="external">http://www.facebook.com的截图如下</a>:<br><img src="/images/Nginx/307_redirect_from_http_to_https_in_facebook.png" alt="307_redirect_from_http_to_https_in_facebook.png"></p>
</li>
<li><p>amazon从http redirect到https时，直接使用的是307。Chrome中访问<a href="http://www.amazon.com的截图如下" target="_blank" rel="external">http://www.amazon.com的截图如下</a>:<br><img src="/images/Nginx/307_redirect_from_http_to_https_in_amazon.png" alt="307_redirect_from_http_to_https_in_amazon.png"></p>
</li>
</ul>
<h4 id="Reference-1"><a href="#Reference-1" class="headerlink" title="Reference"></a>Reference</h4><ul>
<li><a href="https://tools.ietf.org/html/rfc7231" target="_blank" rel="external">RFC 7231</a></li>
<li><a href="https://httpstatuses.com/301" target="_blank" rel="external">301 MOVED PERMANENTLY</a></li>
<li><a href="https://en.wikipedia.org/wiki/List_of_HTTP_status_codes" target="_blank" rel="external">List of HTTP status codes</a></li>
<li><a href="https://stackoverflow.com/questions/39280361/nginx-loses-post-variable-with-http-https-redirect" target="_blank" rel="external">Nginx loses POST variable with http -&gt; https redirect</a></li>
<li><a href="https://softwareengineering.stackexchange.com/questions/99894/why-doesnt-http-have-post-redirect#99966" target="_blank" rel="external">Why doesn’t HTTP have POST redirect?</a></li>
<li><a href="https://stackoverflow.com/questions/4764297/difference-between-http-redirect-codes" target="_blank" rel="external">Difference between HTTP redirect codes</a></li>
<li><a href="https://airbrake.io/blog/http-errors/307-temporary-redirect" target="_blank" rel="external">307 Temporary Redirect: What It Is and How to Fix It</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Nginx </tag>
            
            <tag> HTTP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS C5类型Instance的挂载点变更]]></title>
      <url>/2019/02/25/mount-point-changed-in-C5/</url>
      <content type="html"><![CDATA[<h3 id="问题点"><a href="#问题点" class="headerlink" title="问题点"></a>问题点</h3><p>一直使用的是Amazon自有的Amazon Linux，中国区的C5出来后，将原有的机器升级到C5的时候，发现额外挂载的磁盘没有挂载成功。</p>
<p>调查了一下，C5系列属于<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#ec2-nitro-instances" target="_blank" rel="external">Nitro-based instance</a>, EBS卷默认使用的是NVMe driver,设备名由原来C4的/dev/xvda1和/dev/xvdf变为了/dev/nvme[0-26]n1的格式。官网说明<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/device_naming.html" target="_blank" rel="external">Device Naming on Linux Instances</a>。</p>
<p>以前C4的机器中,/etc/fstab中设定的另外一块EBS的挂载点是/dev/xvdf, 所以导致了换用C5的时候，没能自动挂载。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># C4时的设置</div><div class="line">/dev/xvdf   /mnt/data ext4  auto,nofail,defaults        1   2</div></pre></td></tr></table></figure>
<h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p><code>lsblk</code>查看EBS的挂载路径，然后修改/etc/fstab。解决了问题。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/dev/nvme1n1 /mnt/data ext4  auto,nofail,defaults        1   2</div></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="AWS推荐的mount方法"><a href="#AWS推荐的mount方法" class="headerlink" title="AWS推荐的mount方法"></a>AWS推荐的mount方法</h3><p>关于/etc/fstab的挂载点，AWS官方文档<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html#ebs-mount-after-reboot" target="_blank" rel="external">Automatically Mount an Attached Volume After Reboot</a>推荐使用UUID来进行挂载，而不要直接使用像/dev/xvdf之类的设备名。类似<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">UUID=aebf131c-6957-451e-8d34-ec978d9581ae  /data  xfs  defaults,nofail  0  2</div></pre></td></tr></table></figure></p>
<p>其中的磁盘UUID<code>aebf131c-6957-451e-8d34-ec978d9581ae</code>可以通过命令<code>blkid</code>来查看。此时，无论Instance改为了哪种类型的Instance，启动时都可以正常挂载使用。<br>使用UUID的挂载方法，也适用于于从Instance制作AMI，再从AMI启动新的Instance的情况，从AMI中启动的Instance的相同分区的EBS的UUID都是相同的,所以能够自动挂载。</p>
<h3 id="相关知识"><a href="#相关知识" class="headerlink" title="相关知识"></a>相关知识</h3><h4 id="开启ENA"><a href="#开启ENA" class="headerlink" title="开启ENA"></a>开启ENA</h4><p>使用C5类型时，要求Instance或AMI是支持Elastic Network Adapter(ENA)的才行。<br>将现有的Instance升级为ENA Support的方法参见官网说明<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-ena.html#enable-enhanced-networking-ena-AL" target="_blank" rel="external">Enabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux Instances</a></p>
<p>大概步骤:</p>
<ul>
<li>系统中安装ENA支持</li>
<li>修改Instance为ena-support : <code>aws ec2 modify-instance-attribute --instance-id instance_id --ena-support</code></li>
<li>查看Instance是否ena-support: <code>aws ec2 describe-instances --instance-ids instance_id --query &quot;Reservations[].Instances[].EnaSupport&quot;</code></li>
<li>支持ena的Instance制作的AMI，会继承该Instance的ena属性。</li>
</ul>
<p>如果没有开启ENA，则无法使用C5类型。</p>
<ul>
<li><p>当不支持ENA的Instance被修改为C5时，启动时会报如下的错误:<br><img src="/images/AWS/EC2/Start-instance-error-when-C5.png" alt="Start-instance-error-when-C5.png"></p>
</li>
<li><p>当从不支持ENA的AMI来launch Instance时，直接就没法选择C5系列的类型。<br><img src="/images/AWS/EC2/AMI-ena-required-when-launching-C5.png" alt="AMI-ena-required-when-launching-C5.png"></p>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html" target="_blank" rel="external">Making an Amazon EBS Volume Available for Use on Linux</a></li>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#ec2-nitro-instances" target="_blank" rel="external">Instance Types</a></li>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-ena.html" target="_blank" rel="external">Enabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux Instances</a></li>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/device_naming.html" target="_blank" rel="external">Device Naming on Linux Instances</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> EC2 </tag>
            
            <tag> Linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ruby异常]]></title>
      <url>/2019/02/14/ruby-exception/</url>
      <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>有一个通知服务，需要使用<a href="https://mandrill.com/" target="_blank" rel="external">Mandrill</a>的提供的邮件服务给客户发送邮件。开发时使用了<a href="https://github.com/renz45/mandrill_mailer" target="_blank" rel="external">mandrill_mailer</a>这个GEM来实现和Mandrill服务的交互。</p>
<p>今天监控程序报警提及昨天的邮件发送量比平时低了不少。上线查了一下日志，发现是代码中一处exception没处理好,导致程序崩溃了。</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>Ruby 中捕获非指定类型的异常，会使用如下方式来捕获异常。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">begin</div><div class="line">rescue =&gt; e</div><div class="line">end</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>根据<a href="https://ruby-doc.org/core-2.3.0/Exception.html" target="_blank" rel="external">官方文档</a>的说明，不指定exception类型时，rescue默认捕获的是<code>StandardError</code>类型的exception。</p>
<p>在我这个问题中，由于<a href="https://mandrill.com/" target="_blank" rel="external">Mandrill</a>的服务出现了问题，发送请求后，没有返回正常的json格式，而是返回了<code>504 Gateway Time-out</code>的一段HTML, Gem <a href="https://github.com/renz45/mandrill_mailer" target="_blank" rel="external">mandrill_mailer</a>解析JSON失败，抛出了自定义的<code>Mandrill::Error</code>的异常。</p>
<p>Gem中抛异常的代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">begin</div><div class="line">    error_info = JSON.parse(body)</div><div class="line">    if error_info[&apos;status&apos;] != &apos;error&apos; or not error_info[&apos;name&apos;]</div><div class="line">        raise Error, &quot;We received an unexpected error: #&#123;body&#125;&quot;</div><div class="line">    end</div><div class="line">    if error_map[error_info[&apos;name&apos;]]</div><div class="line">        raise error_map[error_info[&apos;name&apos;]], error_info[&apos;message&apos;]</div><div class="line">    else</div><div class="line">        raise Error, error_info[&apos;message&apos;]</div><div class="line">    end</div><div class="line">rescue JSON::ParserError</div><div class="line">    raise Error, &quot;We received an unexpected error: #&#123;body&#125;&quot;</div><div class="line">end</div></pre></td></tr></table></figure>
<p>Error异常定义的代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">module Mandrill</div><div class="line">    class Error &lt; Exception</div><div class="line">    end</div><div class="line">    class ValidationError &lt; Error</div><div class="line">    end</div></pre></td></tr></table></figure>
<p>可以看到，Mandrill::Error这个类型，是直接继承自<code>Exception</code>的。而在原有的代码中，没有指定exception的类型,捕获不到这个exception，导致程序崩溃了。</p>
<p>在代码中指定固定类型的exception，解决该问题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">begin</div><div class="line">rescue StandardError, Mandrill::Error =&gt; e</div><div class="line">end</div></pre></td></tr></table></figure>
<h3 id="深挖"><a href="#深挖" class="headerlink" title="深挖"></a>深挖</h3><p>Ruby中关于如何throw和catch exception，<a href="https://stackoverflow.com/" target="_blank" rel="external">stackoverflow</a>和<a href="https://ruby-doc.org/" target="_blank" rel="external">ruby-doc.org</a>上的这几个网页值得细读</p>
<ul>
<li><a href="https://stackoverflow.com/questions/10048173/why-is-it-bad-style-to-rescue-exception-e-in-ruby" target="_blank" rel="external">Why is it bad style to `rescue Exception =&gt; e` in Ruby?</a></li>
<li><a href="https://stackoverflow.com/questions/4800698/what-is-the-difference-between-raise-foo-and-raise-exception-newfoo" target="_blank" rel="external">What is the difference between `raise “foo”` and `raise Exception.new(“foo”)`?</a></li>
<li><a href="https://ruby-doc.org/core-2.3.0/Exception.html" target="_blank" rel="external">Exception</a> </li>
</ul>
<p>梳理了一下，罗列如下:</p>
<ul>
<li>三种常用的exception类<ul>
<li><a href="https://ruby-doc.org/core-2.3.0/RuntimeError.html" target="_blank" rel="external">RuntimeError</a></li>
<li><a href="https://ruby-doc.org/core-2.3.0/StandardError.html" target="_blank" rel="external">StandardError</a></li>
<li><a href="https://ruby-doc.org/core-2.3.0/Exception.html" target="_blank" rel="external">Exception</a></li>
</ul>
</li>
<li><code>RuntimeError</code>是<code>StandardError</code>的一个子类，而<code>StandardError</code>又是<code>Exception</code>的一个子类。</li>
<li>直接raise，抛出的是<code>RuntimeError</code>的异常</li>
<li>默认的rescue，捕获的是<code>StandardError</code>的异常</li>
<li><code>Exception</code>包含了全部的异常，ruby中全部异常都可以使用<code>rescue Exception =&gt; e</code>这种写法来捕获。</li>
<li>但强烈建议不要直接粗暴的指定基类<code>Exception</code>来捕获异常<ul>
<li><code>rescue Exception</code>时，会捕获所有的异常，包括<code>SyntaxError</code>, <code>LoadError</code>和<code>SignalException</code>等</li>
<li>捕获了<code>SignalException</code>，会导致除了<code>kill -9</code>的其他信号量失效，包括<code>Ctrl + C</code></li>
<li>捕获<code>SyntaxError</code>，调用eval时，即使失败了也不会有提示</li>
</ul>
</li>
<li>编写库和Gem的自定义异常时，强烈反对直接继承自基类<code>Exception</code>，因为这会导致使用者在没有指定异常类型时，无法成功捕获异常，导致程序崩溃。</li>
</ul>
<h3 id="小实验"><a href="#小实验" class="headerlink" title="小实验"></a>小实验</h3><ol>
<li><p>raise字符串，rescue不指定异常类型</p>
<p> 代码:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">begin</div><div class="line">  raise &quot;error occur&quot;</div><div class="line">rescue</div><div class="line">  puts &quot;here is string exception&quot;</div><div class="line">end</div></pre></td></tr></table></figure>
<p> 运行结果:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">here is string exception</div></pre></td></tr></table></figure>
</li>
<li><p>raise Exception, rescue Exception</p>
<p> 代码:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">begin</div><div class="line">  raise Exception.new(&quot;error occur&quot;)</div><div class="line">rescue Exception</div><div class="line">  puts &quot;here is Exception exception&quot;</div><div class="line">end</div></pre></td></tr></table></figure>
<p> 运行结果:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">here is Exception exception</div></pre></td></tr></table></figure>
</li>
<li><p>raise字符串，rescue Exception</p>
<p> 代码:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">begin</div><div class="line">  raise &quot;error occur&quot;</div><div class="line">rescue Exception</div><div class="line">  puts &quot;here is exception exception&quot;</div><div class="line">end</div></pre></td></tr></table></figure>
<p> 运行结果:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># rescue Exception可以捕获任何异常</div><div class="line">here is exception exception</div></pre></td></tr></table></figure>
</li>
<li><p>raise Exception, rescue不指定异常类型</p>
<p> 代码:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">begin</div><div class="line">  raise Exception.new(&quot;error occur&quot;)</div><div class="line">rescue</div><div class="line">  puts &quot;here is string exception&quot;</div><div class="line">end</div></pre></td></tr></table></figure>
<p> 运行结果:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 没有捕获到异常，程序崩溃了</div><div class="line">Exception: error occur</div><div class="line">    from (irb):39</div><div class="line">    from /Users/carlshen/.rvm/rubies/ruby-2.3.0/bin/irb:11:in `&lt;main&gt;&apos;</div></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> Code </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ruby </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[修改ElasticBeanstalk中Passenger的max-pool-size]]></title>
      <url>/2018/12/25/How-to-change-max-pool-size-of-passenger-on-ElasticBeanstalk/</url>
      <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>在AWS Elastic Beanstalk中，如果选用Ruby with Passenger运行Rails时，会遇到一个问题，就是EB环境中的Passenger的max_pool_size是默认的6。如果EB中的Instance选用的是性能比较高的类型，只起6个Passenger进程会是一个巨大的浪费。下面介绍几个修改EB中Passenger max_pool_size参数的方法。</p>
<h4 id="Passenger官方说明"><a href="#Passenger官方说明" class="headerlink" title="Passenger官方说明"></a>Passenger官方说明</h4><p>最新的官方<a href="https://www.phusionpassenger.com/library/config/standalone/intro.html#location-of-passengerfile-json" target="_blank" rel="external">Introduction to configuring Passenger Standalone</a>文档中，Passenger Standalone可以通过如下几种方式来修改启动参数</p>
<ul>
<li>一、传递参数给启动命令<code>passenger start</code></li>
<li>二、使用环境变量(从5.0.22开始)</li>
<li>三、通过配置文件<code>Passengerfile.json</code></li>
</ul>
<a id="more"></a>
<h4 id="EB的限制"><a href="#EB的限制" class="headerlink" title="EB的限制"></a>EB的限制</h4><p>EB中的Passenger, 版本还是使用的<code>4.0.60</code>, 因此官网所述的三个方法中，在EB中并不完全适用。</p>
<ul>
<li>方法二需要在<code>5.0.22</code>以上版本才支持，因此在EB环境中不可用</li>
<li>方法三中所述的<code>Passengerfile.json</code>是在5.0.1中才被正式使用的，在之前的版本中，使用的是<code>passenger-standalone.json</code>。所以在EB环境中，需要使用<code>passenger-standalone.json</code>来修改启动参数。文件命令的变化可参见<a href="https://blog.phusion.nl/2015/03/04/phusion-passenger-5-0-1-released/" target="_blank" rel="external">Passenger 5.0.1的Release Note</a></li>
</ul>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><h4 id="通过passenger-standalone-json来实现"><a href="#通过passenger-standalone-json来实现" class="headerlink" title="通过passenger-standalone.json来实现"></a>通过passenger-standalone.json来实现</h4><p>在项目代码主目录下，添加文件<code>passenger-standalone.json</code>, 文件内容为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;max_pool_size&quot;: 10</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>eb deploy</code>完成后，使用EC2的key pair登陆EC2, sudo后运行<code>passenger-status</code>。可以看到max_pool_size已成功修改为10。</p>
<p><img src="/images/AWS/EB/passenger-standalone_for_max_pool_size.jpg" alt="passenger-standalone_for_max_pool_size"></p>
<h4 id="通过EB中的passenger命令参数来实现"><a href="#通过EB中的passenger命令参数来实现" class="headerlink" title="通过EB中的passenger命令参数来实现"></a>通过EB中的passenger命令参数来实现</h4><p>EB环境中的EC2的passenger启动脚本是<code>/etc/init.d/passenger</code>，实际运行的是<code>/opt/elasticbeanstalk/support/conf/passenger</code>。</p>
<p><img src="/images/AWS/EB/passenger_link_on_EC2_of_EB.jpg" alt="passenger_link_on_EC2_of_EB.jpg"></p>
<p>只要通过ebextension配置文件，修改<code>/opt/elasticbeanstalk/support/conf/passenger</code>,添加上对<code>max_pool_size</code>的支持即可。</p>
<p>原始的<code>passenger</code>文件参见<a href="https://github.com/jibing57/my-snippet/blob/master/AWS/ElasticBeanstalk/01_increase_passenger_max_pool_size/origin_passenger" target="_blank" rel="external">origin_passenger</a></p>
<p>用来修改<code>passenger</code>，支持读取环境变量中的<code>PASSENGER_MAX_POOL_SIZE</code>的extension config如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div></pre></td><td class="code"><pre><div class="line">files:</div><div class="line">    &quot;/opt/elasticbeanstalk/support/conf/passenger&quot;:</div><div class="line">      mode: &quot;000755&quot;</div><div class="line">      owner: root</div><div class="line">      group: root</div><div class="line">      content: |</div><div class="line">        #!/usr/bin/env bash</div><div class="line">        #</div><div class="line">        # chkconfig: 2345 80 20</div><div class="line">        # description: Passenger</div><div class="line">        #</div><div class="line"></div><div class="line">        EB_HTTP_PORT=$(/opt/elasticbeanstalk/bin/get-config container -k http_port)</div><div class="line">        EB_APP_USER=$(/opt/elasticbeanstalk/bin/get-config container -k app_user)</div><div class="line">        EB_APP_DEPLOY_DIR=$(/opt/elasticbeanstalk/bin/get-config container -k app_deploy_dir)</div><div class="line">        EB_APP_PID_DIR=$(/opt/elasticbeanstalk/bin/get-config container -k app_pid_dir)</div><div class="line">        EB_APP_LOG_DIR=$(/opt/elasticbeanstalk/bin/get-config container -k app_log_dir)</div><div class="line">        EB_SCRIPT_DIR=$(/opt/elasticbeanstalk/bin/get-config container -k script_dir)</div><div class="line">        EB_SUPPORT_DIR=$(/opt/elasticbeanstalk/bin/get-config container -k support_dir)</div><div class="line">        EB_NGINX_VERSION=$(/opt/elasticbeanstalk/bin/get-config container -k nginx_version)</div><div class="line"></div><div class="line">        . $EB_SUPPORT_DIR/envvars</div><div class="line">        . $EB_SCRIPT_DIR/use-app-ruby.sh</div><div class="line"></div><div class="line">        if [ -f /etc/elasticbeanstalk/set-ulimit.sh ]; then</div><div class="line">          . /etc/elasticbeanstalk/set-ulimit.sh</div><div class="line">        fi</div><div class="line"></div><div class="line">        # fixes http://code.google.com/p/phusion-passenger/issues/detail?id=614</div><div class="line">        export HOME=/tmp</div><div class="line">        export PASSENGER_DOWNLOAD_NATIVE_SUPPORT_BINARY=0</div><div class="line"></div><div class="line">        if [ -d /etc/healthd ]; then</div><div class="line">            STARTOPTS=&quot;--nginx-version $EB_NGINX_VERSION --nginx-config-template $EB_SUPPORT_DIR/conf/nginx_config_healthd.erb&quot;</div><div class="line">        else</div><div class="line">            STARTOPTS=&quot;--nginx-version $EB_NGINX_VERSION --nginx-config-template $EB_SUPPORT_DIR/conf/nginx_config.erb&quot;</div><div class="line">        fi</div><div class="line"></div><div class="line">        ENV_STAGE=$&#123;RACK_ENV:-$RAILS_ENV&#125;    # Read from $RAILS_ENV if $RACK_ENV is empty</div><div class="line">        if [ $&#123;ENV_STAGE,,&#125; = &quot;production&quot; ]; then    # Convert $ENV_STAGE to lower case and compare to &quot;production&quot;</div><div class="line">          # Disable passenger friendly page for production stage</div><div class="line">          STARTOPTS=&quot;$STARTOPTS --no-friendly-error-pages&quot;</div><div class="line">        fi</div><div class="line"></div><div class="line">        GENERALOPTS=&quot;-p $EB_HTTP_PORT --pid-file $EB_APP_PID_DIR/passenger.pid&quot;</div><div class="line">        SELFOPTS=&quot;--max-pool-size $&#123;PASSENGER_MAX_POOL_SIZE:-6&#125;&quot;</div><div class="line"></div><div class="line">        function start() &#123;</div><div class="line">          touch $EB_APP_LOG_DIR/passenger.log</div><div class="line"></div><div class="line">          if [ -d /etc/healthd ]; then</div><div class="line">            mkdir -p $EB_APP_LOG_DIR/healthd</div><div class="line">            chown -R $EB_APP_USER:$EB_APP_USER $EB_APP_LOG_DIR/healthd</div><div class="line">          fi</div><div class="line"></div><div class="line">          chown $EB_APP_USER:$EB_APP_USER \</div><div class="line">            $EB_APP_LOG_DIR/passenger.log</div><div class="line">          passenger start $EB_APP_DEPLOY_DIR $STARTOPTS $GENERALOPTS $SELFOPTS\</div><div class="line">            -d -e $&#123;RACK_ENV:-$RAILS_ENV&#125; --user $EB_APP_USER \</div><div class="line">            --log-file $EB_APP_LOG_DIR/passenger.log</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        function stop() &#123;</div><div class="line">          passenger stop $GENERALOPTS</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        function status() &#123;</div><div class="line">          passenger status $GENERALOPTS</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        case &quot;$1&quot; in</div><div class="line">          start)</div><div class="line">            start</div><div class="line">            ;;</div><div class="line">          stop)</div><div class="line">            stop</div><div class="line">            ;;</div><div class="line">          status)</div><div class="line">            status</div><div class="line">            ;;</div><div class="line">          restart|graceful)</div><div class="line">            stop</div><div class="line">            start</div><div class="line">            ;;</div><div class="line">          reload)</div><div class="line">            su -s /bin/bash -c &quot;touch $EB_APP_DEPLOY_DIR/tmp/restart.txt&quot; $EB_APP_USER</div><div class="line">            ;;</div><div class="line">          *)</div><div class="line">            echo &quot;Usage: $0 &#123;start|stop|restart|reload|status&#125;&quot;</div><div class="line">            exit 1</div><div class="line">            ;;</div><div class="line">        esac</div><div class="line"></div><div class="line">        exit 0</div></pre></td></tr></table></figure>
<p>对原<code>passenger</code>脚本进行了两处修改</p>
<ul>
<li>添加了<code>SELFOPTS</code></li>
<li>添加<code>SELFOPTS</code>到<code>passenger start</code>命令之后。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">$ diff origin_passenger  passenger_with_selfopt</div><div class="line">39a40</div><div class="line">&gt; SELFOPTS=&quot;--max-pool-size $&#123;PASSENGER_MAX_POOL_SIZE:-6&#125;&quot;</div><div class="line">51c52</div><div class="line">&lt;   passenger start $EB_APP_DEPLOY_DIR $STARTOPTS $GENERALOPTS \</div><div class="line">---</div><div class="line">&gt;   passenger start $EB_APP_DEPLOY_DIR $STARTOPTS $GENERALOPTS $SELFOPTS\</div><div class="line">87a89</div><div class="line">&gt;</div><div class="line">$</div></pre></td></tr></table></figure>
<p>该修改参考了如下的两篇帖子:</p>
<ul>
<li><a href="https://serverfault.com/questions/466123/elastic-beanstalk-rails-modify-passenger-config-passenger-max-pool-size" target="_blank" rel="external">Elastic Beanstalk Rails - Modify passenger config passenger_max_pool_size</a></li>
<li><a href="https://forums.aws.amazon.com/thread.jspa?messageID=614495" target="_blank" rel="external">change passenger standalone configuration (max_pool_size) for rails app</a></li>
</ul>
<p>但这两篇帖子中，都把<code>--max-pool-size</code>放在了<code>GENERALOPTS</code>变量中，但这么设置是存在问题的。<br>因为脚本里的stop和status也会用到<code>GENERALOPTS</code>这个变量，但<code>passenger stop</code>和<code>passenger status</code>并不支持<code>--max-pool-size</code>这个参数，如果放在<code>GENERALOPTS</code>中，会导致<code>passenger stop</code>和<code>passenger status</code>运行失败。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">[root@ip-172-31-40-19 ec2-user]# passenger stop -h</div><div class="line">Usage: passenger stop [options]</div><div class="line"></div><div class="line">Options:</div><div class="line">    -p, --port NUMBER                The port number of a Phusion Passenger</div><div class="line">                                     Standalone instance (default: 3000)</div><div class="line">        --pid-file FILE              PID file of a running Phusion Passenger</div><div class="line">                                     Standalone instance.</div><div class="line">    -h, --help                       Show this help message</div><div class="line">[root@ip-172-31-40-19 ec2-user]#</div><div class="line">[root@ip-172-31-40-19 ec2-user]# passenger status -h</div><div class="line">Usage: passenger status [options]</div><div class="line"></div><div class="line">Options:</div><div class="line">    -p, --port NUMBER                The port number of a Phusion Passenger</div><div class="line">                                     Standalone instance (default: 3000)</div><div class="line">        --pid-file FILE              PID file of a running Phusion Passenger</div><div class="line">                                     Standalone instance.</div><div class="line">    -h, --help                       Show this help message</div><div class="line">[root@ip-172-31-40-19 ec2-user]#</div></pre></td></tr></table></figure>
<p>所以最终解决方案是额外添加了变量<code>SELFOPTS</code>用来存放参数<code>--max-pool-size</code>， 并且只将<code>SELFOPTS</code>添加到<code>passenger start</code>中。</p>
<p>使用<code>eb deploy</code>发布ebextension到EB中，在EB Console中添加环境变量<code>PASSENGER_MAX_POOL_SIZE</code>并设定所想要的值。</p>
<p><img src="/images/AWS/EB/passenger_max_pool_size_env_on_eb_console.jpg" alt="passenger_max_pool_size_env_on_eb_console.jpg"></p>
<p>登陆Instance后，即可看到<code>Max pool size</code>已经被设置为<code>PASSENGER_MAX_POOL_SIZE</code>相同的值了。</p>
<p><img src="/images/AWS/EB/ebextension_and_env_for_max_pool_size.jpg" alt="ebextension_and_env_for_max_pool_size.jpg"></p>
<h3 id="相关修改的snippet"><a href="#相关修改的snippet" class="headerlink" title="相关修改的snippet"></a>相关修改的snippet</h3><p>原始passenger文件以及ebextension配置文件可参照 <a href="https://github.com/jibing57/my-snippet/tree/master/AWS/ElasticBeanstalk/01_increase_passenger_max_pool_size" target="_blank" rel="external">https://github.com/jibing57/my-snippet/tree/master/AWS/ElasticBeanstalk/01_increase_passenger_max_pool_size</a></p>
<h3 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h3><p>要修改其他的Passenger启动参数，也可以使用类似的方法来实现。Passenger所支持的参数在官网中都有相关说明,<strong><a href="https://www.phusionpassenger.com/library/config/standalone/reference/#standalone-server-options" target="_blank" rel="external">Configuration reference for Passenger Standalone</a></strong>。 唯一要注意的是，EB中的Passenger版本是4.0.60的，版本要求大于4.0.60的选项在EB中是无效的。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://stackoverflow.com/questions/34774310/rails-elastic-beanstalk-passenger-change-passenger-configuration" target="_blank" rel="external">Rails + Elastic Beanstalk + Passenger: change Passenger configuration</a></li>
<li><a href="https://serverfault.com/questions/466123/elastic-beanstalk-rails-modify-passenger-config-passenger-max-pool-size" target="_blank" rel="external">Elastic Beanstalk Rails - Modify passenger config passenger_max_pool_size</a></li>
<li><a href="https://forums.aws.amazon.com/thread.jspa?messageID=614495" target="_blank" rel="external">change passenger standalone configuration (max_pool_size) for rails app</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Elastic Beanstalk </tag>
            
            <tag> Passenger </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ElasticBeanstalk伸缩时偶发404的原因]]></title>
      <url>/2018/12/24/404-on-Scale-Up-of-ElasticBeanstalk/</url>
      <content type="html"><![CDATA[<p>使用EB Passenger with Ruby部署rails时，如果是以默认配置启动，没有做对应的配置，在ASG Scale Up的时候，可能会有请求没法正常处理，返回404。</p>
<h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><ol>
<li>EB中创建的ELB，默认是使用TCP:80的方式来检查Instance的健康状态的。</li>
<li>ELB的<code>health check</code>的默认<code>Interval</code>是10 seconds，<code>Healthy threshold</code>是3 requests。</li>
<li>当ASG Scale Up时，新建的Instance中Passenger在Rails APP部署完成之前就已经在监听80端口了。</li>
<li>Passenger启动30秒之后，因为超过了<code>Healthy threshold * Interval</code>的时间, 并且每次检查都是OK的，ELB会判定Instance是可用的，就会将流量导入到该Instance中。</li>
<li>但此时rails APP可能并没有部署完成(bundle install加上precompile，部署一个新的APP可能需要5分钟左右)，所以client会得到404的错误。</li>
</ol>
<a id="more"></a>
<h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><ol>
<li>通过修改EB中ELB的<code>health check</code>的检查方式为HTTP方式，可以避免该问题。</li>
<li>在Rails中新增一个单纯返回200 OK的路由(可以是任意的路由,此处以<code>/health</code>为例)</li>
<li><p>在EB Console中，设置ELB中<code>Health check path</code>为<code>/health</code>。选择Configuration中的load balancer，点击<code>modify</code>,在<code>Health check</code>设置中设置<code>Health check path</code>的值为对应的链接。</p>
<p><img src="/images/AWS/EB/health_check_setting_on_EB.jpg" alt="health_check_setting_on_EB"></p>
</li>
<li><p>设置过后，当ASG启动新的Instance时，在APP部署完毕，<code>health check</code>链接生效之前, ELB的检查都无法成功，也就不会将流量导给新的Instance了。只有APP部署完成，<code>health check</code>通过后，才会将流量导给Instance。</p>
</li>
</ol>
<p><strong>注意点：</strong></p>
<ol>
<li>建议专门用一个只回应200 OK的链接用来响应<code>health Check</code>, 而不要使用带有业务逻辑的链接。</li>
<li>必须要先发布Rails APP添加/health，接下来才能修改ELB的Health check path。</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environments-cfg-clb.html#using-features.managing.elb.healthchecks" target="_blank" rel="external">EB Health Check</a></li>
<li><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.healthstatus.html#using-features.healthstatus.understanding" target="_blank" rel="external">Elastic Load Balancing Health Checks</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Elastic Beanstalk </tag>
            
            <tag> Elastic Load Balancing </tag>
            
            <tag> AutoScaling </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Node中heap out of memory的解决办法]]></title>
      <url>/2018/12/17/how-to-handle-heap-out-of-memory-in-node/</url>
      <content type="html"><![CDATA[<h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>公司一个ReactOnRails的项目，之前一直都是跑的好好的。上次一个小修改后，发布预编译React代码的时候，屡次出现<code>FATAL ERROR: CALL_AND_RETRY_LAST Allocation failed - JavaScript heap out of memory</code> 的错误。</p>
<h3 id="调查"><a href="#调查" class="headerlink" title="调查"></a>调查</h3><p>项目使用的是webpacker打包js代码，一开始是以为webpacker的问题，调查了一圈下来，最终发现是node中V8的限制。默认情况下，在64位系统下内存使用是有限制的，有一说是1.4GB，有一说是1.76GB。但在node官网搜了一圈也没找到明确说现在的限制是多少。</p>
<a id="more"></a>
<p>在预编译的报错信息中，找到了如下的错误信息，应该是超过了默认的使用限额，使得node进程爆掉了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;--- Last few GCs ---&gt;</div><div class="line"></div><div class="line">  155958 ms: Mark-sweep 1225.9 (1439.4) -&gt; 1225.9 (1439.4) MB, 1078.8 / 0.0 ms [allocation failure] [GC in old space requested].</div><div class="line">  157041 ms: Mark-sweep 1225.9 (1439.4) -&gt; 1225.9 (1439.4) MB, 1082.3 / 0.0 ms [allocation failure] [GC in old space requested].</div><div class="line">  158129 ms: Mark-sweep 1225.9 (1439.4) -&gt; 1235.9 (1423.4) MB, 1087.9 / 0.0 ms [last resort gc].</div><div class="line">  159213 ms: Mark-sweep 1235.9 (1423.4) -&gt; 1246.1 (1423.4) MB, 1083.6 / 0.0 ms [last resort gc].</div></pre></td></tr></table></figure></p>
<p>最终的解决方案是，在使用node之前，设置环境变量<code>NODE_OPTIONS=--max_old_space_size</code>为一个更大的值，此处，设置<code>NODE_OPTIONS=--max_old_space_size=4096</code>后，解决了问题。</p>
<p>注意的是，破折号格式的<code>NODE_OPTIONS=--max-old-space-size</code>似乎是不起作用的。</p>
<p>任何可以使用环境变量来修改配置的软件，都是部署友好的软件, 特别的适用于类似Elastic Beanstalk这种集成的发布环境。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://github.com/rails/webpacker/issues/1189" target="_blank" rel="external">How to run node with flag –max_old_space_size</a></li>
<li><a href="https://segmentfault.com/a/1190000004934938" target="_blank" rel="external">NodeJS中被忽略的内存</a></li>
<li><a href="https://github.com/nodejs/node/issues/16999" target="_blank" rel="external">v6.12.0: max-old-space-size can no longer be set via NODE_OPTIONS</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Code </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Nodejs </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[在S3上开启CORS]]></title>
      <url>/2018/12/11/how-to-enable-cors-on-S3/</url>
      <content type="html"><![CDATA[<p>在浏览器中使用AWS的SDK直接上传文件到S3时，需要在S3 Bucket上配置CORS才能成功上传，否则ajax请求会被浏览器拦截。</p>
<h3 id="普通CORS访问配置"><a href="#普通CORS访问配置" class="headerlink" title="普通CORS访问配置"></a>普通CORS访问配置</h3><p>官方文档<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html#how-do-i-enable-cors" target="_blank" rel="external">Cross-Origin Resource Sharing (CORS)</a>中提供了开启CORS的范例，摘录如下:</p>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">&lt;CORSConfiguration&gt;</div><div class="line"> &lt;CORSRule&gt;</div><div class="line">   &lt;AllowedOrigin&gt;http://www.example1.com&lt;/AllowedOrigin&gt;</div><div class="line"></div><div class="line">   &lt;AllowedMethod&gt;PUT&lt;/AllowedMethod&gt;</div><div class="line">   &lt;AllowedMethod&gt;POST&lt;/AllowedMethod&gt;</div><div class="line">   &lt;AllowedMethod&gt;DELETE&lt;/AllowedMethod&gt;</div><div class="line"></div><div class="line">   &lt;AllowedHeader&gt;*&lt;/AllowedHeader&gt;</div><div class="line"> &lt;/CORSRule&gt;</div><div class="line"> &lt;CORSRule&gt;</div><div class="line">   &lt;AllowedOrigin&gt;http://www.example2.com&lt;/AllowedOrigin&gt;</div><div class="line"></div><div class="line">   &lt;AllowedMethod&gt;PUT&lt;/AllowedMethod&gt;</div><div class="line">   &lt;AllowedMethod&gt;POST&lt;/AllowedMethod&gt;</div><div class="line">   &lt;AllowedMethod&gt;DELETE&lt;/AllowedMethod&gt;</div><div class="line"></div><div class="line">   &lt;AllowedHeader&gt;*&lt;/AllowedHeader&gt;</div><div class="line"> &lt;/CORSRule&gt;</div><div class="line"> &lt;CORSRule&gt;</div><div class="line">   &lt;AllowedOrigin&gt;*&lt;/AllowedOrigin&gt;</div><div class="line">   &lt;AllowedMethod&gt;GET&lt;/AllowedMethod&gt;</div><div class="line"> &lt;/CORSRule&gt;</div><div class="line">&lt;/CORSConfiguration&gt;</div></pre></td></tr></table></figure>
<h3 id="支持Multipart-Upload的配置"><a href="#支持Multipart-Upload的配置" class="headerlink" title="支持Multipart Upload的配置"></a>支持Multipart Upload的配置</h3><p>当上传的文件比较大的时候，AWS的javascript的SDK会使用Multipart upload的方式来上传, 而Multipart upload的机制中是需要用到Header中的Etag的，因此需要在S3的CORS的rule中配置允许暴露ETag, 也即需要添加<code>&lt;ExposeHeader&gt;ETag&lt;/ExposeHeader&gt;</code></p>
<p>示例Rule:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</div><div class="line">&lt;CORSConfiguration xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;</div><div class="line">&lt;CORSRule&gt;</div><div class="line">    &lt;AllowedOrigin&gt;http://localhost:3000&lt;/AllowedOrigin&gt;</div><div class="line">    &lt;AllowedMethod&gt;PUT&lt;/AllowedMethod&gt;</div><div class="line">    &lt;AllowedMethod&gt;POST&lt;/AllowedMethod&gt;</div><div class="line">    &lt;ExposeHeader&gt;ETag&lt;/ExposeHeader&gt;</div><div class="line">    &lt;AllowedHeader&gt;*&lt;/AllowedHeader&gt;</div><div class="line">&lt;/CORSRule&gt;</div><div class="line">&lt;/CORSConfiguration&gt;</div></pre></td></tr></table></figure>
<p>S3 Multipart Upload的原理在官方博客<a href="https://aws.amazon.com/blogs/aws/amazon-s3-multipart-upload/" target="_blank" rel="external">Amazon S3: Multipart Upload</a>中有相关的说明。</p>
<p>如下是一个Parts Uploaded的PUT请求的response的示例。PUT成功上传分片到S3后，S3返回的本次请求的Etag为<code>0a2f92d61cdc4682ba52adb9e077991f</code><br><img src="/images/AWS/S3/Etag_in_response_of_PUT.jpg" alt="Etag_in_response_of_PUT"></p>
<p>分片全部上传完毕后，SDK再调用POST请求，将之前分片的ETag和PartNumber组成完成的S3文件。<br>如下是最后CompleteMultipartUpload的POST请求的Payload, 可以看到<code>0a2f92d61cdc4682ba52adb9e077991f</code>作为请求的payload的一部分被发送至S3。S3会根据payload将对应的partial block组成完整的S3文件。<br><img src="/images/AWS/S3/Etag_on_complete_post.jpg" alt="Etag_on_complete_post"></p>
<p>如果不添加<code>&lt;ExposeHeader&gt;ETag&lt;/ExposeHeader&gt;</code>的话，Multipart upload的时候，AWS Javascript SDK会报<code>Error: No access to ETag property on response. Check CORS configuration to expose ETag header.</code>的错误。<br><img src="/images/AWS/S3/Error_No_access_to_ETag_property_on_response.jpg" alt="Error_No_access_to_ETag_property_on_response.jpg"></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://aws.amazon.com/blogs/aws/amazon-s3-multipart-upload/" target="_blank" rel="external">Amazon S3: Multipart Upload</a></li>
<li><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html#how-do-i-enable-cors" target="_blank" rel="external">Cross-Origin Resource Sharing (CORS)</a></li>
<li><a href="https://stackoverflow.com/questions/28568794/amazon-s3-javascript-no-access-control-allow-origin-header-is-present-on-the" target="_blank" rel="external">Amazon s3 Javascript- No ‘Access-Control-Allow-Origin’ header is present on the requested resource</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> S3 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何配置Nginx的Dynamic Upstream指向ELB]]></title>
      <url>/2018/11/27/nginx-with-dynamic-upstreams-to-ELB/</url>
      <content type="html"><![CDATA[<h3 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h3><p>公司有一个项目，部署有多个AWS环境，但由于一系列复杂的原因，需要先在账号A下备案的域名来使用在账号B中部署的服务，后续再将域名备案转到账号B中。<br>临时的解决办法就是在账号A下起一个Instance，通过Nginx反向代理到账号B的一个Classic ELB中。</p>
<p>起初服务跑的很顺畅，后来一天突然服务不可用，页面显示504 Gateway Timeout，查看了Nginx的日志，发现有多条<code>upstream timed out (110: Connection timed out) while connecting to upstream</code>的错误日志。</p>
<a id="more"></a>
<h3 id="调查"><a href="#调查" class="headerlink" title="调查"></a>调查</h3><p>AWS 的ELB是一个托管的负载均衡器，通过对外提供的域名来进行访问，北京Region的域名类似<code>service-3332222.cn-north-1.elb.amazonaws.com.cn</code>,但实际上底层还是会有对应的ENI来承载实际的流量，这些ENI有自己临时分配的IP地址。这些ENI可能会随着流量的扩展或AWS底层服务器的变动而随时改变。所以这也是AWS要求使用域名来访问ELB的原因。</p>
<p>Nginx侧使用upstream时，里面如果填写的是域名，那么Nginx会在请求DNS后把对应的IP信息缓存起来，后续的请求就一直用缓存的IP。直到下次reload的时候才会再次查询domain</p>
<p>此时问题就出现了，ELB的域名解析是时刻会更新的，当底层有变动时，承载ELB流量的Public IP就会变化。而Nginx的upstream中，如果使用了域名，第一次DNS请求后就会一直使用缓存的Public IP。当ELB的ENI有变化时，原先的Nginx反向代理缓存的IP地址就失效了，导致连接不上服务。出现<code>upstream timed out (110: Connection timed out) while connecting to upstream</code>的错误。</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>有这么几个方法可以用来解决这个问题:</p>
<ol>
<li>写一个监控的cron脚本，时刻检测ELB对应的IP是否有变化，一旦有变化，就reload Nginx。这个方法需要额外的开发。</li>
<li><a href="https://www.nginx.com/products/nginx/" target="_blank" rel="external">Nginx Plus</a>支持在upstream中添加<a href="https://www.nginx.com/products/nginx/load-balancing/#service-discovery" target="_blank" rel="external">resolve</a>，可以周期性的重新解析DNS域名。可以完美解决所遇到的问题，可一个Nginx Plus Instance的授权费用就要<a href="https://www.nginx.com/products/buy-nginx-plus/" target="_blank" rel="external">$2500+/year</a>，穷人表示用不起。</li>
<li>Nginx社区版的解决方案，不使用upstream，使用set将域名设为一个变量，再传递给proxy_pass。</li>
</ol>
<p>Nginx官方这篇<a href="https://www.nginx.com/blog/dns-service-discovery-nginx-plus/#domain-name-upstream-group" target="_blank" rel="external">Using DNS for Service Discovery with NGINX and NGINX Plus</a>的文章讲述了第二点和第三点的解决方案。</p>
<p>Nginx的免费解决方案的配置如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">server &#123;</div><div class="line">  listen        80;</div><div class="line">  server_name   example.com;</div><div class="line"></div><div class="line">  resolver 172.31.0.2;</div><div class="line">  set $upstream_endpoint http://service-3332222.cn-north-1.elb.amazonaws.com.cn;</div><div class="line"></div><div class="line">  location / &#123;</div><div class="line">    proxy_pass $upstream_endpoint$request_uri;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 proxy_pass中使用变量来代替URI或者Upstream Server Group，Nginx就会在解析缓存过期后再次请求DNS服务器来解析域名。</p>
<p><strong>注意: 此处的resolver所要填写的IP地址，取决于EC2所在的子网划分。如果网络是VPC，那需要设置为EC2所在子网的DNS地址，AWS会为每个子网保留5个IP地址，第三个IP地址用作DNS，例如如果子网CIDR为10.0.0.0/24，那10.0.0.2就是AWS保留的用作DNS的地址, 保留IP地址的官网说明<a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html" target="_blank" rel="external">VPCs and Subnets</a>。如果是Classic的网络,那么AWS的DNS服务器地址是固定的172.16.0.23，AWS官网说明<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-classic-platform.html#ip-addressing-differences" target="_blank" rel="external">EC2-Classic Platform</a>。</strong></p>
<h4 id="关于免费方案的缺陷"><a href="#关于免费方案的缺陷" class="headerlink" title="关于免费方案的缺陷"></a>关于免费方案的缺陷</h4><p>当location的参数不是‘/’，并且proxy_pass后面的参数是一个变量时，proxy_pass的转发规则和正常的规则有所不同。如下是相关的说明以及解决方案。<br>以下文字转自<a href="https://tenzer.dk/nginx-with-dynamic-upstreams/" target="_blank" rel="external">Nginx with dynamic upstreams</a></p>
<p><code>proxy_pass</code>不使用变量时的正常转发规则:<br>nginx配置为如下时:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">location /foo/ &#123;</div><div class="line">    proxy_pass http://127.0.0.1:8080;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>如果请求的网页是<code>/foo/bar/baz</code>,那么Nginx会转发请求至<code>http://127.0.0.1:8080/foo/bar/baz</code>。但如果nginx的配置如下，在proxy_pass参数后面加上了‘/’时。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">location /foo/ &#123;</div><div class="line">    # Note the trailing slash       ↓</div><div class="line">    proxy_pass http://127.0.0.1:8080/;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>Nginx在将请求转发到upstream时，会将<code>location</code>中匹配的部分截掉，<code>/foo/bar/baz</code>会转发到<code>http://127.0.0.1:8080/bar/baz</code>。</p>
<p>但当我们在proxy_pass中使用变量后，转发行为会发生变化。<br>当我们在转发的地址后面有‘/’时:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">resolver 172.31.0.2;</div><div class="line">set $upstream_endpoint http://service-1234567890.us-east-1.elb.amazonaws.com/;</div><div class="line">location /foo/ &#123;</div><div class="line">    proxy_pass $upstream_endpoint;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>当你请求<code>/foo/bar/baz</code>时，请求会转发到‘/’而不是期望的<code>/bar/baz</code></p>
<p>解决办法:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">resolver 172.31.0.2;</div><div class="line">set $upstream_endpoint http://service-1234567890.us-east-1.elb.amazonaws.com;</div><div class="line">location /foo/ &#123;</div><div class="line">    rewrite ^/foo/(.*) /$1 break;</div><div class="line">    proxy_pass $upstream_endpoint;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>删掉<code>set $upstream_endpoint</code>后面的‘/’，手动rewrite地址，这样就可以将请求<code>/foo/bar/baz</code>转发到upstream的<code>/bar/baz</code>了。</p>
<h3 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h3><p>来做几个实验来验证下上面的配置是否可以生效。</p>
<h4 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h4><p>来做几个实验来验证下效果，新建几台Instance，配置好对应的SG。如下是几台实验机器的基本信息:</p>
<table>
<thead>
<tr>
<th>Instance</th>
<th>Public IP</th>
<th>Private IP</th>
<th>SG Rule</th>
</tr>
</thead>
<tbody>
<tr>
<td>EC2-A</td>
<td>18.237.0.231</td>
<td>172.31.39.103</td>
<td>SSH&amp;HTTP From 0.0.0.0/0</td>
</tr>
<tr>
<td>EC2-B</td>
<td>52.26.25.237</td>
<td>172.31.35.141</td>
<td>SSH&amp;HTTP from 0.0.0.0/0</td>
</tr>
<tr>
<td>EC2-Reverse</td>
<td>52.36.100.113</td>
<td>172.31.18.5</td>
<td>SSH&amp;HTTP from 0.0.0.0/0</td>
</tr>
</tbody>
</table>
<p>一个域名: dynamic-upstream.jibing57.com</p>
<h4 id="环境设置"><a href="#环境设置" class="headerlink" title="环境设置"></a>环境设置</h4><p>分别在EC2-A和EC2-B上安装简单的httpd服务</p>
<p>登陆EC2-A中执行命令安装httpd服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"># 切换为root用户</div><div class="line">sudo bash</div><div class="line"></div><div class="line"># 安装httpd</div><div class="line">yum install httpd -y</div><div class="line"></div><div class="line"># 建立一个最简单的page</div><div class="line"># EC2-A中建立Hello, I&apos;m EC2-A的首页</div><div class="line">echo &quot;&lt;h1&gt; Hello, I&apos;m EC2-A &lt;/h1&gt;&quot; &gt; /var/www/html/index.html</div><div class="line"></div><div class="line"># 建立多级目录foo/bar/baz/并建立欢迎页</div><div class="line">mkdir -p /var/www/html/foo/bar/baz/</div><div class="line">echo &quot;&lt;h1&gt; Hello, I&apos;m EC2-A in dir foo/bar/baz/ &lt;/h1&gt;&quot; &gt; /var/www/html/foo/bar/baz/index.html</div><div class="line"></div><div class="line"># 建立多级目录bar/baz/并建立欢迎页</div><div class="line">mkdir -p /var/www/html/bar/baz/</div><div class="line">&quot;&lt;h1&gt; Hello, I&apos;m EC2-A in dir bar/baz/ &lt;/h1&gt;&quot; &gt; /var/www/html/bar/baz/index.html</div><div class="line"></div><div class="line"># 启动http服务</div><div class="line">systemctl start httpd.service</div></pre></td></tr></table></figure>
<p>登陆EC2-B中执行命令安装httpd服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"># 切换为root用户</div><div class="line">sudo bash</div><div class="line"></div><div class="line"># 安装httpd</div><div class="line">yum install httpd -y</div><div class="line"></div><div class="line"># EC2-B中建立Hello, I&apos;m EC2-B的首页</div><div class="line">echo &quot;&lt;h1&gt; Hello, I&apos;m EC2-B &lt;/h1&gt;&quot; &gt; /var/www/html/index.html</div><div class="line"></div><div class="line"># 建立多级目录foo/bar/baz/并建立欢迎页</div><div class="line">mkdir -p /var/www/html/foo/bar/baz/</div><div class="line">echo &quot;&lt;h1&gt; Hello, I&apos;m EC2-B in dir foo/bar/baz/ &lt;/h1&gt;&quot; &gt; /var/www/html/foo/bar/baz/index.html</div><div class="line"></div><div class="line"># 建立多级目录bar/baz/并建立欢迎页</div><div class="line">mkdir -p /var/www/html/bar/baz/</div><div class="line">echo &quot;&lt;h1&gt; Hello, I&apos;m EC2-B in dir bar/baz/ &lt;/h1&gt;&quot; &gt; /var/www/html/bar/baz/index.html</div><div class="line"></div><div class="line"># 启动http服务</div><div class="line">systemctl start httpd.service</div></pre></td></tr></table></figure>
<p>配置好后，访问如下页面，确认网页能够正常展示</p>
<ul>
<li><a href="http://18.237.0.231/" target="_blank" rel="external">http://18.237.0.231/</a></li>
<li><a href="http://18.237.0.231/foo/bar/baz/" target="_blank" rel="external">http://18.237.0.231/foo/bar/baz/</a></li>
<li><a href="http://52.26.25.237/" target="_blank" rel="external">http://52.26.25.237/</a></li>
<li><a href="http://52.26.25.237/foo/bar/baz/" target="_blank" rel="external">http://52.26.25.237/foo/bar/baz/</a></li>
</ul>
<p>配置EC2-Reverse的nginx，将请求重定向到dynamic-upstream.jibing57.com去。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">server &#123;</div><div class="line">    listen 80;</div><div class="line">    server_name 52.36.100.113;</div><div class="line">    access_log  /var/log/nginx/access.log  main;</div><div class="line">    error_log  /var/log/nginx/error.log  notice;</div><div class="line"></div><div class="line">    resolver 172.31.0.2;</div><div class="line">    set $upstream_endpoint http://dynamic-upstream.jibing57.com;</div><div class="line">    location / &#123;</div><div class="line">        proxy_pass $upstream_endpoint;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="进行试验"><a href="#进行试验" class="headerlink" title="进行试验"></a>进行试验</h4><p>基本设置设置完后，让我们来进行试验</p>
<ol>
<li><p>首先将dynamic-upstream.jibing57.com解析到EC2-A的IP地址18.237.0.231<br><img src="/images/AWS/ELB/aliyun_parser_to_EC2-A.jpg" alt="aliyun_parser_to_EC2-A"><br>此时访问<a href="http://52.36.100.113" target="_blank" rel="external">http://52.36.100.113</a>, 可以看到的是EC2-A Instance上的主页内容 Hello, I’m EC2-A<br><img src="/images/AWS/ELB/upstream_to_EC2-A.jpg" alt="upstream_to_EC2-A"></p>
</li>
<li><p>再将dynamic-upstream.jibing57.com解析修改为EC2-B的IP地址52.26.25.237<br><img src="/images/AWS/ELB/aliyun_parser_to_EC2-B.jpg" alt="aliyun_parser_to_EC2-B"><br>当修改后的域名解析扩散到AWS的DNS服务器后，此时访问<a href="http://52.36.100.113" target="_blank" rel="external">http://52.36.100.113</a>，可以看到的是EC2-B Instance上的主页内容Hello, I’m EC2-B<br><img src="/images/AWS/ELB/upstream_to_EC2-B.jpg" alt="upstream_to_EC2-B"></p>
</li>
</ol>
<p>说明设置的upstream起作用了，我们并没有重新启动load Nginx，就可以将请求转发到EC2-B中。</p>
<p>再来修改EC2-Reverse的nginx，测试location /foo/的情形</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">server &#123;</div><div class="line">    listen 80;</div><div class="line">    server_name 52.36.100.113;</div><div class="line">    access_log  /var/log/nginx/access.log  main;</div><div class="line">    error_log  /var/log/nginx/error.log  notice;</div><div class="line"></div><div class="line">    resolver 172.31.0.2;</div><div class="line">    set $upstream_endpoint http://dynamic-upstream.jibing57.com/;</div><div class="line">    location /foo/ &#123;</div><div class="line">        proxy_pass $upstream_endpoint;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>注意此时dynamic-upstream.jibing57.com域名后面带有‘/’。<br>此时访问<a href="http://52.36.100.113/foo/bar/baz/" target="_blank" rel="external">http://52.36.100.113/foo/bar/baz/</a>，页面内容是EC2-B中/index.html的内容<code>Hello, I&#39;m EC2-B</code>。<br>需改Nginx配置为rewrite。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">server &#123;</div><div class="line">    listen 80;</div><div class="line">    server_name 52.36.100.113;</div><div class="line">    access_log  /var/log/nginx/access.log  main;</div><div class="line">    error_log  /var/log/nginx/error.log  notice;</div><div class="line"></div><div class="line">    resolver 172.31.0.2;</div><div class="line">    set $upstream_endpoint http://dynamic-upstream.jibing57.com;</div><div class="line">    location /foo/ &#123;</div><div class="line">        rewrite ^/foo/(.*) /$1 break;</div><div class="line">        proxy_pass $upstream_endpoint;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>此时访问<a href="http://52.36.100.113/foo/bar/baz/，" target="_blank" rel="external">http://52.36.100.113/foo/bar/baz/，</a> 页面内容为EC2-B中/bar/baz/index.html的内容<code>Hello, I&#39;m EC2-B in dir bar/baz/</code></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://tenzer.dk/nginx-with-dynamic-upstreams/" target="_blank" rel="external">Nginx with dynamic upstreams</a></li>
<li><a href="https://distinctplace.com/2017/04/19/nginx-resolver-explained/" target="_blank" rel="external">Nginx resolver explained</a></li>
<li><a href="https://serverfault.com/questions/240476/how-to-force-nginx-to-resolve-dns-of-a-dynamic-hostname-everytime-when-doing-p" target="_blank" rel="external">How to force nginx to resolve DNS (of a dynamic hostname) everytime when doing proxy_pass?</a></li>
<li><a href="https://www.nginx.com/blog/dns-service-discovery-nginx-plus/#domain-name-upstream-group" target="_blank" rel="external">Using DNS for Service Discovery with NGINX and NGINX Plus</a></li>
<li><a href="https://stackoverflow.com/questions/26956979/error-with-ip-and-nginx-as-reverse-proxy" target="_blank" rel="external">Error with IP and Nginx as reverse proxy</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Elastic Load Balancing </tag>
            
            <tag> Nginx </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[在Amazon Linux和Elastic Beanstalk中安装ffmpeg的方法]]></title>
      <url>/2018/11/15/a-way-to-install-ffmpeg-to-Amazon-Linux-and-ElasticBeanstalk/</url>
      <content type="html"><![CDATA[<p>因为项目需要，需要在EC2和EB中安装FFMPEG来处理视频。 </p>
<p>但Amazon Linux默认的yum源中没有FFMPEG，因此需要手动进行安装。</p>
<p>网上有各种的安装方法，有使用Nux Dextop YUM的安装，也有直接使用Static Builds的方式。经过比较试验，最终决定使用Static Builds的方式进行安装，安装方法记录如下。 </p>
<a id="more"></a>
<h3 id="在Amazon-Linux中安装FFMPEG"><a href="#在Amazon-Linux中安装FFMPEG" class="headerlink" title="在Amazon Linux中安装FFMPEG"></a>在Amazon Linux中安装FFMPEG</h3><p>FFMPEG官网 <a href="https://www.ffmpeg.org/download.html" target="_blank" rel="external">https://www.ffmpeg.org/download.html</a> 中，推荐的Linux Static Builds的下载地址为: <a href="https://johnvansickle.com/ffmpeg/" target="_blank" rel="external">https://johnvansickle.com/ffmpeg/</a>。 </p>
<p>64位的Release版的下载地址为: <a href="https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-64bit-static.tar.xz" target="_blank" rel="external">https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-64bit-static.tar.xz</a>, 注意网上很多教程和gist中所提的这个下载地址 <a href="http://ffmpeg.gusari.org/static/64bit/ffmpeg.static.64bit.latest.tar.gz" target="_blank" rel="external">http://ffmpeg.gusari.org/static/64bit/ffmpeg.static.64bit.latest.tar.gz</a> 已经不再维护了。</p>
<p>登陆Amazon Linux，下载压缩包，解压后直接拷贝到指定的bin目录即可使用。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 下载</div><div class="line">wget https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-64bit-static.tar.xz</div><div class="line"></div><div class="line"># 解压</div><div class="line">tar xJfv ffmpeg-release-64bit-static.tar.xz</div><div class="line"></div><div class="line"># 拷贝到指定bin目录(此处以latest版本是4.1，目标bin目录为/usr/local/bin/为例)</div><div class="line">cd ffmpeg-4.1-64bit-static/</div><div class="line">sudo cp ffmpeg ffprobe /usr/local/bin/</div><div class="line"></div><div class="line"># 测试命令是否可用</div><div class="line">ffmpeg</div><div class="line">ffprobe</div></pre></td></tr></table></figure>
<p>实际操作中有需要的话，可以再将<code>ffmpeg</code>和<code>ffprobe</code>链到/usr/bin中。</p>
<h3 id="在Elastic-BeanStalk中安装FFMPEG"><a href="#在Elastic-BeanStalk中安装FFMPEG" class="headerlink" title="在Elastic BeanStalk中安装FFMPEG"></a>在Elastic BeanStalk中安装FFMPEG</h3><p>在没有科学上网的情况下，国内访问<a href="https://johnvansickle.com" target="_blank" rel="external">https://johnvansickle.com</a>这个网站很不稳定，在Beijing Region的EC2中，下载速度有时只有几KB。<br>为了能够在Elastic BeanStalk中稳定安装FFMPEG，采取的方案是自己将二进制文件重新打包，先上传到国内的S3中，再从S3下载二进制文件安装到EB的Instance中。<br>除了稳定外，这样还有一个好处，就是使用的Release的版本自己可控。即使johnvansickle.com后续不再维护FFMPEG了或者有最新的Release了，也不会对我们的使用产生影响。</p>
<p>具体步骤如下:</p>
<p>在本地机器或某个EC2中打包二进制文件上传S3, 此处以最新版的4.1为例。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"># 下载</div><div class="line">wget https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-64bit-static.tar.xz</div><div class="line"></div><div class="line"># 解压</div><div class="line">tar xJfv ffmpeg-release-64bit-static.tar.xz</div><div class="line"></div><div class="line"># 制作压缩包</div><div class="line">mkdir ffmpeg-4.1-64bit</div><div class="line">cp ffmpeg-4.1-64bit-static/ffmpeg ffmpeg-4.1-64bit-static/ffprobe ffmpeg-4.1-64bit</div><div class="line">cd ffmpeg-4.1-64bit</div><div class="line">tar czvf ffmpeg-4.1-64bit.tar.gz ffmpeg ffprobe</div><div class="line"></div><div class="line"># 上传压缩包到S3 (此处bucket和路径需要换成实际使用的)</div><div class="line">aws s3 cp ffmpeg-4.1-64bit.tar.gz s3://test-for-ffmpeg-bucket/ffmpeg/ --acl public-read</div></pre></td></tr></table></figure>
<p>配置EB从S3下载压缩包并解压到指定bin目录,此处可以直接使用EB的<a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html#linux-sources" target="_blank" rel="external">sources</a>语法, 比自己用shell写一堆的命令来的更简洁。sources语法目前支持 tar, tar+gzip, tar+bz2, 和zip这四种格式。</p>
<p>在.ebextensions目录下添加配置文件20-install-ffmpeg.config, 内容如下。<code>/usr/local/bin</code>可以修改为实际需要安装的路径。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sources:</div><div class="line">    /usr/local/bin: https://s3.cn-north-1.amazonaws.com.cn/test-for-ffmpeg-bucket/ffmpeg/ffmpeg-4.1-64bit.tar.gz</div></pre></td></tr></table></figure>
<p>使用<code>eb deploy</code>发布，发布后登陆EC2查看是否生效。</p>
<h3 id="yum源方式安装失败"><a href="#yum源方式安装失败" class="headerlink" title="yum源方式安装失败"></a>yum源方式安装失败</h3><p>网上也有文章提及了可以添加Nux Dextop YUM源的方式来进行安装, 这种方式可能适用于CentOS,但测试下来，并不适用于Amazon Linux，安装时会报错。<br>应该还需要安装其他的lib源才可以顺利安装。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">错误：软件包：libavdevice-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libSDL-1.2.so.0()(64bit)</div><div class="line">错误：软件包：ffmpeg-libs-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libgnutls.so.28(GNUTLS_3_0_0)(64bit)</div><div class="line">错误：软件包：ffmpeg-libs-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libtheoraenc.so.1()(64bit)</div><div class="line">错误：软件包：ffmpeg-libs-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libass.so.5()(64bit)</div><div class="line">错误：软件包：libavdevice-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libv4l2.so.0()(64bit)</div><div class="line">错误：软件包：ffmpeg-libs-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libva.so.1()(64bit)</div><div class="line">错误：软件包：ffmpeg-libs-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libgsm.so.1()(64bit)</div><div class="line">错误：软件包：libavdevice-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libcdio_cdda.so.1()(64bit)</div><div class="line">错误：软件包：libavdevice-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libopenal.so.1()(64bit)</div><div class="line">错误：软件包：ffmpeg-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libSDL-1.2.so.0()(64bit)</div><div class="line">错误：软件包：ffmpeg-libs-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libtheoradec.so.1(libtheoradec_1.0)(64bit)</div><div class="line">错误：软件包：libavdevice-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libcdio_paranoia.so.1()(64bit)</div><div class="line">错误：软件包：ffmpeg-libs-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libgnutls.so.28(GNUTLS_1_4)(64bit)</div><div class="line">错误：软件包：libavdevice-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libpulse.so.0()(64bit)</div><div class="line">错误：软件包：ffmpeg-libs-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libschroedinger-1.0.so.0()(64bit)</div><div class="line">错误：软件包：ffmpeg-libs-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libopenjpeg.so.1()(64bit)</div><div class="line">错误：软件包：libavdevice-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libcdio_cdda.so.1(CDIO_CDDA_1)(64bit)</div><div class="line">错误：软件包：ffmpeg-libs-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libtheoraenc.so.1(libtheoraenc_1.0)(64bit)</div><div class="line">错误：软件包：libdc1394-2.2.0-4.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libraw1394.so.11()(64bit)</div><div class="line">错误：软件包：ffmpeg-libs-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libgnutls.so.28()(64bit)</div><div class="line">错误：软件包：ffmpeg-libs-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libtheoradec.so.1()(64bit)</div><div class="line">错误：软件包：libavdevice-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libpulse.so.0(PULSE_0)(64bit)</div><div class="line">错误：软件包：ffmpeg-libs-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libopus.so.0()(64bit)</div><div class="line">错误：软件包：libavdevice-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libcdio_paranoia.so.1(CDIO_PARANOIA_1)(64bit)</div><div class="line">错误：软件包：ffmpeg-2.8.15-1.el7.nux.x86_64 (nux-dextop)</div><div class="line">          需要：libvdpau.so.1()(64bit)</div><div class="line"> 您可以尝试添加 --skip-broken 选项来解决该问题</div><div class="line"> 您可以尝试执行：rpm -Va --nofiles --nodigest</div></pre></td></tr></table></figure>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://stackoverflow.com/questions/23340771/install-ffmpeg-on-elastic-beanstalk-using-ebextensions-config" target="_blank" rel="external">Install ffmpeg on elastic beanstalk using ebextensions config</a></li>
<li><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html" target="_blank" rel="external">Customizing Software on Linux Servers</a></li>
<li><a href="https://www.ffmpeg.org/" target="_blank" rel="external">FFmpeg</a> </li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Elastic Beanstalk </tag>
            
            <tag> EC2 </tag>
            
            <tag> FFMPEG </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用ssh隧道访问内网资源]]></title>
      <url>/2018/09/15/how-to-access-internal-by-ssh-tunnel/</url>
      <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>日常开发过程中，会遇到一些情况需要和对端机构或公司进行联调。此时有可能会需要访问对端机构内部的一些网页或API。</p>
<p>正规流程中，要访问内网，如果对端机构有VPN服务，应该是申请一个VPN来进行操作，这种方式比较安全可靠。但实际操作过程中，可能对端机构没有VPN服务，或者走VPN流程繁琐。此时如果你有对端机构中某个内网Linux机器的ssh登陆权限,而且该Linux机器可以可以访问相关网页或者API，则可以通过ssh tunnel的方式，临时来访问对端机构的内部资源。</p>
<p>接下来，通过使用两台EC2来模拟内网环境,来介绍如何在Mac机器上使用ssh tunnel访问内网资源。</p>
<h3 id="搭建说明"><a href="#搭建说明" class="headerlink" title="搭建说明"></a>搭建说明</h3><h4 id="搭建模拟用的EC2"><a href="#搭建模拟用的EC2" class="headerlink" title="搭建模拟用的EC2"></a>搭建模拟用的EC2</h4><p>首先，开启两台EC2，一台用来模拟ssh的跳板机(Jumpbox)，一台用来模拟内部网页服务器(HttpServer)。</p>
<a id="more"></a>
<p>基本信息如下:</p>
<table>
<thead>
<tr>
<th>Instance</th>
<th>Public IP</th>
<th>Private IP</th>
<th>SG Name</th>
<th>SG Rule</th>
</tr>
</thead>
<tbody>
<tr>
<td>Jumpbox</td>
<td>34.220.122.63</td>
<td>172.31.17.27</td>
<td>jumpbox-sg</td>
<td>SSH From 0.0.0.0/0</td>
</tr>
<tr>
<td>HttpServer</td>
<td>52.33.58.113</td>
<td>172.31.21.213</td>
<td>HttpServer</td>
<td>SSH from 0.0.0.0/0, HTTP from jumpbox-sg</td>
</tr>
</tbody>
</table>
<p>其中Jumpbox的SecurityGroup只允许ssh登陆，HttpServer开放ssh，并开放HTTP端口为只允许Jumpbox的SG访问。</p>
<p>登陆HttpServer，设置一个简易的httpd服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 安装httpd</div><div class="line">sudo yum install httpd -y</div><div class="line"></div><div class="line"># 建立一个最简单的page</div><div class="line">sudo echo &quot;&lt;h1&gt; Page From `curl http://169.254.169.254/latest/meta-data/local-ipv4` &lt;/h1&gt;&quot; &gt; /var/www/html/index.html</div><div class="line"></div><div class="line"># 启动http服务</div><div class="line">sudo systemctl start httpd.service</div></pre></td></tr></table></figure>
<p>在Jumpbox中测试使用curl，可以成功获取到index.html的内容，为<code>&lt;h1&gt; Page From 172.31.21.213 &lt;/h1&gt;</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">[ec2-user@ip-172-31-17-27 ~]$ curl -vL http://172.31.21.213</div><div class="line">* Rebuilt URL to: http://172.31.21.213/</div><div class="line">*   Trying 172.31.21.213...</div><div class="line">* TCP_NODELAY set</div><div class="line">* Connected to 172.31.21.213 (172.31.21.213) port 80 (#0)</div><div class="line">&gt; GET / HTTP/1.1</div><div class="line">&gt; Host: 172.31.21.213</div><div class="line">&gt; User-Agent: curl/7.55.1</div><div class="line">&gt; Accept: */*</div><div class="line">&gt;</div><div class="line">&lt; HTTP/1.1 200 OK</div><div class="line">&lt; Date: Sat, 15 Sep 2018 06:39:46 GMT</div><div class="line">&lt; Server: Apache/2.4.34 ()</div><div class="line">&lt; Upgrade: h2,h2c</div><div class="line">&lt; Connection: Upgrade</div><div class="line">&lt; Last-Modified: Sat, 15 Sep 2018 06:27:16 GMT</div><div class="line">&lt; ETag: &quot;23-575e3081098b7&quot;</div><div class="line">&lt; Accept-Ranges: bytes</div><div class="line">&lt; Content-Length: 35</div><div class="line">&lt; Content-Type: text/html; charset=UTF-8</div><div class="line">&lt;</div><div class="line">&lt;h1&gt; Page From 172.31.21.213 &lt;/h1&gt;</div><div class="line">* Connection #0 to host 172.31.21.213 left intact</div><div class="line">[ec2-user@ip-172-31-17-27 ~]$</div></pre></td></tr></table></figure>
<h4 id="本地设置"><a href="#本地设置" class="headerlink" title="本地设置"></a>本地设置</h4><p>在本地电脑上，先使用<code>ssh -D</code>建立一个本地到Jumpbox的ssh tunnel，然后设置浏览器使用该链接来访问对端的内网服务。</p>
<h5 id="搭建ssh-tunnel"><a href="#搭建ssh-tunnel" class="headerlink" title="搭建ssh tunnel"></a>搭建ssh tunnel</h5><p>打开本地Terminal，在命令行中创建ssh tunnel。ssh有三种方式创建tunnel，分别对应了参数<code>-D</code>,<code>-L</code>和<code>-R</code>。详细用法可参照<a href="http://www.ruanyifeng.com/blog/2011/12/ssh_port_forwarding.html" target="_blank" rel="external">SSH原理与运用（二）：远程操作与端口转发</a>, 此处使用<code>-D</code>即可。</p>
<p>ssh <code>-D</code>参数的man说明如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">-D [bind_address:]port</div><div class="line">        Specifies a local ``dynamic&apos;&apos; application-level port forwarding.  This works by allocating a socket to listen to port on the local side, optionally bound to the speci-</div><div class="line">        fied bind_address.  Whenever a connection is made to this port, the connection is forwarded over the secure channel, and the application protocol is then used to deter-</div><div class="line">        mine where to connect to from the remote machine.  Currently the SOCKS4 and SOCKS5 protocols are supported, and ssh will act as a SOCKS server.  Only root can forward</div><div class="line">        privileged ports.  Dynamic port forwardings can also be specified in the configuration file.</div><div class="line"></div><div class="line">        IPv6 addresses can be specified by enclosing the address in square brackets.  Only the superuser can forward privileged ports.  By default, the local port is bound in</div><div class="line">        accordance with the GatewayPorts setting.  However, an explicit bind_address may be used to bind the connection to a specific address.  The bind_address of</div><div class="line">        ``localhost&apos;&apos; indicates that the listening port be bound for local use only, while an empty address or `*&apos; indicates that the port should be available from all inter-</div><div class="line">        faces.</div></pre></td></tr></table></figure>
<p>使用如下命令,创建隧道</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># ssh -D bind_port -i 登陆instance的private_key 用户名@public_ip</div><div class="line">$ ssh -D 1234 -i OregonKeyPairsInCarlshenCnpem.pem ec2-user@34.220.122.63</div></pre></td></tr></table></figure>
<h5 id="设置Firefox"><a href="#设置Firefox" class="headerlink" title="设置Firefox"></a>设置Firefox</h5><p>设置Firefox来使用刚建立的ssh tunnel, 打开Firefox的【首选项】页面，选择【网络代理】, 在链接设置页面，选择“手动代理配置”，SOCKS主机一栏填入<code>127.0.0.1</code>,端口填入ssh -D绑定的端口，我这边设置的是端口1234。</p>
<p>Firefox设置截图<br><img src="/images/SSH/Tunnel/firefox_socks_settings.png" alt="firefox_socks_settings"></p>
<p>此时，通过浏览器，就可以成功访问到内网HttpServer中的网页了。</p>
<p><img src="/images/SSH/Tunnel/internal_access_from_firefox.png" alt="internal_access_from_firefox"></p>
<h5 id="设置Terminal来访问内网"><a href="#设置Terminal来访问内网" class="headerlink" title="设置Terminal来访问内网"></a>设置Terminal来访问内网</h5><p>在Terminal中也可以设置proxy来让http和https的流量走设置好的ssh proxy。</p>
<p>在打开新的Terminal中，输入如下命令让本terminal中的http和https流量走设置好的ssh tunnel。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export http_proxy=socks5://127.0.0.1:1234;export https_proxy=socks5://127.0.0.1:1234;</div></pre></td></tr></table></figure>
<p>使用curl，可以成功访问HttpServer中的网页。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[carlshen@carl-186 ~]$ curl http://172.31.21.213</div><div class="line">&lt;h1&gt; Page From 172.31.21.213 &lt;/h1&gt;</div><div class="line">[carlshen@carl-186 ~]$</div></pre></td></tr></table></figure>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://unix.stackexchange.com/questions/215571/how-can-i-remotely-access-an-intranet-website-from-an-external-network-via-an-ss" target="_blank" rel="external">How can I remotely access an intranet website from an external network via an SSH tunnel?</a></li>
<li><a href="https://askubuntu.com/questions/53553/how-do-i-retrieve-the-public-key-from-a-ssh-private-key" target="_blank" rel="external">How do I retrieve the public key from a SSH private key?</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> EC2 </tag>
            
            <tag> MacOS </tag>
            
            <tag> SSH </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[由于EC2中no space left引起的EB警报]]></title>
      <url>/2018/08/11/EB-warning-due-to-no-space-left-on-device-issue-on-EC2/</url>
      <content type="html"><![CDATA[<h3 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h3><p>周末早上突然收到大量Elastic Beanstalk的警报邮件，提示一个跑Rails的EB环境中有大量的请求是HTTP 5xx。</p>
<p>登陆AWS Console查看相关Event，已经有持续的WARN提示有大量的HTTP 5xx请求。<br><img src="/images/AWS/EBNoSpaceLeft/amounts_of_5XX_on_EB_event.png" alt="amounts_of_5XX_on_EB_event"></p>
<h3 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h3><p>使用EB console下载了各个Instanc的日志，仔细进行排查。</p>
<p>最后在一个Instance的Rails log中发现有请求提示<code>No space left on device</code>。原来是Instance的磁盘满了，导致了部分需要写临时文件的请求失败了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">F, [2018-08-11T02:10:43.084147 #11559] FATAL -- :</div><div class="line">Errno::ENOSPC (No space left on device @ rb_sysopen - /var/app/current/tmp/cache/rack%3A%3Aattack%3A306790688%3Alocations%2Fip%3A46.229.168.8120180811-11559-4156fj):</div><div class="line">  app/middleware/catch_json_parse_errors.rb:8:in `call&apos;</div></pre></td></tr></table></figure>
<a id="more"></a>
<h4 id="初步调查"><a href="#初步调查" class="headerlink" title="初步调查"></a>初步调查</h4><p>登陆出问题的Instance，<code>df -h</code>查看磁盘空间，发现磁盘上明明还有大量空间没使用。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[ec2-user@ip-172-31-47-93 ~]$ df -h</div><div class="line">文件系统        容量  已用  可用 已用% 挂载点</div><div class="line">devtmpfs        3.7G   60K  3.7G    1% /dev</div><div class="line">tmpfs           3.7G     0  3.7G    0% /dev/shm</div><div class="line">/dev/xvda1      7.8G  6.0G  1.8G   78% /</div><div class="line">[ec2-user@ip-172-31-47-93 ~]$</div></pre></td></tr></table></figure>
<p>尝试touch文件, 失败，系统提示确实没有磁盘空间不足<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[ec2-user@ip-172-31-47-93 ~]$ touch 1</div><div class="line">touch: 无法创建&quot;1&quot;: 设备上没有空间</div><div class="line">[ec2-user@ip-172-31-47-93 ~]$</div></pre></td></tr></table></figure></p>
<p>查看磁盘的inode节点，找到了问题，原来是磁盘的inode节点已经使用了100%。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[ec2-user@ip-172-31-47-93 log]# df -i</div><div class="line">文件系统        Inode 已用(I) 可用(I) 已用(I)% 挂载点</div><div class="line">devtmpfs       503511     405  503106       1% /dev</div><div class="line">tmpfs          506232       1  506231       1% /dev/shm</div><div class="line">/dev/xvda1     524288  524288       0     100% /</div><div class="line">[root@ip-172-31-47-93 log]#</div></pre></td></tr></table></figure></p>
<p>有磁盘空间，但是inode已经使用完毕了,说明系统中有大量的小文件存在, 极大可能是运行的Rails程序中的问题。</p>
<h4 id="恢复服务"><a href="#恢复服务" class="headerlink" title="恢复服务"></a>恢复服务</h4><p>因为需要保留该Instance进行进一步的调查，所以不能简单的使用EB Console将有问题的EC2替换掉。</p>
<p>选择将有问题的Instance从ASG中Detach出来，这样既可以让服务恢复，又能保留存在问题的Instance进行详细调查。</p>
<p>进入AutoScaling的操作界面，找到EB使用的AutoScaling组，在AutoScaling组的Instances配置界面上输入问题Instance的ID，然后将该Instance Detach。</p>
<p><img src="/images/AWS/EBNoSpaceLeft/detach_instance_from_ASG.png" alt="detach_instance_from_ASG"></p>
<p>Detach后，EB自动检测到少了一台Instance，自动启了新的Instance，等新Instance启动完成后，EB服务恢复，不再有HTTP 5xx的WARN了。</p>
<h4 id="深挖问题"><a href="#深挖问题" class="headerlink" title="深挖问题"></a>深挖问题</h4><p>服务恢复后，登陆有问题的Instance，使用命令<code>find . -xdev -type f | cut -d &quot;/&quot; -f 2 | sort | uniq -c | sort -nr</code>逐级查看各个目录中inode的使用量。</p>
<p>最终发现应用程序的<code>/var/app/current/tmp</code>目录消耗了巨大的inode。经过排查是某个固定的请求写了太多的缓存文件导致的问题。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p> 遇到只有某个Instance中才出现的问题时，可以选择将有问题的Instance隔离出EB环境，先恢复服务后再进行调查。</p>
<p>临时将EC2移出AutoScaling的两个方法:</p>
<ul>
<li>set to standby <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html" target="_blank" rel="external">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html</a><ul>
<li>设置为standby后，排查完毕后还可以恢复入AutoScaling</li>
</ul>
</li>
<li>detach <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/detach-instance-asg.html" target="_blank" rel="external">https://docs.aws.amazon.com/autoscaling/ec2/userguide/detach-instance-asg.html</a><ul>
<li>detach后，就脱离AutoScaling，不再占用desired capacity，AutoScaling会根据desired capacity再创建一个Instance来弥补</li>
</ul>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html" target="_blank" rel="external">Temporarily Removing Instances from Your Auto Scaling Group</a></li>
<li><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/detach-instance-asg.html" target="_blank" rel="external">Detach EC2 Instances from Your Auto Scaling Group</a></li>
<li><a href="https://serverfault.com/a/633150" target="_blank" rel="external">https://serverfault.com/a/633150</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Elastic Beanstalk </tag>
            
            <tag> AutoScaling </tag>
            
            <tag> EC2 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[macOS 10.13.2下SSH命令Invalid key length错误的原因]]></title>
      <url>/2018/05/21/ssh-key-Invalid-key-length-Error-on-Macos/</url>
      <content type="html"><![CDATA[<p>升级到Macos 10.13.2后，使用ssh以key方式登陆一个12年配置的服务器的时候，ssh命令报告了<code>Invalid key length</code>的错误。</p>
<p>很是纳闷，服务器和本地的key都是配置好后就没动过的，key pair本身肯定没问题啊，怎么会报错了呢?</p>
<h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>Google了一下，发现是MacOS使用的OpenSSH版本升级到了7.6，小于1024bits的RSA keys已经不被支持了。</p>
<p>使用<code>ssh-keygen</code>查看了一下public key，发现是1023bit的。所以ssh时候报了<code>Invalid key length</code>的错误。</p>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ ssh-keygen -lf carl.pub</div><div class="line">1023 82:3e:bc:4c:e8:34:0e:f1:5b:44:e3:db:9b:c2:11:88 carl.pub (RSA)</div><div class="line">$</div></pre></td></tr></table></figure>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>重新生成了一个长度为2048bit的新ssh keypair，并将公钥添加到服务器端，就可以解决该问题。</p>
<p><strong>这个<code>Invalid key length</code>的错误，只是Macos的ssh客户端的行为，如果不换key，而使用一个允许小于1024bit的ssh客户端，原有的key仍然是可以登陆服务器的。</strong></p>
<h3 id="小测试"><a href="#小测试" class="headerlink" title="小测试"></a>小测试</h3><h4 id="试验1024bit-key-length"><a href="#试验1024bit-key-length" class="headerlink" title="试验1024bit key length"></a>试验1024bit key length</h4><p>使用<code>ssh-keygen -t rsa -b 1024 -C &quot;carl.shen@hello_aws&quot; -f carl_1024</code>生成key length为1024的keypair</p>
<p>使用私钥carl_1024登陆服务器，ssh客户端不会报告<code>Invalid key length</code> 的错误<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ ssh -i carl_1024 carl.shen@hello_aws</div><div class="line">carl.shen@hello_aws: Permission denied (publickey).</div><div class="line">$</div></pre></td></tr></table></figure></p>
<h4 id="试验2048bit-key-length"><a href="#试验2048bit-key-length" class="headerlink" title="试验2048bit key length"></a>试验2048bit key length</h4><p>使用<code>ssh-keygen -t rsa -b 2048 -C &quot;carl.shen@hello_aws&quot; -f carl_2048</code>生成key length为2048的keypair</p>
<p>使用私钥carl_2048登陆服务器，ssh客户端就不会报告<code>Invalid key length</code>的错误<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ ssh -i carl_2048 carl.shen@hello_aws</div><div class="line">carl.shen@hello_aws: Permission denied (publickey).</div><div class="line">$</div></pre></td></tr></table></figure></p>
<h4 id="试验4096bit-key-length"><a href="#试验4096bit-key-length" class="headerlink" title="试验4096bit key length"></a>试验4096bit key length</h4><p>使用<code>ssh-keygen -t rsa -b 4096 -C &quot;carl.shen@hello_aws&quot; -f carl_4096</code>生成key length为4096的keypair</p>
<p>使用私钥carl_4096登陆服务器，ssh客户端就不会报告<code>Invalid key length</code>的错误<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ ssh -i carl_4096 carl.shen@hello_aws</div><div class="line">carl.shen@hello_aws: Permission denied (publickey).</div><div class="line">$</div></pre></td></tr></table></figure></p>
<h4 id="尝试生成小于1024-bit的key失败"><a href="#尝试生成小于1024-bit的key失败" class="headerlink" title="尝试生成小于1024 bit的key失败"></a>尝试生成小于1024 bit的key失败</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ ssh-keygen -t rsa -b 1023 -C &quot;carl.shen@hello_aws&quot; -f carl_1023</div><div class="line">Invalid RSA key length: minimum is 1024 bits</div><div class="line">$</div></pre></td></tr></table></figure>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://discussions.apple.com/thread/8203610" target="_blank" rel="external">ssh_dispatch_run_fatal Invalid key length</a></li>
<li><a href="https://sachin.ranadive.org/index.php/2017/12/09/os-x-10-13-2-high-sierra-ssh-1024-invalid-key-length/" target="_blank" rel="external">OS X 10.13.2 High Sierra SSH Invalid key length</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Shell </category>
            
        </categories>
        
        
        <tags>
            
            <tag> MacOS </tag>
            
            <tag> SSH </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[macOS 10.13 High Sierra读写NTFS移动硬盘]]></title>
      <url>/2018/05/10/how-to-write-NTFS-Deives-on-a-Mac-10.13/</url>
      <content type="html"><![CDATA[<p>系统升级到10.13后，没用过移动硬盘，所以一直也没发现以前装的osxfuse+ntfs-3g在系统更新后已经失效了。结果么在要使用的时候发现悲剧了，在10.13中没法写NTFT格式的移动硬盘了。</p>
<p>上网搜了一下，How-To Geek上面有一篇文章，详细讲述了Macos下挂载NTFS移动硬盘的几种方法,有收费方案也有免费方案，并比较了每个方案的优缺点。 虽然文章是基于macOS 10.12 Sierra的，但各个方案的优缺点同样适用于10.13。<a href="https://www.howtogeek.com/236055/how-to-write-to-ntfs-drives-on-a-mac/" target="_blank" rel="external">How to Write to NTFS Drives on a Mac</a></p>
<a id="more"></a>
<h3 id="收费可靠方案"><a href="#收费可靠方案" class="headerlink" title="收费可靠方案"></a>收费可靠方案</h3><p>19.95美元的<a href="https://www.paragon-software.com/home/ntfs-mac/" target="_blank" rel="external">Paragon NTFS For Mac</a></p>
<h3 id="针对Seagate硬盘的免费可靠方案"><a href="#针对Seagate硬盘的免费可靠方案" class="headerlink" title="针对Seagate硬盘的免费可靠方案"></a>针对Seagate硬盘的免费可靠方案</h3><p>在搜索的过程中，无意中发现针对Macos无法写NTFS的情况，Seagate早就提供了免费的Paragon驱动程序。下载过后就能直接读写Seagate自家的硬盘了。这么方便的工具才发现，不由得感叹自己的孤陋寡闻。<br>下载地址: <a href="https://www.seagate.com/cn/zh/support/external-hard-drives/portable-hard-drives/goflex/ntfs-driver-for-mac-os-master-dl/" target="_blank" rel="external">适用于 macOS (10.10 及以上) 的 Paragon 驱动程序</a></p>
<h3 id="osxfuse-ntfs-3g-免费方案"><a href="#osxfuse-ntfs-3g-免费方案" class="headerlink" title="osxfuse+ntfs-3g 免费方案"></a>osxfuse+ntfs-3g 免费方案</h3><p>开源的<a href="https://github.com/osxfuse/osxfuse" target="_blank" rel="external">osxfuse</a>+ntfs-3g方案, 可以参见stackoverflow的回答 <a href="https://apple.stackexchange.com/questions/20889/how-do-i-write-to-ntfs-drives-in-os-x/213575#213575" target="_blank" rel="external">how-do-i-write-to-ntfs-drives-in-os-x</a> 以及osxfuse的<a href="https://github.com/osxfuse/osxfuse/wiki/NTFS-3G" target="_blank" rel="external">官方wiki</a></p>
<p>缺点是每次系统升级后得重新升级osxfuse和ntfs-3g。</p>
<h3 id="终极方案"><a href="#终极方案" class="headerlink" title="终极方案"></a>终极方案</h3><p>终极方案当然是将移动硬盘重格式化为exFAT，就可以无缝兼容PC和MAC了。</p>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> MacOS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Mac下几个处理图片的命令]]></title>
      <url>/2018/04/25/command-line-tools-to-check-images-on-macos/</url>
      <content type="html"><![CDATA[<h3 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h3><p>Macos下，查看图片，默认就是使用Preview。Preview基本包含了普通人能用到的各种功能。<br>但有时候需要批量处理图片时，常规的GUI工具会显得不太方便，使用命令来操作能够更快捷高效。下面介绍几个Macos下和图片相关的几个命令。</p>
<h3 id="查看图片尺寸"><a href="#查看图片尺寸" class="headerlink" title="查看图片尺寸"></a>查看图片尺寸</h3><h4 id="系统自带file命令"><a href="#系统自带file命令" class="headerlink" title="系统自带file命令"></a>系统自带file命令</h4><p>在命令行下查看图片的分辨率，可以直接使用Macos自带的file命令即可。<br><a id="more"></a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ ls</div><div class="line">Diamond.png kulipa.jpg  steve.jpeg</div><div class="line">$</div><div class="line">$ file *</div><div class="line">Diamond.png: PNG image data, 256 x 256, 4-bit colormap, non-interlaced</div><div class="line">kulipa.jpg:  JPEG image data, JFIF standard 1.01, resolution (DPI), density 1x1, segment length 16, baseline, precision 8, 220x220, frames 3</div><div class="line">steve.jpeg:  JPEG image data, JFIF standard 1.01, aspect ratio, density 1x1, segment length 16, baseline, precision 8, 220x220, frames 3</div><div class="line">$</div></pre></td></tr></table></figure>
<p>其中256 x 256就是Diamond.png的分辨率，220x220是kulipa.jpg和steve.jpeg的分辨率。</p>
<h4 id="ImageMagick中的identify"><a href="#ImageMagick中的identify" class="headerlink" title="ImageMagick中的identify"></a>ImageMagick中的identify</h4><p>如果还需要图片的更多内容，那么可以使用<code>identify</code>命令。<code>identify</code>是强大的开源图片处理工具<a href="https://www.imagemagick.org/script/index.php" target="_blank" rel="external">ImageMagick</a>的一个组件。</p>
<p>Macos下可以通过使用<a href="https://brew.sh/" target="_blank" rel="external">Homebrew</a>来安装<a href="https://www.imagemagick.org/script/index.php" target="_blank" rel="external">ImageMagick</a>: <code>brew install imagemagick</code></p>
<p>安装完毕后，就可以使用<code>identify</code>命令了。 简单的用法就是 <code>identify ${image_name}</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ ls</div><div class="line">Diamond.png kulipa.jpg  steve.jpeg</div><div class="line">$</div><div class="line">$ identify *</div><div class="line">Diamond.png PNG 256x256 256x256+0+0 8-bit sRGB 499B 0.000u 0:00.000</div><div class="line">kulipa.jpg[1] JPEG 220x220 220x220+0+0 8-bit sRGB 10.9KB 0.000u 0:00.000</div><div class="line">steve.jpeg[2] JPEG 220x220 220x220+0+0 8-bit sRGB 9.62KB 0.000u 0:00.000</div><div class="line">$</div></pre></td></tr></table></figure></p>
<p>就能够显示分辨率，图片大小等基本信息了。</p>
<p>也可以使用-format参数来输出指定的信息。-format的具体参数参照<a href="https://www.imagemagick.org/script/escape.php" target="_blank" rel="external">ImageMagick官网说明</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ ls</div><div class="line">Diamond.png kulipa.jpg  steve.jpeg</div><div class="line">$</div><div class="line"># 下面-format参数中，%f代表filename，%w代表width in pixels，%h代表height in pixels,%m表示文件格式</div><div class="line">$ identify -format &quot;%f: %wx%h file_format:%m\n&quot; *</div><div class="line">Diamond.png: 256x256 file_format:PNG</div><div class="line">kulipa.jpg: 220x220 file_format:JPEG</div><div class="line">steve.jpeg: 220x220 file_format:JPEG</div><div class="line">$</div></pre></td></tr></table></figure></p>
<p>也可以使用-verbose参数来打印图片的全部信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div></pre></td><td class="code"><pre><div class="line">$ identify -verbose steve.jpeg</div><div class="line">Image: steve.jpeg</div><div class="line">  Format: JPEG (Joint Photographic Experts Group JFIF format)</div><div class="line">  Mime type: image/jpeg</div><div class="line">  Class: DirectClass</div><div class="line">  Geometry: 220x220+0+0</div><div class="line">  Units: Undefined</div><div class="line">  Type: TrueColor</div><div class="line">  Endianess: Undefined</div><div class="line">  Colorspace: sRGB</div><div class="line">  Depth: 8-bit</div><div class="line">  Channel depth:</div><div class="line">    red: 8-bit</div><div class="line">    green: 8-bit</div><div class="line">    blue: 8-bit</div><div class="line">  Channel statistics:</div><div class="line">    Red:</div><div class="line">      min: 0 (0)</div><div class="line">      max: 255 (1)</div><div class="line">      mean: 102.506 (0.401985)</div><div class="line">      standard deviation: 95.4329 (0.374247)</div><div class="line">      kurtosis: -1.49406</div><div class="line">      skewness: 0.402081</div><div class="line">    Green:</div><div class="line">      min: 0 (0)</div><div class="line">      max: 255 (1)</div><div class="line">      mean: 154.898 (0.607445)</div><div class="line">      standard deviation: 55.8926 (0.219187)</div><div class="line">      kurtosis: -0.0319889</div><div class="line">      skewness: -0.290039</div><div class="line">    Blue:</div><div class="line">      min: 0 (0)</div><div class="line">      max: 255 (1)</div><div class="line">      mean: 127.662 (0.500634)</div><div class="line">      standard deviation: 71.2261 (0.279318)</div><div class="line">      kurtosis: -1.0386</div><div class="line">      skewness: 0.188506</div><div class="line">  Image statistics:</div><div class="line">    Overall:</div><div class="line">      min: 0 (0)</div><div class="line">      max: 255 (1)</div><div class="line">      mean: 128.355 (0.503355)</div><div class="line">      standard deviation: 75.9486 (0.297838)</div><div class="line">      kurtosis: -0.862748</div><div class="line">      skewness: -0.0758529</div><div class="line">  Rendering intent: Perceptual</div><div class="line">  Gamma: 0.454545</div><div class="line">  Chromaticity:</div><div class="line">    red primary: (0.64,0.33)</div><div class="line">    green primary: (0.3,0.6)</div><div class="line">    blue primary: (0.15,0.06)</div><div class="line">    white point: (0.3127,0.329)</div><div class="line">  Background color: white</div><div class="line">  Border color: srgb(223,223,223)</div><div class="line">  Matte color: grey74</div><div class="line">  Transparent color: black</div><div class="line">  Interlace: None</div><div class="line">  Intensity: Undefined</div><div class="line">  Compose: Over</div><div class="line">  Page geometry: 220x220+0+0</div><div class="line">  Dispose: Undefined</div><div class="line">  Iterations: 0</div><div class="line">  Compression: JPEG</div><div class="line">  Quality: 75</div><div class="line">  Orientation: Undefined</div><div class="line">  Properties:</div><div class="line">    date:create: 2018-04-25T19:46:03+08:00</div><div class="line">    date:modify: 2016-09-13T09:57:32+08:00</div><div class="line">    jpeg:colorspace: 2</div><div class="line">    jpeg:sampling-factor: 2x2,1x1,1x1</div><div class="line">    signature: 6d01e3f2f45a94c2ec30e4928fdb3fbbc82c7d5e556e71a9a981868f6cdbc778</div><div class="line">  Artifacts:</div><div class="line">    filename: steve.jpeg</div><div class="line">    verbose: true</div><div class="line">  Tainted: False</div><div class="line">  Filesize: 9.62KB</div><div class="line">  Number pixels: 48.4K</div><div class="line">  Pixels per second: 4.84MB</div><div class="line">  User time: 0.000u</div><div class="line">  Elapsed time: 0:01.009</div><div class="line">  Version: ImageMagick 6.8.8-6 Q16 x86_64 2014-02-17 http://www.imagemagick.org</div><div class="line">$</div></pre></td></tr></table></figure></p>
<h3 id="命令行中打开图片"><a href="#命令行中打开图片" class="headerlink" title="命令行中打开图片"></a>命令行中打开图片</h3><h4 id="Macos自带的open命令"><a href="#Macos自带的open命令" class="headerlink" title="Macos自带的open命令"></a>Macos自带的open命令</h4><p>Macos自带了强大的<code>open</code>命令，<code>open</code>后面加上图片路径，就会使用系统的preview来打开这个图片。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ ls</div><div class="line">Diamond.png kulipa.jpg  steve.jpeg</div><div class="line">$</div><div class="line">$ open *</div><div class="line">$</div></pre></td></tr></table></figure></p>
<p>输入<code>open *</code>指令后，就会使用preview来打开Diamond.png,kulipa.jpg和steve.jpeg这三张图片。效果和在Finder中选中这三张图片，然后选择使用Preview打开效果是一样的。<br><img src="/images/Shell/open_image_on_terminal.jpg" alt="open_image_on_terminal.jpg"></p>
<p><code>open</code>是Macos的一个强大的命令，能做很多事情，比如</p>
<ul>
<li>使用Finder打开当前目录 <code>open ./</code></li>
<li>使用Chrome打开链接 <code>open &quot;https://www.google.com&quot; -a Google\ Chrome</code></li>
</ul>
<h4 id="直接在Terminal中查看图片"><a href="#直接在Terminal中查看图片" class="headerlink" title="直接在Terminal中查看图片"></a>直接在Terminal中查看图片</h4><p>有一些工具，安装配置后能直接在Terminal中展示图片。目前没有这方面的需求，没有试过。摘录几个网址，有空了可以试了玩玩。</p>
<ul>
<li><a href="https://www.everythingcli.org/ranger-image-preview-on-osx-with-iterm2/" target="_blank" rel="external">RANGER IMAGE PREVIEW ON OSX WITH ITERM2</a></li>
<li><a href="https://www.reddit.com/r/osx/comments/5qlbzf/fehstyle_image_viewer/" target="_blank" rel="external">Feh-style image viewer?</a></li>
<li><a href="https://github.com/stefanhaustein/TerminalImageViewer" target="_blank" rel="external">TerminalImageViewer</a></li>
</ul>
<h3 id="命令行转换图片格式"><a href="#命令行转换图片格式" class="headerlink" title="命令行转换图片格式"></a>命令行转换图片格式</h3><h4 id="系统自带的sips命令"><a href="#系统自带的sips命令" class="headerlink" title="系统自带的sips命令"></a>系统自带的sips命令</h4><p>Macos自带了一个命令可以用来转换图片格式，名字叫做<a href="https://developer.apple.com/legacy/library/documentation/Darwin/Reference/ManPages/man1/sips.1.html" target="_blank" rel="external">sips</a></p>
<p>转换格式的用法如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sips -s format [转换的目标格式] --out [目标文件名字] [输入文件]</div></pre></td></tr></table></figure></p>
<p>如下是一个将jpg图片转换为png图片的例子:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">$ ls</div><div class="line">kulipa.jpg</div><div class="line"># 将kulipa.jpg转换为png格式，输出名字叫kulipa.png</div><div class="line">$ sips -s format png --out kulipa.png kulipa.jpg</div><div class="line">/Users/carlshen/tmp/command_line_image_check/kulipa.jpg</div><div class="line">  /Users/carlshen/tmp/command_line_image_check/kulipa.png</div><div class="line">$</div><div class="line">$ ls</div><div class="line">kulipa.jpg kulipa.png</div><div class="line">$</div><div class="line">$ file kulipa.jpg kulipa.png</div><div class="line">kulipa.jpg: JPEG image data, JFIF standard 1.01, resolution (DPI), density 1x1, segment length 16, baseline, precision 8, 220x220, frames 3</div><div class="line">kulipa.png: PNG image data, 220 x 220, 8-bit/color RGB, non-interlaced</div><div class="line">$</div></pre></td></tr></table></figure></p>
<p>支持的格式有<code>jpeg | tiff | png | gif | jp2 | pict | bmp | qtif | psd | sgi | tga</code><br>不同系统的sips支持的格式可能不同，可以使用<code>man sips</code>来查看sips支持的转换格式</p>
<p>sips除了能转换图片格式以外，还可以对图片进行调整大小(resize)，旋转(rotate)和翻转(flip)等。</p>
<ul>
<li>拿一个原size是1200x896的house.jpg为例:<ul>
<li><strong>限定范围缩放命令</strong>: <code>sips -Z pixelsWH [file]</code><ul>
<li>例子: <code>sips -Z 300 house.jpg</code></li>
<li>将原图缩放到300x300像素的方框内，保持图片的长宽比不变</li>
</ul>
</li>
<li><strong>调整大小命令</strong>: <code>sips -z height width [file]</code><ul>
<li>例子: <code>sips -z 400 400 house.jpg</code></li>
<li>图片会被缩小拉伸到400x400，原图片的长宽比会改变</li>
</ul>
</li>
<li><strong>旋转图片命令</strong>: <code>sips -r degreesCW [file]</code><ul>
<li>例子: <code>sips -r 90 house.jpg</code></li>
<li>图片会顺时针旋转90度</li>
</ul>
</li>
<li><strong>翻转图片命令</strong>: <code>sips -f horizontal|vertical [file]</code><ul>
<li>例子: <code>sips -f horizontal house.jpg</code></li>
<li>图片会水平翻转。如果使用vertical, 则会垂直翻转</li>
</ul>
</li>
<li><strong>注意，上述命令会直接修改原图片，如果要保留原图片，则可以加上–out 参数指定输出的文件名。比如<code>sips -f horizontal house.jpg --out house_horizontal.jpg</code></strong></li>
</ul>
</li>
</ul>
<h4 id="使用convert命令"><a href="#使用convert命令" class="headerlink" title="使用convert命令"></a>使用convert命令</h4><p>convert也是<a href="https://www.imagemagick.org/script/index.php" target="_blank" rel="external">ImageMagick</a>的组件之一。简单的转换格式用法非常的简单<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">convert [源文件] [目标文件]</div></pre></td></tr></table></figure></p>
<p>convert会根据文件后缀自动转换为想要的格式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">$</div><div class="line">$ convert kulipa.jpg kulipa.png # 转换为png文件</div><div class="line">$ convert kulipa.jpg kulipa.bmp # 转换为bmp文件</div><div class="line">$</div><div class="line">$ ls</div><div class="line">kulipa.bmp kulipa.jpg kulipa.png</div><div class="line">$</div><div class="line"># 检查各个图片的格式</div><div class="line">$ identify *</div><div class="line">kulipa.bmp BMP 220x220 220x220+0+0 8-bit sRGB 145KB 0.000u 0:00.000</div><div class="line">kulipa.jpg[1] JPEG 220x220 220x220+0+0 8-bit sRGB 10.9KB 0.000u 0:00.009</div><div class="line">kulipa.png[2] PNG 220x220 220x220+0+0 8-bit sRGB 15.1KB 0.000u 0:00.000</div><div class="line">$</div></pre></td></tr></table></figure></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://unix.stackexchange.com/questions/75635/shell-command-to-get-pixel-size-of-an-image" target="_blank" rel="external">https://unix.stackexchange.com/questions/75635/shell-command-to-get-pixel-size-of-an-image</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Shell </category>
            
        </categories>
        
        
        <tags>
            
            <tag> MacOS </tag>
            
            <tag> Image </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[豆瓣图片403问题]]></title>
      <url>/2018/04/24/douban-image-403/</url>
      <content type="html"><![CDATA[<h3 id="起由"><a href="#起由" class="headerlink" title="起由"></a>起由</h3><p>发布了一版blog到github上。偶然间发现<a href="/douban">豆瓣页面</a>中书的图片都无法显示了。打开Inspector调查了下，发现请求豆瓣图片的链接返回的都是403 Forbidden。<br><img src="/images/Hexo/douban_image_403.png" alt="douban_image_403.png"></p>
<p>网上搜索了一下，可能的原因就是豆瓣为了封杀微信小程序在页面上直接请求豆瓣的图片，封杀了图片的外链。</p>
<h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>尝试着将豆瓣API返回的图片链接拷贝到浏览器中直接访问，发现可以加载图片，那估计豆瓣是通过检测HTTP Request中的referer Header来实现防盗链。</p>
<a id="more"></a>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>有下面这么几种方法可以来解决该403问题</p>
<ol>
<li>架设一个中转服务器，页面请求中转服务器，中转服务器请求豆瓣获取图片后再返回给前端页面。<ul>
<li>优点是可以避开Referer Header限制</li>
<li>缺点一是需要假设服务器，成本大</li>
<li>缺点二是貌似豆瓣对请求的IP地址也有限制，请求量大时可能也会被禁。</li>
</ul>
</li>
<li>通过某些代理网站来获取原始图片，比如要获取图片的原始链接是<a href="https://img3.doubanio.com/view/subject/l/public/s29542864.jpg" target="_blank" rel="external">https://img3.doubanio.com/view/subject/l/public/s29542864.jpg</a>，添加代理网站的信息后，访问链接变为<a href="https://images.weserv.nl/?url=img3.doubanio.com/view/subject/l/public/s29542864.jpg" target="_blank" rel="external">https://images.weserv.nl/?url=img3.doubanio.com/view/subject/l/public/s29542864.jpg</a>，原理和第一种类似，只是中转服务器变为使用第三方了。<ul>
<li>优点是方便，快捷，不操心</li>
<li>缺点是依赖于第三方的服务，可用性和稳定性不可控。</li>
</ul>
</li>
<li>修改网页Html，使得在访问豆瓣图片时，不发送Referer的Header。2017年的时候，W3C制定了一个名为<a href="https://www.w3.org/TR/referrer-policy/" target="_blank" rel="external">Referrer Policy</a>的规范，可以用来告诉浏览器是否需要发送Referer Header。但非所有浏览器都能支持referrerpolicy这个属性。<ul>
<li>好像除了自己可控外, 没什么其它优点。</li>
<li>缺点是并非所有浏览器都能支持referrerpolicy这个属性。在MacOS 10.13.2下试验了几个在用的浏览器,结果如下:<ul>
<li>66.0.3359.117版本的Chrome<strong>支持</strong></li>
<li>59.0.2版本的Firefox<strong>支持</strong>该属性</li>
<li>版本为11.0.2 (13604.4.7.1.6)的Safari<strong>不支持</strong>该属性。即使设置了referrerpolicy=”no-referrer”,还是会发送Referer Header。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="方案选取"><a href="#方案选取" class="headerlink" title="方案选取"></a>方案选取</h3><p>上面所述的几个方法中:</p>
<ul>
<li>方法一对于一个托管在Github上，追求简单的Blog来说，太过于沉重了。</li>
<li>方法二是个很好的解决方案，但是自己的Blog就随手写点东西，抱着越简单越好的原则，不想引入任何不必要的第三方服务。</li>
<li>方法三最搓，没有完整的解决问题。但<a href="/douban">豆瓣页面</a>只是为了记录一下读书记录，没有图片展示也无所谓，自己能看就行。</li>
</ul>
<p>最终决定采用第三种方法</p>
<p>BLog的豆瓣页面使用的是<a href="https://github.com/Yikun/hexo-generator-douban" target="_blank" rel="external">hexo-generator-douban</a>插件，插件最新维护日期是2016年。因为只是一个自用的不完整的解决方案，没提issue, 就Fork了一份代码到自己的Github，自己改了改，添加了referrerpolicy=”no-referrer”。然后安装github版本到Hexo。</p>
<p>代码修改diff: <a href="https://github.com/jibing57/hexo-generator-douban/commit/21f2bc7d2b259c6a9221a5cd3935c222ea0429cc" target="_blank" rel="external">https://github.com/jibing57/hexo-generator-douban/commit/21f2bc7d2b259c6a9221a5cd3935c222ea0429cc</a></p>
<p>安装Github代码到本地:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">$ npm install git+https://github.com/jibing57/hexo-generator-douban.git --save</div><div class="line">+ hexo-generator-douban@0.1.2</div><div class="line">added 1 package and updated 3 packages in 11.748s</div><div class="line"></div><div class="line"></div><div class="line">   ╭─────────────────────────────────────╮</div><div class="line">   │                                     │</div><div class="line">   │   Update available 5.5.1 → 5.8.0    │</div><div class="line">   │     Run npm i -g npm to update      │</div><div class="line">   │                                     │</div><div class="line">   ╰─────────────────────────────────────╯</div><div class="line"></div><div class="line">$</div></pre></td></tr></table></figure>
<p>本地启动Hexo，在Chrome中就可以正常看到豆瓣的图片了。<br><img src="/images/Hexo/douban_image_showed_in_chrome.png" alt="douban_image_showed_in_chrome.png"></p>
<h3 id="小故事"><a href="#小故事" class="headerlink" title="小故事"></a>小故事</h3><p>Header中的referer，其实是referer的拼写错误。参见Wiki <a href="https://en.wikipedia.org/wiki/HTTP_referer" target="_blank" rel="external">HTTP referer</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://www.w3.org/TR/referrer-policy/" target="_blank" rel="external">Referrer Policy</a></li>
<li><a href="http://blog.ipsfan.com/3889.html" target="_blank" rel="external">解决豆瓣api图片403禁止访问问题</a></li>
<li><a href="https://github.com/LingYanSi/blog/issues/89" target="_blank" rel="external">豆瓣API访问图片403的解决方案</a></li>
<li><a href="https://github.com/izzyleung/ZhihuDailyPurify/issues/24" target="_blank" rel="external">api获取图片403，请问你是怎么解决的?</a></li>
<li><a href="https://stackoverflow.com/questions/17509669/how-to-install-an-npm-package-from-github-directly" target="_blank" rel="external">How to install an npm package from GitHub directly</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Blog </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Douban </tag>
            
            <tag> Nodejs </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[S3进阶系列(二) ——权限管理]]></title>
      <url>/2018/04/21/aws-s3-master-permission/</url>
      <content type="html"><![CDATA[<h3 id="什么是S3的权限管理"><a href="#什么是S3的权限管理" class="headerlink" title="什么是S3的权限管理"></a>什么是S3的权限管理</h3><p>S3的权限管理是任何使用S3的人都无法绕开的功能,决定了S3中的Object是否能够被访问。</p>
<p>权限管理本质上解决了这么一个问题: 谁(Principal)能够在何种条件(condition)下被允许或拒绝(Effect)对哪些资源(Resource)做何种操作(Action)</p>
<a id="more"></a>
<h3 id="S3权限管理概述"><a href="#S3权限管理概述" class="headerlink" title="S3权限管理概述"></a>S3权限管理概述</h3><p>S3权限管理，需要了解什么是S3的策略(Policy)，有哪些策略，有多个策略时S3是如何进行评估的，以及如何选取合适的策略。</p>
<h4 id="策略-Policies"><a href="#策略-Policies" class="headerlink" title="策略(Policies)"></a>策略(Policies)</h4><ul>
<li>默认所有S3的资源都是私有的<ul>
<li>只有资源的拥有者（创建资源的账号）能访问资源</li>
<li>拥有者可以通过policy来赋予其他人来访问资源</li>
</ul>
</li>
<li>策略（policy）分为两类<ul>
<li>直接应用于某个资源（一个Object或者一个Bucket）的策略称为资源策略(Resources Policies)</li>
<li>直接应用于某个IAM用户的被称为用户策略（User Resources）</li>
</ul>
</li>
<li>Resources Based Polices<ul>
<li>总共有两类的资源策略<ul>
<li>访问控制列表(Access Control Lists)<ul>
<li>可以赋予简单的对object和bucket的读写权限给AWS账号和预定义组</li>
<li>使用XML schema</li>
<li>由一个权限条目的列表组成</li>
</ul>
</li>
<li>桶策略（Bucket Policies）<ul>
<li>用来赋予AWS账号或者IAM用户访问Bucket和object的权限</li>
<li>使用JSON</li>
<li>可以进行精细化控制</li>
<li>大多数情况下，可以用来替换ACL</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>用户策略(User Policies)<ul>
<li>使用AWS IAM添加到用户(users), 组(groups)或者角色(roles)</li>
<li>可以进行精细化控制</li>
<li>使用JSON</li>
<li>因为是直接赋给IAM users/groups/role的，因此不支持给予匿名访问的权限</li>
<li>不能被赋予root account</li>
</ul>
</li>
<li>跨账户访问(Cross Account Access) <ul>
<li>跨账户访问是一个账户授权另一个账户访问它的资源<ul>
<li>比如Account A可以通过Bucket Policy授权Account B访问权限<ol>
<li>Account A通过Bucket Policy授权了Account B可以访问某个Bucket</li>
<li>Account B通过User Policy授权Account B的某个IAM User访问权限</li>
<li>Account B中的IAM user就可以访问Account A中Bucket的资源了</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="S3怎么评估polices"><a href="#S3怎么评估polices" class="headerlink" title="S3怎么评估polices"></a>S3怎么评估polices</h4><ul>
<li>整体规则:<ul>
<li>当有显式拒绝时，会被判定为拒绝访问</li>
<li>没有显式拒绝，有显示允许时，会被判定为允许访问。</li>
<li>没有显式拒绝也没有显示允许时，就适用默认拒绝规则，会被判定为拒绝访问。</li>
</ul>
</li>
<li>原理和例子可以参照这两篇文章 <a href="/2017/12/14/deep-dig-on-s3-bucket-permission/" title="深挖AWS S3的权限管理">深挖AWS S3的权限管理</a> 和 <a href="/2017/12/19/deep-dig-on-s3-bucket-permission-exam/" title="深挖AWS S3的权限管理(实验篇)">深挖AWS S3的权限管理(实验篇)</a></li>
</ul>
<h4 id="怎么选取使用哪个策略"><a href="#怎么选取使用哪个策略" class="headerlink" title="怎么选取使用哪个策略"></a>怎么选取使用哪个策略</h4><ul>
<li>一般情况下，优先使用User Policy和Bucket Policy，而不是ACL，因为它们能提供细粒度的控制。</li>
<li>如下情况下，需要使用ACL<ul>
<li>需要控制不是Bucket拥有者的object的权限</li>
<li>需要控制大量零碎的，不同prefix的object的权限</li>
<li>赋予“S3 Log Delivery Group”权限到Bucket时</li>
</ul>
</li>
<li>如下情况时，需要使用Bucket Policy<ul>
<li>无法使用ACL授权跨账户访问时</li>
</ul>
</li>
</ul>
<h3 id="S3的访问控制列表-ACL"><a href="#S3的访问控制列表-ACL" class="headerlink" title="S3的访问控制列表(ACL)"></a>S3的访问控制列表(ACL)</h3><ul>
<li>什么是ACL<ul>
<li>ACL是基于XML的，可用于bucket和object</li>
<li>每一个bucket和object都有一条ACL作为它的sub-resource来定义访问权限</li>
<li>默认的ACL是给object的拥有者完全的访问权限</li>
</ul>
</li>
<li>ACL 特性<ul>
<li>只能被用来给AWS account和预定义的groups来赋予权限，不能管理IAM user的权限</li>
<li>只能赋予基本的读写权限<ul>
<li>不能基于条件来控制读写权限</li>
<li>不能赋予显式拒绝的权限</li>
</ul>
</li>
<li>ACL 可以拥有最多 100 个授权</li>
</ul>
</li>
<li><p><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/acl-overview.html" target="_blank" rel="external">ACL 例子</a>解析</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</div><div class="line">&lt;AccessControlPolicy xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;</div><div class="line">  &lt;Owner&gt;</div><div class="line">    &lt;ID&gt;*** Owner-Canonical-User-ID ***&lt;/ID&gt;</div><div class="line">    &lt;DisplayName&gt;owner-display-name&lt;/DisplayName&gt;</div><div class="line">  &lt;/Owner&gt;</div><div class="line">  &lt;AccessControlList&gt;</div><div class="line">    &lt;Grant&gt;</div><div class="line">      &lt;Grantee xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; </div><div class="line">               xsi:type=&quot;Canonical User&quot;&gt;</div><div class="line">        &lt;ID&gt;*** Owner-Canonical-User-ID ***&lt;/ID&gt;</div><div class="line">        &lt;DisplayName&gt;display-name&lt;/DisplayName&gt;</div><div class="line">      &lt;/Grantee&gt;</div><div class="line">      &lt;Permission&gt;FULL_CONTROL&lt;/Permission&gt;</div><div class="line">    &lt;/Grant&gt;</div><div class="line">  &lt;/AccessControlList&gt;</div><div class="line">&lt;/AccessControlPolicy&gt;</div></pre></td></tr></table></figure>
<ul>
<li><p>注意<strong>Owner.ID</strong>一栏中，是拥有这个object的aws account的规范用户ID（canonical ID)，<strong>而不是12位的account ID</strong>, 关于如何查找规范用户ID，可以参阅<a href="https://docs.aws.amazon.com/zh_cn/general/latest/gr/acct-identifiers.html#FindingCanonicalId" target="_blank" rel="external">查找账户的规范用户ID</a></p>
</li>
<li><p>Grant的实体，分为好几类，可以是AWS account,也可以是S3预定义的组。</p>
<ul>
<li><p>当为AWS account时，则<strong>Grantee</strong>中xsi:type为CanonicalUser,Grant中以ID, DisplayName字段展示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&lt;Grant&gt;</div><div class="line">&lt;Grantee xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:type=&quot;CanonicalUser&quot;&gt;</div><div class="line">    &lt;ID&gt;user2-canonical-user-ID&lt;/ID&gt;</div><div class="line">    &lt;DisplayName&gt;display-name&lt;/DisplayName&gt;</div><div class="line">&lt;/Grantee&gt;</div><div class="line">&lt;Permission&gt;READ&lt;/Permission&gt;</div><div class="line">&lt;/Grant&gt;</div></pre></td></tr></table></figure>
</li>
<li><p>当为预定义组时，则<strong>Grantee</strong>中xsi:type为Group，Grant中以group的URI字段来展示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;Grant&gt;</div><div class="line">&lt;Grantee xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:type=&quot;Group&quot;&gt;</div><div class="line">    &lt;URI&gt;http://acs.amazonaws.com/groups/s3/LogDelivery&lt;/URI&gt;</div><div class="line">&lt;/Grantee&gt;</div><div class="line">&lt;Permission&gt;WRITE&lt;/Permission&gt;</div><div class="line">&lt;/Grant&gt;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>ACL 权限</p>
<ul>
<li>ACL只能几个设定好的权限集，针对Object和Bucket的ACL权限集相同。<br><img src="/images/AWS/S3/Master/s3-acl-permissions.png" alt="s3-acl-permissions"></li>
</ul>
</li>
<li><p><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/acl-overview.html#specifying-grantee-predefined-groups" target="_blank" rel="external">ACL 预定义组(ACL Predefined Groups)</a></p>
<ul>
<li>ACL能授权给预定义组<ul>
<li>授权预定义组时,在Grant中，使用Group URI，而不是Canonical ID</li>
</ul>
</li>
<li>几个预定义组<ul>
<li>经身份验证的用户组（Authenticated Users)<ul>
<li>表明可以允许任何AWS账号访问资源</li>
<li>Group URI=<a href="http://acs.amazonaws.com/groups/global/AuthenticatedUsers" target="_blank" rel="external">http://acs.amazonaws.com/groups/global/AuthenticatedUsers</a></li>
</ul>
</li>
<li>所有的用户组（All Users）<ul>
<li>表明可以允许世界上任何人进行访问，也就是Public</li>
<li>Group URI=<a href="http://acs.amazonaws.com/groups/global/AllUsers" target="_blank" rel="external">http://acs.amazonaws.com/groups/global/AllUsers</a></li>
</ul>
</li>
<li>日志传输组(Log Delivery Group)<ul>
<li>允许将S3访问日志写入Bucket中，用于S3 Bucket logging功能</li>
<li>Group URI=<a href="http://acs.amazonaws.com/groups/s3/LogDelivery" target="_blank" rel="external">http://acs.amazonaws.com/groups/s3/LogDelivery</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/acl-overview.html#canned-acl" target="_blank" rel="external">标准ACL(Canned ACL)</a></p>
<ul>
<li>AWS定义了一系列预定义的ACL授权，称为标准ACL(Canned ACL)<br><img src="/images/AWS/S3/Master/s3-acl-canned-acl.png" alt="s3-acl-canned-acl"></li>
<li>设置时只能使用其中的一个canned ACL</li>
<li>最常用于AWS CLI, 如下命令拷贝xxx.jpg到S3中，并且设置为public可读<ul>
<li><code>aws s3 cp xxx.jpg s3://bucket_name/public/ --acl public-read</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Bucket-amp-User-Policies"><a href="#Bucket-amp-User-Policies" class="headerlink" title="Bucket &amp; User Policies"></a>Bucket &amp; User Policies</h3><ul>
<li>什么是Bucket和User Policies<ul>
<li>Bucket和User Polices都是策略，以JSON格式定义，用来规定bucket和object的访问权限<ul>
<li>Bucket policies是attached在Bucket上的, 在S3中设置</li>
<li>User Policies是基于用户的，在IAM中设置</li>
</ul>
</li>
<li>Bucket或者IAM默认都是没有任何策略的</li>
</ul>
</li>
<li>Bucket &amp; User Policy特性<ul>
<li>都可以规定IAM User的访问权限。Bucket Policy还可以规定AWS root account的访问权限，而IAM Policy因为没法attach到root account，因此无法规定root account的权限</li>
<li>可以定义非常精细的访问权限</li>
<li>可以使用显式拒绝</li>
<li>可以设置带条件的访问权限</li>
<li>JSON格式</li>
<li>大小限制在20KB</li>
</ul>
</li>
<li>Policy的必备要素<ul>
<li>一个Policy包含如下几个元素<ul>
<li><a href="https://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/reference_policies_elements_principal.html" target="_blank" rel="external">Principal</a>服务委托人 - 被允许对某个资源进行某种操作的主体，只在Bucket Policy中使用。分为以下几种:<ul>
<li>特定AWS账户: <code>&quot;Principal&quot;: { &quot;AWS&quot;: &quot;arn:aws:iam::AWS-account-ID:root&quot; }</code></li>
<li>IAM用户: <code>&quot;Principal&quot;: { &quot;AWS&quot;: &quot;arn:aws:iam::AWS-account-ID:user/user-name&quot; }</code></li>
<li>联合身份用户(federated identity) - web identity federation<ul>
<li><code>&quot;Principal&quot;: { &quot;Federated&quot;: &quot;cognito-identity.amazonaws.com&quot; }</code></li>
<li><code>&quot;Principal&quot;: { &quot;Federated&quot;: &quot;www.amazon.com&quot; }</code></li>
<li><code>&quot;Principal&quot;: { &quot;Federated&quot;: &quot;graph.facebook.com&quot; }</code></li>
<li><code>&quot;Principal&quot;: { &quot;Federated&quot;: &quot;accounts.google.com&quot; }</code></li>
</ul>
</li>
<li>联合身份用户(federated identity) - SAML identity provider <ul>
<li><code>&quot;Principal&quot;: { &quot;Federated&quot;: &quot;arn:aws:iam::AWS-account-ID:saml-provider/provider-name&quot; }</code></li>
</ul>
</li>
<li>IAM角色(IAM Role)<ul>
<li><code>&quot;Principal&quot;: { &quot;AWS&quot;: &quot;arn:aws:iam::AWS-account-ID:role/role-name&quot; }</code></li>
</ul>
</li>
<li>AWS 服务<ul>
<li><code>&quot;Principal&quot;: { &quot;Service&quot;: &quot;elasticmapreduce.amazonaws.com&quot; }</code></li>
</ul>
</li>
<li>Everyone（匿名用户）<ul>
<li><code>&quot;Principal&quot;: &quot;*&quot;</code>或<code>&quot;Principal&quot; : { &quot;AWS&quot; : &quot;*&quot; }</code></li>
</ul>
</li>
<li>规范用户ID<ul>
<li><code>&quot;Principal&quot;: { &quot;CanonicalUser&quot;: &quot;79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be&quot; }</code></li>
</ul>
</li>
</ul>
</li>
<li><a href="https://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/reference_policies_elements_effect.html" target="_blank" rel="external">Effect</a> - 规则的结果<ul>
<li>有效值为Allow和Deny</li>
</ul>
</li>
<li><a href="https://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/reference_policies_elements_action.html" target="_blank" rel="external">Action</a> - 指定的策略权限，能进行何种操作<ul>
<li>AWS S3相关的所有策略权限，参考官网<a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/using-with-s3-actions.html" target="_blank" rel="external">AWS S3 在策略中指定权限</a></li>
<li>例子<ul>
<li>Amazon SQS操作 - <code>&quot;Action&quot;: &quot;sqs:SendMessage&quot;</code></li>
<li>Amazon EC2操作 - <code>&quot;Action&quot;: &quot;ec2:StartInstances&quot;</code></li>
</ul>
</li>
</ul>
</li>
<li><a href="https://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/reference_policies_elements_resource.html" target="_blank" rel="external">Resource</a> - 资源名<ul>
<li>Resource就是ARN(aws resource name)的集合</li>
<li><a href="https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html#genref-arns" target="_blank" rel="external">ARN</a>有三种格式<ul>
<li>格式一: arn:partition:service:region:account-id:resource<ul>
<li>例子 arn:aws:iam:us-west-2:123456789012:user/David</li>
</ul>
</li>
<li>格式二: arn:partition:service:region:account-id:resourcetype/resource<ul>
<li>例子 arn:aws:s3:::my_corporate_bucket/exampleobject.png</li>
</ul>
</li>
<li>格式三: arn:partition:service:region:account-id:resourcetype:resource<ul>
<li>例子 arn:aws:rds:eu-west-1:123456789012:db:mysql-db</li>
</ul>
</li>
<li><strong>其中全球区的partition为aws，而中国区的partition为aws-cn</strong></li>
</ul>
</li>
<li>Resource可以使用通配符*和?<ul>
<li>某个bucket中所有的object arn:aws:s3:::mypublicbucket/*</li>
</ul>
</li>
</ul>
</li>
<li><a href="https://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/reference_policies_elements_sid.html" target="_blank" rel="external">SID</a> - 每份策略的标识符             <ul>
<li>策略的名字，在整个Policy文件中需要唯一</li>
</ul>
</li>
</ul>
</li>
<li><strong>注意: 在User Policy中没有Principal，执行User Policy的主体就是Principal</strong></li>
</ul>
</li>
<li><p><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/amazon-s3-policy-keys.html" target="_blank" rel="external">Policy中的可选元素 – Condition</a></p>
<ul>
<li>Condition是一个可选元素</li>
<li>Condition元素可用来在Policy中授予条件时指定某些条件, 一条Policy可以有一条或者多条Condition</li>
<li>类似于在policy中添加了一个”if”语句</li>
<li>详细用法参见<a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/amazon-s3-policy-keys.html" target="_blank" rel="external">官网文档</a></li>
<li><p>一个Condition包含如下要素</p>
<ul>
<li>一个条件运算符(Condition Operators),例如StringEquals，NumericEquals以及IpAddress等，所有的条件运算符参见官网文档<a href="https://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/reference_policies_elements_condition_operators.html" target="_blank" rel="external">IAM JSON 策略元素：条件运算符</a></li>
<li>Key/Value的键值对，包含条件键（Condition Key)和允许的值（Value)</li>
<li>例子， 如下Condition的片段中，<code>StringEquals</code>就是条件运算符，<code>s3:x-amz-acl</code>就是条件键, <code>public-read</code>就是值。这个Condition会被解读为: s3:x-amz-acl的字符串等于(StringEquals) public-read的时候，Condition检查通过<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&quot;Condition&quot;: &#123;</div><div class="line">    &quot;StringEquals&quot;: &#123;</div><div class="line">        &quot;s3:x-amz-acl&quot;:[&quot;public-read&quot;]</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/example-bucket-policies.html" target="_blank" rel="external">用途举例</a></p>
<ul>
<li><p>允许用户只有在设置”public-read” ACL的时候才能”put” object</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;Version&quot;:&quot;2012-10-17&quot;,</div><div class="line">    &quot;Statement&quot;:[</div><div class="line">        &#123;</div><div class="line">        &quot;Sid&quot;:&quot;AddCannedAcl&quot;,</div><div class="line">        &quot;Effect&quot;:&quot;Allow&quot;,</div><div class="line">        &quot;Principal&quot;: &#123;&quot;AWS&quot;: [&quot;arn:aws:iam::111122223333:root&quot;,&quot;arn:aws:iam::444455556666:root&quot;]&#125;,</div><div class="line">        &quot;Action&quot;:[&quot;s3:PutObject&quot;,&quot;s3:PutObjectAcl&quot;],</div><div class="line">        &quot;Resource&quot;:[&quot;arn:aws:s3:::examplebucket/*&quot;],</div><div class="line">        &quot;Condition&quot;:&#123;&quot;StringEquals&quot;:&#123;&quot;s3:x-amz-acl&quot;:[&quot;public-read&quot;]&#125;&#125;</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>仅允许用户在开启加密的时候才允许”put” object, 例子参考官网博文<a href="https://aws.amazon.com/cn/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/" target="_blank" rel="external">How to Prevent Uploads of Unencrypted Objects to Amazon S3</a></p>
</li>
<li>仅允许用户在使用multi-factor验证的时候才能删除某个object<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</div><div class="line">    &quot;Id&quot;: &quot;123&quot;,</div><div class="line">    &quot;Statement&quot;: [</div><div class="line">    &#123;</div><div class="line">        &quot;Sid&quot;: &quot;&quot;,</div><div class="line">        &quot;Effect&quot;: &quot;Deny&quot;,</div><div class="line">        &quot;Principal&quot;: &quot;*&quot;,</div><div class="line">        &quot;Action&quot;: &quot;s3:*&quot;,</div><div class="line">        &quot;Resource&quot;: &quot;arn:aws:s3:::examplebucket/taxdocuments/*&quot;,</div><div class="line">        &quot;Condition&quot;: &#123; &quot;Null&quot;: &#123; &quot;aws:MultiFactorAuthAge&quot;: true &#125;&#125;</div><div class="line">    &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>带有Condition的完整Policy例子</p>
<ul>
<li>允许来自源IP在192.168.143.0/24的任何人获取Bucket examplebucket中的任意object，但IP地址192.168.143.188除外。<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</div><div class="line">    &quot;Id&quot;: &quot;S3PolicyId1&quot;,</div><div class="line">    &quot;Statement&quot;: [</div><div class="line">        &#123;</div><div class="line">            &quot;Sid&quot;: &quot;statement1&quot;,</div><div class="line">            &quot;Effect&quot;: &quot;Allow&quot;,</div><div class="line">            &quot;Principal&quot;: &quot;*&quot;,</div><div class="line">            &quot;Action&quot;:[&quot;s3:GetObject&quot;]  ,</div><div class="line">            &quot;Resource&quot;: &quot;arn:aws:s3:::examplebucket/*&quot;,</div><div class="line">            &quot;Condition&quot; : &#123;</div><div class="line">                &quot;IpAddress&quot; : &#123;</div><div class="line">                    &quot;aws:SourceIp&quot;: &quot;192.168.143.0/24&quot; </div><div class="line">                &#125;,</div><div class="line">                &quot;NotIpAddress&quot; : &#123;</div><div class="line">                    &quot;aws:SourceIp&quot;: &quot;192.168.143.188/32&quot; </div><div class="line">                &#125; </div><div class="line">            &#125; </div><div class="line">        &#125; </div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>记忆Policy元素的小技巧，<a href="https://acloud.guru/" target="_blank" rel="external">AcloudGuru</a>课程中有一个容易记忆的方法</p>
<ul>
<li>总共6个要素(Condition可选)<ul>
<li><strong>C</strong>onditions</li>
<li><strong>P</strong>rincipal</li>
<li><strong>E</strong>ffect</li>
<li><strong>A</strong>ction</li>
<li><strong>R</strong>esource</li>
<li><strong>S</strong>ID</li>
</ul>
</li>
<li>对于Bucket policy来说<ul>
<li>I “C PEARS” – 字母C读成成see，意为”我在Bucket Policy中看见了梨子”</li>
</ul>
</li>
<li>对于IAM Policy来说<ul>
<li>I “C EARS” – 字母C读成see，意为”我在IAM Policy中看见了耳朵”</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="跨账户策略注意点"><a href="#跨账户策略注意点" class="headerlink" title="跨账户策略注意点"></a>跨账户策略注意点</h3><ol>
<li><p>跨账户访问时，假设账户A设置了允许账户B上传object到A的Bucket中, 当账户B上传object到A的Bucket中时，object的owner是B，A默认是没有权限访问该Object的。<br>可以通过设置Bucket Policy的方式，让账户B必须要授权A能够完全控制B上传的object。<br>官网文章 <a href="https://aws.amazon.com/cn/premiumsupport/knowledge-center/s3-bucket-owner-access/" target="_blank" rel="external">How can I make sure the bucket owner has access to resources that are copied or moved between Amazon S3 buckets owned by different AWS accounts?</a></p>
</li>
<li><p>跨账户授权时，如果通过Bucket Policy授权其他账户的IAM用户访问自己的Bucket，IAM用户还需要他自己的Root账户也同时授权权限访问该Bucket，否则IAM用户没有权限访问该Bucket。  被授权的其他账户的IAM账户，仍然需要自己的root账户添加授权permission才能访问源Bucket。<br>如果账户A使用Bucket Policy授权账户B的User C能够访问账户A的Bucket_A, 此时User C还会没法访问Bucket_A, 必须账户B在给User C的Policy中添加允许访问Bucket_A的策略后，User C才能访问Bucket_A。<br><strong>总结</strong>为: 如果一个Bucket要授权给其他账号的IAM User使用时，必须要两个Account都显式授权IAM User访问权限才行。</p>
</li>
</ol>
<h3 id="使用签名-signed-的方式访问私有Object"><a href="#使用签名-signed-的方式访问私有Object" class="headerlink" title="使用签名(signed)的方式访问私有Object"></a>使用签名(signed)的方式访问私有Object</h3><h4 id="什么是签名"><a href="#什么是签名" class="headerlink" title="什么是签名"></a>什么是签名</h4><p>AWS提供了一个<a href="https://docs.aws.amazon.com/zh_cn/general/latest/gr/signing_aws_api_requests.html" target="_blank" rel="external">签署签名</a>的功能，在HTTP请求中附加签名信息后，AWS能够识别发送请求的用户,并验证请求是否是该用户授权签名的。</p>
<p>AWS S3中可以使用该功能来短暂的让用户访问或操作某个私有Object。<br>Bucket的Owner通过指定如下信息来生成一个pre-signed的URL</p>
<ul>
<li>加密凭证(Security credentials)</li>
<li>Bucket名字</li>
<li>Object名字</li>
<li>HTTP方法(GET, PUT, DELETE等)</li>
<li>过期时间</li>
</ul>
<p>得到pre-signed URL的任何人，都能够在指定的过期时间之前，访问指定的object。</p>
<h4 id="签名版本"><a href="#签名版本" class="headerlink" title="签名版本"></a>签名版本</h4><p>AWS提供了两种签名版本Signature Version 4 和Signature Version 2。<br>对于S3来说，所有的Region都支持Signature Version 4，而Signature Version 2的签名方式，只有在2014年1月30日之前的Region才支持，在此日期之后建立的Region，一律都只支持Signature Version 4的签名方式。此处是官方说明文档<a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/API/sig-v4-authenticating-requests.html" target="_blank" rel="external">Authenticating Requests (AWS Signature Version 4)</a></p>
<p>AWS CLI生成pre-signed的一个例子, 拿韩国首尔Region为例。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 不指定Region的话，aws s3 cli会使用Signature Version 2方式签名</div><div class="line">$ aws s3 presign s3://carl-test-at-seoul/fox.jpg --expires-in 3600</div><div class="line">https://carl-test-at-seoul.s3.amazonaws.com/fox.jpg?AWSAccessKeyId=AKIAIJQFRFPQCLKRF5MA&amp;Signature=M1teYMCFbGKlfOBt0d%2BAgl8NvFs%3D&amp;Expires=1523590452</div><div class="line">$</div></pre></td></tr></table></figure></p>
<p>但是首尔Region是不支持该签名方式的，因此使用生成的上述URL访问S3时，会报告如下错误<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&lt;Error&gt;</div><div class="line">&lt;Code&gt;InvalidRequest&lt;/Code&gt;</div><div class="line">&lt;Message&gt;The authorization mechanism you have provided is not supported. Please use AWS4-HMAC-SHA256.&lt;/Message&gt;</div><div class="line">&lt;RequestId&gt;F7B09345800A56B8&lt;/RequestId&gt;</div><div class="line">&lt;HostId&gt;Fjr3JFqFxLI7Ar2HYyOJk/ZFuMfreIFa5CnFRBq8A1J8/g9/3XbaK3bkiY2gXtNFuZ6AcN/+8Is=&lt;/HostId&gt;</div><div class="line">&lt;head&gt;</div><div class="line">&lt;link/&gt;</div><div class="line">&lt;/head&gt;</div><div class="line">&lt;/Error&gt;</div></pre></td></tr></table></figure></p>
<p>在AWS CLI中指定首尔Region后，CLI会判断Region，从而使用Signature Version 4的加密方式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ aws s3 presign s3://carl-test-at-seoul/fox.jpg --expires-in 3600 --region ap-northeast-2</div><div class="line">https://carl-test-at-seoul.s3.amazonaws.com/fox.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIJQFRFPQCLKRF5MA%2F20180413%2Fap-northeast-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20180413T023641Z&amp;X-Amz-Expires=3600&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=a97309f2de68d16afcc3f380dec5ec06d0de09f3b748081598107a0b3842c255</div><div class="line">$</div></pre></td></tr></table></figure></p>
<p>使用上述生成的URL，就能够顺利访问到原本私有的Object了。</p>
<h3 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h3><ul>
<li><a href="/2018/04/18/aws-s3-master-basic/" title="S3进阶系列(一) —— S3基础知识">S3进阶系列(一) —— S3基础知识</a>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/Welcome.html" target="_blank" rel="external">S3官方文档</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> IAM </tag>
            
            <tag> S3 </tag>
            
            <tag> AWS CLI </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何去除SSH key中的passphrase]]></title>
      <url>/2018/04/19/how-to-remove-the-passphrase-of-ssh-key/</url>
      <content type="html"><![CDATA[<h3 id="去除passphrase的方法"><a href="#去除passphrase的方法" class="headerlink" title="去除passphrase的方法"></a>去除passphrase的方法</h3><p>使用<code>ssh-keygen</code>来重新设置一个空的passphrase, 就相当于去除了原来的passphrase, 答案来自<a href="https://stackoverflow.com/a/112409" target="_blank" rel="external">stackoverflow</a>, 回答者还很贴心的提示，输入的passphrase会被记录在~/.bash_history中，别忘记处理这个情况。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ssh-keygen -p [-P old_passphrase] [-N new_passphrase] [-f keyfile]</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ol>
<li><p>首先使用ssh-keygen生成一对带passphrase的key, 此处输入的passphrase为helloworld.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">$ ssh-keygen -f ./id_rsa_for_passphrase</div><div class="line">Generating public/private rsa key pair.</div><div class="line">Enter passphrase (empty <span class="keyword">for</span> no passphrase):</div><div class="line">Enter same passphrase again:</div><div class="line">Your identification has been saved <span class="keyword">in</span> ./id_rsa_for_passphrase.</div><div class="line">Your public key has been saved <span class="keyword">in</span> ./id_rsa_for_passphrase.pub.</div><div class="line">The key fingerprint is:</div><div class="line">SHA256:l40jWfZPFe7ItcUJvduSyx/CXVmI27T6y75i/yuAPxs carlshen@carl-186.local</div><div class="line">The key<span class="string">'s randomart image is:</span></div><div class="line"><span class="string">+---[RSA 2048]----+</span></div><div class="line"><span class="string">|             ... |</span></div><div class="line"><span class="string">|             .o+o|</span></div><div class="line"><span class="string">|          o . o+*|</span></div><div class="line"><span class="string">|         + =.+++=|</span></div><div class="line"><span class="string">|        S * +o+=+|</span></div><div class="line"><span class="string">|         + o.++.o|</span></div><div class="line"><span class="string">|          .Eo+.= |</span></div><div class="line"><span class="string">|           o+++ .|</span></div><div class="line"><span class="string">|           oo+O*+|</span></div><div class="line"><span class="string">+----[SHA256]-----+</span></div><div class="line"><span class="string">$</span></div><div class="line"><span class="string">$ ll</span></div><div class="line"><span class="string">total 16</span></div><div class="line"><span class="string">-rw-------  1 carlshen  staff  1766  4 19 16:05 id_rsa_for_passphrase</span></div><div class="line"><span class="string">-rw-r--r--  1 carlshen  staff   405  4 19 16:05 id_rsa_for_passphrase.pub</span></div><div class="line"><span class="string">$</span></div></pre></td></tr></table></figure>
</li>
<li><p>去除private key的passphrase</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ ssh-keygen -p -P helloworld -N <span class="string">""</span> -f id_rsa_for_passphrase</div><div class="line">Your identification has been saved with the new passphrase.</div><div class="line">$</div><div class="line">$ ll</div><div class="line">total 16</div><div class="line">-rw-------  1 carlshen  staff  1679  4 19 16:07 id_rsa_for_passphrase</div><div class="line">-rw-r--r--  1 carlshen  staff   405  4 19 16:05 id_rsa_for_passphrase.pub</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
<li><p>使用去除passphrase的private key来生成public key的内容，此时已经不需要输入passphrase。同原先的public key比较，可以看出内容是相同的。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ ssh-keygen -y -f id_rsa_for_passphrase</div><div class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCnIM/b01uMtYWlEyWN7RsIynhTPqodAaMkK5jhWqrLj29FUFHbSZwXaMlbxi1yxsp9VHX/YjFXQjesUv+wv+We2I9gNN/emd1zIrFDIU1sLZTwdPPsZ/nBA9e19tncVvUHk07wFmFGE1pH7mCpVHSjYgqhJYKZFn5RVcTn7lir4pIvjGl94+wNCtTueMsAiH8K+F3gcivwFQK9Gng7Fiv1PKwBVuJzlLabM90uaFuGcaVo7s+PE3E70TReXsRkUYCR5CtA4ja4JIVf1rMt0WwSb09KnmRFanEfEYPeZX7I44EPIYEAJWRccOTbWb/ywd5tbKJhgJBnTzcsxtvZHR/v</div><div class="line">$</div><div class="line">$ cat id_rsa_for_passphrase.pub</div><div class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCnIM/b01uMtYWlEyWN7RsIynhTPqodAaMkK5jhWqrLj29FUFHbSZwXaMlbxi1yxsp9VHX/YjFXQjesUv+wv+We2I9gNN/emd1zIrFDIU1sLZTwdPPsZ/nBA9e19tncVvUHk07wFmFGE1pH7mCpVHSjYgqhJYKZFn5RVcTn7lir4pIvjGl94+wNCtTueMsAiH8K+F3gcivwFQK9Gng7Fiv1PKwBVuJzlLabM90uaFuGcaVo7s+PE3E70TReXsRkUYCR5CtA4ja4JIVf1rMt0WwSb09KnmRFanEfEYPeZX7I44EPIYEAJWRccOTbWb/ywd5tbKJhgJBnTzcsxtvZHR/v carlshen@carl-186.local</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://stackoverflow.com/questions/112396/how-do-i-remove-the-passphrase-for-the-ssh-key-without-having-to-create-a-new-ke" target="_blank" rel="external">How do I remove the passphrase for the SSH key without having to create a new key?</a></li>
<li><a href="https://serverfault.com/questions/426394/how-to-check-if-a-rsa-public-private-key-pair-matched" target="_blank" rel="external">How to check if a RSA public / private key pair matched</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Shell </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> SSH </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[S3进阶系列(一) —— S3基础知识]]></title>
      <url>/2018/04/18/aws-s3-master-basic/</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>S3是AWS服务最早推出的几项服务之一，也是使用最为频繁的几个AWS服务之一。官网文档以及网上文章零零散散看了不少,实际使用中也用过S3的大多数功能，但“纸上得来终觉浅，绝知此事要躬行”，决定给自己挖个坑，结合已读资料和实际使用经验，系统地来梳理一下S3的已有功能。</p>
<p>本篇是第一篇,介绍一下S3的一些基础知识。希望能够坚持把这个系列写完。</p>
<a id="more"></a>
<h3 id="S3-是什么"><a href="#S3-是什么" class="headerlink" title="S3 是什么"></a>S3 是什么</h3><ul>
<li>Amazon Simple Storage Service(简称S3)，是亚马逊AWS服务在2006年第一个正式对外推出的云计算服务。</li>
<li>Amazon S3 是互联网存储解决方案。该服务旨在降低开发人员进行网络规模级计算的难度。<br>  Amazon S3 提供了一个简单 Web 服务接口，可用于随时在 Web 上的任何位置存储和检索任何数量的数据。此服务让所有开发人员都能访问同一个具备高扩展性、可靠性、安全性和快速价廉的数据存储基础设施， Amazon 用它来运行其全球的网站网络。此服务旨在为开发人员带来最大化的规模效益。</li>
<li>对象存储<ul>
<li>数据(包括文件/视频/图片)以及相关的源数据都是以对象(objects)的方式来存储的</li>
<li>不能以S3来当做操作系统的文件系统</li>
<li>对象（object）最大支持5TB</li>
</ul>
</li>
<li>高耐用<ul>
<li>Object 提供11个9，99.999999999%的持久性<ul>
<li>意味着每一亿个object才会丢失一个object</li>
<li>S3存储的对象会在同一个Region的多个AZ中保存多份拷贝</li>
</ul>
</li>
</ul>
</li>
<li>高可用<ul>
<li>提供99.99%的可用性</li>
</ul>
</li>
<li>高扩展<ul>
<li>提供无限的存储空间</li>
</ul>
</li>
<li>基于WEB<ul>
<li>上传和下载数据基于HTTP/HTTPS请求</li>
</ul>
</li>
<li>安全性<ul>
<li>可以选择多种方式来加密数据</li>
</ul>
</li>
<li>计费模式<ul>
<li>Pay as you go - Pay only for what you use</li>
</ul>
</li>
</ul>
<h3 id="S3-能用来做什么"><a href="#S3-能用来做什么" class="headerlink" title="S3 能用来做什么"></a>S3 能用来做什么</h3><ul>
<li>备份</li>
<li>存储内容</li>
<li>大数据分析</li>
<li>挂载静态网站</li>
<li>灾难恢复</li>
</ul>
<h3 id="S3桶-Bucket"><a href="#S3桶-Bucket" class="headerlink" title="S3桶(Bucket)"></a><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/UsingBucket.html" target="_blank" rel="external">S3桶(Bucket)</a></h3><ul>
<li>Bucket就是用来存储对象（object）的一个集合<ul>
<li>Bucket和Object都是资源(resources), 也就是aws能操作的一个实体(entity)</li>
</ul>
</li>
<li>可以容纳无限的Object</li>
<li>默认一个账户可以创建100个bucket，这个上限是软性限制，可以通过向AWS提交case来提高上限</li>
<li>Bucket需要归属于某个Region，不是Global的。虽然S3的web console页面是Global的。</li>
<li>Bucket由附属的子资源(subresources)来定义bucket的配置</li>
</ul>
<h3 id="S3-命名空间-Namespace"><a href="#S3-命名空间-Namespace" class="headerlink" title="S3 命名空间(Namespace)"></a>S3 命名空间(Namespace)</h3><ul>
<li>S3需要唯一的命名空间<ul>
<li>Bucket名字必须在AWS的整个S3生态中是唯一的,而不仅仅是只在Region中唯一。</li>
<li>访问Bucket的方式<ul>
<li>Virtual<ul>
<li><a href="https://bucket.s3.amazonaws.com" target="_blank" rel="external">https://bucket.s3.amazonaws.com</a></li>
<li><a href="https://bucket.s3-aws-region.amazonaws.com" target="_blank" rel="external">https://bucket.s3-aws-region.amazonaws.com</a></li>
</ul>
</li>
<li>Path<ul>
<li><a href="https://s3-aws-region.amazonaws.com/bucket" target="_blank" rel="external">https://s3-aws-region.amazonaws.com/bucket</a></li>
</ul>
</li>
<li>Bucket名字必须要全局唯一的原因就是因为Bucket名字也会是域名的一部分</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="区域-Region"><a href="#区域-Region" class="headerlink" title="区域(Region)"></a>区域(Region)</h3><ul>
<li>Region就是bucket实际存放的位置</li>
<li>存放在某个Region的Object永远不会离开这个Region，除非你显式地将它传输出去</li>
<li>选取Bucket的Region的时候，需要考虑延迟，费用等因素。因为建造和运营成本不同，AWS各个Region的S3费用并不相同。而一般来说，选择最近的Region，延迟就越小。</li>
</ul>
<h3 id="对象Object"><a href="#对象Object" class="headerlink" title="对象Object"></a><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/UsingObjects.html" target="_blank" rel="external">对象Object</a></h3><ul>
<li>S3是一个可以存储近似无限数量Object的Key-Value存储系统</li>
<li>Object包括如下元素<ul>
<li>Key - object的名字</li>
<li>Value - 存储的数据，大小范围是0~5TB</li>
<li>Version ID - 当versioning功能开启后，每个Object都会拥有一个Version ID，用来区分有相同Key的不同的Object<ul>
<li>Bucket + Key + Version ID 三要素唯一定义了S3中的一个Object</li>
</ul>
</li>
<li>Metadata - 用来存储Object其他相关信息的Name-value键值对</li>
<li>Subresources - 定义了一个Object的额外的资源</li>
<li>Access Control Information - 控制每个对象的访问策略</li>
</ul>
</li>
</ul>
<h3 id="Object名字"><a href="#Object名字" class="headerlink" title="Object名字"></a><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/UsingMetadata.html" target="_blank" rel="external">Object名字</a></h3><ul>
<li>S3是平面结构的<ul>
<li>和文件系统不同，S3没有目录</li>
<li>可以通过使用<strong>prefixed</strong>来模拟目录</li>
<li>Object的键的名称是一序列的 Unicode 字符，它的 UTF-8 编码长度最大为 1024 个字节</li>
<li>强烈建议Object的命名要符合DNS命名要求，采用如下字符集<ul>
<li>字母数字字符 [0-9a-zA-Z]和特殊字符 !、-、_、.、*、’、( 以及 )</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="对象标签-Object-Tagging"><a href="#对象标签-Object-Tagging" class="headerlink" title="对象标签(Object Tagging)"></a><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/object-tagging.html" target="_blank" rel="external">对象标签(Object Tagging)</a></h3><ul>
<li>利用对象标签，您可以对存储进行分类。每个标签都是一个键-值对<ul>
<li>例如<ul>
<li>Department=Development</li>
<li>DeployEnv=demo</li>
</ul>
</li>
<li>好处<ul>
<li>可以支持精细的访问控制</li>
<li>可以支持精细的生命周期管理(Lifecycle management)</li>
<li>使用S3 Analytics时，可以使用tag来配置筛选条件</li>
<li>可以自定义Amazon CloudWatch和CloudTrail的筛选条件</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="子资源-Subresources"><a href="#子资源-Subresources" class="headerlink" title="子资源(Subresources)"></a>子资源(Subresources)</h3><ul>
<li>子资源<ul>
<li>S3子资源提供了存储和管理Bucket配置信息的支持</li>
<li>S3子资源只能依附于某一个确定的bucket或者object</li>
<li>bucket和object可以有一组关联的子资源</li>
<li>S3子资源必须归属于某个Bucket或者Object，而不能独立存在</li>
<li>Bucket和Object分别提供了不同的子资源</li>
</ul>
</li>
<li><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/UsingBucket.html" target="_blank" rel="external">Bucket提供的子资源</a><ul>
<li>位置(Location)</li>
<li>策略(Policy)和访问控制列表(ACL)</li>
<li>跨资源共享(CORS)</li>
<li>静态网站托管(Static Website Hosting)</li>
<li>日志记录(Logging)</li>
<li>时间通知(Event Notification)</li>
<li>版本控制(Versioning)</li>
<li>生命周期(Lifecycle)</li>
<li>跨区域复制(Cross-Region Replication)</li>
<li>标记(Tagging)</li>
<li>下载人员付费(RequestPayment)</li>
<li>传输加速(Transfer Acceleration)</li>
</ul>
</li>
<li><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/ObjectAndSoubResource.html" target="_blank" rel="external">Object提供的子资源</a><ul>
<li>访问控制列表(ACL)</li>
<li>torrent (用于支持BitTorrent协议)</li>
</ul>
</li>
</ul>
<h3 id="一致性模型-Consistency-Model"><a href="#一致性模型-Consistency-Model" class="headerlink" title="一致性模型(Consistency Model)"></a><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/Introduction.html#ConsistencyModel" target="_blank" rel="external">一致性模型(Consistency Model)</a></h3><ul>
<li>对put的新object，S3提供了read-after-write的一致性<ul>
<li>read-after-write: 只有在对象成功写入所有的设备，返回成功的返回值后，对象才能被读取</li>
</ul>
</li>
<li>对更新和删除操作，S3提供了最终一致性(eventual consistency)<ul>
<li>正在更新时，此时请求对象，旧数据可能会被返回</li>
<li>在删除旧数据时，此时请求对象，旧数据仍然可能会被返回</li>
</ul>
</li>
<li>最终一致性（Eventual consistency）提供了低延迟和高吞吐的性能</li>
<li>S3不提供锁的机制<ul>
<li>如果两个写操作并发，结果最终会以最后一个为准</li>
<li>如果需要类似锁的机制，那只能在使用S3的Application层进行控制</li>
</ul>
</li>
</ul>
<h3 id="存储类别-Storage-Classes-Tiers"><a href="#存储类别-Storage-Classes-Tiers" class="headerlink" title="存储类别(Storage Classes/Tiers)"></a><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/storage-class-intro.html" target="_blank" rel="external">存储类别(Storage Classes/Tiers)</a></h3><ul>
<li>S3提供了如下几种存储类型<ul>
<li>S3 Standard: 默认的存储类型，提供99.99%可用性，99.999999999%持久性。数据文件会冗余地存储在多个设施的多个设备中。设计目的是可以支持同时两个设施损坏。适合性能敏感的使用案例和需要经常访问的数据。</li>
<li>S3 Standard - Infrequent Access: 提供99.99%可用性，99.999999999%持久性。 针对需要长期储存且不常访问, 但是需要立即能够访问到的数据 (例如备份以及访问频率已减少的较旧数据) 进行了优化。存储费用比S3 Standard要低，但是每个访问请求的费用要远高于S3 Standard。</li>
<li>Reduced Redundancy Storage(RRS): 提供99.99%可用性和99.99%的持久性。适用于能再次生成的数据，比如图片的缩略图等。</li>
<li>Glacier: 最便宜的存储方案，但是没法实时访问数据，必须要先恢复数据后才能进行访问。不适用于需要经常访问的数据。</li>
</ul>
</li>
<li>S3 提供了生命管理周期（Lifecycle Management）的机制，可以将Object从S3 Standard转换到S3 Standard-Infrequent Access，再到Glacier。也可以在设定的时间过后自动删除object。但需要注意的是，不是一到达设定的时间后就立马开始转换，设定Lifecycle Management后，S3会将需要转换的object标记上转换时间，实际转换时间一般会延后。</li>
<li><a href="https://aws.amazon.com/cn/s3/storage-classes/" target="_blank" rel="external">存储类别之间额比较如下</a>:<br><img src="/images/AWS/S3/Master/s3-storage-class.png" alt="s3-storage-class"></li>
</ul>
<h3 id="安全性"><a href="#安全性" class="headerlink" title="安全性"></a>安全性</h3><ul>
<li>S3提供了11个9的持久性和99.99%的可用性<ul>
<li>提供数据的checksums机制，如果数据有损坏，可以使用冗余数据来修复</li>
<li>跨域复制(Cross-Region replication)提供了更强的数据保护措施</li>
</ul>
</li>
<li>版本控制(Versioning)，可以提供额外的保护。开启Versioning后，提供了存储在S3中每个Object的每个版本的恢复能力，即使Object已经被删除。</li>
<li>S3的Object默认是私有的，只有Bucket和object的拥有者才能访问他们创建的资源<ul>
<li>可以通过Policies和ACL来授予访问objects和buckets的权限</li>
</ul>
</li>
<li>S3提供了服务端加密和传输加密的功能</li>
<li>所有访问S3 resources的请求都可被记录，用于审计</li>
</ul>
<h3 id="发送请求的方式"><a href="#发送请求的方式" class="headerlink" title="发送请求的方式"></a>发送请求的方式</h3><ul>
<li>S3是一个RESTful的web service<ul>
<li>提供http&amp;https的交互方式</li>
<li>通过REST API来发送请求</li>
</ul>
</li>
<li>可以通过以下几种方式来操作S3<ul>
<li>AWS Management console</li>
<li>AWS CLI (Command Line Interface)</li>
<li>AWS SDK’s (Software Development Kits)</li>
</ul>
</li>
<li>操作方式<ul>
<li>GET 对等于 Download/Read</li>
<li>PUT 对等于 Upload/Write</li>
<li>DELETE 对等于 Delete</li>
</ul>
</li>
</ul>
<h3 id="计费"><a href="#计费" class="headerlink" title="计费"></a>计费</h3><ul>
<li>使用多少就计费多少的原则</li>
<li>计费项目<ul>
<li>存储（Storage）</li>
<li>请求（Requests）</li>
<li>数据传输费用（Data Transfer Pricing）</li>
<li>数据传输加速（Transfer Acceleration）</li>
<li>管理功能（Management Functions）<ul>
<li>Metrics</li>
<li>存储类别分析（Storage Class Analysis）</li>
<li>S3存储清单(S3 Inventory)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h3><ul>
<li><a href="/2017/12/20/expired-of-S3-RRS/" title="S3 RRS价格注意点">S3 RRS价格注意点</a></li>
<li><a href="/2018/04/21/aws-s3-master-permission/" title="S3进阶系列(二) ——权限管理">S3进阶系列(二) ——权限管理</a>
</li>
</ul>
<h3 id="小故事"><a href="#小故事" class="headerlink" title="小故事"></a>小故事</h3><p>2017-03-01当天，AWS的一个程序员在调试系统时，输错了一个字母，导致US-EAST-1 region的大量S3服务器被误删除，导致大半个互联网将近瘫痪了4个小时。</p>
<p>问题发生后，AWS的<a href="http://status.aws.amazon.com/" target="_blank" rel="external">SERVICE HEALTH DASHBOARD</a>却显示Service一切正常。听说是因为Dashboard本身也是依赖于US-EAST-1的S3的, S3出了问题导致Dashboard也出问题了。</p>
<p>从这个小事故中可以看出AWS S3服务应用的普遍程度。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/Welcome.html" target="_blank" rel="external">S3官方文档</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> S3 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[订阅RDS事件]]></title>
      <url>/2018/03/16/how-to-subscribe-RDS-event/</url>
      <content type="html"><![CDATA[<p>关于RDS状态的监控，除了常规的CloudWatch Metric以外。RDS还有一个事件机制，能够将RDS的事件通知到SNS。</p>
<h3 id="RDS-Event简介"><a href="#RDS-Event简介" class="headerlink" title="RDS Event简介"></a>RDS Event简介</h3><h4 id="RDS-Event-Source-type"><a href="#RDS-Event-Source-type" class="headerlink" title="RDS Event - Source type"></a>RDS Event - Source type</h4><p>RDS的事件类型分为如下六大类:</p>
<ul>
<li>数据库实例(DB instance)</li>
<li>数据库集群(DB cluster)</li>
<li>数据库快照(DB snapshot)</li>
<li>数据库集群快照(DB cluster snapshot)</li>
<li>数据库安全组(DB security group)</li>
<li>数据库参数组(DB parameter group)</li>
</ul>
<a id="more"></a>
<h4 id="RDS-Event-Event-Categories"><a href="#RDS-Event-Event-Categories" class="headerlink" title="RDS Event - Event Categories"></a>RDS Event - Event Categories</h4><p>每个事件大类下还有不同类别的事件类型，各个事件类型下的类别如下:</p>
<ul>
<li>数据库实例(DB instance)的Event Categories<br><img src="/images/AWS/RDS/rds_event_source_type_instance.png" alt="rds_event_source_type_instance"></li>
<li>数据库集群(DB cluster)的Event Categories<br><img src="/images/AWS/RDS/rds_event_source_type_db_cluster.png" alt="rds_event_source_type_db_cluster"></li>
<li>数据库快照(DB snapshot)的Event Categories<br><img src="/images/AWS/RDS/rds_event_source_type_snapshots.png" alt="rds_event_source_type_snapshots"></li>
<li>数据库集群快照(DB cluster snapshot)的Event Categories<br><img src="/images/AWS/RDS/rds_event_source_type_db_cluster_snapshots.png" alt="rds_event_source_type_db_cluster_snapshots"></li>
<li>数据库安全组(DB security group)的Event Categories<br><img src="/images/AWS/RDS/rds_event_source_type_security_group.png" alt="rds_event_source_type_security_group"></li>
<li>数据库参数组(DB parameter group)的Event Categories<br><img src="/images/AWS/RDS/rds_event_source_type_parameter_group.png" alt="rds_event_source_type_parameter_group"></li>
</ul>
<h4 id="RDS-Event-Event-ID"><a href="#RDS-Event-Event-ID" class="headerlink" title="RDS Event - Event ID"></a>RDS Event - Event ID</h4><p>每个Event Category下还有多个Event，太多了，没法一一罗列了。详情可以参见<a href="https://docs.aws.amazon.com/zh_cn/AmazonRDS/latest/UserGuide/USER_Events.html" target="_blank" rel="external">AWS官网文档</a></p>
<h3 id="RDS-Event和CloudWatch-Metric比较"><a href="#RDS-Event和CloudWatch-Metric比较" class="headerlink" title="RDS Event和CloudWatch Metric比较"></a>RDS Event和CloudWatch Metric比较</h3><p>RDS的CloudWatch Metric主要针对的是RDS实例内在运行情况的监控，比如CPU利用率，DB连接，剩余磁盘空间，剩余内存，写和读的IOPS等。</p>
<p>而RDS的Event是对RDS的配置组以及Instance实例的状态变更的监控。比如Instance的创建，删除，备份和是否可用等；参数组是否修改了；快照的创建，删除等。</p>
<h3 id="创建RDS-Event"><a href="#创建RDS-Event" class="headerlink" title="创建RDS Event"></a>创建RDS Event</h3><p>RDS的Console正在改版，虽然设置原理都是一样的，但是在界面和操作的交互上，新Console和老Console还是有所区别的。</p>
<h4 id="老Console中设置RDS-Event"><a href="#老Console中设置RDS-Event" class="headerlink" title="老Console中设置RDS Event"></a>老Console中设置RDS Event</h4><ol>
<li><p>进入RDS Console，左侧栏中选中“Event Subscriptions”,在展开的页面中点击“Create Event Subscription”来创建Event Subscription<br><img src="/images/AWS/RDS/rds_event_old_console_create_event_subscription.png" alt="rds_event_old_console_create_event_subscription"></p>
</li>
<li><p>在“Create Event Subscription”页面中，填入对应的内容</p>
<ul>
<li>Name一栏填入要设置的Event的名字，此处设为rds-mydb-instance-event</li>
<li>Send notifications to一栏选择要发送的SNS Topic，如果没有，可以点击旁边的create topic按钮新建一个SNS Topic</li>
<li>Source Type一栏，根据实际需要选择要监控的Event类型，此处选择Instances</li>
<li>Enabled一栏，可以设置是否将这个Subscription设为开启状态</li>
<li>Event Categories一栏，选上要监控的Event类别, 此处可以多选。在Mac下，按住Command键后再点击鼠标就能够选取多个。图中选择了availability，deletion，failover，failure，low storage和recovery这六个Event Categories。</li>
<li>Instances一栏，可以选择只对某些RDS Instance监控,或者对全部RDS Instance进行监控<br><img src="/images/AWS/RDS/rds_event_old_console_create_event_subscription_detail.png" alt="rds_event_old_console_create_event_subscription_detail"></li>
</ul>
</li>
<li><p>点击Create建立Subscription</p>
</li>
<li><p>在Console中就能够看到Subscription正在创建中，等待一小段时间后，就会显示Subscription创建成功了。<br><img src="/images/AWS/RDS/rds_event_old_console_create_event_subscription_list.png" alt="rds_event_old_console_create_event_subscription_list"></p>
</li>
<li><p>当有监控的事件发生时，AWS就会发送通知到设置的SNS Topic中。SNS再以Email，SMS或者HTTP Endpoint的形式发送给Topic的订阅者。设置完后，在SNS中订阅的邮件地址马上就收到了一封主题为“RDS Notification Message”的邮件。提示将会收到来自SNS的通知，如果不需要，可以退订。</p>
</li>
</ol>
<h4 id="新Console中设置RDS-Event"><a href="#新Console中设置RDS-Event" class="headerlink" title="新Console中设置RDS Event"></a>新Console中设置RDS Event</h4><ol>
<li><p>进入RDS Console，左侧栏中选中“Event Subscriptions”,在展开的页面中点击“Create Event Subscription”来创建Event Subscription<br><img src="/images/AWS/RDS/rds_event_new_console_create_event_subscription.png" alt="rds_event_new_console_create_event_subscription"></p>
</li>
<li><p>在“Create Event Subscription”页面中，填入对应的内容</p>
<ul>
<li>Name一栏填入要设置的Event的名字，此处设为rds-mydb-instance-event</li>
<li>Enabled一栏，可以设置是否将这个Subscription设为开启状态</li>
<li>Target一栏，选择已有的SNS Topic或者新建一个Topic<br><img src="/images/AWS/RDS/rds_event_new_console_create_event_subscription_detail_1.png" alt="rds_event_new_console_create_event_subscription_detail_1"></li>
</ul>
</li>
<li><p>再往下翻,设置Event。<br> 3.1 初始画面，只有Source Type可选。<br> <img src="/images/AWS/RDS/rds_event_new_console_create_event_subscription_detail_source_type.png" alt="rds_event_new_console_create_event_subscription_detail_source_type"><br> 3.2 Source Type选择Instances，出现Instances to include和Event categories to include的页面。<br> <img src="/images/AWS/RDS/rds_event_new_console_create_event_subscription_detail_source_type_2.png" alt="rds_event_new_console_create_event_subscription_detail_source_type_2"><br> 3.3 在Instances to include中选择Select specific instances, 出现Specific instances的下拉菜单，在菜单中选择要监控的RDS<br> <img src="/images/AWS/RDS/rds_event_new_console_create_event_subscription_detail_source_type_3.png" alt="rds_event_new_console_create_event_subscription_detail_source_type_3"><br> 3.4 在Event categories to include中选择Select specific event categories, 出现Specific event下拉菜单，在菜单中选取availability，deletion，failover，failure，low storage和recovery这六个Event Categories。不得不吐槽的是，目前操作界面中每次只能选一个，要选六个Event category就需要打开下拉菜单六次，再点击对应的event category名字。<br> <img src="/images/AWS/RDS/rds_event_new_console_create_event_subscription_detail_source_type_4.png" alt="rds_event_new_console_create_event_subscription_detail_source_type_4"></p>
</li>
<li><p>点击Create建立Subscription</p>
</li>
<li><p>在Console中就能够看到Subscription正在创建中，等待一小段时间后，就会显示Subscription创建成功了。<br><img src="/images/AWS/RDS/rds_event_new_console_create_event_subscription_list.png" alt="rds_event_new_console_create_event_subscription_list"></p>
</li>
<li><p>当有监控的事件发生时，AWS就会发送通知到设置的SNS Topic中。SNS再以Email，SMS或者HTTP Endpoint的形式发送给Topic的订阅者。设置完后，在SNS中订阅的邮件地址马上就收到了一封主题为“RDS Notification Message”的邮件。提示将会收到来自SNS的通知，如果不需要，可以退订。</p>
</li>
</ol>
<h4 id="接收通知"><a href="#接收通知" class="headerlink" title="接收通知"></a>接收通知</h4><p>设置好RDS Subscription后，一旦有对应的Event出现，RDS就会发送消息到设置的SNS Topic中，订阅此Topic的人员就可以立马收到通知，从而可以及时处理情况。</p>
<h4 id="新老Console比较"><a href="#新老Console比较" class="headerlink" title="新老Console比较"></a>新老Console比较</h4><p>在RDS Event设置这个Feature上，新Console并没有添加新的功能，只是在UI界面和交互方式上做了调整。但个人感觉，新Console还不如老Console的交互来得人性化。</p>
<p>先来一张旧版Console和新版Console的对比图。<br><img src="/images/AWS/RDS/rds_event_compare_with_old_and_new_console.png" alt="rds_event_compare_with_old_and_new_console"></p>
<p>谈谈个人对新Console的看法:</p>
<ol>
<li>界面不太紧凑，新建一个Event Subscription，页面上需要滚动两屏才能设置完。设计风格应该是借鉴了新版的S3 Console。</li>
<li>设置Event Source的界面时，交互太过于强调分层。以设置Instance的Event为例，必须先设置好Source type为Instance，才能看到”Instance to include”和”Event Categories to include”。个人还是比较喜欢旧版中，将所有可选的内容直观的显示在页面上给用户选取这样的交互方式。</li>
<li>设置event categories时的下拉菜单的交互称不上是用户友好的。设置每一个event categories，都需要经历两步操作: 1.点开下拉菜单。2. 选择event category。如果要设置六个event category，那就需要12次点击操作，其中选取event category这6次点击是无法避免的，可是需要用户点开6次下拉菜单，这个设定不太友好。或许有办法可以点开下拉菜单一次，选取多个event category，只是我没试出来而已。</li>
<li>还有“All instances”和“All event categories”的选项，选取后在界面上没有像旧版Console那样，显示“All instances”和“All event categories”都包含了哪些具体内容。新版console的设计者应该是认为选了这两个选项，就代表你已经明确知道目前系统中有哪几个Instance和这个Source type下的所有Event Categories是什么了。在这个操作上，潜在要求了Console的使用者需要和像使用AWS CLI那样,预先了解资源，这种设计对新手不够友好。</li>
</ol>
<h3 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h3><ul>
<li><a href="/2018/03/14/AWS-RDS-recovery-due-to-issue-with-underlying-hardware/" title="AWS RDS recovery due to issue with underlying hardware">AWS RDS recovery due to issue with underlying hardware</a>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html#USER_Events.Subscribing" target="_blank" rel="external">AWS Doc – Using Amazon RDS Event Notification</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> RDS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[在Chrome中设置Google搜索为NoCountryRedirect]]></title>
      <url>/2018/03/15/how-to-set-chrome-to-use-google-with-NoCountryRedirect/</url>
      <content type="html"><![CDATA[<p>添加代理来科学上网时，使用Google进行搜索，Chrome浏览器默认会重定向到代理国所在的Google域名。比如<code>https://www.google.co.kr/</code>,<code>https://www.google.co.jp/</code>等, 已经在<code>www.google.com</code>登录的Google账号就还得重登, 极度烦躁。</p>
<p>下面是几个固定使用<code>www.google.com</code>来进行搜索的方法</p>
<a id="more"></a>
<h3 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h3><p>介绍三种方式来实现。</p>
<h4 id="固定访问地址"><a href="#固定访问地址" class="headerlink" title="固定访问地址"></a>固定访问地址</h4><p>使用<code>google.com/ncr</code>来访问Google，ncr是NoCountryRedirect的缩写，此时Google会重定向请求到主站<code>https://www.google.com/</code>，而不会定向到访问IP所在的Google域名了</p>
<h4 id="修改Chrome设置"><a href="#修改Chrome设置" class="headerlink" title="修改Chrome设置"></a>修改Chrome设置</h4><ol>
<li><p>地址栏输入<code>chrome://settings/</code>进入Chrome设置页面</p>
</li>
<li><p>找到“搜索引擎”项，点击“管理搜索引擎”<br><img src="/images/Chrome/chrome_setting.png" alt="chrome_setting"></p>
</li>
<li><p>在“管理搜索引擎”页面，点击的“添加”按钮。</p>
</li>
<li><p>在“添加搜索引擎”页面中，搜索引擎和关键字一栏设定为自己能识别的名字，此处命名为“Google NCR”，在网址一栏中填入”<a href="https://www.google.com/search?q=%s”" target="_blank" rel="external">https://www.google.com/search?q=%s”</a><br><img src="/images/Chrome/chrome_add_search_engine.png" alt="chrome_add_search_engine"></p>
</li>
<li><p>再设置刚添加的“Google NCR”条目为默认搜索引擎<br><img src="/images/Chrome/chrome_set_default_search_engine.png" alt="chrome_set_default_search_engine"></p>
</li>
<li><p>之后在地址栏直接输入要搜索的内容时候，就会使用Google主站<code>https://www.google.com</code>来进行搜索了。</p>
</li>
</ol>
<h4 id="使用Chrome扩展"><a href="#使用Chrome扩展" class="headerlink" title="使用Chrome扩展"></a>使用Chrome扩展</h4><p>有一个名为“NoCountryRedirect (NCR)”的Chrome extension是专门用来解决这个问题的。<a href="https://chrome.google.com/webstore/detail/nocountryredirect-ncr/ciboebddidackjicoeoiigdnbmchkdll" target="_blank" rel="external">插件传送门</a></p>
<p>根据扩展的描述，其实就是在输入<code>www.google.com</code>时自动在URLS后面添加上<code>&#39;/ncr&#39;</code>来实现的。此插件只对google.com和blogspot.com有效。插件描述如下</p>
<blockquote>
<p>Makes sure you stay on google.com and blogspot.com by automatically adding ‘no country redirect’ (‘/ncr’) to those URLs<br>THIS EXTENSION ONLY WORKS FOR BLOGSPOT.COM AND GOOGLE.COM, NO OTHER DOMAINS!</p>
</blockquote>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本着能不加扩展就不加扩展和最小操作的原则，最终选择了修改Chrome设置的方法。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://productforums.google.com/forum/#!topic/chrome/zXRG7-AEgfw" target="_blank" rel="external">How do I change my default search country? Google thinks I am in a different country</a></li>
<li><a href="https://www.v2ex.com/t/159920" target="_blank" rel="external">如何强制 chrome 地址栏使用 google.com 搜索？</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Chrome </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS RDS recovery due to issue with underlying hardware]]></title>
      <url>/2018/03/14/AWS-RDS-recovery-due-to-issue-with-underlying-hardware/</url>
      <content type="html"><![CDATA[<p>早上刚上班就收到一封主题为”RDS Notification Message”的订阅邮件，message内容为”Recovery of the DB instance has started. Recovery time will vary with the amount of data to be recovered”。</p>
<p>上Web Console查看发生了什么事，发现RDS已经自我恢复完毕了。最新的Event消息提示为”Recovery of the DB instance is complete.”</p>
<p><img src="/images/AWS/RDS/rds_recovery_event.png" alt="rds_recovery_event"></p>
<p>提了Case询问了下，也在网上搜了一下，基本搞清楚了是怎么个回事，记录如下。</p>
<a id="more"></a>
<h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>根据<a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html" target="_blank" rel="external">AWS 官方文档</a>的描述，”Recovery of the DB instance has started. Recovery time will vary with the amount of data to be recovered.”的消息的RDS Event ID为RDS-EVENT-0020，属于RDS中<code>recovery</code>类别下。</p>
<p>但官方文档中没有提及，什么时候会导致发出这类Event。</p>
<p>经过调查，大致明白了出现这种消息，是因为承载了RDS Instance的底层硬件出问题，监控程序连接不上RDS Instance，就触发执行了recovery操作，应该算是RDS的一个自我恢复的机制。</p>
<p>像RDS这种PAAS服务，就能提供这种自我恢复的机制。而像EC2这种IAAS服务，当底层硬件down掉后，因为aws无法知道Instance里面运行了啥，所能做的就只能维护着现场的“尸体”，发送个通知给管理员，等管理员过来“收尸”了。不久前的”收尸”经历还历历在目。→_→ <a href="/2018/03/12/how-to-handle-Retirement-Notification-Amazon-EC2-Instance-scheduled-for-retirement/" title="EC2底层硬件Retirement的处理方法">EC2底层硬件Retirement的处理方法</a>。</p>
<h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>AWS针对这种情况的解决方案是建议是开启RDS的Multi-AZ功能, 开启Multi-AZ后，会启动一个备用RDS。主机和备用机之间使用同步复制技术，来保证数据的一致性。当主机出问题后，会转而使用备用机。切换是AWS自动进行的，RDS对外的Endpoint不会改变。唯一的影响就是原来连接到RDS主机的DB connect会中断，当client重连Endpoint的时候，就会连接到备用机上。</p>
<p>关于Multi-AZ的详细情况参见官网说明 –&gt; <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html" target="_blank" rel="external">High Availability (Multi-AZ)</a>和<a href="https://aws.amazon.com/rds/details/multi-az/" target="_blank" rel="external">Amazon RDS Multi-AZ Deployments</a></p>
<h4 id="Multi-AZ的优点"><a href="#Multi-AZ的优点" class="headerlink" title="Multi-AZ的优点"></a>Multi-AZ的优点</h4><p>简单罗列使用Multi-AZ的优点如下:</p>
<ul>
<li>万一整个AZ都挂掉了，另一个AZ中还有一个同步的数据库可用</li>
<li>Auto Backup的时候，在standby Instance上进行，不会造成primary Instance IO的下降和可能的短暂的不可用。</li>
<li>RDS打patch的时候，不会有中断</li>
<li>Primary Instance fail后，会自动切换为使用standby</li>
</ul>
<h4 id="Multi-AZ的缺点"><a href="#Multi-AZ的缺点" class="headerlink" title="Multi-AZ的缺点"></a>Multi-AZ的缺点</h4><p>唯一的缺点，那就是费用是Single-AZ的两倍，这个容易理解，因为实际上有两台Instance在运行。 PostgreSQL Multi-AZ的价格<a href="https://aws.amazon.com/rds/postgresql/pricing/" target="_blank" rel="external">在此</a></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>是机器就有可能会坏，无论是在自建的数据中心还是AWS的机房里面。绝大多数情况下，成熟云厂商的解决方案，都要比自己来运维要更靠谱。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html#USER_Events.Messages" target="_blank" rel="external">Using Amazon RDS Event Notification</a></li>
<li><a href="https://forums.aws.amazon.com/thread.jspa?messageID=415143" target="_blank" rel="external">AWS Forums – Unexplained: “Recovering DB Instance”</a></li>
<li><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html" target="_blank" rel="external">AWS Doc – High Availability (Multi-AZ)</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> RDS </tag>
            
            <tag> Failover </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[EC2底层硬件Retirement的处理方法]]></title>
      <url>/2018/03/12/how-to-handle-Retirement-Notification-Amazon-EC2-Instance-scheduled-for-retirement/</url>
      <content type="html"><![CDATA[<p>AWS的EC2的实例是在物理机上虚拟化出来的，当承载EC2的物理机出现问题或者要退役时，AWS会发送一封主题为<strong>[Retirement Notification] Amazon EC2 Instance scheduled for retirement</strong>的email到AWS Account的注册邮箱中。</p>
<p>以前多次收到过这类邮件，但提示的目标instance-ID都是AutoScaling中运行的EC2，而AutoScaling中的Instance自动伸缩不会影响业务，因此也从没有主动去处理过。</p>
<p>这次突然收到通知邮件，说一台运行后台任务的Instance的underlying hardware hosting出问题了。在此记录一下相关的现象和解决方法。</p>
<a id="more"></a>
<h3 id="现象"><a href="#现象" class="headerlink" title="现象:"></a>现象:</h3><ol>
<li><p>注册邮箱中，收到主题为<strong>[Retirement Notification] Amazon EC2 Instance scheduled for retirement</strong>的邮件, 提示某个Instnace的底层硬件要退役了，提醒做好对应的处理:</p>
</li>
<li><p>点击邮件中Events页面的链接登陆EC2 Console，类似为<a href="https://console.aws.amazon.com/ec2/v2/home?region=us-west-2#Events" target="_blank" rel="external">https://console.aws.amazon.com/ec2/v2/home?region=us-west-2#Events</a></p>
</li>
<li><p>此时，对应的EC2可能有两种情况: 一是还在正常工作，二是已经不在正常工作了。不幸的是，在这次事件中，虽然Instance State还是处于running, 但是已经无法登陆出错的Instance了。Console中提示如下的信息, 提示Instance已经不可使用了。<br><img src="/images/AWS/EC2/ec2_retirement_event_description.jpeg" alt="ec2_retirement_event_description.jpeg"></p>
</li>
<li><p>而且Instance的System Status Checks和Instnace Status Checks都处于Failed状态。</p>
</li>
</ol>
<h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><h4 id="Instance的root-device是EBS时"><a href="#Instance的root-device是EBS时" class="headerlink" title="Instance的root device是EBS时"></a>Instance的root device是EBS时</h4><p>如果Instance的root device是EBS, 可行的步骤如下:</p>
<ol>
<li><p>直接Stop Instance，然后再Start即可。此时在大多数情况下，instance会迁移到一个新的承载物理机上。<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html" target="_blank" rel="external">官方文档</a>的描述如下:</p>
<blockquote>
<p>In most cases, the instance is migrated to a new underlying host computer when it’s started.</p>
</blockquote>
</li>
<li><p>如果Instance使用了Elastic IP，那么就不需要做任何操作。如果是自动分配的Public IP,则需要注意，stop后再start，Public IP会发生改变。如果原先脚本或者程序中有直接用到原Public IP的地方，需要做对应的修改。</p>
</li>
<li><p>在实际操作中，先stop 对应的instance，然后再start，instance就可用了。只是期间发现stop instance操作比平时会长很多。可能和正常的stop操作相比，会有额外的数据同步操作。但或许也只是我这边的个例。</p>
</li>
</ol>
<h4 id="Instance的root-device是Instance-store时"><a href="#Instance的root-device是Instance-store时" class="headerlink" title="Instance的root device是Instance store时"></a>Instance的root device是Instance store时</h4><p>如果Instnace的root device是Instance store的，可行的操作步骤如下:</p>
<ol>
<li>先基于现有的Instance建立一个AMI。</li>
<li>从AMI创建一个新的Instance。</li>
<li>private IP，Public IP地址都会发生改变，如果有用到的地方，需要对应修改。</li>
</ol>
<h4 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h4><p>如果恢复Instance的事情，不是异常紧急的话，建议即使root device是EBS，也先做一个AMI后再执行stop，start操作。这样更靠谱,更保险一点。</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><h4 id="邮件示例"><a href="#邮件示例" class="headerlink" title="邮件示例"></a>邮件示例</h4><p>主题为<strong>[Retirement Notification] Amazon EC2 Instance scheduled for retirement</strong>的邮件大致内容如下:</p>
<pre><code>Dear Amazon EC2 Customer,

We have important news about your account (AWS Account ID: 888888888888). EC2 has detected degradation of the underlying hardware hosting your Amazon EC2 instance (instance-ID: i-aaaaaaaaaaaaaaaaa) in the us-west-2 region. Due to this degradation, your instance could already be unreachable. After 2018-03-26 14:00 UTC your instance, which has an EBS volume as the root device, will be stopped.

You can see more information on your instances that are scheduled for retirement in the AWS Management Console (https://console.aws.amazon.com/ec2/v2/home?region=us-west-2#Events)

* How does this affect you?

Your instance will be stopped after the specified retirement date, but you can start it again at any time. Note that if you have EC2 instance store volumes attached to the instance, any data on these volumes will be lost when the instance is stopped or terminated as these volumes are physically attached to the host computer

* What do you need to do?

You can wait for the scheduled retirement date - when the instance is stopped - or stop the instance yourself any time before then. Once the instances has been stopped, you can start the instance again at any time. For more information about stopping and starting your instance, and what to expect when your instance is stopped, such as the effect on public, private and Elastic IP addresses associated with your instance, see Stop and Start Your Instance in the EC2 User Guide (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html).
</code></pre><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><a href="https://forums.aws.amazon.com/thread.jspa?threadID=167250" target="_blank" rel="external">AWS forums - Degraded hardware, EC2 instance scheduled for retirement - options</a></li>
<li><a href="https://forums.aws.amazon.com/thread.jspa?threadID=255981" target="_blank" rel="external">AWS forums - [Retirement Notification] Amazon EC2 Instance scheduled for retirement.</a></li>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-retirement.html#instance-retirement-actions" target="_blank" rel="external">AWS Doc - Instance Retirement</a></li>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html" target="_blank" rel="external">AWS Doc - Stop and Start Your Instance</a></li>
</ol>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> EC2 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用seed将csv格式的db文件导入DB]]></title>
      <url>/2018/02/11/a-way-to-use-csv-file-to-populate-the-database-by-rake-db-seed-on-rails/</url>
      <content type="html"><![CDATA[<p>一段将csv格式的db文件导入对应表结构的代码。</p>
<h3 id="前提要求"><a href="#前提要求" class="headerlink" title="前提要求"></a>前提要求</h3><ol>
<li>csv文件名字和Model Class需要一一对应， 比如用户表users, Model中定义是<code>class User &lt; ActiveRecord::Base</code>, 那么csv文件名字得是user.csv</li>
<li>csv文件需要有header,并且header名字需要同数据库中对应表的字段名相同</li>
</ol>
<a id="more"></a>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>以下代码在Rails 4.2下能够成功运行。 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">require &apos;csv&apos;</div><div class="line"></div><div class="line"># 禁止Class的callback的函数</div><div class="line"># https://stackoverflow.com/a/6538007</div><div class="line">def skip_all_callbacks(klass)</div><div class="line">  [:validation, :save, :create, :commit].each do |name|</div><div class="line">    klass.send(&quot;_#&#123;name&#125;_callbacks&quot;).each do |_callback|</div><div class="line">    # HACK - the oracle_enhanced_adapter write LOBs through an after_save callback (:enhanced_write_lobs)</div><div class="line">    if (_callback.filter != :enhanced_write_lobs)</div><div class="line">      klass.skip_callback(name, _callback.kind, _callback.filter)</div><div class="line">    end</div><div class="line">    end</div><div class="line">  end</div><div class="line">end</div><div class="line"></div><div class="line">seed_files = Dir[Rails.root.join(&apos;lib&apos;, &apos;seeds&apos;, &apos;*.csv&apos;)]</div><div class="line">seed_files.each do |path|</div><div class="line">  model = &quot;#&#123;File.basename(path, &apos;.*&apos;).camelize&#125;&quot;.constantize</div><div class="line"></div><div class="line">  # 一些field可能并没有声明为attr_accessible, 因此在seeds中需要临时声明为attr_accessible, 否则对应的字段不会被更新，并且不会报错。</div><div class="line">  model.class_eval do</div><div class="line">    attr_accessible *column_names</div><div class="line">  end</div><div class="line"></div><div class="line">  # 设置要插入数据的model为禁止callback, 此处是纯粹的导入数据，所以需要禁止Model的callback，防止数据错乱。</div><div class="line">  skip_all_callbacks(model)</div><div class="line"></div><div class="line">  csv = CSV.parse(File.read(path), :headers =&gt; true, :encoding =&gt; &apos;ISO-8859-1&apos;)</div><div class="line">  csv.each do |row|</div><div class="line">    model.new(row.to_hash).save(validate: false)</div><div class="line">  end</div><div class="line">end</div></pre></td></tr></table></figure>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>将代码保存为<code>db/seeds.rb</code>, 并将需要import的csv保存在<code>lib/seeds/</code>目录下。执行<code>rake db:seed</code>就可以将csv文件的数据导入到DB中对应的表中。</p>
<h3 id="PostgreSQL-dump命令"><a href="#PostgreSQL-dump命令" class="headerlink" title="PostgreSQL dump命令"></a>PostgreSQL dump命令</h3><p>另外，在PostgreSQL中，将对应的某些记录dump成csv格式文件的命令如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 在psql中输入如下命令，将所选记录dump成带header的csv文件</div><div class="line">\copy (SELECT * FROM users where id &gt;=10 and id &lt;20) To &apos;/tmp/user.csv&apos; With CSV HEADER</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> Code </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ruby </tag>
            
            <tag> Rails </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用AWS CLI来备份和恢复S3 Bucket的ACL]]></title>
      <url>/2018/02/11/aws-s3-cli-to-backup-and-restore-bucket-acl/</url>
      <content type="html"><![CDATA[<p>S3 Bucket层面的权限管理，推荐使用Bucket Policy来管理，但可能会有一些遗留的S3 Bucket, 还使用着Bucket ACL来进行权限管理。</p>
<p>在将Bucket ACl升级为Bucket Policy或者是更新Bucket ACL的时候，会有备份Bucket ACL的需求。此处介绍一下如何使用AWS CLI来备份和恢复Bucket的Bucket ACL。</p>
<h3 id="备份-Bucket-ACL"><a href="#备份-Bucket-ACL" class="headerlink" title="备份 Bucket ACL"></a>备份 Bucket ACL</h3><p>使用CLI中s3api中的<a href="https://docs.aws.amazon.com/cli/latest/reference/s3api/get-bucket-acl.html" target="_blank" rel="external">get-bucket-acl</a>命令来保存bucket acl。</p>
<h4 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">  get-bucket-acl</div><div class="line">--bucket &lt;value&gt;</div><div class="line">[--cli-input-json &lt;value&gt;]</div><div class="line">[--generate-cli-skeleton &lt;value&gt;]</div></pre></td></tr></table></figure>
<a id="more"></a>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p><code>aws s3api get-bucket-acl</code>的输出默认是json格式。如下例子中，表示bucket允许任意AWS User都可以READ bucket中的object, 用户b0a30227944666c6a28c66c35d06e096a813a0c533e0a16ffb48ca8e6ba36149可以有FULL_CONTROL整个bucket。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">$ aws s3api get-bucket-acl --bucket carl-test-at-seoul</div><div class="line">&#123;</div><div class="line">    &quot;Owner&quot;: &#123;</div><div class="line">        &quot;ID&quot;: &quot;b0a30227944666c6a28c66c35d06e096a813a0c533e0a16ffb48ca8e6ba36149&quot;</div><div class="line">    &#125;,</div><div class="line">    &quot;Grants&quot;: [</div><div class="line">        &#123;</div><div class="line">            &quot;Grantee&quot;: &#123;</div><div class="line">                &quot;Type&quot;: &quot;Group&quot;,</div><div class="line">                &quot;URI&quot;: &quot;http://acs.amazonaws.com/groups/global/AuthenticatedUsers&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;Permission&quot;: &quot;READ&quot;</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            &quot;Grantee&quot;: &#123;</div><div class="line">                &quot;Type&quot;: &quot;CanonicalUser&quot;,</div><div class="line">                &quot;ID&quot;: &quot;b0a30227944666c6a28c66c35d06e096a813a0c533e0a16ffb48ca8e6ba36149&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;Permission&quot;: &quot;FULL_CONTROL&quot;</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h4 id="备份"><a href="#备份" class="headerlink" title="备份"></a>备份</h4><p>将命令的输出重定向到某个文件中，就可以备份该bucket的bucket acl了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">aws s3api get-bucket-acl --bucket carl-test-at-seoul &gt; bucket_acl_of_carl-test-at-seoul.json</div></pre></td></tr></table></figure>
<h3 id="恢复-Bucket-ACL"><a href="#恢复-Bucket-ACL" class="headerlink" title="恢复 Bucket ACL"></a>恢复 Bucket ACL</h3><p>使用CLI中s3api中的<a href="https://docs.aws.amazon.com/cli/latest/reference/s3api/put-bucket-acl.html" target="_blank" rel="external">put-bucket-acl</a>命令来恢复bucket acl。</p>
<h4 id="用法-1"><a href="#用法-1" class="headerlink" title="用法"></a>用法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">  put-bucket-acl</div><div class="line">[--acl &lt;value&gt;]</div><div class="line">[--access-control-policy &lt;value&gt;]</div><div class="line">--bucket &lt;value&gt;</div><div class="line">[--content-md5 &lt;value&gt;]</div><div class="line">[--grant-full-control &lt;value&gt;]</div><div class="line">[--grant-read &lt;value&gt;]</div><div class="line">[--grant-read-acp &lt;value&gt;]</div><div class="line">[--grant-write &lt;value&gt;]</div><div class="line">[--grant-write-acp &lt;value&gt;]</div><div class="line">[--cli-input-json &lt;value&gt;]</div><div class="line">[--generate-cli-skeleton &lt;value&gt;]</div></pre></td></tr></table></figure>
<p>其中，从文件中恢复ACL只需要关注 <code>--bucket</code>(指定bucket)和<code>--access-control-policy</code>(添加bucket ACL文件)这两个参数即可。</p>
<h4 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h4><ol>
<li><p>使用先将Bucket carl-test-at-seoul中允许任意AWS User都可以READ object的权限去掉。去掉后，相关bucket acl如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">$ aws s3api get-bucket-acl --bucket carl-test-at-seoul</div><div class="line">&#123;</div><div class="line">    &quot;Owner&quot;: &#123;</div><div class="line">        &quot;ID&quot;: &quot;b0a30227944666c6a28c66c35d06e096a813a0c533e0a16ffb48ca8e6ba36149&quot;</div><div class="line">    &#125;,</div><div class="line">    &quot;Grants&quot;: [</div><div class="line">        &#123;</div><div class="line">            &quot;Grantee&quot;: &#123;</div><div class="line">                &quot;Type&quot;: &quot;CanonicalUser&quot;,</div><div class="line">                &quot;ID&quot;: &quot;b0a30227944666c6a28c66c35d06e096a813a0c533e0a16ffb48ca8e6ba36149&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;Permission&quot;: &quot;FULL_CONTROL&quot;</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
<li><p>再使用之前保存的acl文件来恢复该bucket的acl。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ aws s3api put-bucket-acl --bucket carl-test-at-seoul --access-control-policy file://bucket_acl_of_carl-test-at-seoul.json</div></pre></td></tr></table></figure>
</li>
<li><p>检查bucket的acl，可以看到已经恢复成acl文件中相应的权限了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">$ aws s3api get-bucket-acl --bucket carl-test-at-seoul</div><div class="line">&#123;</div><div class="line">    &quot;Owner&quot;: &#123;</div><div class="line">        &quot;ID&quot;: &quot;b0a30227944666c6a28c66c35d06e096a813a0c533e0a16ffb48ca8e6ba36149&quot;</div><div class="line">    &#125;,</div><div class="line">    &quot;Grants&quot;: [</div><div class="line">        &#123;</div><div class="line">            &quot;Grantee&quot;: &#123;</div><div class="line">                &quot;Type&quot;: &quot;Group&quot;,</div><div class="line">                &quot;URI&quot;: &quot;http://acs.amazonaws.com/groups/global/AuthenticatedUsers&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;Permission&quot;: &quot;READ&quot;</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            &quot;Grantee&quot;: &#123;</div><div class="line">                &quot;Type&quot;: &quot;CanonicalUser&quot;,</div><div class="line">                &quot;ID&quot;: &quot;b0a30227944666c6a28c66c35d06e096a813a0c533e0a16ffb48ca8e6ba36149&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;Permission&quot;: &quot;FULL_CONTROL&quot;</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> S3 </tag>
            
            <tag> AWS CLI </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Macos下如何实现grep的非贪婪模式]]></title>
      <url>/2018/02/09/non-greedy-match-in-grep-on-macos/</url>
      <content type="html"><![CDATA[<p>有时候需要使用正则的<code>非贪婪模式(non-greedy或者叫lazy模式)</code>来搜一些内容，但Macos系统自带的<code>grep</code>无法胜任。<br>Mac下的<code>grep</code>是BSD系的，不能使用-P参数来使用perl模式。Linux下的<code>GNU grep</code>就可以使用-P参数来支持非贪婪模式。</p>
<p>MacOS自带的<code>grep</code>的示例:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">$ cat grep_on_macos.txt</div><div class="line">carl has a nice car. the car has four wheels.</div><div class="line">$</div><div class="line"># 支持普通正则</div><div class="line">$ grep -o &quot;car.*has&quot; grep_on_macos.txt</div><div class="line">carl has a nice car. the car has</div><div class="line">$</div><div class="line"># 不支持带?的非贪婪模式</div><div class="line">$ grep -o &quot;car.*?has&quot; grep_on_macos.txt</div><div class="line">$</div></pre></td></tr></table></figure>
<p>此处记录两个可行的方法来解决该问题</p>
<a id="more"></a>
<h3 id="使用egrep"><a href="#使用egrep" class="headerlink" title="使用egrep"></a>使用egrep</h3><p>在Terminal中敲击<code>man egrep</code>, 在man page的Description中可以看到有如下一行描述<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">grep is used for simple patterns and basic regular expressions (BREs); </div><div class="line">egrep can handle extended regular expressions (EREs).</div></pre></td></tr></table></figure></p>
<p>可以看到Mac默认的<code>grep</code>只支持基本政策表达式，而<code>egrep</code>支持扩展正则表达式</p>
<p>因此在上面的例子中，使用<code>egrep</code>来替代<code>grep</code>就能达到目的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$ cat grep_on_macos.txt</div><div class="line">carl has a nice car. the car has four wheels.</div><div class="line">$</div><div class="line"># 普通正则表达式没问题</div><div class="line">$ egrep -o &quot;car.*has&quot; grep_on_macos.txt</div><div class="line">carl has a nice car. the car has</div><div class="line">$</div><div class="line"># egrep也支持?的非贪婪模式</div><div class="line">$ egrep -o &quot;car.*?has&quot; grep_on_macos.txt</div><div class="line">carl has</div><div class="line">car. the car has</div><div class="line">$</div></pre></td></tr></table></figure>
<h3 id="Install-GNU-grep"><a href="#Install-GNU-grep" class="headerlink" title="Install GNU grep"></a>Install GNU grep</h3><p>使用<code>egrep</code>的方法，在偶尔手动操作来查找的时候是可行的。但如果是要在Macos下要运行某个脚本，而脚本中使用了<code>grep</code>的-P参数，此时<code>egrep</code>方法就有局限性，总不能还得改脚本吧。</p>
<p>此时，可以使用<code>Homebrew</code>来安装<code>GNU grep</code>。安装时候也有两种办法</p>
<h4 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h4><p>简单粗暴，<code>Homebrew</code>安装时直接覆盖系统的<code>grep</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">brew install grep --with-default-names</div></pre></td></tr></table></figure>
<p>此时，直接会将系统的<code>grep</code>替换为brew安装的<code>GNU grep</code></p>
<h4 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h4><p>不覆盖系统自带的<code>grep</code>，通过~/.bashrc alias的方式来实现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">brew install grep</div></pre></td></tr></table></figure>
<p>此时，brew安装的<code>GNU grep</code>安装路径为/usr/local/bin/ggrep。注意是<code>ggrep</code>，而不是<code>grep</code>。</p>
<p>再通过alias的方式让shell在运行<code>grep</code>的时候运行<code>ggrep</code>, 可以达到同样的效果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo &quot;alias grep=&apos;ggrep&apos;&quot; &gt;&gt; ~/.bashrc</div></pre></td></tr></table></figure>
<p>更推荐方法二，alias的grep只在自己账号下使用，不会影响系统。运行结果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">$ cat grep_on_macos.txt</div><div class="line">carl has a nice car. the car has four wheels.</div><div class="line">$</div><div class="line"># 查看grep的alias</div><div class="line">$ alias grep</div><div class="line">alias grep=&apos;ggrep&apos;</div><div class="line">$</div><div class="line"># 普通正则表达式没问题</div><div class="line">$ grep -o &quot;car.*has&quot; grep_on_macos.txt</div><div class="line">carl has a nice car. the car has</div><div class="line">$</div><div class="line"># 运行的grep也支持?的非贪婪模式</div><div class="line">$ grep -P -o &quot;car.*?has&quot; grep_on_macos.txt</div><div class="line">carl has</div><div class="line">car. the car has</div><div class="line">$</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Shell </category>
            
        </categories>
        
        
        <tags>
            
            <tag> MacOS </tag>
            
            <tag> Linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS Certified Solutions Architect - Professional Road Map]]></title>
      <url>/2018/02/07/AWS-Certified-Solutions-Architect-Professional/</url>
      <content type="html"><![CDATA[<h2 id="Official-AWS-Certification-Page"><a href="#Official-AWS-Certification-Page" class="headerlink" title="Official AWS Certification Page"></a>Official AWS Certification Page</h2><p>访问官网<a href="https://amazonaws-china.com/certification/certification-prep/" target="_blank" rel="external">AWS Certification</a></p>
<ul>
<li>参加 AWS 培训课程</li>
<li>查看考试指南和样题<ul>
<li>了解考试涉及的概念并整体了解需要学习哪些内容, <a href="http://d0.awsstatic-china.com/Train%20&amp;%20Cert/docs/AWS_certified_solutions_architect_professional_blueprint.pdf" target="_blank" rel="external">AWS Certified Solutions Architect – Professional 考试指南</a> 相当于考试大纲, 必看,而且需要反复的看。因为学习过一阵后再来看Guide，会有更深的体会。</li>
<li><a href="https://d0.awsstatic-china.com/training-and-certification/docs/AWS_certified_solutions_architect_professional_examsample.pdf" target="_blank" rel="external">考试样题</a>用于熟悉题目题型。几个样题都是大段大段的文字，实际考试题目中也有字数少的题目。但题目的阅读量绝对是比Asoocaiated的大得多。</li>
</ul>
</li>
<li>练习试验<ul>
<li>注册一个AWS全球账号，使用一年的免费额度结合Blueprint中的各个内容进行试验。</li>
</ul>
</li>
<li>学习 AWS 白皮书<ul>
<li>白皮书是纯英文的，每个白皮书篇幅都很长。</li>
<li>在BluePrint的描述中，和AWS Certified Solutions Architect - Associate的白皮书列表相比，多了<strong>Defining Fault Tolerant Applications in the AWS Cloud</strong>这一份白皮书，官网找了一下，只有一个名字类似的名为<a href="https://d1.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf" target="_blank" rel="external">Building Fault-Tolerant Applications on AWS</a>的白皮书，时间还是October 2011的。不管怎样，但愿好理念永不过时，还是整个念一遍吧。</li>
</ul>
</li>
<li>查看 AWS 常见问题<ul>
<li>官网推荐的FAQ都建议看完。</li>
</ul>
</li>
<li>参加模拟考试<ul>
<li>40美刀一次。是否值得因人而异。参加Professional的考试，肯定都已经经历过AWS Certified Solutions Architect – Associate的洗礼了, 因此熟悉考试界面肯定不是决定要参加模拟考试的目的。一个合理的理由是，可以提前熟悉一下Professional考题阅读难度, 为正式考试的时候做好心理准备。网上都说AWS Certified Solutions Architect – Professional这个考试最大的难度就是要在170分钟内阅读，理解77~80道字数不算少的题，然后选出最优的答案。网上一些母语是英语的老外都感慨阅读量比较大，何况是我们这种非英语系的国家的考生了。</li>
</ul>
</li>
<li>报名考试并获得认证<ul>
<li>登陆<a href="https://www.aws.training/certification" target="_blank" rel="external">https://www.aws.training/certification</a>注册进行考试</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="考试指南"><a href="#考试指南" class="headerlink" title="考试指南"></a>考试指南</h2><p><a href="http://d0.awsstatic-china.com/Train%20&amp;%20Cert/docs/AWS_certified_solutions_architect_professional_blueprint.pdf" target="_blank" rel="external">AWS Certified Solutions Architect – Professional 考试指南</a> 读三遍，读三遍，读三遍</p>
<p>总计8个Domain, 各个Domain占的百分比如下</p>
<ul>
<li>High Availability and Business Continuity - 15%</li>
<li>Costing                                   - 5%</li>
<li>Deployment Management                     - 10%</li>
<li>Network Design                            - 10%</li>
<li>Data Storage                              - 15%</li>
<li>Security                                  - 20%</li>
<li>Scalability &amp; Elasticity                  - 15%</li>
<li>Cloud Migration &amp; Hybrid Architecture     - 10%</li>
</ul>
<h2 id="涉及到的AWS-Services"><a href="#涉及到的AWS-Services" class="headerlink" title="涉及到的AWS Services"></a>涉及到的AWS Services</h2><ul>
<li>AWS KMS</li>
<li>AWS Import/Export</li>
<li>AWS STS</li>
<li>CloudFormation</li>
<li>CloudFront</li>
<li>CloudHSM</li>
<li>CloudSearch</li>
<li>CloudWatch</li>
<li>CloudTrail</li>
<li>Data Pipeline</li>
<li>Direct Connect</li>
<li>DynamoDB</li>
<li>EBS</li>
<li>EC2</li>
<li>AutoScaling</li>
<li>ELB</li>
<li>EMR</li>
<li>ElastiCache</li>
<li>Elastic Beanstalk</li>
<li>Elastic Transcoder</li>
<li>Glacier</li>
<li>IAM</li>
<li>Kinesis</li>
<li>OpsWorks</li>
<li>RDS</li>
<li>RedShift</li>
<li>Route 53</li>
<li>S3</li>
<li>SES</li>
<li>SNS</li>
<li>SQS</li>
<li>SWF</li>
<li>Storage Gateway</li>
<li>VPC</li>
</ul>
<h2 id="视频学习"><a href="#视频学习" class="headerlink" title="视频学习"></a>视频学习</h2><p>网上有多种视频教程，当时个人觉得<a href="https://acloud.guru" target="_blank" rel="external">Acloudguru</a>的AWS Associated系列的教程非常好(三份Associated的教程都买了)，因此此次也购买了<a href="https://acloud.guru" target="_blank" rel="external">Acloudguru</a>的<a href="https://acloud.guru/learn/aws-certified-solutions-architect-professional" target="_blank" rel="external">AWS Certified Solutions Architect – Professional</a>的教程。但学下来整体感觉这个Professional的教程一般，总感觉内容和AWS Certified Solutions Architect – Associated系列的差不多。不够深入和细致。其余的教程没有学习过，因此无法做评价。</p>
<h2 id="要点摘录"><a href="#要点摘录" class="headerlink" title="要点摘录"></a>要点摘录</h2><h3 id="Domain-1-High-Availability-and-Business-Continuity-15"><a href="#Domain-1-High-Availability-and-Business-Continuity-15" class="headerlink" title="Domain 1 - High Availability and Business Continuity - 15%"></a>Domain 1 - High Availability and Business Continuity - 15%</h3><ul>
<li><strong>Disaster Recovery</strong><ul>
<li>WhitePaper : <a href="https://media.amazonwebservices.com/AWS_Disaster_Recovery.pdf" target="_blank" rel="external">https://media.amazonwebservices.com/AWS_Disaster_Recovery.pdf</a></li>
<li>What is Disaster Recovery<ul>
<li>Disaster recovery (DR) is about preparing for and recovering from a disaster. Any event that has a negative impact on a company’s business continuity or finances could be termed a disaster. This includes hardware or software failure, a network outage, a power outage, physical damage to a building like fire or flooding, human error, or some other significant event.</li>
</ul>
</li>
<li>Recovery Time Objective (RTO)<ul>
<li>Recovery Time Objective is the amount of time that it takes for your business to recover from an outage or disruption.</li>
<li>It can include the time for trying to fix the problem without a recovery, the recovery itself, testing and the communication to the users</li>
</ul>
</li>
<li>Recovery Point Objective (RPO)<ul>
<li>Recovery Point Objective (RPO) is the maximum period of time in which data might be lost from an IT service due to a major incident.</li>
<li>In other words, how much data can your organization afford to lose? An hour’s worth? A day’s worth? None at all?</li>
</ul>
</li>
<li>Traditional Approaches to DR<ul>
<li>A traditional approach to DR usually involves an N+1 approach and has different levels of off-site duplication of data and infrastructure.<ul>
<li>Facilities to house the infrastructure, including power and cooling</li>
<li>Security to ensure the physical protection of assets</li>
<li>Suitable capacity to scale the environment</li>
<li>Support for repairing, replacing, and refreshing the infrastructure</li>
<li>Contractual agreements with an Internet service provider (ISP) to provide Internet connectivity that can sustain bandwidth utilization for the environment under a full load</li>
<li>Network infrastructure such as firewalls, routers, switches, and load balancers</li>
<li>Enough server capacity to run all mission-critical services, including storage appliances for the supporting data, and servers to run applications and backend services such as user authentication, Domain Name System (DNS)</li>
<li>Dynamic Host Configuration Protocol (DHCP), monitoring, and alerting</li>
</ul>
</li>
</ul>
</li>
<li>Why use aws for DR<ul>
<li>Only minimum hardware is required for ‘data replication’</li>
<li>Allows you to be flexible depending on what your disaster is and how to recover from it</li>
<li>Open cost model (pay as you use) rather than heavy investment upfront. Scaling is quick and easy</li>
<li>Automate disaster recovery deployment</li>
</ul>
</li>
<li>What Services<ul>
<li>Regions</li>
<li>Storage<ul>
<li>S3 - 99.999999999% durability and Cross Region Replication</li>
<li>Glacier</li>
<li>Elastic Block Store (EBS)</li>
<li>Direct Connect</li>
<li>AWS Storage Gateway<ul>
<li>Gateway-cached volumes - store primary data and cache most recently used data locally.</li>
<li>Gateway-stored volumes - store entire dataset on site and asynchronously replicate data back to S3</li>
<li>Gateway-virtual tape library - Store your virtual tapes in either S3 or Glacier</li>
</ul>
</li>
</ul>
</li>
<li>Compute<ul>
<li>EC2</li>
<li>EC2 VM Import Connector - Virtual appliance which allows you to import virtual machine images from your existing environment to Amazon EC2 instances.</li>
</ul>
</li>
<li>Networking<ul>
<li>Route53</li>
<li>Elastic Load Balancing</li>
<li>Amazon Virtual Private Cloud (VPC)</li>
<li>Amazon Direct Connect</li>
</ul>
</li>
<li>Database<ul>
<li>RDS</li>
<li>DynamoDB</li>
<li>Redshift</li>
</ul>
</li>
<li>Orchestration<ul>
<li>CloudFormation</li>
<li>ElasticBeanstalk</li>
<li>OpsWork</li>
</ul>
</li>
<li>Lambda</li>
</ul>
</li>
<li>DR Scenarios<ul>
<li>Four Scenarios<ul>
<li>Backup &amp; Restore</li>
<li>Pilot Light</li>
<li>Warm Standby</li>
<li>Multi Site</li>
</ul>
</li>
<li>Backup &amp; Restore<ul>
<li>In most traditional environments, data is backed up to tape and sent off-site regularly. If you use this method, it can take a long time to restore your system in the event of a disruption or disaster. Amazon S3 is an ideal destination for backup data that might be needed quickly to perform a restore. Transferring data to and from Amazon S3 is typically done through the network, and is therefore accessible from any location.</li>
<li>You can use AWS Import/Export to transfer very large data sets by shipping storage devices directly to AWS. For longer-term data storage where retrieval times of several hours are adequate, there is Amazon Glacier, which has the same durability model as Amazon S3. . Amazon Glacier and Amazon S3 can be used in conjunction to produce a tiered backup solution.</li>
<li>Data Backup Options to Amazon S3 from On-Site Infrastructure or from AWS.<br>  <img src="/images/AWS/Sysops/data_backup_options_to_S3_from_on_site_infrastructure_from_aws.jpg" alt="data_backup_options_to_S3_from_on_site_infrastructure_from_aws"></li>
<li>Restoring a System from Amazon S3 Backups to Amazon EC2<br>  <img src="/images/AWS/Sysops/restoring_a_system_from_amazon_s3_backups_to_amazon_ec2.jpg" alt="restoring_a_system_from_amazon_s3_backups_to_amazon_ec2"></li>
<li>Key steps for backup &amp; restore<ul>
<li>Select an appropriate tool or method to back up your data into AWS.</li>
<li>Ensure that you have an appropriate retention policy for this data.</li>
<li>Ensure that appropriate security measures are in place for this data, including encryption and access policies.</li>
<li>Regularly test the recovery of this data and the restoration of your system.</li>
</ul>
</li>
</ul>
</li>
<li>Pilot Light<ul>
<li>The term pilot light is often used to describe a DR scenario in which a minimal version of an environment is always running in the cloud. The idea of the pilot light is an analogy that comes from the gas heater. In a gas heater, a small flame that’s always on can quickly ignite the entire furnace to heat up a house.</li>
<li>This scenario is similar to a backup-and-restore scenario. For example, with AWS you can maintain a pilot light by configuring and running the most critical core elements of your system in AWS. When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core.</li>
<li>Infrastructure elements for the pilot light itself typically include your database servers, which would replicate data to Amazon EC2 or Amazon RDS. Depending on the system, there might be other critical data outside of the database that needs to be replicated to AWS. This is the critical core of the system (the pilot light) around which all other infrastructure pieces in AWS (the rest of the furnace) can quickly be provisioned to restore the complete system.</li>
<li>To provision the remainder of the infrastructure to restore business-critical services, you would typically have some preconfigured servers bundled as Amazon Machine Images (AMIs), which are ready to be started up at a moment’s notice.  When starting recovery, instances from these AMIs come up quickly with their pre-defined role (for example, Web or App Server) within the deployment around the pilot light.</li>
<li>From a networking point of view, you have two main options for provisioning:<ul>
<li>Use pre-allocated elastic IP address and associate them with your instances when invoking DR. You can also use pre-allocated elastic network interfaces (ENIs) with pre-allocated Mac Addresses for applications with special licensing requirements</li>
<li>Use Elastic Load Balancing (ELB) to distribute traffic to multiple instances. You would then update your DNS records to point at your Amazon EC2 instance or point to your load balancer using a CNAME</li>
</ul>
</li>
<li>Preparation phase<ul>
<li>The Preparation Phase of the Pilot Light Scenario<br>  <img src="/images/AWS/Sysops/the_preparation_phase_of_the_pilot_light_scenario.jpg" alt="the_preparation_phase_of_the_pilot_light_scenario"></li>
<li>Key steps for preparation:<ul>
<li>Set up Amazon EC2 instances to replicate or mirror data.</li>
<li>Ensure that you have all supporting custom software packages available in AWS.</li>
<li>Create and maintain AMIs of key servers where fast recovery is required.</li>
<li>Regularly run these servers, test them, and apply any software updates and configuration changes.</li>
<li>Consider automating the provisioning of AWS resources</li>
</ul>
</li>
</ul>
</li>
<li>Recovery phase<ul>
<li>The Recovery Phase of the Pilot Light Scenario<br>  <img src="/images/AWS/Sysops/the_recovery_phase_of_the_pilot_light_scenario.jpg" alt="the_recovery_phase_of_the_pilot_light_scenario"></li>
<li>Key steps for recovery:<ul>
<li>Start your application Amazon EC2 instances from your custom AMIs.</li>
<li>Resize existing database/data store instances to process the increased traffic.</li>
<li>Add additional database/data store instances to give the DR site resilience in the data tier; if you are using Amazon RDS, turn on Multi-AZ to improve resilience.</li>
<li>Change DNS to point at the Amazon EC2 servers.</li>
<li>Install and configure any non-AMI based systems, ideally in an automated way.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Warm Standby<ul>
<li>The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud. A warm standby solution extends the pilot light elements and preparation. It further decreases the recovery time because some services are always running. By identifying your business-critical systems, you can fully duplicate these systems on AWS and have them always on.</li>
<li>These servers can be running on a minimum-sized fleet of Amazon EC2 instances on the smallest sizes possible. This solution is not scaled to take a full-production load, but it is fully functional. It can be used for non-production work, such as testing, quality assurance, and internal use.</li>
<li>In a disaster, the system is scaled up quickly to handle the production load. In AWS, this can be done by adding more instances to the load balancer and by resizing the small capacity servers to run on larger Amazon EC2 instance types.</li>
<li>Horizontal scaling is preferred over vertical scaling</li>
<li>Preparation phase<ul>
<li>The Preparation Phase of the Warm Standby Scenario<br>  <img src="/images/AWS/Sysops/the_preparation_phase_of_the_warm_standby_scenario.jpg" alt="the_preparation_phase_of_the_warm_standby_scenario"></li>
<li>Key steps for preparation:<ul>
<li>Set up Amazon EC2 instances to replicate or mirror data.</li>
<li>Create and maintain AMIs.</li>
<li>Run your application using a minimal footprint of Amazon EC2 instances or AWS infrastructure.</li>
<li>Patch and update software and configuration files in line with your live environment.</li>
</ul>
</li>
</ul>
</li>
<li>Recovery phase<ul>
<li>The Recovery Phase of the Warm Standby Scenario<br>  <img src="/images/AWS/Sysops/the_recovery_phase_of_the_warm_standby_scnario.jpg" alt="the_recovery_phase_of_the_warm_standby_scnario"></li>
<li>Key steps for recovery:<ul>
<li>Increase the size of the Amazon EC2 fleets in service with the load balancer (horizontal scaling).</li>
<li>Start applications on larger Amazon EC2 instance types as needed (vertical scaling).</li>
<li>Either manually change the DNS records, or use Amazon Route 53 automated health checks so that all traffic is routed to the AWS environment.</li>
<li>Consider using Auto Scaling to right-size the fleet or accommodate the increased load.</li>
<li>Add resilience or scale up your database.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Multi Site<ul>
<li>A multi-site solution runs in AWS as well as on your existing on-site infrastructure, in an active-active configuration. The data replication method that you employ will be determined by the recovery point that you choose.</li>
<li>You can use Route53 to root traffic to both sites either symmetrically or asymmetrically.</li>
<li>In an on-site disaster situation, you can adjust the DNS weighting and send all traffic to the AWS servers. The capacity of the AWS service can be rapidly increased to handle the full production load. You can use Amazon EC2 Auto Scaling to automate this process. You might need some application logic to detect the failure of the primary database services and cut over to the parallel database services running in AWS.</li>
<li>Preparation phase<ul>
<li>The Preparation Phase of the Multi-Site Scenario<br>  <img src="/images/AWS/Sysops/the_preparation_phase_of_the_multi_site_scenario.jpg" alt="the_preparation_phase_of_the_multi_site_scenario"></li>
<li>Key steps for preparation:<ul>
<li>Set up your AWS environment to duplicate your production environment.</li>
<li>Set up DNS weighting, or similar traffic routing technology, to distribute incoming requests to both sites.  Configure automated failover to re-route traffic away from the affected site.</li>
</ul>
</li>
</ul>
</li>
<li>Recovery phase<ul>
<li>The Recovery Phase of the Multi-Site Scenario Involving On-Site and AWS Infrastructure.<br>  <img src="/images/AWS/Sysops/the_recovery_phase_of_the_multi_site_scenario_involving_on_site_and_aws_infrastructure.jpg" alt="the_recovery_phase_of_the_multi_site_scenario_involving_on_site_and_aws_infrastructure"></li>
<li>Key steps for recovery:<ul>
<li>Either manually or by using DNS failover, change the DNS weighting so that all requests are sent to the AWS site.</li>
<li>Have application logic for failover to use the local AWS database servers for all queries.</li>
<li>Consider using Auto Scaling to automatically right-size the AWS fleet.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Failing Back<ul>
<li>Backup and restore<ol>
<li>Freeze data changes to the DR site</li>
<li>Take a backup</li>
<li>Restore the backup to the primary site</li>
<li>Re-point users to the primary site</li>
<li>Unfreeze the changes</li>
</ol>
</li>
<li>Pilot light, warm standby, and multi-site:<ol>
<li>Establish reverse mirroring/replication from the DR site back to the primary site, once the primary site has caught up with the changes.</li>
<li>Freeze data changes to the DR site</li>
<li>Re-point users to the primary site</li>
<li>Unfreeze the changes</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>You will be given different RTO’s and RPO’s and then asked which AWS services you should choose. All choices may be correct, just some are more correct than others.</li>
<li>Key AWS Back Up &amp; DR Technologies<ul>
<li>S3 provides a highly durable storage infrastructure designed for mission critical and primary data storage. Objects are redundantly stored on multiple devices across multiple facilities within a region, designed to provide a durability of 99.999999999% (11 9s)</li>
<li>Archives (think objects) are optimized for infrequent access, for which retrieval times of several hours are adequate. It can take at least <strong>3 Hours</strong> to recover a file from glacier.</li>
<li>EBS provides the ability to create point-in-time snapshots of data volumes.</li>
<li>DynamoDB offers cross region replication. You can read more about how to set it up here.<ul>
<li><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.CrossRegionRepl.html" target="_blank" rel="external">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.CrossRegionRepl.html</a></li>
</ul>
</li>
<li>RDS gives you the ability to snapshot data from one region to another, and also to have a read replica running in another region.</li>
<li>Redshift: snapshot your data warehouse to be durably stored in Amazon S3 within the same region or copied to another region.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>DR&amp;BC For Database</strong></p>
<ul>
<li>HA for Popular Database<ul>
<li>SQL Server = AlwaysOn Availability Groups, SQL Mirroring</li>
<li>MySQL = Asynchronous replication</li>
<li>Oracle = Oracle Data Guard, Oracle RAC</li>
</ul>
</li>
<li>RDS Multi-AZ Failover<ul>
<li>Automatic failover in case of<ul>
<li>Loss of availability in primary AZ</li>
<li>Loss of connectivity to primary DB</li>
<li>Storage or host failure to primary DB</li>
<li>Software patching</li>
<li>Rebooting of primary DB.</li>
</ul>
</li>
<li>MultiAZ deployments for Oracle, PostgreSQL, MySQL, and MariaDB DB instances use Amazon’s failover technology.</li>
<li>SQL Server DB instances use SQL Server Mirroring.</li>
<li>Amazon Aurora instances stores copies of the data in a DB cluster across multiple Availability Zones in a single region.</li>
<li>All approaches safeguard your data in the event of a DB instance failure or loss of an Availability Zone.</li>
</ul>
</li>
<li>Read Replica<ul>
<li>What are Read Replica<ul>
<li>Read Replicas make it easy to take advantage of supported engine’s built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB Instance for read-heave database workloads.</li>
<li>Read only copies of your database.</li>
<li>You can create a Read Replica with a few clicks in the AWS Management Console or using the CreateDBInstanceReadReplica API. Once the Read Replica is created, database updates on the source DB Instance will be replicated using a supported engine’s native, asynchronous replication. You can create multiple Read Replicas for a given source DB Instance and distribute your application’s read traffic amongst them.</li>
</ul>
</li>
<li>When would you use read replica’s<ul>
<li>Scaling beyond the compute or I/O capacity of a single DB Instance for read-heavy database workloads. This excess read traffic can be directed to one or more Read Replicas</li>
<li>Serving read traffic while the source DB Instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your Read Replica</li>
<li>Business reporting or data warehousing scenarios; you may want business reporting queries to run against a Read Replica, rather than your primary, production DB Instance.</li>
</ul>
</li>
<li>Supported Versions<ul>
<li>MySQL<ul>
<li>MySQL 5.6 (NOT 5.1 or 5.5)</li>
<li>Can use both MySQL engines (MyISAM and InnoDB) however only InnoDB is supported by AWS</li>
</ul>
</li>
<li>PostgreSQL<ul>
<li>PostgreSQL 9.3.5 or newer</li>
</ul>
</li>
<li>MariaDB<ul>
<li>All current versions</li>
<li>For all 3 Amazon uses these engines native asynchronous replication to update the read replica</li>
</ul>
</li>
<li>Oracle and MSSQL<ul>
<li>All current versions</li>
</ul>
</li>
<li>Aurora<ul>
<li>Aurora employees an SSD-backed virtualized storage layer purpose-built for database workloads. Amazon Aurora replica share the same underlying storage as the source instance, lowering costs and avoiding the need to copy data to the replica nodes.</li>
</ul>
</li>
</ul>
</li>
<li>Creating Read Replicas<ul>
<li>When creating a new Read Replica, AWS will take a snapshot of your database.</li>
<li>If Multi-AZ is not enabled:<ul>
<li>This snapshot will be of your primary database and can cause brief I/O suspension for around 1 minute.</li>
</ul>
</li>
<li>If Multi-AZ is enabled:<ul>
<li>The snapshot will be of your secondary database and you will not experience any performance hits on your primary database.</li>
</ul>
</li>
</ul>
</li>
<li>Connecting to Read Replica<ul>
<li>When a new read replica is created you will be able to connect to it using a new end point DNS address.</li>
</ul>
</li>
<li>Read Replica’s Can Be Promoted<ul>
<li>You can promote a read replica to it’s own standalone database. Doing this will break the replication link between the primary and the secondary.</li>
</ul>
</li>
<li>Multi-region Read Replicas<ul>
<li>With Amazon Relational Database Service (Amazon RDS), you can create a MySQL, PostgreSQL, or MariaDB Read Replica in a different AWS Region than the source DB instance. You create a Read Replica to do the following:<ul>
<li>Improve your disaster recovery capabilities.</li>
<li>Scale read operations into a region closer to your users.</li>
<li>Make it easier to migrate from a data center in one region to a data center in another region.</li>
</ul>
</li>
<li>You can create an <strong>Amazon Aurora DB cluster</strong> as a Read Replica in a different AWS Region than the source DB cluster. Taking this approach can improve your disaster recovery capabilities, let you scale read operations into a region that is closer to your users, and make it easier to migrate from one region to another.</li>
<li>You can create Read Replicas of both encrypted and unencrypted DB clusters. The Read Replica must be encrypted if the source DB cluster is encrypted.</li>
</ul>
</li>
</ul>
</li>
<li>Few More Exam Tips<ul>
<li>You can have up to 5 read replicas.</li>
<li>You can have read replicas in different REGIONS (with the exception of SQL Server and Oracle).</li>
<li>For Read Replicas Replication is Asynchronous only, not synchronous.</li>
<li>Read Replicas can be built off Multi-AZ’s databases.</li>
<li><strong>BUT</strong> Read Replicas themselves <strong>cannot</strong> be Multi-AZ currently.</li>
<li>You can have Read Replicas of Read Replicas, however <strong>only</strong> for MySQL 5.6 and MariaDB and this will further increase latency, PostgreSQL is not currently supported.</li>
<li>DB Snapshots and Automated backups <strong>cannot</strong> be taken of read replicas.</li>
<li><strong>Synchronous</strong> replication is used for <strong>Multi-AZ</strong></li>
<li><strong>Asynchronous</strong> replication is used for <strong>Read Replicas</strong></li>
<li>If your application does not require transaction support, Atomicity, Consistency, Isolation, Durability (ACID) compliance, joins &amp; SQL, consider using DynamoDB rather than RDS.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Storage Gateway</strong></p>
<ul>
<li>How can I backup My Data<ul>
<li>Write backup data to S3 directly, using <strong>API calls</strong>.</li>
<li>Write backup data to <strong>Storage Gateway</strong>, which then securely replicates it to S3.</li>
</ul>
</li>
<li>Three type Storage Gateway Interface<ul>
<li>File Interface<ul>
<li>NFS</li>
<li>unlimited amount of storage, However maximum file size is 5TB.</li>
</ul>
</li>
<li>Volume Interface<ul>
<li>Gateway-Cached Volumes</li>
<li>Gateway-Stored Volumes</li>
</ul>
</li>
<li>Tape Interface<ul>
<li>Gateway-Virtual Tape Library</li>
</ul>
</li>
</ul>
</li>
<li>Storage Gateway<ul>
<li>File Volumes<ul>
<li>NFS</li>
<li>Unlimited amount of storage. However maximum file size is 5TB.</li>
</ul>
</li>
<li>Volume Gateway<ul>
<li>Cached (Gateway-Cached Volumes)<ul>
<li>iSCSI based block storage</li>
<li>Each Volume can store up to 32TB in Size.</li>
<li>32 Volumes supported. 1PB of data can be stored(32*32)</li>
</ul>
</li>
<li>Stored (Gateway-Stored Volumes)<ul>
<li>iSCSI based block storage</li>
<li>Each Volume can store up to 16TB in Size.</li>
<li>32 Volumes supported. 512TB of data can be stored (32*16)</li>
</ul>
</li>
<li>Tape Gateway (Gateway-Virtual Tape Library)<ul>
<li>iSCSI based virtual tape solution</li>
<li>Virtual Tape Library (S3) 1500 virtual tapes (1PB)</li>
<li>Virtual Tape Shelf (Glacier) unlimited tapes.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Storage Gateway Introduction</p>
<ul>
<li>File Gateway<br>  File gateway provides a virtual file server, which enables you to store and retrieve <strong>Amazon S3</strong> objects through standard file storage protocols. File gateway allows your existing file-based applications or devices to use secure and durable cloud storage without needing to be modified. With file gateway, your configured S3 buckets will be available as Network File System (NFS) mount points. Your applications read and write files and directories over NFS, interfacing to the gateway as a file server.<br>  In turn, the gateway translates these file operations into object requests on your S3 buckets. Your most recently used data is cached on the gateway for low-latency access, and data transfer between your data center and AWS is fully managed and optimized by the gateway.</li>
<li>Gateway-Cached Volumes<br>  You can store your primary data in <strong>Amazon S3</strong>, and retain your frequently accessed data locally. Gateway-cached volumes provide substantial cost savings on primary storage, minimize the need to scale your storage on-premises, and retain low-latency access to your frequently accessed data.</li>
<li>Gateway-Stored Volumes<br>  In the event you need low-latency access to your entire data set, you can configure your on-premises data gateway to store your primary data locally, and asynchronously back up point-in-time snapshots of this data to <strong>Amazon S3</strong>.</li>
<li>Gateway-Virtual Tape Library<br>  You can have a limitless collection of virtual tapes. Each virtual tape can be stored in a Virtual Tape Library backed by <strong>Amazon S3</strong> or a Virtual Tape Shelf(VTS) backed by <strong>Amazon Glacier</strong>.</li>
</ul>
</li>
<li><p>Storage Gateway - General Facts</p>
<ul>
<li>Can be deployed on-premise, or as an <strong>EC2</strong> instance.</li>
<li>Can schedule snapshots.</li>
<li>You can use Storage Gateway with <strong>Direct Connect</strong>.</li>
<li>You can implement bandwidth throttling.</li>
<li>On-Premise needs with either <strong>Vmware’s ESXi</strong> or <strong>Hyper-V</strong>.</li>
<li>Hardware Requirements:<ul>
<li>4 or 8vCPUs</li>
<li>7.5 GB of RAM</li>
<li>75 GB for installation of VM image and system data</li>
</ul>
</li>
</ul>
</li>
<li><p>Storage Gateway - Storage Requirements</p>
<ul>
<li>For gateway-cached volume configuration, you will need storage for the local cache and an upload buffer.</li>
<li>For gateway-stored volume configuration, you will need storage for your entire dataset and an upload buffer. Gateway-stored volumes can range from 1GiB to 1 TB. Each gateway configured for gateway-stored volumes can support up to 12 volumes and a total volume storage of 16TB.</li>
<li>For gateway-VTL configuration, you will need storage for the local cache and an upload buffer.</li>
</ul>
</li>
<li><p>Storage Gateway - Networking Requirements</p>
<ul>
<li>Open <strong>port 443</strong> on your firewalls.</li>
<li>Internally, you will need to allow <strong>port 80</strong> (activation only), <strong>port 3260</strong> (by local systems to connect to iSCSI targets exposed by the gateway) and <strong>port UDP 53 (DNS)</strong></li>
</ul>
</li>
<li><p>Storage Gateway - Encryption</p>
<ul>
<li>Data in transit is secured using SSL</li>
<li>Data at rest can be encrypted using AES-256</li>
</ul>
</li>
<li><p>Gateway-Cached and Gateway-Stored Volumes</p>
<ul>
<li>You can take point-in-time, incremental snapshots of your volume and store them in <strong>Amazon S3</strong> in the form of <strong>Amazon EBS</strong> snapshots.</li>
<li>Snapshots can be initiated on a scheduled or ad-hoc basis.</li>
<li>Gateway Stored Snapshots<ul>
<li>If your volume data is stored on-premises, snapshots provide durable, off-site backups in <strong>Amazon S3</strong>.</li>
<li>You can create a new <strong>Gateway-Stored</strong> volume from a snapshot in the event you need to recover a backup.</li>
<li>You can also use a snapshot of your Gateway-Stored volume as the starting point for a new <strong>Amazon EBS</strong> volume which you can then attach to an <strong>Amazon EC2</strong> instance.</li>
</ul>
</li>
<li>Gateway Cached Snapshots<ul>
<li>Snapshots can be used to preserve versions of your data, allowing you to revert to a prior version when required or to repurpose a point-in-time version as a new <strong>Gateway-Cached volume</strong>.</li>
</ul>
</li>
</ul>
</li>
<li><p>Gateway-Virtual Tape Library Retrieval<br>  The virtual tape containing your data must be stored in a Virtual Tape Library before it can be accessed. Access to virtual tapes in your Virtual Tape Library is <strong>instantaneous</strong>.</p>
<p>  If the virtual tape containing your data is in your Virtual Tape Shelf, you must first retrieve the virtual tape from your Virtual Tape Shelf. It takes about <strong>24 Hours</strong> for the retrieved virtual tape to be available in the selected Virtual Tape Library.</p>
</li>
<li><p>Gateway-Virtual Tape Library Supports</p>
<ul>
<li>Symantec NetBackup version 7.x</li>
<li>Symantec Backup Exec 2012</li>
<li>Symantec Backup Exec 2014</li>
<li>Symantec Backup Exec 15</li>
<li>Microsoft System Center 2012 R2 Data Protection Manager</li>
<li>Veeam Backup &amp; Replication V7</li>
<li>Veeam Backup &amp; Replication V8</li>
<li>Dell NetVault Backup 10.0</li>
</ul>
</li>
<li><p>Storage Gateway Exam Tips</p>
<ul>
<li>Know the four different Storage Gateway Types:<ul>
<li>File Gateway</li>
<li>Volume Gateway<ul>
<li>Cached - OLD NAME (Gateway-Cached Volumes)</li>
<li>Stored - OLD NAME (Gateway-Stored Volumes)</li>
</ul>
</li>
<li>Tape Gateway - OLD NAME (Gateway-Virtual Tape Library)</li>
</ul>
</li>
<li>Remember that access to virtual tapes in your virtual tape library are instantaneous. If your tape is in the virtual tape shelf(glacier) it can take 24 hours to get back to your virtual tape library.</li>
<li>Encrypted using SSL for transit and is encrypted at rest in Amazon S3 using AES-256.</li>
<li>Gateway-Stored Volumes - stores data as Amazon EBS Snapshots in S3.</li>
<li>Snapshot can be scheduled.</li>
<li>Bandwidth can be throttled (good for remote sites)</li>
<li>You need a storage gateway in each site if using multiple locations.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Snowball (Import/Export)</strong></p>
<ul>
<li>Snowball<ul>
<li>Types<ul>
<li>Snowball</li>
<li>Snowball Edge</li>
<li>Snowmobile</li>
</ul>
</li>
<li>Understand what Snowball is</li>
<li>Understand what Import Export is</li>
<li>Snowball Can<ul>
<li>Import to S3</li>
<li>Export from S3</li>
</ul>
</li>
</ul>
</li>
<li>Import/Export<ul>
<li>Import/Export Disk<ul>
<li>Import to S3, EBS, Glacier</li>
<li>export from S3</li>
</ul>
</li>
<li>Import/Export Snowball<ul>
<li>Import to S3</li>
<li>Export to S3</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Automated Backups</strong></p>
<ul>
<li>AWS Services and Automated Backups<ul>
<li>Services that have Automated Backup<ul>
<li>RDS</li>
<li>Elasticache (Redis only)</li>
<li>Redshift</li>
</ul>
</li>
<li>Services that do not have Automated Backup<ul>
<li>EC2</li>
</ul>
</li>
<li>RDS Automated Backups<ul>
<li>For MySQL you need <strong>innoDB</strong> (transactional engine)</li>
<li>There is a performance hit if Multi-AZ is not enabled</li>
<li>If you delete an instance, then <strong>ALL</strong> automated backups are deleted</li>
<li>However, manual DB snapshots will <strong>NOT</strong> be deleted</li>
<li>All stored on S3</li>
<li>When you do a restore, you can change the engine type (<strong>SQL Standard to SQL Enterprise for example</strong>). Provided you have enough storage space</li>
</ul>
</li>
<li>Elasticache Backups<ul>
<li>Available for <strong>Redis Cache Cluster</strong> only</li>
<li>The entire cluster is snapshotted</li>
<li>Snapshot will degrade performance</li>
<li>Therefore only set your snapshot window during the least busy part of the day</li>
<li>Stored on <strong>S3</strong></li>
</ul>
</li>
<li>Redshift Backups<ul>
<li>Stored on <strong>S3</strong></li>
<li>By default, <strong>Amazon Redshift</strong> enables automated backups of your data warehouse cluster with a <strong>1-day</strong> retention period</li>
<li><strong>Amazon Redshift</strong> only backs up data that has changed so most snapshots only use up a small amount of your free backup storage</li>
</ul>
</li>
<li>EC2<ul>
<li>No automated backups</li>
<li>Backups <strong>degrade</strong> you performance, schedule these times wisely</li>
<li>Snapshots are stored in <strong>S3</strong></li>
<li>Can create automated backups using either the command line interface or Python</li>
<li>They are <strong>incremental</strong>:<ul>
<li>Snapshots only store incremental changes since last snapshot</li>
<li>Only charged for incremental storage</li>
<li>Each snapshot still contains the base snapshot data</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Domain 1 - Summary</strong></p>
<ul>
<li>Domain 1</li>
<li>Domain 1.0: <strong>High Availability</strong> and <strong>Business Continuity</strong></li>
<li>Demonstrate ability to architect the appropriate level of availability based on stakeholder requirements<ul>
<li>1.2 Demonstrate ability to implement <strong>DR</strong> for systems based on <strong>RPO</strong> and <strong>RTO</strong></li>
<li>1.3 Determine appropriate use of <strong>multi-Availability Zones</strong> vs. <strong>multi-Region architectures</strong></li>
<li>1.4 Demonstrate ability to implement self-healing capabilities</li>
<li><strong>15%</strong> of the exam</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Domain-2-Costing-5"><a href="#Domain-2-Costing-5" class="headerlink" title="Domain 2 -Costing - 5%"></a>Domain 2 -Costing - 5%</h3><ul>
<li>Cross Account Access Role &amp; Permissions<ul>
<li>Steps<ul>
<li>Identify our account numbers</li>
<li>Create a group in IAM - Dev</li>
<li>Create a user in IAM - Dev</li>
<li>Log in to Production</li>
<li>Create the “read-write-app-bucket” policy</li>
<li>Create the “UpdateApp” Cross Account Role</li>
<li>Apply the newly created policy to the role</li>
<li>Log in to the Developer Account</li>
<li>Create a new inline policy</li>
<li>Apply it to the Developer group</li>
<li>Login as John</li>
<li>Switch Accounts</li>
</ul>
</li>
</ul>
</li>
<li><p>AWS Organizations &amp; Consolidated Billing</p>
<ul>
<li>AWS Organizations<br>  <strong>AWS Organizations</strong> is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage.<ul>
<li>Available in <strong>two</strong> feature sets:<ul>
<li>Consolidated Billing</li>
<li>ALL Features</li>
</ul>
</li>
<li>consist of<ul>
<li>Root</li>
<li>Organization Unit (OU)</li>
<li>AWS Account</li>
</ul>
</li>
</ul>
</li>
<li>Consolidated Billing<ul>
<li>Accounts<ul>
<li>Paying Account (Paying account is independent. Cannot access resources of the other accounts)</li>
<li>Linked Accounts (All linked accounts are independent)</li>
</ul>
</li>
<li>Advantages<ul>
<li><strong>One bill</strong> per AWS account</li>
<li>Very easy to <strong>track</strong> charges and <strong>allocate</strong> costs</li>
<li>Volume pricing <strong>discount</strong></li>
</ul>
</li>
<li>S3 pricing</li>
<li>Reserved EC2 Instances</li>
<li>Best Practices<ul>
<li>Always <strong>enable</strong> multi-factor authentication on root account.</li>
<li>Always use a <strong>strong</strong> and <strong>complex</strong> password on root account.</li>
<li>Paying account should be used for billing purposes <strong>only</strong>. Do not deploy resources in to paying account.</li>
</ul>
</li>
<li>Notes<ul>
<li>Linked Accounts<ul>
<li><strong>20</strong> linked accounts only</li>
<li>To add more visit <a href="https://aws-portal.amazon.com/gp/aws/html-forms-controller/contactus/aws-account-and-billing" target="_blank" rel="external">https://aws-portal.amazon.com/gp/aws/html-forms-controller/contactus/aws-account-and-billing</a></li>
</ul>
</li>
<li>Billing Alerts<ul>
<li>When monitoring is enabled on the paying account the billing data for all linked accounts is included</li>
<li>You can still create billing alerts per individual account</li>
</ul>
</li>
<li>CloudTrail<ul>
<li>Per <strong>AWS Account</strong> and is enabled <strong>per region</strong>.</li>
<li>Can consolidate logs using an <strong>S3 bucket</strong>.<ol>
<li>Turn on CloudTrail in the paying account</li>
<li>Create a <strong>bucket policy</strong> that allows cross account access</li>
<li>Turn on CloudTrail in the other accounts and use the bucket in the paying account</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips:<ul>
<li>Consolidated billing allows you to get <strong>volume discounts</strong> on all your accounts.</li>
<li>Unused reserved instances for <strong>EC2</strong> are applied across the group.</li>
<li><strong>CloudTrail</strong> is on a per account and per region basis but can be aggregate in to a single bucket in the paying account.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Tagging &amp; Resource Groups</p>
<ul>
<li>What Are Tags<ul>
<li><strong>Key Value Pairs</strong> attached to AWS resources</li>
<li>Metadata <strong>(data about data)</strong></li>
<li>Tags can sometimes be <strong>inherited</strong><ul>
<li>Autoscaling, CloudFormation and Elastic Beanstalk can create other resources</li>
</ul>
</li>
</ul>
</li>
<li>What Are Resource Groups<ul>
<li>Resource groups make it easy to <strong>group your resources</strong> using the tags that are assigned to them. You can group resources that share one or more tags.</li>
<li>Resource groups contain information such as:<ul>
<li>Region</li>
<li>Name</li>
<li>Health Checks</li>
</ul>
</li>
<li>Specific information<ul>
<li>For <strong>EC2</strong> - Public &amp; Private IP Addresses</li>
<li>For <strong>ELB</strong> - Port Configurations</li>
<li>For <strong>RDS</strong> - Database Engine etc.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Reserved Instance for EC2 &amp; RDS</p>
<ul>
<li>EC2<ul>
<li>EC2 - Pricing Models<ul>
<li><strong>On Demand</strong> - allow you to pay a fixed rate by the hour with no commitment.</li>
<li><strong>Reserved</strong> - provide you with a capacity reservation, and offer a significant discount on the hourly charge for an instance 1 year or 3 year terms.</li>
<li><strong>Spot</strong> - enable you to bid whatever price you want for instance capacity, providing for even greater savings if your applications have flexible start and end times.</li>
<li><strong>Dedicated</strong> - Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that’s dedicated to a single customer. Your Dedicated Instances are physically isolated at the host hardware level from your instances that aren’t Dedicated Instances and from instances that belong to other AWS accounts.</li>
</ul>
</li>
<li>EC2 - Pricing Models detail<ul>
<li><strong>On Demand</strong><ul>
<li>Users that want the low cost and flexibility of Amazon EC2 without any up-front payment or long-term commitment</li>
<li>Applications with short term, spiky, or unpredictable workloads that cannot be interrupted.</li>
<li>Applications being developed or tested on Amazon EC2 for the first time</li>
</ul>
</li>
<li><strong>Reserved</strong><ul>
<li>Applications with steady state or predictable usage</li>
<li>Applications that require reserved capacity</li>
<li>Users able to make upfront payments to reduce their total computing costs even further.</li>
</ul>
</li>
<li><strong>Spot</strong><ul>
<li>Applications that have flexible start and end times</li>
<li>Applications that are only feasible at very low compute prices</li>
<li>Users with urgent computing needs for large amounts of additional capacity</li>
</ul>
</li>
</ul>
</li>
<li>Understanding Reserved Instances<ul>
<li>All Up Front = <strong>Largest Discount</strong> (Up to 75%)</li>
<li>Partial Up Front = <strong>Middle Discount</strong></li>
<li>No Upfront = <strong>Least Discount</strong> (Still cheaper than on demand)</li>
</ul>
</li>
<li>Different Types of RIs<ul>
<li><strong>Standard RIs</strong><ul>
<li>These provide the most significant discount <strong>(up to 75% off On-Demand)</strong> and are best suited for steady-state usage.</li>
</ul>
</li>
<li><strong>Convertible RIs</strong><ul>
<li>These provide a discount <strong>(up to 45% off On-Demand)</strong> and the capability to change the attributes of the RI as long as the exchange results in the creation of Reserved Instances of equal or greater value. Like Standard RIs, Convertible RIs are best suited for steady-state usage.</li>
</ul>
</li>
<li><strong>Scheduled RIs</strong><ul>
<li>These are available to launch within the time windows you reserve. This option allows you to match your capacity reservation to a predictable recurring schedule that only requires a fraction of a day, a week, or a month.</li>
</ul>
</li>
<li><strong>Convertible RIs</strong><ul>
<li>Change instance families, operating system, tenancy and payment option</li>
</ul>
</li>
</ul>
</li>
<li>Understanding Reserved Instances<ul>
<li>You can modify reserved instances<ul>
<li>Switching Availability Zones</li>
<li>Change the instance type within the same instance family</li>
</ul>
</li>
</ul>
</li>
<li>Change Your Reserved Instances<ul>
<li>You can change your standard reserved instances by submitting a modification request. This request will be processed providing the footprint remains the same. This is calculated by using <strong>normalization factors</strong>.</li>
</ul>
</li>
<li>Normalization Factors<ul>
<li>Each <strong>Reserved Instance</strong> has an instance size footprint, which is determined by the normalization factor of the instance type and the number of instances in the reservation.</li>
<li>A modification request is <strong>not</strong> processed if the footprint of the target configuration does not match the size of the original configuration. In the <strong>Amazon EC2</strong> console, the footprint is measured in units.</li>
<li>Example:<ul>
<li>Take the quantity of reserved instances and multiply it by the normalization factor.</li>
<li>So a quantity of 2 large instances = (2*4) = <strong>8</strong></li>
<li>This can be changed to 8 small instances <strong>(8*1)</strong> or 4 medium instances <strong>(2*4)</strong></li>
</ul>
</li>
<li><strong>Reserved Instances can only be modified if they are within the SAME family</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>RDS</p>
<ul>
<li><p>RDS - Reserved Instances</p>
<ul>
<li>Each reservation is associated with the following set of attributes:<ul>
<li>DB Engine</li>
<li>DB Instance class</li>
<li>Deployment type</li>
<li>License Model</li>
<li>Region</li>
</ul>
</li>
<li>Each reservation can only be applied to a DB Instance with the <strong>same</strong> attributes for the duration of the term. If you decide to modify any of these attributes of your running DB Instance class before the end of the reservation term, your hourly usage rates for that DB Instance will <strong>revert</strong> to on demand hourly rates.</li>
<li>If you later modify the running DB Instance’s attributes to <strong>match</strong> those of the original reservation, or <strong>create a new</strong> DB Instance with the <strong>same</strong> attributes as your original reservation, your reserved pricing will be applied to it until the end of your reservation term.</li>
<li>You can have <strong>reserved</strong> instances for <strong>RDS Multi-AZ’s</strong> as well as <strong>Read Replicas</strong>.</li>
<li>For Read Replica’s the DB instance class and region <strong>must</strong> be the same.</li>
</ul>
</li>
<li><p>RDS - Move AZs</p>
<ul>
<li>Each Reserved Instance is associated with a specific <strong>Region</strong>, which is fixed for the lifetime of the reservation and cannot be changed. Each reservation <strong>can</strong>, however, be used in any of the available AZs within the associated Region.</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Largest discount is applied for all Upfront <strong>(up to 75%)</strong></li>
<li>Contract length is <strong>1 or 3 years</strong>. The longer the contract, the more you save.</li>
<li>Three types of RIs<ul>
<li><strong>Standard, Convertible &amp; Scheduled</strong></li>
</ul>
</li>
<li>Standard RIs for EC2 can be <strong>modified</strong>, but only if they are in the <strong>same family</strong> and only if the normalization factors are equal and only for linux (excluding RedHat or SUSE Linux)<ul>
<li>Therefore it is better to purchase an RI with more <strong>capacity</strong> than you actually need, so as to allow for growth.</li>
</ul>
</li>
<li>You can <strong>switch</strong> EC2 RIs between AZs, but <strong>not</strong> between regions.</li>
<li>You can sell EC2 RIs on <a href="https://aws.amazon.com/ec2/purchasing-options/reserved-instances/marketplace/" target="_blank" rel="external">https://aws.amazon.com/ec2/purchasing-options/reserved-instances/marketplace/</a></li>
<li>You can have reserved <strong>RDS instances</strong></li>
<li>You can move AZ’s but <strong>not</strong> regions</li>
<li>Understand the different use cases of <strong>reserved, on demand</strong> &amp; <strong>spot instances</strong>.</li>
<li>Typically you only want to use on demand instances for things like <strong>autoscaling</strong>. Spot is going to be the cheapest way of doing something, but is might not be the best answer technically.</li>
<li>Read the question <strong>carefully</strong>. Is it asking you what is the most commercially feasible way of designing a solution, or is it asking for high availability with low RTO/RPOs.</li>
<li>Knowing the <strong>different</strong> instance type by name <strong>REALLY</strong> helps.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Knowing your EC2 Instance Type</p>
<ul>
<li><p>EC2 Instance Types</p>
<p>| Family | Speciality| Use case|<br>| ————- |:————-:| —–:|<br>| D2 | Dense Storage | Fileservers/Data Wareshousing/Hadoop|<br>| R4 | Memory Optimized  | Memory Intensive Apps/DBs|<br>| M4 | General Purpose| Application Servers |<br>| C4 | Compute Optimized | CPU Intensive Apps/DBs |<br>| G2 | Graphics Intensive | Video Encoding/3D Application Streaming |<br>| I2 | High Speed Storage | NoSQL DBs, Data Warehousing etc |<br>| F1 | Field Programmable Gate Array| Hardware acceleration for your code |<br>| T2 | Lowest Cost, General Purpose | Web Servers/Small DBs |<br>| P2 | Graphics/General Purpose GPU | Machine Learning, Bit Coin Mining etc |<br>| X1 | Memory Optimize | SAP HANA/Apache Spark etc |</p>
</li>
<li><p>How to remember Instance type</p>
<ul>
<li>D for Density</li>
<li>R for RAM</li>
<li>M - main choice for general purpose apps</li>
<li>C for Compute</li>
<li>G - Graphics</li>
<li>I for IOPS</li>
<li>F for FPGA</li>
<li>T cheap general purpose (think T2 micro)</li>
<li>P - Graphics (think Pics)</li>
<li>X - Extreme Memory</li>
<li><strong>DR Mc GIFT PX</strong></li>
</ul>
</li>
</ul>
</li>
<li>Domain 2 Summary<ul>
<li>Domain 2.0: Costing<ul>
<li>2.1 Demonstrate ability to make architectural decisions that minimize and optimize infrastructure cost</li>
<li>2.2 Apply the appropriate AWS account and billing set-up options based on scenario</li>
<li>2.3 Ability to compare and contrast the cost implications of different architectures</li>
<li>5% of the exam</li>
</ul>
</li>
<li>Steps To Enable Cross Account Access<ul>
<li>If you need a custom policy (such as read and write access to a custom S3 bucket) create this policy first.</li>
<li>Create role with cross account access</li>
<li>Apply the policy to that role and note down the ARN.</li>
<li>Grant access to the Role</li>
<li>Switch to the Role</li>
<li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html" target="_blank" rel="external">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></li>
<li><a href="https://aws.amazon.com/blogs/security/how-to-enable-cross-account-access-to-the-aws-management-console/" target="_blank" rel="external">https://aws.amazon.com/blogs/security/how-to-enable-cross-account-access-to-the-aws-management-console/</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Domain-3-Deployment-Management-10"><a href="#Domain-3-Deployment-Management-10" class="headerlink" title="Domain 3 - Deployment Management - 10%"></a>Domain 3 - Deployment Management - 10%</h3><ul>
<li><p>CloudFormation</p>
<ul>
<li><strong>READ</strong> the CloudFormation FAQs: <a href="https://aws.amazon.com/cloudformation/faqs/" target="_blank" rel="external">https://aws.amazon.com/cloudformation/faqs/</a> and do the deep dive lab</li>
<li>What is CloudFormation?<ul>
<li>One of the most powerful parts of AWS, <strong>CloudFormation</strong> allows you to take what was once traditional hardware infrastructure and convert it into code.</li>
<li><strong>CloudFormation</strong> gives developers an systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.</li>
<li>You don’t need to figure out the order for provisioning AWS services or the subtleties of making those dependencies work. <strong>CloudFormation</strong> takes care of this for you.</li>
<li>After the AWS resources are deployed, you can <strong>modify and update</strong> them in a controlled and predictable way, in effect applying version control to your AWS infrastructure the same way you do with your software.</li>
</ul>
</li>
<li>CloudFormation Stack vs. Template<ul>
<li>A <strong>CloudFormation Template</strong> is essentially an architectural diagram an a <strong>CloudFormation Stack</strong> is the end result of that diagram (i.e. what is actually provisioned).</li>
<li>You create, update and delete a collection of resources by creating, updating an deleting stacks using CloudFormation templates.</li>
<li>CloudFormation templates are in the <strong>JSON</strong> format or <strong>YAML</strong>.</li>
</ul>
</li>
<li>Elements Of A Template<ul>
<li><strong>Mandatory Elements</strong><ul>
<li>List of AWS Resources and their associated configuration values</li>
</ul>
</li>
<li><strong>Optional Elements</strong><ul>
<li>The template’s file format &amp; version number</li>
<li>Template Parameters<ul>
<li>The input values that are supplied at stack creation time. Limit of 60.</li>
</ul>
</li>
<li>Output Values<ul>
<li>The output values required once a stack has finished building (such as the public IP address, ELB address, etc.) Limit of 60.</li>
</ul>
</li>
<li>List of data tables<ul>
<li>Used to look up static configuration values such as AMI’s etc.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Outputting Data</p>
<ul>
<li>You can use <strong>Fn:GetAtt</strong> to output data.<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&quot;Public&quot;: &#123;</div><div class="line">&quot;Description&quot;: &quot;Public IP address of the web server&quot;,</div><div class="line">&quot;Value&quot;: &#123;</div><div class="line">&quot;Fn::GetAtt&quot;:[</div><div class="line">&quot;WebServerHost&quot;,</div><div class="line">&quot;PublicIp&quot;</div><div class="line">]</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Chef &amp; Puppet Integration</p>
<ul>
<li>Cloud Formation supports <strong>Chef &amp; Puppet Integration</strong>, meaning that you can deploy and configure right down to the application layer.</li>
<li><strong>Bootstrap scripts</strong> are also supported enabling you to install packages, files, an services on your <strong>EC2 instances</strong> by simply describing them in your CloudFormation template.</li>
</ul>
</li>
<li>Stack Creation Errors<ul>
<li>By Default, the **”automatic rollback on error” feature is enabled. This will cause all AWS resources that AWS CloudFormation created successfully for a stack up to the point where an error occurred to be deleted.</li>
<li>You will be <strong>charged</strong> for resources that are provisioned, even if there is an error.</li>
<li>CloudFormation, itself, is <strong>free</strong>.</li>
</ul>
</li>
<li>Stacks Can Wait For Applications<ul>
<li>AWS CloudFormation provides a <strong>WaitCondition</strong> resources that acts as a barrier, blocking the creation of other resources until a completion signal is received from an external source, such as your application or management system.</li>
</ul>
</li>
<li>You can Specify Deletion Policies<ul>
<li>AWS CloudFormation allows you to <strong>define deletion policies</strong> for resources in the template. You can specify that snapshots be created for Amazon EBS volumes or Amazon RDS database instances before they are deleted.</li>
<li>You can also specify that a resource should be preserved and not deleted when the stack is deleted. This is useful for preserving <strong>Amazon S3 buckets</strong> when the stack is deleted.</li>
</ul>
</li>
<li>You Can Update Your Stack After It Is Created<ul>
<li>You can use AWS CloudFormation to <strong>modify</strong> and <strong>update</strong> the resources in your existing stacks in a controlled and predictable way. By using templates to manage your stack changes, you have the ability to apply version control to your <strong>AWS infrastructure</strong>, just as you do with the software running on it.</li>
</ul>
</li>
<li><p>CloudFormation &amp; Roles</p>
<ul>
<li><p>CloudFormation can be used to create roles in <strong>IAM</strong>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">&quot;Type&quot;: &quot;AWS::IAM::Role&quot;,</div><div class="line">&quot;Properties&quot;:&#123;</div><div class="line">  &quot;AssumeRolePolicyDocument&quot;: &#123; JSON &#125;,</div><div class="line">  &quot;ManagedPolicyArns&quot;: [String,...],</div><div class="line">  &quot;Path&quot;: String,</div><div class="line">  &quot;Policies&quot;: [Policies,...]</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>CloudFormation can be used to grant EC2 instances access to roles.</p>
</li>
</ul>
</li>
<li><p>VPCs Can Be Created And Customized</p>
<ul>
<li><strong>CloudFormation</strong> supports creating VPCs, Subnets, Gateways, Route Tables and Network ACLs, as well as creating resources such as Elastic IPs, Amazon EC2 Instances, EC2 Security Groups, Auto Scaling Groups, Elastic Load Balancers, Amazon RDS Database Instances, and Amazon RDS Security Groups in a VPC.</li>
<li>You can specify IP address ranges both in terms of <strong>CIDR ranges</strong> as well as <strong>individual IP addresses</strong> for specific instances. You can specify pre-existing EIPs.  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;AWSTemplateFormatVersion&quot; : &quot;2010-09-09&quot;,</div><div class="line">    &quot;Description&quot; : &quot;Simple Standalone ENI&quot;,</div><div class="line">    &quot;Resources&quot; : &#123;</div><div class="line">        &quot;myENI&quot; : &#123;</div><div class="line">            &quot;Type&quot; : &quot;AWS::EC2::NetworkInterface&quot;,</div><div class="line">            &quot;Properties&quot; : &#123;</div><div class="line">                &quot;Tags&quot;: [&#123;&quot;Key&quot;:&quot;foo&quot;,&quot;Value&quot;:&quot;bar&quot;&#125;],</div><div class="line">                &quot;Description&quot;: &quot;A nice description.&quot;,</div><div class="line">                &quot;SourceDestCheck&quot;: &quot;false&quot;,</div><div class="line">                &quot;GroupSet&quot;: [&quot;sg-75zzz219&quot;],</div><div class="line">                &quot;SubnetId&quot;: &quot;subnet-3z648z53&quot;,</div><div class="line">                &quot;PrivateIpAddress&quot;: &quot;10.0.0.16&quot;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>VPC Peering</p>
<ul>
<li>You can create <strong>multiple</strong> VPCs inside one template.</li>
<li>You can enable <strong>VPC Peering</strong> using <strong>CloudFormation</strong>, but only within the <strong>same</strong> AWS Account.</li>
</ul>
</li>
<li>Route53 Supported<ul>
<li>You can create <strong>new</strong> hosted zones or <strong>update existing</strong> hosted zones using <strong>CloudFormation Templates</strong>.</li>
<li>This includes <strong>adding</strong> or <strong>changing</strong> items such as A records, Aliases, CNAMEs, etc.</li>
</ul>
</li>
<li>Exam Tips<ul>
<li><strong>CloudFormation</strong> - a big topic in the exam.</li>
<li>Know <strong>all</strong> the services that are supported.</li>
<li>Remember what is mandatory for a template - <strong>“Resources”</strong></li>
<li>You can create <strong>multiple VPCs</strong> inside one template.</li>
<li>You can <strong>enable VPC Peering</strong> using CloudFormation, but only within the <strong>same</strong> AWS Account.</li>
<li><strong>Chef, Puppet</strong> &amp; <strong>Bootstrap Scripts</strong> are supported.</li>
<li>You can use <strong>Fn::GetAtt</strong> to output data.</li>
<li>By default, the <strong>“automatic rollback on error”</strong> feature is <strong>enabled</strong>.</li>
<li>You are <strong>charged</strong> for errors.</li>
<li>CloudFormation is <strong>free</strong>.</li>
<li>Stacks can wait for applications to be provisioned using the <strong>“WaitCondition”</strong>.</li>
<li>Route53 is completely <strong>supported</strong>. This includes creating <strong>new</strong> hosted zones or <strong>updating</strong> existing ones.</li>
<li>You can create A Records, Aliases etc.</li>
<li>IAM Role Creation and Assignment is also <strong>supported</strong>.</li>
</ul>
</li>
<li><p>CloudFormation material</p>
<ul>
<li>CloudFormation Basic<ul>
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/gettingstarted.templatebasics.html" target="_blank" rel="external">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/gettingstarted.templatebasics.html</a></li>
<li>Learn about the following about templates:<ul>
<li>Declaring resources and their properties</li>
<li><strong>Ref</strong>erencing(提取) other resources with the <strong>Ref</strong> function and resource attributes using the Fn::GetAtt function</li>
<li>Using parameters to enable users to specify values at stack creation time and using constraints to validate parameter input</li>
<li>Using mappings to determine conditional values</li>
<li>Using the Fn::Join function to construct values based on parameters, resource attributes, and other things</li>
<li>Using output values based to capture information about the stack’s resources.</li>
</ul>
</li>
</ul>
</li>
<li><p>CloudFormation intrinsic Function</p>
<ul>
<li>Fn::Base64<ul>
<li>returns the Base64 representation of the input string. this function is typically used to pass encoded data to Amazon EC2 instances by way of the UserData property.</li>
<li>JSON Format { “Fn::Base64” : valueToEncode }</li>
</ul>
</li>
<li>Fn::FindInMap<ul>
<li>returns the value corresponding to keys in a two-level map that is declared in the Mapping section.</li>
<li>JSON Format { “Fn::FindInMap” : [ “MapName”, “TopLevelKey”, “SecondLevelKey”] }</li>
</ul>
</li>
<li>Fn::GetAtt<ul>
<li>returns the value of an attribute from a resource in the template</li>
<li>JSON Format { “Fn::GetAtt” : [ “logicalNameOfResource”, “attributeName” ] }</li>
</ul>
</li>
<li>Fn::Join<ul>
<li>Fn::Join appends a set of values into a single value, separated by the specified delimiter. If a delimiter is the empty string, the set of values are concatenated with no delimiter.</li>
<li>JSON Format { “Fn::Join” : [ “delimiter”, [ comma-delimited list of values ] ] }</li>
<li>JSON example {“Fn::Join” : [ “:”, [ “a”, “b”, “c” ] ]} will returns “a:b:c”</li>
</ul>
</li>
<li>Fn::Select<ul>
<li>returns a single object from a list of objects by index</li>
<li>JSON Format { “Fn::Select” : [ index, listOfObjects ] }</li>
<li>JSON example { “Fn::Select” : [ “1”, [ “apples”, “grapes”, “oranges”, “mangoes” ] ] } will returns “grapes”</li>
</ul>
</li>
<li>Fn::Split<ul>
<li>To split a string into a list of string values so that you can select an element from the resulting string list.</li>
<li>JSON Format { “Fn::Split” : [ “delimiter”, “source string” ] }</li>
<li>JSON example { “Fn::Split” : [ “|” , “a|b|c” ] } will return [“a”, “b”, “c”]</li>
</ul>
</li>
<li><p>Fn::Sub</p>
<ul>
<li>将输入字符串中的变量替换为您指定的值</li>
<li>JSON Format  { “Fn::Sub” : [ String, { Var1Name: Var1Value, Var2Name: Var2Value } ] }</li>
<li>JSON example - 将AWS::Region和AWS::StackName替换为实际的值<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&quot;UserData&quot;: &#123; &quot;Fn::Base64&quot;: &#123; &quot;Fn::Join&quot;: [&quot;\n&quot;, [</div><div class="line">&quot;#!/bin/bash -xe&quot;,</div><div class="line">&quot;yum update -y aws-cfn-bootstrap&quot;,</div><div class="line">&#123; &quot;Fn::Sub&quot;: &quot;/opt/aws/bin/cfn-init -v --stack $&#123;AWS::StackName&#125; --resource LaunchConfig --configsets wordpress_install --region $&#123;AWS::Region&#125;&quot; &#125;,</div><div class="line">&#123; &quot;Fn::Sub&quot;: &quot;/opt/aws/bin/cfn-signal -e $? --stack $&#123;AWS::StackName&#125; --stack $&#123;AWS::StackName&#125; --resource WebServer --region $&#123;AWS::Region&#125;&quot; &#125;]]</div><div class="line">&#125;&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Ref</p>
<ul>
<li>returns the value of the specified parameter or resource.<ul>
<li>When you specify a parameter’s logical name, it returns the value of the parameter.</li>
<li>When you specify a resource’s logical name, it returns a value that you can typically use to refer to that resource, such as a physical ID.</li>
</ul>
</li>
<li>JSON Format { “Ref” : “logicalName” }</li>
</ul>
</li>
</ul>
</li>
<li>CloudFormation limit<ul>
<li>You can include up to 60 parameters and 60 outputs in a template.</li>
<li>There are no limit to the number of templates.</li>
<li>Each AWS CloudFormation account is limited to a maximum of 200 stacks.</li>
</ul>
</li>
<li>CloudFormation – Ref, Fn::Join, GetAtt, Fn::split, Fn::select and etc function</li>
</ul>
</li>
</ul>
</li>
<li><p>Elastic Beanstalk</p>
<ul>
<li>Elastic Beanstalk<ul>
<li><strong>AWS Elastic Beanstalk</strong> makes it even easier for developers to quickly deploy and manage applications in the AWS cloud. Developers simply upload their application, and Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring.</li>
</ul>
</li>
<li>Difference Between CloudFormation &amp; Elastic Beanstalk?<ul>
<li><strong>CloudFormation</strong> supports Elastic Beanstalk, but Elastic Beanstalk will <strong>not</strong> provision CloudFormation Templates.</li>
</ul>
</li>
<li>Supported Languages * Stacks<ul>
<li>Packer Builder</li>
<li>Apache<ul>
<li>Apache Tomcat for Java applications (Java 6,7 &amp; 8)</li>
<li>Apache HTTP Server for PHP applications (PHP 5.4 - 7.0)</li>
<li>Apache HTTP Server for Python applications</li>
<li>Nginx or Apache HTTP Server for Node.js applications</li>
</ul>
</li>
<li>Docker<ul>
<li>Single Container Docker</li>
<li>Multicontainer Docker</li>
<li>Preconfigured Docker</li>
</ul>
</li>
<li>Ruby (Passenger or Puma) (Ruby 1.9 - 2.3)</li>
<li>Go 1.6</li>
<li>Java with Tomcat</li>
<li>Java SE (Java 7 &amp; 8)</li>
<li>.NET on Windows Server with IIS 7.5, 8.0 and 8.5</li>
<li>Node.js (nginx &amp; Apache)</li>
<li>Python (2.6 - 3.4)</li>
<li>PHP</li>
</ul>
</li>
<li>What Can Be Controlled With Elastic Beanstalk<ul>
<li>Access built-in <strong>Amazon CloudWatch</strong> monitoring and get notifications on application health and other important events</li>
<li>Adjust application server settings <strong>(e.g. JVM settings)</strong> and pass environment variables</li>
<li>Run other application components, such as a <strong>memory caching service</strong>, side-by-side in Amazon EC2</li>
<li>Access log files <strong>without</strong> logging in to the application servers</li>
</ul>
</li>
<li>Provisioning With Elastic Beanstalk<ul>
<li>You can simply upload your deployable code <strong>(e.g. WAR file)</strong> or push your Git repository, and AWS Elastic Beanstalk does the rest. The AWS Toolkit for <strong>Visual Studio</strong> and the AWS Toolkit for <strong>Eclipse</strong> allow you to deploy your application to AWS Elastic Beanstalk and manage it, without leaving your IDE.</li>
</ul>
</li>
<li>Updating Elastic Beanstalk<ul>
<li>You can push out <strong>updates from GIT</strong>: only the modified files are transmitted to AWS Elastic Beanstalk.</li>
<li>Elastic Beanstalk is designed to support <strong>multiple</strong> running environments such as one for integration testing, one for <strong>pre-production</strong>, and one for <strong>production</strong>. Each environment is independently configured and runs on its <strong>own</strong> separate AWS resources. Elastic Beanstalk also <strong>stores</strong> and <strong>tracks</strong> application versions over time, so an existing environment can be easily rolled back to a prior version or a new environment can be launched using an older version to try and reproduce a customer problem.</li>
</ul>
</li>
<li>How Does It Work?<ul>
<li>Elastic Beanstalk stores your application files and, <strong>optionally</strong>, your server log files in Amazon S3. If you are using the AWS Management Console, Git, the AWS Toolkit for Visual Studio, or AWS Toolkit for Eclipse, an Amazon S3 bucket will be created in your account, and the files you upload will be <strong>automatically</strong> copied from your local client to Amazon S3.</li>
<li>Optionally, you may configure Elastic Beanstalk to copy your server log files <strong>every hour</strong> to Amazon S3. You do this by editing the environment configuration settings.</li>
</ul>
</li>
<li>Can It Store Application Data Like Images?<ul>
<li>You can use Amazon S3 for <strong>application storage</strong>. The easiest way to do this is by including the <strong>AWS SDK</strong> as part of your application’s deployable file. For example, you can <strong>include</strong> the AWS SDK for Java as port of your application’s WAR file.</li>
</ul>
</li>
<li>Can Elastic Beanstalk Provision RDS Instances?<ul>
<li><strong>Yes:</strong> Elastic Beanstalk can <strong>automatically</strong> provision an Amazon RDS DB Instance. The connectivity information to the DB Instance is exposed to your application by <strong>environment variables</strong>.</li>
</ul>
</li>
<li>Elastic Beanstalk Fault Tolerance<ul>
<li>Elastic Beanstalk can be configured to be <strong>fault tolerant</strong> within a <strong>single region</strong> using <strong>multiple</strong> availability zones.</li>
</ul>
</li>
<li>Elastic Beanstalk Security<ul>
<li>By default, your application is available publicly at <strong>myapp.elasticbeanstalk.com</strong></li>
<li>Integrates with <strong>AWS VPC</strong>. You can then <strong>restrict</strong> who can access the app via <strong>white-listed IP addresses</strong> (either at the security group level or NACL level.)</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Elastic Beanstalk can provision <strong>RDS</strong> instances</li>
<li>Elastic Beanstalk supports <strong>IAM</strong></li>
<li>Elastic Beanstalk supports <strong>VPC</strong></li>
<li>You have <strong>full access</strong> to the resources under Elastic Beanstalk</li>
<li>Code is stored in <strong>S3</strong></li>
<li><strong>Multiple</strong> environments are allowed to support version control. You can <strong>roll</strong> back changes.</li>
<li><strong>Only</strong> the changes from <strong>Git repositories</strong> are replicated.</li>
<li>Amazon Linux AMI &amp; Windows 2012 R2 <strong>supported</strong>.\</li>
</ul>
</li>
</ul>
</li>
<li><p>OpsWorks</p>
<ul>
<li><p>What is OpsWorks</p>
<ul>
<li>Cloud-based applications usually require a group of related resources—application servers, database servers, and so on—that must be created and managed collectively. This collection of instances is called a stack. A simple application stack might look something like the following.<br><img src="/images/AWS/Sysops/what_is_opswork.png" alt="what_is_opswork"></li>
<li>AWS OpsWorks Stacks provides a simple and straightforward way to create and manage stacks and their associated applications and resources</li>
<li><strong>Amazon Definition</strong>:<ul>
<li>AWS OpsWorks is an application management service that helps you <strong>automate</strong> operational tasks like code deployment, software configurations, package installations, database setups, and server scaling using <strong>Chef</strong>. <strong>OpsWorks</strong> gives you the flexibility to define your <strong>application architecture</strong> and <strong>resource configuration</strong> and handles the <strong>provisioning</strong> and <strong>management</strong> of your AWS resources for you. OpsWorks includes automation to scale your application based on <strong>time</strong> or <strong>load</strong>, monitoring to help you troubleshoot and take automated action based on the state of your resources, and permissions and policy management to make management of multi-user environments <strong>easier</strong>.</li>
</ul>
</li>
</ul>
</li>
<li><p>What is Chef</p>
<ul>
<li>Chef turns <strong>infrastructure</strong> into <strong>code</strong>. With Chef, you can <strong>automate</strong> how you <strong>build</strong>, <strong>deploy</strong>, and <strong>manage</strong> your infrastructure. Your infrastructure becomes as versionable, testable, and repeatable as application code.</li>
<li>Chef server stores your <strong>recipes</strong> as well as other configuration <strong>data</strong>. The Chef client is installed on each server, virtual machine, container, or networking device you manage - we’ll call these <strong>nodes</strong>. The client periodically polls Chef server latest policy and state of your network. If anything on the node is out of date, the client brings it up to date.</li>
</ul>
</li>
<li><p>What is OpsWorks</p>
<ul>
<li>A <strong>GUI</strong> to deploy and configure your infrastructure quickly. OpsWorks consists of <strong>two</strong> elements, <strong>Stacks</strong> and <strong>Layers</strong>.</li>
<li>A stack is a <strong>container</strong> (or <strong>group</strong>) of resources such as <strong>ELBs</strong>, <strong>EC2</strong> instances, <strong>RDS</strong> instances etc.</li>
<li>A <strong>layer</strong> exists <strong>within</strong> a stack and consists of things like a <strong>web application</strong> layer. An <strong>application processing</strong> layer or a <strong>Database</strong> layer.</li>
<li>When you <strong>create</strong> a layer, rather than going and configuring everything manually (like installing Apache, PHP etc) OpsWorks takes <strong>care</strong> of this for you.</li>
</ul>
</li>
<li><p>Layers</p>
<ul>
<li><strong>1 or more</strong> layers in the stack</li>
<li>An instance must be assigned to <strong>at least</strong> 1 layer</li>
<li>Which chef layers run, are <strong>determined by</strong> the layer the instance belongs to</li>
<li>Preconfigured Layers <strong>include</strong>:<ul>
<li>Applications</li>
<li>Databases</li>
<li>Load Balancers</li>
<li>Caching</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Domain 3<ul>
<li>Domain 3.0: Deployment Management<ul>
<li>3.1 Ability to manage the lifecycle of an application on AWS</li>
<li>3.2 Demonstrate ability to implement the right architecture for development, testing, and staging environments</li>
<li>3.3 Position and select most appropriate AWS deployment mechanism based on scenario</li>
</ul>
</li>
<li>OpsWorks Exam Tips:<ul>
<li>Do the <strong>OpsWorks Lab</strong></li>
<li>You need to know the difference between:<ul>
<li>A <strong>Stack</strong></li>
<li>A <strong>Layer</strong></li>
<li>A <strong>Recipe</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Domain-4-Network-Design-10"><a href="#Domain-4-Network-Design-10" class="headerlink" title="Domain 4 - Network Design - 10%"></a>Domain 4 - Network Design - 10%</h3><ul>
<li><p>VPC</p>
<ul>
<li>What You Should Know Already<ul>
<li>What a <strong>VPC</strong> is</li>
<li>How to <strong>build</strong> your own VPC</li>
<li>How to make a subnet <strong>public</strong></li>
<li>How to make a subnet <strong>private</strong></li>
<li>What a <strong>NAT</strong> is<ul>
<li>Disable Source/Destination Checks</li>
</ul>
</li>
<li>What a <strong>route table</strong> is </li>
<li><strong>Subnets</strong> can communicate with each other by default</li>
</ul>
</li>
<li>Basic Info<ul>
<li>Think of a VPC as a logical datacenter in AWS</li>
<li>Consists of IGW’s (Or Virtual Private Gateways), Route Tables, Network Access Control Lists, Subnets, Security Groups</li>
<li>1 Subnet = 1 Availability Zone</li>
<li>Security Groups are Stateful, Network Access Control Lists are Stateless.</li>
<li>Can Peer VPCs both in the same account and with other AWS accounts.</li>
<li>No Transitive Peering</li>
<li>Custom VPC network block size has to be between a /16 netmask and /28 netmask.</li>
</ul>
</li>
<li>What can you do with a VPC<ul>
<li>Launch instances into a subnet of your choosing</li>
<li>Assign custom IP address ranges in each subnet</li>
<li>Configure route tables between subnets</li>
<li>Create internet gateway and attach it to our VPC</li>
<li>Much better security control over your AWS resources</li>
<li>Instance security groups</li>
<li>Subnet network access control lists (ACLS)</li>
</ul>
</li>
<li><p>Default VPC vs Custom VPC</p>
<ul>
<li>Default VPC is user friendly, allowing you to immediately deploy instances</li>
<li>All Subnets in default VPC have a route out to the internet.</li>
<li>Each EC2 instance has both a public and private IP address</li>
<li>If you delete the default VPC the only way to get it back is to contact AWS.</li>
</ul>
</li>
<li><p>VPC peering</p>
<ul>
<li>Allows you to connect one VPC with another via a direct network route using private IP addresses.</li>
<li>Instances behave as if they were on the same private network</li>
<li>You can peer VPC’s with other AWS accounts as well as with other VPCs in the same account.</li>
<li>Peering is in a star configuration, ie 1 central VPC peers with 4 others, <strong>NO TRANSITIVE PEERING!!!</strong></li>
</ul>
</li>
<li><p>Create VPC</p>
<ul>
<li>things automatically created<ul>
<li>Route tables</li>
<li>Network ACLs</li>
<li>Security Groups</li>
<li>DHCP options set</li>
</ul>
</li>
<li>things are not automatically created<ul>
<li>Internet Gateways</li>
<li>Subnets</li>
</ul>
</li>
</ul>
</li>
<li><p>VPC Subnet</p>
<ul>
<li>There are 5 IP address reserved in each subnet by AWS, take CIDR block 10.0.0.0/24 as example<ul>
<li>10.0.0.0 Network address</li>
<li>10.0.0.1 Reserved by AWS for the VPC router</li>
<li>10.0.0.2 Reserved by AWS for DNS</li>
<li>10.0.0.3 Reserved by AWS for future use.</li>
<li>10.0.0.255 Network broadcast address, we do not support broadcast in a VPC, therefore we reserve this address.</li>
</ul>
</li>
</ul>
</li>
<li><p>NAT instances</p>
<ul>
<li>When creating a NAT instance, Disable Source/Destination Check on the Instance</li>
<li>NAT instance must be in a public subnet</li>
<li>Must have an elastic IP address to work</li>
<li>There must be a route out of the private subnet to the NAT instance, in order for this to work</li>
<li>The amount of traffic that NAT instances supports, depends on the instance size. If you are bottlenecking, increase the instance size</li>
<li>You can create high availability using Autoscaling Groups, multiple subnets in different AZ’s and a script to automate failover</li>
<li>Behind a Security Group.</li>
</ul>
</li>
<li><p>NAT Gateways</p>
<ul>
<li>Very new</li>
<li>Preferred by the enterprise</li>
<li>Scale automatically up to 10Gbps</li>
<li>No need to patch</li>
<li>Not associated with security groups</li>
<li>Automatically assigned a public ip address</li>
<li>Remember to update your route tables.</li>
<li>No need to disable Source/Destination Checks.</li>
</ul>
</li>
<li><p>NAT instances vs NAT Gateways<br>  <img src="/images/AWS/CSAP/CSAP_comparing_of_NAT_instance_an_NAT_gateways.png" alt="CSAP_comparing_of_NAT_instance_an_NAT_gateways"></p>
</li>
<li><p>Network ACL’s</p>
<ul>
<li>Your VPC automatically comes a default network ACL and by default it allows all outbound and inbound traffic.</li>
<li>You can create a custom network ACL. By default, each custom network ACL denies all inbound and outbound traffic until you add rules.</li>
<li>Each subnet in your VPC must be associated with a network ACL. If you don’t explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL.</li>
<li>You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed.</li>
<li>A network ACL contains a numbered list of rules that is evaluated in order, starting with the lowest numbered rule.</li>
<li>A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic.</li>
<li>Network ACLs are stateless responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa)</li>
<li>Block IP Addresses using network ACL’s not Security Groups</li>
</ul>
</li>
<li><p>Security Group vs Network ACL<br>  <img src="/images/AWS/CSAP/CSAP_comparison_of_SecurityGroups_and_NetworkACLs.png" alt="CSAP_comparison_of_SecurityGroups_and_NetworkACLs"></p>
</li>
<li><p>NAT vs Bastions</p>
<ul>
<li>A NAT is used to provide internet traffic to EC2 instances in private subnets</li>
<li>A Bastion is used to securely administer EC2 instance (using SSH or RDP) in private subnets. In Australia we call them jump boxes.</li>
</ul>
</li>
<li><p>Resilient Architecture</p>
<ul>
<li>If you want resiliency, always have 2 public subnets and 2 private subnets. Make sure each subnet is in different availability zones.</li>
<li>With ELB’s make sure they are in 2 public subnets in 2 different availability zones.</li>
<li>With Bastion hosts, put them behind an autoscaling group with a minimum size of 2. Use Route53 (either round robin or using a health check) to automatically fail over.</li>
<li>NAT instances are tricky to make resilient. You need 1 in each public subnet, each with their own public IP address, and you need to write a script to fail between the two. Instead where possible, use NAT gateways.</li>
</ul>
</li>
<li><p>VPC Flow Logs</p>
<ul>
<li>You can monitor network traffic within your custom VPC’s using VPC Flow Logs.</li>
</ul>
</li>
<li><p>VPC limit</p>
<ul>
<li>Currently you can create 200 subnets per VPC by default</li>
</ul>
</li>
</ul>
</li>
<li><p>VPC Peering</p>
<ul>
<li>What is VPC Peering<ul>
<li>VPC Peering is simply a connection between two VPCs that enables you to route traffic between them using private IP addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account within a <strong>single region</strong>.</li>
<li>AWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor a VPN connection, and <strong>does not</strong> rely on a separate pieces of physical hardware. There is no single point of failure for communication or a bandwidth bottleneck.</li>
</ul>
</li>
<li>Transitive Peering NOT Supported</li>
<li>VPC Peering Limitations<ul>
<li>Soft limit of <strong>50 VPC</strong> peers per VPC, can be increased to <strong>125</strong> by request.</li>
<li>You <strong>cannot</strong> create a VPC peering connection between VPCs that have matching or overlapping CIDR blocks.</li>
<li>You <strong>cannot</strong> create a VPC peering connection between VPCs in different regions.</li>
<li>VPC peering does <strong>not</strong> support transitive peering relationships.</li>
<li>A placement group can span peered VPCs; however, you will <strong>not</strong> get full bandwidth between instances in peered VPCs.</li>
<li>Private DNS values <strong>cannot</strong> be resolved between instances in peered VPCs.</li>
</ul>
</li>
<li>Steps To Setup VPC Peering<ul>
<li>Owner of the <strong>local</strong> VPC sends a request to the owner of the second VPC to peer.</li>
<li>Owner of the <strong>second</strong> VPC has to accept.</li>
<li>Owner of the <strong>local</strong> VPC adds a route to their route table allowing their subnets to route out to the peer VPC</li>
<li>Owner of the <strong>peer</strong> VPC adds a route to their route table allowing their subnets to route back to the other VPC.</li>
<li>Security Groups in both VPCs have to <strong>both</strong> allow traffic.</li>
</ul>
</li>
<li>Troubleshooting<ul>
<li><strong>Setting up the peer:</strong><ul>
<li>Are the VPCs in the same region?</li>
<li>If you can’t create a VPC peer, check to see if the CIDR blocks are overlapping.</li>
</ul>
</li>
<li><strong>After the peer is setup:</strong><ul>
<li>Check that the relevant security groups and NACLs are allowing traffic through.</li>
<li>Check that a route has been created in BOTH VPCs routing tables.</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Remember the <strong>steps</strong> to setting up a VPC peer.</li>
<li>Remember how to <strong>troubleshoot</strong> issues with VPC peering.</li>
</ul>
</li>
</ul>
</li>
<li><p>Direct Connect</p>
<ul>
<li><p>What is Direct Connect <a href="https://aws.amazon.com/directconnect/?nc1=h_ls" target="_blank" rel="external">https://aws.amazon.com/directconnect/?nc1=h_ls</a></p>
<ul>
<li><strong>AWS Direct Connect</strong> makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.</li>
<li><strong>AWS Direct Connect</strong> lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. </li>
<li>This allows you to use the <strong>same</strong> connection to access public resources such as objects stored in Amazon S3 using public IP address space, and private resources such as Amazon EC2 instances running within an Amazon Virtual Private Cloud (<strong>VPC</strong>) using private IP space, while maintaining network separation between the public and private environments. Virtual interfaces can be reconfigured at any time to meet your changing needs.</li>
</ul>
</li>
<li><p>Direct Connect Benefits</p>
<ul>
<li>Reduce <strong>costs</strong> when using large volumes of traffic</li>
<li>Increase <strong>reliability</strong></li>
<li>Increase <strong>bandwidth</strong></li>
</ul>
</li>
<li>How Is Direct Connect Different From A VPN?<ul>
<li><strong>VPN Connections</strong> can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.</li>
<li><strong>AWS Direct Connect</strong> does not involve the Internet. Instead, it uses dedicated, private network connections between your intranet and <strong>Amazon VPC</strong>.</li>
</ul>
</li>
<li>Direct Connect Connections<ul>
<li>Available In:<ul>
<li>10 Gbps</li>
<li>1 Gbps</li>
</ul>
</li>
<li>Sub 1Gbps can be purchased through <strong>AWS Direct Connect Partners</strong></li>
<li>Uses <strong>Ethernet VLAN trunking (802.1Q)</strong></li>
<li>This dedicated connection can be partitioned in to multiple virtual interfaces (<strong>VIFs</strong>)<ul>
<li>Allows <strong>public</strong> connections to <strong>EC2</strong> or <strong>S3</strong> using public IP addresses</li>
<li>Allows <strong>private</strong> connections to <strong>VPC</strong> using internal IP addresses<br><img src="/images/AWS/CSAP/CSAP_direct_connect_diagram_1.png" alt="CSAP_direct_connect_diagram_1"><br><img src="/images/AWS/CSAP/CSAP_direct_connect_diagram_2.png" alt="CSAP_direct_connect_diagram_2"></li>
</ul>
</li>
<li>Use BGP to Fail Over Automatically from Direct Connect to Site to Site VPN<br><img src="/images/AWS/CSAP/CSAP_direct_connect_use_bgp_to_failover_automatically_from_direct_connect_to_site_to_site_VPN.png" alt="CSAP_direct_connect_use_bgp_to_failover_automatically_from_direct_connect_to_site_to_site_VPN"></li>
</ul>
</li>
<li>CGW vs. VPG<ul>
<li>When using a <strong>VPN</strong> to connect to a <strong>VPC</strong>, you need an anchor on each side of that connection. A <strong>Customer Gateway</strong> is the anchor on your side of that connection. It can be a physical or software appliance.</li>
<li>The anchor on the AWS side of the VPN connection is called a <strong>Virtual Private Gateway</strong>.</li>
</ul>
</li>
<li>Key Points To Remember<ul>
<li>If you are accessing public services using HTTPS endpoints (think DynamoDB, S3) then use <strong>public</strong> VIFs.</li>
<li>If you are accessing VPCs using private IP address ranges, then use <strong>private</strong> VIFs.</li>
<li>In the US, you only need <strong>1</strong> direct connect connection to connect in to all 4 US regions. Data transferred between regions goes over AWS’s internal lines, not the public internet.</li>
<li>Direct connect itself is not redundant. You can <strong>add</strong> redundancy by having <strong>2</strong> connections(<strong>2 routes, 2 direct connects</strong>), or by having a site-to-site VPN in place.</li>
<li>Layer 2 connections are <strong>not</strong> supported.</li>
<li>Know the difference between a <strong>Customer Gateway CGW</strong> (Customer side) and <strong>Virtual Private Gateway</strong> (AWS side.)</li>
</ul>
</li>
</ul>
</li>
<li><p>HPC &amp; Enhanced Networking</p>
<ul>
<li>HPC - Guide Lines<br>High performance compute is used by many different industries, such as the pharmaceutical or automotive industries. <strong>HPC</strong> typically involves:<ul>
<li>Batch processing with large and compute intensive workloads</li>
<li>Demands High Performance CPU, Network &amp; Storage</li>
<li>Usually Jumbo Frames are required</li>
</ul>
</li>
<li>What Are Jumbo Frames?<ul>
<li><strong>Jumbo Frames</strong> are ethernet frames with more than <strong>1500 bytes</strong> of payload. A Jumbo Frame can carry up to <strong>9000 bytes</strong> of payload.</li>
<li>Shared file systems such as <strong>Lustre</strong> and <strong>NFS</strong> use Jumbo Frames frequently. HPC applications use a lot of disk I/O and need access to a shared file system, this makes Jumbo Frames critical.</li>
</ul>
</li>
<li>SR-IOV<ul>
<li>The use of Jumbo Frames is supported on AWS through enhanced networking. Enhanced networking is available using single-root I/O virtualization (<strong>SR-IOV</strong>) on supported instance types:<ul>
<li>C3</li>
<li>C4</li>
<li>D2</li>
<li>I2</li>
<li>M4</li>
<li>R3</li>
</ul>
</li>
<li>Must be on HVM VMs, not PV</li>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html" target="_blank" rel="external">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html</a></li>
</ul>
</li>
<li>Placement Groups<br>  A placement group is a logical grouping of instances within a single <strong>Availability Zone</strong>. Using placement groups enables applications to participate in a low-latency, 10Gbps network. Placement groups are <strong>recommended</strong> for applications that benefit from low network latency, high network throughput, or both.<br>  To provide the lowest latency and the highest packet-per-second network performance for your placement group, choose an instance type that supports <strong>enhanced networking</strong>.<ul>
<li>Don’t span availability zones: <strong>1</strong> placement group = <strong>1 AZ</strong>.</li>
<li>Placement groups can span subnets, but the subnets must be in the <strong>same</strong> AZ.</li>
<li>Only certain instance types (<strong>enhanced networking instances</strong>) may be launched in to a placement group:<ul>
<li>C3</li>
<li>C4</li>
<li>D2</li>
<li>I2</li>
<li>M4</li>
<li>R3</li>
</ul>
</li>
<li>Existing instances <strong>cannot</strong> be moved in to a placement group.</li>
<li>Although launching multiple instance types into a placement group is possible, this <strong>reduces</strong> the likelihood that the required capacity will be available for your launch to succeed.</li>
<li>There may not be <strong>sufficient</strong> capacity to add extra instances later on. It is best practice to size the placement group for the peak load and launch all instances at the same time.</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Enhanced Networking is available using single root I/O virtualization (<strong>SR-IOV</strong>). This requires <strong>HVM virtualization</strong>.</li>
<li>A placement group <strong>cannot</strong> span availability zones, but it <strong>can</strong> span subnets, provided they are in the <strong>same</strong> VPC.</li>
<li>You <strong>cannot</strong> move existing instances into a placement group.</li>
<li>Try to use <strong>homogenous instance types</strong> when launching placement groups.</li>
<li>Provision your placement group for peak load. You may <strong>not</strong> be able to add instances later.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>ELB<ul>
<li>What is ELB<ul>
<li><strong>Elastic Load Balancing</strong> automatically distributes incoming application traffic across multiple Amazon EC2 instances in the cloud.</li>
<li>It enables you to achieve greater levels of <strong>fault tolerance</strong> in your applications, seamlessly providing the required amount of load balancing capacity needed to distribute application traffic.</li>
</ul>
</li>
<li>ELB Tips<ul>
<li>2 Types of Load Balancers:<ul>
<li>Classic Load Balancer</li>
<li>Application Load Balancer</li>
</ul>
</li>
</ul>
</li>
<li>Classic Load Balancer<ul>
<li><strong>High Availability</strong><ul>
<li>You can deploy Classic Load Balancers in either a single or multiple availability zones</li>
</ul>
</li>
<li><strong>Health Checks</strong><ul>
<li>You can detect the health of Amazon EC2 instances. When a health check detects an unhealthy EC2 instance, it no longer routes traffic to that instance and it will spread the load across the remaining healthy instances.</li>
</ul>
</li>
<li><strong>Security Features</strong><ul>
<li>When using a VPC, you can create and manage security groups associated with Classic Load Balancers to provide additional networking and security options. You can also create a Classic Load Balancer without a public IP addresses to serve as an internal (non-internet-facing) load balancer.</li>
</ul>
</li>
<li><strong>SSL Offloading</strong><ul>
<li>You can offload your SSL connections to your classic load balancer. Flexible cipher support allows you to control the ciphers and protocols the load balancer presents to clients.</li>
</ul>
</li>
<li><strong>Sticky Sessions</strong><ul>
<li>Classic Load Balancers support the ability to stick user sessions to specific EC2 instances using cookies. Traffic will be routed to the same instances as the user continues to access your application.</li>
</ul>
</li>
<li><strong>IPv6</strong><ul>
<li>Classic Load Balancers support the use of both the Internet Protocol version 4 and 6 (IPv4 an IPv6)</li>
</ul>
</li>
<li><strong>Layer 4 or Layer 7 Load Balancing</strong><ul>
<li>You can load balance HTTP/HTTPS applications and use layer 7-specific features, such as X-Forwarded and sticky sessions. You can also use strict layer 4 load balancing for applications that rely purely on the TCP protocol.</li>
</ul>
</li>
<li><strong>Operational Monitoring</strong><ul>
<li>Classic Load Balancer metrics, such as request count and request latency, are reported by Amazon CloudWatch.</li>
</ul>
</li>
<li><strong>Logging</strong><ul>
<li>You can use the Access Logs feature to record all requests sent to your load balancer, and store the logs in Amazon S3 for later analysis. You can use these logs to diagnose application failures and for analyzing web traffic. You can also use AWS CloudTrail to record classic load balancer API calls for your account and deliver log files to S3. The API call history enables you to perform security analysis, resource change tracking, and compliance auditing.</li>
</ul>
</li>
</ul>
</li>
<li>Application Load Balancer<ul>
<li>Content-Based Routing<ul>
<li>If your application is composed of several individual services, an Application Load Balancer can route a request to a service based on the content of the request.</li>
</ul>
</li>
<li>Host-Based Routing<ul>
<li>You can route a client request based on Host field of the HTTP header allowing you to route to multiple domains from the same load balancer.</li>
</ul>
</li>
<li>Path-based Routing<ul>
<li>You can route a client request based on the URL path of the HTTP header.</li>
<li>e.g. <a href="https://domain.com/uploads" target="_blank" rel="external">https://domain.com/uploads</a> vs <a href="https://domain.com/downloads" target="_blank" rel="external">https://domain.com/downloads</a></li>
</ul>
</li>
<li>Containerized Application Support (integrates with ECS)</li>
<li>HTTP/2 Support</li>
<li>WebSockets Support</li>
<li>Native IPv6 Support</li>
<li>Sticky Sessions</li>
<li>Health Checks</li>
<li>High Availability</li>
<li>Security Features</li>
<li>Integrates with WAF</li>
<li>Layer-7 Load Balancing</li>
<li>HTTPS Support</li>
<li>Operational Monitoring</li>
<li>Logging</li>
<li>Delete Protection</li>
<li>Request Tracing - custom identifier “X-Amzn-Trace-Id” HTTP header on all requests</li>
</ul>
</li>
<li>Classic Load Balancer Tips<ul>
<li>Know which ports ELBs support<ul>
<li><strong>[EC2-VPC]</strong> 1-65535</li>
<li><strong>[EC2-Classic]</strong> 25,80,443, 465, 587, 1024-65535</li>
</ul>
</li>
<li>You <strong>cannot</strong> assign an Elastic IP address to an Elastic Load Balancer.</li>
<li><strong>IPv4</strong> &amp; <strong>IPv6</strong> is supported.</li>
<li>You can load balance to the “<strong>Zone Apex</strong>“ of your domain name.</li>
</ul>
</li>
<li>ELB Tips<ul>
<li>You can get a <strong>history</strong> of Elastic Load Balancing API calls made on my account for security analysis and operational troubleshooting purposes by turning on CloudTrail.</li>
<li>If you have <strong>multiple</strong> SSL certificates, you should use multiple Elastic Load Balancers, unless you have a wildcard certificate.</li>
</ul>
</li>
</ul>
</li>
<li><p>Scaling NATs</p>
<ul>
<li>NATs<br>  Instances that you launch into a private subnet in a virtual private cloud (<strong>VPC</strong>) can’t communicate with the Internet. You can <strong>optionally</strong> use a network address translation (<strong>NAT</strong>) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound traffic to the Internet, but prevent the instances from receiving inbound traffic initiated by someone on the Internet.</li>
<li>NAT Bottlenecks<br>  Bottlenecks can occur when you have a single NAT that has too much traffic passing through it. There are several approaches to reducing bottlenecks:<ul>
<li><strong>Scale Up</strong><ul>
<li>Increase your instance size</li>
<li>Choose an instance family which supports Enhanced Networking</li>
</ul>
</li>
<li><strong>Scale Out</strong><ul>
<li>Add an additional NAT &amp; subnet and migrate half your workloads to the new subnet.</li>
</ul>
</li>
</ul>
</li>
<li>HA For NATs<br>  You can create <strong>HA for NAT</strong> instances, but each subnet can <strong>only</strong> route to 1 NAT at a time. You can fail over a subnet to another NAT.<br>  Good blog on how to do it here: <a href="https://aws.amazon.com/cn/articles/high-availability-for-amazon-vpc-nat-instances-an-example/" target="_blank" rel="external">https://aws.amazon.com/cn/articles/high-availability-for-amazon-vpc-nat-instances-an-example/</a></li>
<li>Blog On Scaling NATs<br>  Great Blog by Matthew Barlocker: <a href="http://nineofclouds.blogspot.com/2013/01/vpc-migration-nats-bandwidth-bottleneck.html" target="_blank" rel="external">http://nineofclouds.blogspot.com/2013/01/vpc-migration-nats-bandwidth-bottleneck.html</a></li>
</ul>
</li>
<li><p>Domain 4 Summary</p>
<ul>
<li>Domain 4<ul>
<li>Domain 4.0: Network Design for a complex large scale deployment</li>
<li>4.1 Demonstrate ability to design and implement networking features of AWS</li>
<li>4.2 Demonstrate ability to design and implement connectivity features of AWS</li>
</ul>
</li>
<li>Direct Connect<ul>
<li>If you are accessing public services using HTTPS endpoints (think DynamoDB, S3) then use <strong>public</strong> VIFs.</li>
<li>If you are accessing VPCs using private IP address ranges, then use <strong>private</strong> VIFs.</li>
<li>In the US, you only need <strong>1</strong> direct connect connection to connect in to all 4 US regions. Data transferred between regions goes over AWS’s internal lines, not the public internet.</li>
<li>Direct connect itself is not redundant. You can <strong>add</strong> redundancy by having <strong>2</strong> connections(<strong>2 routes, 2 direct connects</strong>), or by having a site-to-site VPN in place.</li>
<li>Layer 2 connections are <strong>not</strong> supported.</li>
<li>Know the difference between a <strong>Customer Gateway CGW</strong> (Customer side) and <strong>Virtual Private Gateway</strong> (AWS side.)</li>
</ul>
</li>
<li>ELB’s<ul>
<li>Know which ports ELBs support<ul>
<li><strong>[EC2-VPC]</strong> 1-65535</li>
<li><strong>[EC2-Classic]</strong> 25,80,443, 465, 587, 1024-65535</li>
</ul>
</li>
<li>You <strong>cannot</strong> assign an Elastic IP address to an Elastic Load Balancer.</li>
<li><strong>IPv4</strong> &amp; <strong>IPv6</strong> is supported (although VPC’s do <strong>not</strong> support IPv6 at this time)</li>
<li>You can load balance to the “<strong>Zone Apex</strong>“ of your domain name.</li>
<li>You can get a <strong>history</strong> of Elastic Load Balancing API calls made on my account for security analysis and operational troubleshooting purposes by turning on CloudTrail.</li>
<li>If you have <strong>multiple</strong> SSL certificates, you should use multiple Elastic Load Balancers, unless you have a wildcard certificate.</li>
</ul>
</li>
<li>HPC &amp; Enhanced Networking<ul>
<li>Enhanced Networking is available using single root I/O virtualization (<strong>SR-IOV</strong>). This requires <strong>HVM virtualization</strong>.</li>
<li>A placement group <strong>cannot</strong> span availability zones, but it <strong>can</strong> span subnets, provided they are in the <strong>same</strong> VPC.</li>
<li>You <strong>cannot</strong> move existing instances into a placement group.</li>
<li>Try to use <strong>homogenous instance types</strong> when launching placement groups.</li>
<li>Provision your placement group for peak load. You may <strong>not</strong> be able to add instances later.</li>
</ul>
</li>
<li>Scaling NATs<br>  Bottlenecks can occur when you have a single NAT that has too much traffic passing through it. There are several approaches to reducing bottlenecks:<ul>
<li><strong>Scale Up</strong><ul>
<li>Increase your instance size</li>
<li>Choose an instance family which supports Enhanced Networking</li>
</ul>
</li>
<li><strong>Scale Out</strong><ul>
<li>Add an additional NAT &amp; subnet and migrate half your workloads to the new subnet.</li>
</ul>
</li>
</ul>
</li>
<li>VPC Peering<ul>
<li>Create Step<ul>
<li>Owner of the <strong>local</strong> VPC sends a request to the owner of the second VPC to peer.</li>
<li>Owner of the <strong>second</strong> VPC has to accept.</li>
<li>Owner of the <strong>local</strong> VPC adds a route to their route table allowing their subnets to route out to the peer VPC</li>
<li>Owner of the <strong>peer</strong> VPC adds a route to their route table allowing their subnets to route back to the other VPC.</li>
<li>Security Groups in both VPCs have to <strong>both</strong> allow traffic.</li>
</ul>
</li>
<li>Troubleshooting<ul>
<li><strong>Setting up the peer:</strong><ul>
<li>Are the VPCs in the same region?</li>
<li>If you can’t create a VPC peer, check to see if the CIDR blocks are overlapping.</li>
</ul>
</li>
<li><strong>After the peer is setup:</strong><ul>
<li>Check that the relevant security groups and NACLs are allowing traffic through.</li>
<li>Check that a route has been created in BOTH VPCs routing tables.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Domain-5-Data-Storage-15"><a href="#Domain-5-Data-Storage-15" class="headerlink" title="Domain 5 Data Storage - 15%"></a>Domain 5 Data Storage - 15%</h3><ul>
<li>Optimizing S3<ul>
<li>Optimizing For Puts<ul>
<li><strong>Two different types of network scenarios:</strong><ul>
<li>Strong internet connection, with fast speeds (such as fibre.)</li>
<li>Less reliable internet connection with inconsistent network performance.</li>
</ul>
</li>
<li><strong>How can we optimize?</strong><ul>
<li>For strong networks, we want to take advantage of the network itself and make the network the bottleneck.</li>
<li>For weaker networks, we want to prevent large files having to restart their uploads.</li>
</ul>
</li>
</ul>
</li>
<li>Parallelizing For PUTs<ul>
<li>By <strong>dividing</strong> your files in to small parts and then uploading those parts simultaneously, you are “<strong>parallelizing</strong>“ your puts.</li>
<li>If 1 part fails, it can be <strong>restarted</strong> an there are fewer large restarts on an unreliable network.</li>
<li>Moves the bottleneck to the network itself, which can help <strong>increase</strong> the aggregate throughput.</li>
<li><strong>25-50 MB</strong> on high bandwidth networks, and around <strong>10 MB</strong> on mobile networks.</li>
<li>Need to find a <strong>balance</strong>. Too many increases the connection overhead, too few doesn’t give you any resiliency.</li>
</ul>
</li>
<li>Optimizing For GETs<ul>
<li><strong>Use CloudFront:</strong><ul>
<li>Multiple Endpoints Globally</li>
<li>Low Latency</li>
<li>High Transfer Speeds Available</li>
<li>Caches Objects from S3</li>
<li><strong>Two varieties:</strong><ul>
<li>RTMP</li>
<li>Web Distribution</li>
</ul>
</li>
</ul>
</li>
<li>Need to use <strong>range-based GETs</strong> to get multithreaded performance.</li>
<li>Using the Range HTTP header in a GET request, you can <strong>retrieve</strong> a specific range of bytes in an object stored in Amazon S3.</li>
<li>Allows you to send <strong>multiple</strong> GETs at once, hence parallelizing for GETs.</li>
<li><strong>Compensates</strong> for unreliable network performance.</li>
<li><strong>Maximizes</strong> bandwidth throughput.</li>
</ul>
</li>
<li>S3 is Lexicographical<ul>
<li>Introduce some randomness in to the name.</li>
<li>The more random, the better the performance</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Use <strong>parallelization</strong> to <strong>optimize</strong> both GETs &amp; PUTs</li>
<li>S3 stores data in <strong>Lexicographical</strong> order.</li>
<li>Introduce <strong>randomness</strong> to maximize performance.</li>
</ul>
</li>
</ul>
</li>
<li>S3 Best Practices<ul>
<li>Versioning - Recap<ul>
<li>Allows you to <strong>protect</strong> your files from being accidentally deleted or overwritten.<ul>
<li><strong>No</strong> penalty on performance.</li>
</ul>
</li>
<li>Every time you upload a new file, a new version is created <strong>automatically</strong>.</li>
<li>Makes it easy to <strong>retrieve</strong> deleted versions.</li>
<li>Makes it easy to <strong>roll-back</strong> to previous versions.</li>
<li>Once you turn versioning on, you <strong>cannot</strong> turn it off – you can only <strong>suspend</strong> it.</li>
<li>There are <strong>further steps</strong> you can take to secure your S3 environment.</li>
</ul>
</li>
<li>Securing S3<ul>
<li>You can use <strong>bucket policier</strong> to restrict deletes.</li>
<li>You can also use <strong>MFA Delete</strong>:<ul>
<li>You have to use two-factor authentication to <strong>delete</strong> objects permanently.</li>
<li>You have to use two-factor authentication to <strong>change</strong> the state of versioning within your bucket.</li>
</ul>
</li>
<li>Requires both your <strong>security credentials</strong>, as well as a <strong>code</strong> from an approved authentication device ( such as <strong>Google Authenticator</strong>).</li>
</ul>
</li>
<li>Securing S3 - Use Another Account<ul>
<li>Versioning will protect you on an <strong>individual</strong> file level.</li>
<li>Does <strong>not</strong> protect you against deleting a bucket.</li>
<li>Bucket deletion could be either <strong>malicious</strong> or <strong>accidental</strong>.</li>
<li><strong>Back</strong> your bucket up to another bucket owned by another account using cross account access.</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>You can <strong>secure S3</strong> by:<ul>
<li>Using Bucket Policies</li>
<li>Using MFA Delete</li>
<li>Backing Your Bucket Up To Another S3 Bucket Owned By A Separate Account</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Database Design<ul>
<li>Recommended White Paper<ul>
<li>AWS Storage Options</li>
<li><a href="https://d1.awsstatic.com/whitepapers/Storage/AWS%20Storage%20Services%20Whitepaper-v9.pdf" target="_blank" rel="external">https://d1.awsstatic.com/whitepapers/Storage/AWS%20Storage%20Services%20Whitepaper-v9.pdf</a></li>
<li>Take particular note of the Anti-Patterns for the different technologies.</li>
</ul>
</li>
<li>Multi-AZ vs Read Replicas<ul>
<li><strong>Multi-AZ</strong><ul>
<li>Used for DR only, not for scaling</li>
<li>Is synchronous replication</li>
</ul>
</li>
<li><strong>Read Replicas</strong><ul>
<li>Used for Scaling Out, not DR</li>
<li>Is Asynchronous replication</li>
</ul>
</li>
</ul>
</li>
<li>RDS Use Cases<ul>
<li><strong>Amazon RDS</strong> is ideal for existing applications that rely on MySQL, Oracle, SQL, PostgreSQL, MariaDB and Aurora.</li>
<li>Since Amazon RDS offers full compatibility and direct access to native database engines, most code, libraries, and tools designed for these databases should work <strong>unmodified</strong> with Amazon RDS.</li>
<li>Amazon RDS is also <strong>optimal</strong> for new applications with structured data that requires more sophisticated querying and joining capabilities than those provided by Amazon’s NoSQL database offering, Amazon DynamoDB.</li>
</ul>
</li>
<li>ACID? Think RDS<ul>
<li>The <strong>ACID</strong> concept is described in <strong>ISO/IEC 10026-1:1998 Section 4</strong>.</li>
<li><strong>A</strong>tomicity. In a transaction involving two or more discrete pieces of information, either all of the pieces are committed or none are.</li>
<li><strong>C</strong>onsistency. A transaction either creates a new and valid state of data, or if any failure occurs, returns all data to its state before the transaction was started.</li>
<li><strong>I</strong>solation. A transaction in process and not yet committed must remain isolated from any other transaction.</li>
<li><strong>D</strong>urability. Committed data is saved by the system such that, even in the event of a failure and system restart, the data is available in its correct state.</li>
</ul>
</li>
<li>Where Not To Use RDS<br>  <strong>Amazon RDS</strong> is a great solution for cloud-based, fully-managed relational databases; but in a number of scenarios, it may not be the right choice.<ul>
<li><strong>Index and query-focused data</strong> - Many cloud-based solutions don’t require advanced features found in a relational database, such as joins an complex transactions. If your application is more oriented toward indexing and querying data, you may find Amazon DynamoDB to be more appropriate for your needs.</li>
<li><strong>Numerous BLOBs</strong> - While all of the database engines provided by Amazon RDS support binary large objects (BLOBs), if your application makes heavy use of them (audio files, videos, images and so on), you may fin Amazon S3 to be a better choice.</li>
<li><strong>Automated scalability</strong> - Amazon RDS provides pushbutton scaling. If you need fully automated scaling, Amazon DynamoDB may be a better choice.</li>
<li><strong>Other database platforms</strong> - At this time, Amazon RDS provides MySQL, Oracle, SQL Server, PostgreSQL, MariaDB and Aurora databases. If you need another database platform (such as IBM DB2, Informix, or Sybase), you need to deploy a self-managed database on an Amazon EC2 instance by using a relational database AMI, or by installing database software on an Amazon EC2 instance.</li>
<li><strong>Complete control</strong> - If your application requires complete, OS-level control of the database server, with full root or admin login privileges (for example, to install additional third-party software on the same server), a self managed database on Amazon EC2 may be a better match.</li>
</ul>
</li>
<li>DynamoDB Use Cases<br>  <strong>Amazon DynamoDB</strong> is ideal for existing or new applications that need a flexible NoSQL database with low read and write latencies, and the ability to scale storage an throughput up or down as needed without code changes or downtime.<br>  <strong>Common use cases</strong> include: mobile apps, gaming, digital ad serving, live voting and audience interaction for live events, sensor networks, log ingestion, access control for web-based content, metadata storage for Amazon S3 objects, e-commerce shopping carts, and web session management.<br>  Many of these use cases require a highly available and scalable database because downtime or performance degradation has an immediate negative impact on an organization’s business.<br>  <strong>Need to automatically scale your database, think DynamoDB.</strong></li>
<li>Where Not To Use DynamoDB<ul>
<li><strong>Prewritten application tied to a traditional relational database</strong> - If you are attempting to port an existing application to the AWS cloud, and need to continue using a relational database.</li>
<li><strong>Joins and/or complex transactions</strong> - While many solutions are able to leverage Amazon DynamoDB to support their users, it’s possible that your application may require joins, complex transactions, and other relational infrastructure provided by traditional database platforms. If this is the case, you may want to explore Amazon RDS or Amazon EC2 with a self-managed database.</li>
<li><strong>Blob Data</strong> - If you plan on storing large (greater than 64 KB) BLOB data, such as digital video, images or music, you’ll want to consider Amazon S3. However, Amazon DynamoDB still has a role to play in this scenario, for keeping track of metadata (e.g. item name, size, date created, owner, location and so on) about your binary objects.</li>
<li><strong>Large data with low I/O rate</strong> - Amazon DynamoDB uses SSD drives and is optimized for workloads with a high I/O rate per GB stored. If you plan to store very large amounts of data that are infrequently accessed, other storage options, such as Amazon S3, may be a better choice.</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Multi-AZ = <strong>Synchronous</strong> Replication</li>
<li>Read Replica = <strong>Asynchronous</strong> Replication</li>
<li><strong>RDS</strong> is optimal for new applications with structured data that requires more sophisticated querying and joining capabilities</li>
<li><strong>ACID</strong> = <strong>RDS</strong></li>
<li>Don’t use RDS for:<ul>
<li>Index and query-focused data (<strong>DynamoDB</strong>)</li>
<li>Numerous BLOBs (<strong>S3</strong>)</li>
<li>Automated scalability (<strong>DynamoDB</strong>)</li>
<li>Other database platforms such as IBM DB2, Informix, or Sybase (<strong>EC2</strong>)</li>
<li>Complete Control (<strong>EC2</strong>)</li>
</ul>
</li>
<li><strong>Don’t use DynamoDB for:</strong><ul>
<li>Prewritten application tied to a traditional relational database (<strong>RDS</strong>)</li>
<li>Joins and/or complex transactions (<strong>RDS</strong>)</li>
<li>Blob Data (<strong>S3</strong>)</li>
<li>Large data with low I/O rate (<strong>S3</strong>)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Domain 5<ul>
<li>**Domain 5.0: Data Storage for a complex large scale deployment<ul>
<li><strong>5.1</strong> Demonstrate ability to make architectural trade off decisions involving storage options</li>
<li><strong>5.2</strong> Demonstrate ability to make architectural trade off decisions involving database options</li>
<li><strong>5.3</strong> Demonstrate ability to implement the most appropriate data storage architecture</li>
<li><strong>5.4</strong> Determine use of synchronous versus asynchronous replication</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Read the <strong>White Paper</strong>!</li>
<li>AWS Storage Options</li>
<li><a href="https://d0.awsstatic.com/whitepapers/Storage/aws-storage-options.pdf" target="_blank" rel="external">https://d0.awsstatic.com/whitepapers/Storage/aws-storage-options.pdf</a></li>
<li>Take particular note of the Anti-Patterns for the different technologies.</li>
</ul>
</li>
<li>Exam Tips - S3 Performance<ul>
<li>Use <strong>parallelization</strong> to optimize both GETs &amp; PUTs.</li>
<li>S3 stores data in <strong>Lexicographical</strong> order.</li>
<li>Introduce <strong>randomness</strong> to maximize performance.</li>
</ul>
</li>
<li>Exam Tips - Securing S3<ul>
<li><strong>You can secure S3 by:</strong><ul>
<li>Using Bucket Policies</li>
<li>Using MFA Delete</li>
<li>Backing Your Bucket Up To Another S3 Bucket Owned By A Separate Account</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Multi-AZ = <strong>Synchronous</strong> Replication</li>
<li>Read Replica = <strong>Asynchronous</strong> Replication</li>
<li><strong>RDS</strong> is optimal for new applications with structured data that requires more sophisticated querying and joining capabilities</li>
<li><strong>ACID</strong> = <strong>RDS</strong></li>
<li>Don’t use RDS for:<ul>
<li>Index and query-focused data (<strong>DynamoDB</strong>)</li>
<li>Numerous BLOBs (<strong>S3</strong>)</li>
<li>Automated scalability (<strong>DynamoDB</strong>)</li>
<li>Other database platforms such as IBM DB2, Informix, or Sybase (<strong>EC2</strong>)</li>
<li>Complete Control (<strong>EC2</strong>)</li>
</ul>
</li>
<li><strong>Don’t use DynamoDB for:</strong><ul>
<li>Prewritten application tied to a traditional relational database (<strong>RDS</strong>)</li>
<li>Joins and/or complex transactions (<strong>RDS</strong>)</li>
<li>Blob Data (<strong>S3</strong>)</li>
<li>Large data with low I/O rate (<strong>S3</strong>)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Domain-6-Security-20"><a href="#Domain-6-Security-20" class="headerlink" title="Domain 6 Security - 20%"></a>Domain 6 Security - 20%</h3><ul>
<li>AWS Directory Services<ul>
<li>AWS Directory Services<br>  <strong>AWS Directory Service</strong> is a managed service that allows you to connect your AWS resources with an existing on-premises Microsoft Active Directory or to set up a new, stand-alone directory in the AWS cloud.<ul>
<li><strong>Comes in 2 flavors:</strong><ul>
<li>AD Connector</li>
<li>Simple AD</li>
</ul>
</li>
</ul>
</li>
<li>AD Connector<br>  <strong>AD Connector</strong> enables you to easily connect your Microsoft Active Directory to the AWS cloud, without requiring complex directory synchronization technologies or the cost an complexity of hosting a federation infrastructure.<br>  Once set up, your end users and ID administrators can use their existing corporate credentials to log on to AWS applications such as Amazon Workspaces, Amazon WorkDocs or Amazon WorkMail and to manage AWS resources (e.g. Amazon EC2 instances or S3 buckets) via AWS Identity and Access Management (IAM) role-based access to the AWS Management Console.<br>  Your existing security policies, such as password expiration, password history and account lockouts can be <strong>enforced consistently</strong> – whether users to IT administrators are accessing resources in your on-premises infrastructure or the AWS cloud.<br>  You can also use <strong>AD Connector</strong> to enable multi-factor authentication by integrating with your existing <strong>RADIUS-based MFA infrastructure</strong> to provide an additional layer of security when users access AWS applications.</li>
<li>Simple AD<br>  <strong>Simple AD</strong> is a managed directory which is powered by <strong>Samba 4 Active Directory Compatible Server</strong>. It supports commonly used features such as user accounts, group memberships, domain-joining Amazon EC2 instances running Linux and Microsoft Windows, as well as Kerberos based single sign-on (<strong>SSO</strong>), and Group Policies.</li>
<li>Exam Tips<ul>
<li><strong>2 Types of Directory Services:</strong><ul>
<li>**AD Connector (Used for existing AD Deployments)<ul>
<li>Great way to connect in to existing AD environments.</li>
<li>Supports MFA</li>
</ul>
</li>
<li>**Simple AD (Used for new AD Deployments)<ul>
<li>MFA Not Supported</li>
<li>Cannot add additional AD servers</li>
<li>No Trust Relationships</li>
<li>Cannot Transfer FSMO Roles</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Security Token Service(STS)</p>
<ul>
<li>Security Token Service<ul>
<li>Grants users <strong>limited</strong> and <strong>temporary</strong> access to AWS resources. Users can come from three sources:</li>
<li><strong>Federation (typically Active Directory)</strong><ul>
<li>Uses Security Assertion Markup Language (SAML)</li>
<li>Grants temporary access based off the users Active Directory credentials. Does not need to be a user in IAM</li>
<li>Single sign on allows users to log in to AWS console without assigning IAM credentials</li>
<li>Federation with Mobile Apps<ul>
<li>Use Facebook/Amazon/Google or other OpenID providers to log in</li>
</ul>
</li>
<li>Cross Account Access<ul>
<li>Let’s users from one AWS account access resources in another</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Understanding The Key Terms<ul>
<li><strong>Federation</strong><ul>
<li>Combining or joining a list of users in one domain (such as IAM) with a list of users in another domain (such as Active Directory, Facebook etc.)</li>
</ul>
</li>
<li><strong>Identity Broker</strong><ul>
<li>A service that allows you to take an identity from point A and join it (federate it) to point B</li>
</ul>
</li>
<li><strong>Identity Store</strong><ul>
<li>Services like Active Directory, Facebook, Google etc.</li>
</ul>
</li>
<li><strong>Identities</strong><ul>
<li>A user of a service like Facebook etc.</li>
</ul>
</li>
</ul>
</li>
<li>Scenario<ul>
<li>AWS blog [<a href="https://aws.amazon.com/cn/blogs/aws/aws-identity-and-access-management-now-with-identity-federation/" target="_blank" rel="external">AWS Identity and Access Management – Now With Identity Federation</a>]</li>
<li>You are hosting a company website on some EC2 web servers in your VPC. Users of the website must log in to the site which then authenticates against the companies active directory servers which are based on site at the companies head quarters. Your VPC is connected to your company HQ via a secure IPSEC VPN. Once logged in the user can only have access to their own S3 bucket. <strong>How do you set this up?</strong><br>  <img src="/images/AWS/Sysops/sts_scenario.jpg" alt="sts_scenario"></li>
<li>Steps of scenario<ul>
<li>Employee enters their username and password</li>
<li>The application calls an Identity Broker. The broker captures the username and password.</li>
<li>The Identity Broker uses the organization’s LDAP directory to validate the employee’s identity.</li>
<li>The Identity Broker calls the new GetFederationToken function using IAM credentials. The call must include an IAM policy and a duration (1 to 36 hours), along with a policy that specifies the permissions to be granted to the temporary security credentials.</li>
<li>The Security Token Service confirms that the policy of the IAM user making the call to GetFederationToken gives permission to create new tokens and then returns four values to the application: An access key, a secret access key, a token, and a duration (the token’s lifetime).</li>
<li>The Identity Broker returns the temporary security credentials to the reporting application.</li>
<li>The data storage application uses the temporary security credentials (including the token) to make requests to Amazon S3.</li>
<li>Amazon S3 uses IAM to verify that the credentials allow the requested operation on the given S3 bucket and key</li>
<li>IAM provides S3 with the go-ahead to perform the requested operation.</li>
</ul>
</li>
<li>In the Exam<ul>
<li>Develop an <strong>Identity Broker</strong> to communicate with <strong>LDAP</strong> and <strong>AWS STS</strong></li>
<li><strong>Identity Broker</strong> always authenticates with <strong>LDAP</strong> first, <strong>Then</strong> with <strong>AWS STS</strong></li>
<li>Application then gets <strong>temporary access</strong> to AWS resources</li>
</ul>
</li>
</ul>
</li>
<li>Scenario 2<ul>
<li>steps of scenario<ul>
<li>Develop an Identity Broker to communicate with LDAP and AWS STS</li>
<li>Identity Broker always authenticates with LDAP first, gets an IAM Role associate with a user</li>
<li>Application then authenticates with STS and assumes that IAM Role</li>
<li>Application uses that IAM role to interact with S3</li>
</ul>
</li>
<li>In the Exam<ul>
<li>Develop an Identity Broker to communicate with LDAP and AWS STS</li>
<li>Identity Broker always authenticates with LDAP first, Then with AWS STS</li>
<li>Application then gets temporary access to AWS resources</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Monitoring In The Cloud</p>
<ul>
<li>Monitoring<ul>
<li><strong>It’s important to know the difference:</strong><ul>
<li>CloudWatch vs CloudTrail</li>
</ul>
</li>
<li><strong>Use cases as to where to store your logs</strong><ul>
<li>EBS volumes attached to EC2 instances?</li>
<li>S3?</li>
<li>CloudWatch?</li>
</ul>
</li>
</ul>
</li>
<li>CloudWatch vs CloudTrail<ul>
<li>CloudTrail<br>  <strong>CloudTrail</strong> enables you to retrieve a history of <strong>API calls</strong> and other events for all of the regions in your account. This includes calls and events made by the AWS Management Console and command line tools, by any of the AWS SDKs, or by other AWS services.<br>  CloudTrail is used for <strong>auditing</strong> and collecting a <strong>history of API calls</strong> made on your environment. It is not a logging service per se.</li>
<li>CloudWatch<ul>
<li><strong>Amazon CloudWatch</strong> is a <strong>monitoring service</strong> for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files and set alarms.</li>
<li>Amazon CloudWatch can monitor AWS resources such as Amazon EC2 instances, Amazon DynamoDB tables, and Amazon RDS DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate.</li>
<li>By <strong>default</strong>, CloudWatch Logs will store your log data <strong>indefinitely</strong>. You can change the retention for each Log Group at any time.</li>
<li>CloudWatch Alarm history is only stored for <strong>14 days</strong>.</li>
</ul>
</li>
<li>Billing<br>  Billing for CloudWatch Logging is:<br>  <strong>$0.50</strong> per GB ingested per month<br>  <strong>$0.03</strong> per GB archived per month<br>  <strong>$0.10</strong> per alarm per month<br>  S3 lowest tier is <strong>$0.03 per GB</strong>. Can be cheaper to store logs in S3 than CloudWatch, but depends on your environment.</li>
</ul>
</li>
<li>Using CloudWatch To Monitor CloudTrail<ul>
<li>One of the ways that you can work with CloudTrail logs is to monitor them in <strong>real time</strong> by sending them to CloudWatch Logs. For a trail that is enabled in all regions in your account, CloudTrail sends log files from all those regions to a CloudWatch Logs log group.</li>
<li>You define CloudWatch Logs metric filters that will evaluate your CloudTrail log events for matches in terms, phrases, or values. You assign CloudWatch metrics to the <strong>metric filters</strong>. You also create CloudWatch alarms that are triggered according to thresholds and time periods that you specify.</li>
</ul>
</li>
<li>Monitoring<ul>
<li><strong>You can monitor events and ship those logs to:</strong><ul>
<li>CloudWatch</li>
<li>A Centralized Logging System (AlertLogic, Splunk, SumoLogic)</li>
<li>S3 (via a script that exports the log files.)</li>
</ul>
</li>
<li><strong>Avoid</strong> storing your logs anywhere that’s not persistent:<ul>
<li>Root device volume on EC2 instance.</li>
<li>Ephemeral Storage</li>
</ul>
</li>
<li>Best answers are usually either <strong>S3</strong> or <strong>CloudWatch</strong></li>
<li>CloudTrail can be used across <strong>multiple</strong> accounts in a single S3 bucket.</li>
<li>CloudWatch now <strong>supports</strong> Cross Account Subscriptions (Very recent, August 10, 2015) <a href="https://aws.amazon.com/about-aws/whats-new/2015/08/amazon-cloudwatch-logs-cross-account-subscriptions/" target="_blank" rel="external">https://aws.amazon.com/about-aws/whats-new/2015/08/amazon-cloudwatch-logs-cross-account-subscriptions/</a></li>
</ul>
</li>
<li>Exam Tips<ul>
<li>CloudWatch for Logs</li>
<li>CloudTrail for Audits, tracing API calls</li>
<li>CloudWatch can monitor CloudTrail</li>
<li><strong>You can send your logs to:</strong><ul>
<li>CloudWatch</li>
<li>S3</li>
<li>Third Parties (ALertLogic, SumoLogic, Splunk etc.)</li>
</ul>
</li>
<li>CloudTrail can log events from multiple accounts to a single S3 bucket.</li>
<li>CloudWatch supports cross account subscriptions, however this a very new service.</li>
<li>Always look to store your logs in a <strong>persistent</strong> &amp; <strong>safe</strong> place. S3 or CloudWatch are the ideal places.</li>
<li>By <strong>default</strong>, CloudWatch Logs will store your log data <strong>indefinitely</strong>.</li>
<li>CloudWatch Alarm History is only <strong>14 days</strong>.</li>
</ul>
</li>
</ul>
</li>
<li><p>Cloud Hardware Security Modules (HSMs)</p>
<ul>
<li>What Is A HSM?<ul>
<li>HSM stands for <strong>Hardware Security Module</strong>, and is a physical device that safeguards and manages digital keys for strong authentication and provides cryptoprocessing. These modules traditionally come in the form of a plug-in card or an external device that attaches directly to a computer or network server.</li>
</ul>
</li>
<li>Cloud HSMs<ul>
<li>Prior to Cloud, HSM’s customers had to store these devices on premise, introducing a large amount of latency into their environment.</li>
<li>The <strong>AWS CloudHSM</strong> service allows you to <strong>protect</strong> your encryption keys within HSMs designed and validated to government standards for secure key management. You can securely <strong>generate, store and manage</strong> the cryptographic keys used for data encryption such that they are accessible only by you.</li>
</ul>
</li>
<li>Cloud HSM - Pricing<ul>
<li>You will be <strong>charged</strong> an upfront fee for each CloudHSM instance you launch, and an hourly fee for each hour thereafter until you terminate the instance. If you want to try the CloudHSM service for free, you can request a <strong>two week trial</strong>.<br>  <img src="/images/AWS/CSAP/CSAP_classic_CloudHSM_pricing.png" alt="CSAP_classic_CloudHSM_pricing"></li>
<li>Now. There are no upfront costs to use AWS CloudHSM. With CloudHSM, you pay an hourly fee for each HSM you launch until you terminate the HSM.  <a href="https://aws.amazon.com/cloudhsm/pricing/" target="_blank" rel="external">https://aws.amazon.com/cloudhsm/pricing/</a><br>  <img src="/images/AWS/CSAP/CSAP_current_CloudHSM_pricing.png" alt="CSAP_current_CloudHSM_pricing"></li>
</ul>
</li>
<li>Exam Tips - CloudHSM<ul>
<li>Single Tenanted (<strong>1 physical device</strong>, for you only)</li>
<li><strong>Must</strong> be used within a <strong>VPC</strong></li>
<li>You can use <strong>VPC peering</strong> to connect to a <strong>CloudHSM</strong></li>
<li>You can use <strong>EBS volume encryption</strong>, <strong>S3 object encryption</strong> and <strong>key management</strong> with CloudHSM, but this does require <strong>custom application scripting</strong>.</li>
<li>If you need fault tolerance, you need to build a <strong>cluster</strong>, so you will need <strong>2</strong>. If you have only one and it fails, you will <strong>lose</strong> your keys.</li>
<li>Can integrate with <strong>RDS</strong> (Oracle &amp; SQL) as well as <strong>Redshift</strong></li>
<li>Monitor via <strong>Syslog</strong></li>
</ul>
</li>
</ul>
</li>
<li>DDOS<ul>
<li>What Is A DDos Attach?<ul>
<li><strong>A Distributed Denial of Service (DDos)</strong> attack is an attack that attempts to make your website or application unavailable to your end users.</li>
<li>This can be achieved by <strong>multiple</strong> mechanisms, such as large packet floods, by using a combination of reflection and amplification techniques, or by using large botnets.</li>
</ul>
</li>
<li>Amplification/Reflection Attacks<ul>
<li><strong>Amplification/Reflection</strong> attacks can include things such as NTP, SSDP, DNS, Chargen, SNMP attacks etc. and is where an attacker may send a third party server (such as an NTP server) a request using a spoofed IP address. That server will then respond to that request with a greater payload than initial request (usually within the region of 28*54 times larger than the request) to the spoofed IP address.</li>
<li>This means that if the attacker sends a packet with a spoofed IP address of 64 bytes, the NTP server would respond with up to 3,456 bytes of traffic. Attackers can co-ordinate this and use multiple NTP servers a second to send legitimate NTP traffic to the target.</li>
</ul>
</li>
<li>Application Attacks (L7)<ul>
<li>Flood of Get request</li>
<li>Slowloris</li>
</ul>
</li>
<li>How To Mitigate DDoS?<ul>
<li>Minimize the Attack Surface Area</li>
<li>Be Ready to Scale to Absorb the Attack</li>
<li>Safeguard Exposed Resources</li>
<li>Learn Normal Behavior</li>
<li>Create a Plan for Attacks</li>
</ul>
</li>
<li>Minimize The Attack Surface Area<ul>
<li>Some production environments have <strong>multiple</strong> entry points in to them. Perhaps they allow direct SSH or RDP access to their web servers/application and DB servers for management.</li>
<li>This can be <strong>minimized</strong> by using a <strong>Bastion/Jump Box</strong> that only allows access to specific white list IP addresses to these bastion servers and move the web, application and DB servers to private subnets. By minimizing the attack surface area, you are <strong>limiting</strong> your exposure to just a few hardened entry points.<br>  <img src="/images/AWS/CSAP/CSAP_DDos_architecture.png" alt="CSAP_DDos_architecture"></li>
</ul>
</li>
<li>Be Ready To Scale To Absorb The Attack<ul>
<li>The key strategy behind a DDoS attack is to bring your infrastructure to <strong>breaking point</strong>. This strategy assumes one thing: that you <strong>can’t</strong> scale to meet the attack.</li>
<li>The easiest way to defeat this strategy is to design your infrastructure to scale as, and when, it is needed.</li>
<li>You can scale both <strong>Horizontally</strong> &amp; <strong>Vertically</strong>.</li>
</ul>
</li>
<li>Scaling Has The Following Benefits<ul>
<li>The attack is spread over a <strong>larger area</strong>.</li>
<li>Attackers then have to <strong>counter attack</strong>, taking up more of their resources.</li>
<li>Scaling buys you time to <strong>analyze</strong> the attack and to respond with the appropriate countermeasures.</li>
<li>Scaling has the added benefit of providing you with <strong>additional</strong> levels of redundancy.</li>
</ul>
</li>
<li>Safeguard Exposed Resources<ul>
<li>In situations where you <strong>cannot</strong> eliminate Internet entry points to your applications, you’ll need to take <strong>additional</strong> measures to restrict access and protect those entry points without interrupting legitimate en user traffic.</li>
<li><strong>Three resources</strong> that can provide this control and flexibility are <strong>Amazon CloudFront</strong>, <strong>Amazon Route 53</strong> and <strong>web application firewalls(WAFs)</strong>.<ul>
<li><strong>CloudFront:</strong><ul>
<li><strong>Geo Restriction/Blocking</strong> – Restrict access to users in specific countries (using whitelists or blacklists).</li>
<li><strong>Origin Access Identity</strong> – Restrict Access to your S3 bucket so that people can only access S3 using CloudFront URLs.</li>
</ul>
</li>
<li><strong>Route53:</strong><ul>
<li><strong>Alias Record Sets</strong> – You can use these to immediately redirect your traffic to an Amazon CloudFront distribution, or to a different ELB load balancer with higher capacity EC2 instances running WAFs or your own security tools. No DNS change, and no need to worry about propagation.</li>
<li><strong>Private DNS</strong> – Allow you to manage internal DNS names for your application resources (web servers, application servers, database etc.) without exposing this information to the public Internet.</li>
</ul>
</li>
<li><strong>WAFs:</strong><ul>
<li>DDos attacks that happen at the application layer commonly target web applications with lower volumes of traffic compared to infrastructure attacks. To mitigate these types of attacks, you’ll want to include a WAF as port of your infrastructure.<ul>
<li><strong>New WAF Service</strong> – You can use the new AWS WAF service.</li>
<li><strong>AWS Market Place</strong> – You can buy other WAF’s on the AWS Market Place.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Learn Normal Behavior<ul>
<li><strong>Be aware of normal and unusual behavior</strong><ul>
<li>Know the different types of traffic and what normal levels of this traffic should be.</li>
<li>Understand expected and unexpected resource spikes.</li>
</ul>
</li>
<li><strong>What are the benefits?</strong><ul>
<li>Allows you to spot abnormalities fast.</li>
<li>You can create alarms to alert you of abnormal behavior.</li>
<li>Helps you to collect forensic data to understand the attack.</li>
</ul>
</li>
</ul>
</li>
<li>Create A Plan For Attacks<ul>
<li><strong>Having a plan in place before an attack ensures that:</strong><ul>
<li>You’ve validated the design of your architecture.</li>
<li>You understand the costs for your increased resiliency and already know what techniques to employ when you come under attack.</li>
<li>You know who to contact when an attack happens.</li>
</ul>
</li>
</ul>
</li>
<li>DDoS Exam Tips<ul>
<li>Definitely worth a few points in the exam.</li>
<li>Worth reading the white paper: <a href="https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf" target="_blank" rel="external">https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf</a> and <a href="https://aws.amazon.com/blogs/security/updated-whitepaper-available-aws-best-practices-for-ddos-resiliency/" target="_blank" rel="external">https://aws.amazon.com/blogs/security/updated-whitepaper-available-aws-best-practices-for-ddos-resiliency/</a></li>
<li>Remember the technologies you can use to mitigate a DDoS attack:<ul>
<li>CloudFront</li>
<li>Route53</li>
<li>ELB’s</li>
<li>WAFs</li>
<li>Autoscaling (Use for both WAFs an Web Servers)</li>
<li>CloudWatch</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>IDS &amp; IPS<ul>
<li>What Is IDS?<ul>
<li>An <strong>Intrusion Detection System (IDS)</strong> inspects all inbound and outbound network activity and identifies suspicious patterns that may indicate a network or system attack from someone attempting to break into, or compromise, a system.</li>
</ul>
</li>
<li>What Is IPS?<ul>
<li>An <strong>Intrusion Prevention System (IPS)</strong> is a network security/threat prevention technology that examines network traffic flows to detect and prevent vulnerability exploits.<br><img src="/images/AWS/CSAP/CSAP_IDS_IPS.png" alt="CSAP_IDS_IPS"></li>
</ul>
</li>
<li>IDS/IPS Exam Tips<ul>
<li><strong>IDS</strong> is intrusion <strong>Detection</strong>.</li>
<li><strong>IPS</strong> is intrusion <strong>Prevention</strong>.</li>
<li>Generally, you have an appliance in a public subnet, and then agents installed on <strong>each</strong> EC2 instance.</li>
<li>Log files can be sent to a <strong>SOC (Security Operation Centre)</strong> or stored in <strong>S3</strong>.</li>
</ul>
</li>
<li>Great Youtube Video<ul>
<li><a href="https://www.youtube.com/watch?v=wZ8sB7hfyvw" target="_blank" rel="external">https://www.youtube.com/watch?v=wZ8sB7hfyvw</a></li>
<li>AltertLogic Threat Manager AWS Youtube.</li>
</ul>
</li>
</ul>
</li>
<li>Domain 6 - Summary<ul>
<li>Domain 6<ul>
<li><strong>Domain 6.0: Security</strong><ul>
<li>6.1 Design information security management systems and compliance controls</li>
<li>6.2 Design security controls with the AWS shared responsibility model and global infrastructure</li>
<li>6.3 Design identity and access management controls</li>
<li>6.4 Design protection of Data at Rest controls</li>
<li>6.5 Design protection of Data in Flight and Network Perimeter controls</li>
<li>Worth 20%</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips - CloudHSM<ul>
<li>Single Tenanted (<strong>1 physical device</strong>, for you only)</li>
<li><strong>Must</strong> be used within a <strong>VPC</strong></li>
<li>You can use <strong>VPC peering</strong> to connect to a <strong>CloudHSM</strong></li>
<li>You can use <strong>EBS volume encryption</strong>, <strong>S3 object encryption</strong> and <strong>key management</strong> with CloudHSM, but this does require <strong>custom application scripting</strong>.</li>
<li>If you need fault tolerance, you need to build a <strong>cluster</strong>, so you will need <strong>2</strong>. If you have only one and it fails, you will <strong>lose</strong> your keys.</li>
<li>Can integrate with <strong>RDS</strong> (Oracle &amp; SQL) as well as <strong>Redshift</strong></li>
<li>Monitor via <strong>Syslog</strong></li>
</ul>
</li>
<li>Directory Services - Exam Tips<ul>
<li><strong>2 Types of Directory Services:</strong><ul>
<li><strong>AD Connector (Used for existing AD Deployments)</strong><ul>
<li>Great way to connect in to existing AD environments.</li>
<li>Supports MFA</li>
</ul>
</li>
<li><strong>Simple AD (Used for new AD Deployments)</strong><ul>
<li>MFA Not Supported</li>
<li>Cannot add additional AD servers</li>
<li>No Trust Relationships</li>
<li>Cannot Transfer FSMO Roles</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>IDS/IPS Exam Tips<ul>
<li><strong>IDS</strong> is intrusion <strong>Detection</strong>.</li>
<li><strong>IPS</strong> is intrusion <strong>Prevention</strong>.</li>
<li>Generally, you have an appliance in a public subnet, and then agents installed on <strong>each</strong> EC2 instance.</li>
<li>Log files can be sent to a <strong>SOC (Security Operation Centre)</strong> or stored in <strong>S3</strong>.<br><img src="/images/AWS/CSAP/CSAP_IDS_IPS.png" alt="CSAP_IDS_IPS"></li>
</ul>
</li>
<li>Monitoring Exam Tips<ul>
<li>CloudWatch for Logs</li>
<li>CloudTrail for Audits, tracing API calls</li>
<li>CloudWatch can monitor CloudTrail</li>
<li><strong>You can send your logs to:</strong><ul>
<li>CloudWatch</li>
<li>S3</li>
<li>Third Parties (ALertLogic, SumoLogic, Splunk etc.)</li>
</ul>
</li>
<li>CloudTrail can log events from multiple accounts to a single S3 bucket.</li>
<li>CloudWatch supports cross account subscriptions, however this a very new service.</li>
<li>Always look to store your logs in a <strong>persistent</strong> &amp; <strong>safe</strong> place. S3 or CloudWatch are the ideal places.</li>
<li>By <strong>default</strong>, CloudWatch Logs will store your log data <strong>indefinitely</strong>.</li>
<li>CloudWatch Alarm History is only <strong>14 days</strong>.</li>
</ul>
</li>
<li>Identity Brokers<ul>
<li>Develop an <strong>Identity Broker</strong> to communicate with <strong>LDAP</strong> and <strong>AWS STS</strong></li>
<li><strong>Identity Broker</strong> always authenticates with <strong>LDAP</strong> first, <strong>Then</strong> with <strong>AWS STS</strong></li>
<li>Application then gets <strong>temporary access</strong> to AWS resources<br><img src="/images/AWS/Sysops/sts_scenario.jpg" alt="sts_scenario"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Domain-7-Scalability-amp-Elasticity-15"><a href="#Domain-7-Scalability-amp-Elasticity-15" class="headerlink" title="Domain 7 - Scalability &amp; Elasticity - 15%"></a>Domain 7 - Scalability &amp; Elasticity - 15%</h3><ul>
<li>CloudFront<ul>
<li>CloudFront<br>  <strong>Amazon CloudFront</strong> can be used to deliver your entire website, including dynamic, static, streaming, and interactive content using a global network of edge locations.<br>  Requests for your content are automatically routed to the nearest edge location, so content is delivered with the best possible performance. Amazon CloudFront is optimized to work with other Amazon Web Services, like S3, EC2, ELB and Route 53.</li>
<li>Key Concepts<ul>
<li>Distribution Types</li>
<li>Geo Restriction</li>
<li>Support for POST, PUT &amp; Other HTTP Methods</li>
<li>SSL Configurations</li>
<li>Wildcard CNAME support</li>
<li>Invalidation</li>
<li>Zone Apex Support</li>
<li>Edge Caching</li>
</ul>
</li>
<li>Distribution Types<ul>
<li><strong>Two types of Distributions:</strong><ul>
<li>Web Distributions</li>
<li>RTMP Distributions</li>
</ul>
</li>
</ul>
</li>
<li>Geo Restriction<ul>
<li><strong>Geo Restriction</strong> or <strong>Geoblocking</strong> lets you choose the countries in which you want to restrict access to your content</li>
<li><strong>Whitelists</strong> or <strong>Blacklists</strong> particular countries</li>
<li>Can be done either using the <strong>API</strong> or the <strong>console</strong></li>
<li>Viewer from blacklisted countries will see a <strong>HTTP 403</strong> error</li>
<li>You can also create <strong>custom</strong> error pages</li>
</ul>
</li>
<li>Support For<ul>
<li><strong>GET, HEAD, POST, PUT, PATCH, DELETE</strong> and <strong>OPTIONS</strong>.</li>
<li>Amazon CloudFront does <strong>not</strong> cache the responses to POST, PUT, DELETE and PATCH requests – these requests are proxied back to the origin server.</li>
</ul>
</li>
<li>SSL<br>  You can use either <strong>HTTP</strong> or <strong>HTTPS</strong> with <strong>CloudFront</strong>. With <strong>SSL</strong>, you can use the <strong>default</strong> CloudFront URL, or your own <strong>custom</strong> URL with your own <strong>SSL</strong> certificate.<ul>
<li><strong>Dedicated IP Custom SSL:</strong> Allocates dedicated IP addresses to serve your SSL content at each CloudFront edge location. VERY Expensive. $600 USD per certificate per month. However will support older browsers.</li>
<li><strong>SNI Custom SSL:</strong> Relies on the SNI extension of the Transport Layer Security protocol, which allows multiple domains to serve SSL traffic over the same IP address by including the hostname viewers are trying to connect to. Older browsers won’t support it.</li>
</ul>
</li>
<li>CNAME Support<ul>
<li><strong>CNAMEs are supported</strong><ul>
<li>You can have 10 CNAMES aliases to each distribution.</li>
<li>CloudFront also supports wildcard CNAMEs.</li>
</ul>
</li>
</ul>
</li>
<li>Invalidation Requests<br>  There are <strong>multiple</strong> options for removing a file from the edge locations. You can simply delete the file from your origin and, as content in the edge locations reaches the expiration period defined in each object’s HTTP header, it will be removed.<br>  In the event that offensive or potentially harmful material needs to be removed before the specified expiration time, you can use the <strong>Invalidation API</strong> to remove the object from all Amazon CloudFront edge locations.</li>
<li>Zone Apex Support<br>  Using <strong>Amazon Route S3</strong>, AWS’s authoritative DNS service, you can configure an ‘<strong>Alias</strong>‘ record that lets you map the apex or root (example.com) of your DNS name to your Amazon CloudFront distribution.<br>  <a href="http://example.com" target="_blank" rel="external">http://example.com</a> =&gt; <a href="http://d1111eft.cloufront.net" target="_blank" rel="external">http://d1111eft.cloufront.net</a><br>  Amazon Route 53 will then respond to each request for an Alias record with the right IP address(es) for your CloudFront distribution. Route 53 doesn’t charge for queries to Alias records that are mapped to a CloudFront distribution.</li>
<li>Dynamic Content Support<br>  <strong>Amazon CloudFront</strong> supports delivery of dynamic content that is customized or personalized using HTTP cookies. To use this feature, you specify whether you want Amazon CloudFront to forward some or all of your cookies to your custom origin server. Amazon CloudFront then considers the forwarded cookie values when identifying a unique object in its cache.<br>  This way, your end users get both the benefit of content that is personalized just for them with a cookie and the performance benefits of Amazon CloudFront. You can also optionally choose to log the cookie values in Amazon CloudFront access logs.</li>
<li>Exam Tips - CloudFront<ul>
<li><strong>2 distribution types:</strong> Web &amp; RTMP</li>
<li>You can use <strong>Geo Restriction</strong> to white list or black list a country.</li>
<li>You can use <strong>SSL</strong> with CloudFront. You can either use the <strong>default</strong> URL or your own <strong>custom</strong> URL. If you use custom URL you can use <strong>Dedicated IP Custom SSL</strong> or <strong>SNI Custom SSL</strong>.</li>
<li>CloudFront Supports <strong>GET, HEAD, POST, PUT, PATCH, DELETE</strong> and <strong>OPTIONS</strong>.</li>
<li>Amazon CloudFront does <strong>not</strong> cache the responses to POST, PUT, DELETE and PATCH requests – these requests are <strong>proxied</strong> back to the origin server.</li>
<li><strong>CNAMES</strong> are supported</li>
<li>In the event that offensive or potentially harmful material needs to be removed before the specified expiration time, you can use the <strong>Invalidation API</strong> to remove the object from all Amazon CloudFront edge locations. This is called an invalidation request.</li>
<li><strong>Zone Apex</strong> records are supported if you use <strong>Route53</strong>. This is achieved by using an <strong>Alias</strong> record.</li>
<li><strong>Dynamic Content</strong> is supported! Amazon CloudFront supports delivery of dynamic content that is <strong>customized</strong> or <strong>personalized</strong> using HTTP cookies.</li>
</ul>
</li>
</ul>
</li>
<li>Memcached vs Redis<ul>
<li>Memcached vs Redis<br>  In the SA Pro exam, you will be given a number of different scenarios. In some of these you will need to decide which <strong>ElastiCache Engine</strong> you need to use. ElastiCache has <strong>two</strong> different engines: <strong>Memcached</strong> &amp; <strong>Redis</strong>.<br>  Memcached and Redis look similar on the surface: both are <strong>in-memory key stores</strong>. However, in practice, there are significant differences.</li>
<li>Requirement in Memcached and Redis<br><img src="/images/AWS/CSAP/CSAP_Memcached_vs_Redis.png" alt="CSAP_Memcached_vs_Redis.png"></li>
<li>Exam Tips - Use Memcached If …<ul>
<li>You want the <strong>simplest</strong> model possible.</li>
<li>You need to run large <strong>nodes</strong> with <strong>multiple</strong> cores or threads.</li>
<li>You need the ability to <strong>scale out</strong>, <strong>adding</strong> and <strong>removing</strong> nodes as demand on your system increases and decreases.</li>
<li>You want to <strong>shard</strong> your data across <strong>multiple</strong> nodes.</li>
<li>You need to <strong>cache</strong> objects, such as a <strong>database</strong>.</li>
</ul>
</li>
<li>Exam Tips - Use Redis If…<ul>
<li>You need complex <strong>data types</strong>, such as <strong>strings, hashed, lists</strong> and <strong>sets</strong>.</li>
<li>You need to <strong>sort</strong> or <strong>rank</strong> in-memory data-sets.</li>
<li>You want <strong>persistence</strong> of your key store.</li>
<li>You want to <strong>replicate</strong> your data from the <strong>primary</strong> to one or more read replicas for availability.</li>
<li>You need <strong>automatic failover</strong> if any of your primary nodes fail.</li>
<li>You want <strong>publish</strong> and <strong>subscribe</strong> (pub/sub) capabilities – the client being informed of events on the server.</li>
<li>You want <strong>backup</strong> and <strong>restore</strong> capabilities.</li>
</ul>
</li>
</ul>
</li>
<li><p>Kinesis</p>
<ul>
<li>Kinesis<br>  <strong>Kinesis</strong> can be involved in many different scenario questions and you need to have a good understanding of what Kinesis is, what the different components are, and where you should use Kinesis.</li>
<li>What Is Kinesis?<br>  <strong>Amazon Kinesis Streams</strong> enable you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data, such as clickstreams, application logs, and social media to an Amazon Kinesis stream from hundreds of thousands of sources. Within seconds, the data will be available for your Amazon Kinesis Applications to read and process from the stream.<br>  <strong>Data in Kinesis is stored for 24 hours by default. You can increase this to 7 days if required.</strong><br>  <img src="/images/AWS/CSAP/CSAP_kinesis_flow.png" alt="CSAP_kinesis_flow.png"></li>
<li>Key Concepts<ul>
<li><strong>Data Producers</strong></li>
<li><strong>Shards</strong></li>
<li><strong>Records</strong><ul>
<li>Sequence Number</li>
<li>Partition Key</li>
<li>Data Itself (Blob)</li>
</ul>
</li>
<li><strong>Data Consumers</strong> (Kinesis Streams Applications)</li>
</ul>
</li>
<li>Data Producers<ul>
<li><strong>Amazon Kinesis Streams API</strong><ul>
<li>PutRecord (Single data record)</li>
<li>PutRecords (Multiple data records)</li>
</ul>
</li>
<li><strong>Amazon Kinesis Producer Library (KPL)</strong><ul>
<li>On Github, By using the KPL, customers do not need to develop the same logic every time they create a new application for data ingestion.</li>
</ul>
</li>
<li><strong>Amazon Kinesis Agent</strong><ul>
<li>Prebuilt Java Application you can install on your linux devices.</li>
</ul>
</li>
</ul>
</li>
<li>What Is A Shard?<br>  A <strong>Shard</strong> is simply the unit of measurement of data when referring to Kinesis.<br>  One shard provides a capacity of <strong>1MB/sec</strong> data input and <strong>2MB/sec</strong> data output. One shard can support up to <strong>1000 PUT</strong> records per second. You will specify the number of shards needed when you create a stream. For example, you can create a stream with two shards. This stream has a throughput of 2MB/sec data input and 4MB/sec data output, and allows up to 2000 PUT records per second. You can dynamically <strong>add</strong> or <strong>remove</strong> shards from your stream as your data throughput changes via <strong>resharding</strong>.</li>
<li>What Is A Partition Key?<br>  <strong>Kinesis Streams</strong> can consist of many different shards. You can group the data by shard by using a <strong>partition key</strong>.<br>  Essentially, the partition key tells you which shard the data belongs to. A partition key is specified by the applications putting the data into a stream.</li>
<li>What Is A Sequence Number?<br>  Each data record has a <strong>unique sequence number</strong>. Think of it as a unique key.<br>  The sequence number is assigned by Streams <strong>after</strong> you write to the stream with <strong>client.putRecord</strong> or <strong>client.putRecords</strong>.<br>  You can’t use sequence numbers to logically separate data in terms of what shards they have come from - you can <strong>only</strong> do this using partition keys.</li>
<li>Blobs<ul>
<li><strong>Data blobs</strong> are the data your data producer adds to a stream. The maximum size of a data blob (the data payload after Base64-decoding) is <strong>1 megabyte (MB)</strong>.</li>
</ul>
</li>
<li>Exam Tips - Kinesis<ul>
<li>Any kind of scenario where you are streaming large amounts of data that need to be processed quickly, think <strong>Kinesis</strong>.</li>
<li>Concepts<ul>
<li><strong>Data Producers</strong></li>
<li><strong>Shards</strong></li>
<li><strong>Records</strong><ul>
<li>Sequence Number</li>
<li>Partition Key</li>
<li>Data Itself (Blob)</li>
</ul>
</li>
<li><strong>Data Consumers</strong> (Kinesis Streams Applications)</li>
</ul>
</li>
<li>Data is stored for <strong>24 hours</strong> by default within streams and can be increased to <strong>7 days</strong>.</li>
<li>Use <strong>S3, Redshift</strong> etc. to store processed data for longer term. Kinesis streams <strong>IS NOT PERSISTENT</strong> storage.</li>
</ul>
</li>
</ul>
</li>
<li><p>SNS Mobile Push</p>
<ul>
<li>SNS Mobile Push<ul>
<li>A topic that can come up a fair bit in the various different scenarios. You send push notification messages to both mobile devices and desktops using one of the following supported push notification services:<ul>
<li>Amazon Device Messaging (ADM)</li>
<li>Apple Push Notification Service (APNS) for both iOS and Mac OS X</li>
<li>Baidu Cloud Push (Baidu)</li>
<li>Google Cloud Messaging for Android (GCM)</li>
<li>Microsoft Push Notification Service for Windows Phone (MPNS)</li>
<li>Windows Push Notification Services (WNS)<br><img src="/images/AWS/CSAP/CSAP_SNS_Mobile_Push.png" alt="CSAP_SNS_Mobile_Push.png"></li>
</ul>
</li>
</ul>
</li>
<li>Steps<ol>
<li>Request Credentials from Mobile Platforms</li>
<li>Request Token from Mobile Platforms</li>
<li>Create Platform Application Object</li>
<li>Create Platform Endpoint Object</li>
<li>Publish Message to Mobile Endpoint</li>
</ol>
<ul>
<li><strong>Further reading:</strong><ul>
<li><a href="https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html" target="_blank" rel="external">https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html</a></li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips - SNS Mobile Push<ul>
<li>You can extend SNS to mobile applications using <strong>SNS Mobile Push</strong>. The following push notifications are supported:<ul>
<li>Amazon Device Messaging (ADM)</li>
<li>Apple Push Notification Service (APNS) for both iOS and Mac OS X</li>
<li>Baidu Cloud Push (Baidu)</li>
<li>Google Cloud Messaging for Android (GCM)</li>
<li>Microsoft Push Notification Service for Windows Phone (MPNS)</li>
<li>Windows Push Notification Services (WNS)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Domain 7</p>
<ul>
<li>Domain 7.0 - Scalability &amp; Elasticity<ul>
<li>7.1 Demonstrate the ability to design a loosely coupled system</li>
<li>7.2 Demonstrate ability to implement the most appropriate front-end scaling architecture</li>
<li>7.3 Demonstrate ability to implement the most appropriate middle-tier scaling architecture</li>
<li>7.4 Demonstrate ability to implement the most appropriate data storage scaling architecture</li>
<li>7.5 Determine trade-offs between vertical and horizontal scaling</li>
</ul>
</li>
<li>Exam Tips - CloudFront<ul>
<li><strong>2 distribution types:</strong> Web &amp; RTMP</li>
<li>You can use <strong>Geo Restriction</strong> to white list or black list a country.</li>
<li>You can use <strong>SSL</strong> with CloudFront. You can either use the <strong>default</strong> URL or your own <strong>custom</strong> URL. If you use custom URL you can use <strong>Dedicated IP Custom SSL</strong> or <strong>SNI Custom SSL</strong>.</li>
<li>CloudFront Supports <strong>GET, HEAD, POST, PUT, PATCH, DELETE</strong> and <strong>OPTIONS</strong>.</li>
<li>Amazon CloudFront does <strong>not</strong> cache the responses to POST, PUT, DELETE and PATCH requests – these requests are <strong>proxied</strong> back to the origin server.</li>
<li><strong>CNAMES</strong> are supported</li>
<li>In the event that offensive or potentially harmful material needs to be removed before the specified expiration time, you can use the <strong>Invalidation API</strong> to remove the object from all Amazon CloudFront edge locations. This is called an invalidation request.</li>
<li><strong>Zone Apex</strong> records are supported if you use <strong>Route53</strong>. This is achieved by using an <strong>Alias</strong> record.</li>
<li><strong>Dynamic Content</strong> is supported! Amazon CloudFront supports delivery of dynamic content that is <strong>customized</strong> or <strong>personalized</strong> using HTTP cookies.</li>
</ul>
</li>
<li>Exam Tips - Use Memcached If …<ul>
<li>You want the <strong>simplest</strong> model possible.</li>
<li>You need to run large <strong>nodes</strong> with <strong>multiple</strong> cores or threads.</li>
<li>You need the ability to <strong>scale out</strong>, <strong>adding</strong> and <strong>removing</strong> nodes as demand on your system increases and decreases.</li>
<li>You want to <strong>shard</strong> your data across <strong>multiple</strong> nodes.</li>
<li>You need to <strong>cache</strong> objects, such as a <strong>database</strong>.</li>
</ul>
</li>
<li>Exam Tips - Use Redis If…<ul>
<li>You need complex <strong>data types</strong>, such as <strong>strings, hashed, lists</strong> and <strong>sets</strong>.</li>
<li>You need to <strong>sort</strong> or <strong>rank</strong> in-memory data-sets.</li>
<li>You want <strong>persistence</strong> of your key store.</li>
<li>You want to <strong>replicate</strong> your data from the <strong>primary</strong> to one or more read replicas for availability.</li>
<li>You need <strong>automatic failover</strong> if any of your primary nodes fail.</li>
<li>You want <strong>publish</strong> and <strong>subscribe</strong> (pub/sub) capabilities – the client being informed of events on the server.</li>
<li>You want <strong>backup</strong> and <strong>restore</strong> capabilities.</li>
</ul>
</li>
<li>Exam Tips - SNS Mobile Push<ul>
<li>You can extend SNS to mobile applications using <strong>SNS Mobile Push</strong>. The following push notifications are supported:<ul>
<li>Amazon Device Messaging (ADM)</li>
<li>Apple Push Notification Service (APNS) for both iOS and Mac OS X</li>
<li>Baidu Cloud Push (Baidu)</li>
<li>Google Cloud Messaging for Android (GCM)</li>
<li>Microsoft Push Notification Service for Windows Phone (MPNS)</li>
<li>Windows Push Notification Services (WNS)</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips - Kinesis<ul>
<li>Any kind of scenario where you are streaming large amounts of data that need to be processed quickly, think <strong>Kinesis</strong>.</li>
<li>Concepts<ul>
<li><strong>Data Producers</strong></li>
<li><strong>Shards</strong></li>
<li><strong>Records</strong><ul>
<li>Sequence Number</li>
<li>Partition Key</li>
<li>Data Itself (Blob)</li>
</ul>
</li>
<li><strong>Data Consumers</strong> (Kinesis Streams Applications)</li>
</ul>
</li>
<li>Data is stored for <strong>24 hours</strong> by default within streams and can be increased to <strong>7 days</strong>.</li>
<li>Use <strong>S3, Redshift</strong> etc. to store processed data for longer term. Kinesis streams <strong>IS NOT PERSISTENT</strong> storage.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Domain-8-Cloud-Migration-amp-Hybrid-Architecture-10"><a href="#Domain-8-Cloud-Migration-amp-Hybrid-Architecture-10" class="headerlink" title="Domain 8 - Cloud Migration &amp; Hybrid Architecture - 10%"></a>Domain 8 - Cloud Migration &amp; Hybrid Architecture - 10%</h3><ul>
<li>Vmware Migrations<ul>
<li>Migrations<ul>
<li><strong>AWS Management Portal</strong> for vCenter enables you to manage your AWS resources using VMware vCenter.</li>
<li>The portal installs as a vCenter plug-in within your existing vCenter environment. Once installed, it enables you to migrate VMware VMs to Amazon EC2 and manage AWS resources from within vCenter.</li>
</ul>
</li>
<li>Common Use Cases<ul>
<li>Migrate VMware VMs to Amazon EC2</li>
<li>Reach New Geographies from vCenter</li>
<li>Self-Service AWS Portal within vCenter</li>
<li>Leverage vCenter Experience While Getting Started with AWS</li>
</ul>
</li>
<li>Product Demo<ul>
<li><a href="https://aws.amazon.com/ec2/vcenter-portal/" target="_blank" rel="external">https://aws.amazon.com/ec2/vcenter-portal/</a></li>
</ul>
</li>
<li>Exam Tips - VMware<ul>
<li>vCenter has a plug-in that it enables you to migrate VMware VMs to Amazon EC2 and manage AWS resources from within vCenter.</li>
<li><strong>Use Cases Include:</strong><ul>
<li>Migrate VMware VMs to Amazon EC2</li>
<li>Reach New Geographies from vCenter</li>
<li>Self-Server AWS Portal within vCenter</li>
<li>Leverage vCenter Experience While Getting Started with AWS</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Storage Gateway<ul>
<li>Storage Gateway<ul>
<li><strong>Gateway-Cached Volumes</strong><ul>
<li>iSCSI based block storage</li>
</ul>
</li>
<li><strong>Gateway-Stored Volumes</strong><ul>
<li>iSCSI based block storage</li>
</ul>
</li>
<li><strong>Gateway-Virtual Tape Library</strong><ul>
<li>iSCSI based virtual tape solution</li>
</ul>
</li>
</ul>
</li>
<li>Storage Gateway Migrations<br><img src="/images/AWS/CSAP/CSAP_storage_gateway_migration.png" alt="CSAP_storage_gateway_migration.png"></li>
<li>Consistent Snapshots<ul>
<li>Snapshots provide a <strong>point-in-time view of data</strong> that has been written to your AWS Storage Gateway volumes. However, snapshots only capture data that has been written to your storage volumes, which can exclude data that has been buffered by either the client or OS.</li>
<li>Your application and operating system will eventually <strong>flush</strong> this buffered data to your storage volumes.</li>
<li>If you need to guarantee that your operating system and file system have flushed their buffered data to disk prior to taking a snapshot, you can do this by taking your storage volume <strong>offline</strong> before taking a snapshot.</li>
<li>Doing this forces your operating system to <strong>flush</strong> its data to disk. After the snapshot is complete. you can bring the volume back online.</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>You can use <strong>Storage Gateway</strong> to migrate your existing VMs to AWS.</li>
<li>You need to ensure that the snapshots are <strong>consistent</strong>. The best way to do this is to your VM offline and then do the snapshot.</li>
</ul>
</li>
</ul>
</li>
<li>Data Pipeline<ul>
<li>Data Pipeline<ul>
<li><strong>AWS Data Pipeline</strong> is a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premise data sources, at specified intervals. </li>
</ul>
</li>
<li>Data Pipeline - Key Concepts<ul>
<li>Pipeline</li>
<li>Datanode</li>
<li>Activity</li>
<li>Procondition</li>
<li>Schedule</li>
</ul>
</li>
<li>Pipeline<ul>
<li><strong>Pipeline</strong><ul>
<li>A pipeline is simply the name of the container that contains the datanodes, activities, preconditions, and schedules required in order to move your data from one location to another.</li>
<li>A pipeline can run either on an EC2 instance or an EMR instance.</li>
<li>Data Pipeline will provision and terminate these resources for you automatically.</li>
</ul>
</li>
</ul>
</li>
<li>Data Pipeline - Use on Premise<ul>
<li><strong>Pipeline - On Premise</strong><ul>
<li>AWS Data Pipeline supplies a Task Runner package that can be installed on your on-premise hosts. This package continuously polls the AWS Data Pipeline service for work to perform. When it’s time to run a particular activity on your on-premise resources, for example, executing a DB stored procedure or a database dump, AWS Data Pipeline will issue the appropriate command to the Task Runner.</li>
</ul>
</li>
</ul>
</li>
<li>Data Pipeline - Data Nodes<ul>
<li><strong>DataNode</strong><ul>
<li>A data node is essentially the end destination for your data. For example, a data node can reference a specific Amazon S3 path. AWS Data Pipeline supports an expression language that makes it easy to reference data which is generated on a regular basis. For example, you could specify that your Amazon S3 data format is s3://example-bucket/my-logs/logdata-#{scheduledStartTime(‘YYYY-MM-dd-HH’)}.tgz.</li>
</ul>
</li>
</ul>
</li>
<li>Data Pipeline - Activity<ul>
<li><strong>Activity</strong><ul>
<li>An activity is an action that AWS Data Pipeline initiates on your behalf as part of a pipeline. Example activities are EMR or Hive jobs, copies, SQL queries, or command-line scripts.</li>
<li>You can specify your own custom activities using the ShellCommandActivity</li>
</ul>
</li>
</ul>
</li>
<li>Data Pipeline - Precondition<ul>
<li><strong>Precondition</strong><ul>
<li>A precondition is a readiness check that can be optionally associated with a data source or activity. If a data source has a precondition check, then that check must complete successfully before any activities consuming the data source are launched. If an activity has a precondition, then the precondition check must complete successfully before the activity is run. This can be useful if you are running an activity that is expensive to compute, and should not run until specific criteria are met.</li>
<li>You can specify custom preconditions.</li>
</ul>
</li>
<li><strong>Precondition</strong><ul>
<li>DynamoDBDataExists - Does data exist?</li>
<li>DynamoDBTableExists - Does Table exist?</li>
<li>S3KeyExists - Does S3 path Exist?</li>
<li>S3PrefixExists - Does a file exist within that S3 path?</li>
<li>ShellCommandPrecondition - Custom Preconditions</li>
</ul>
</li>
</ul>
</li>
<li>Data Pipeline - Schedule<ul>
<li><strong>Schedule</strong><ul>
<li>Schedules define when your pipeline activities run and the frequency with which the service expects your data to be available.</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips - Data Pipeline<ul>
<li><strong>AWS Data Pipeline</strong> is a web service that helps you reliably process and move data between different AWS compute and storage services.</li>
<li>Can be integrated with <strong>on-premise environments</strong>.</li>
<li>Can be <strong>scheduled</strong>.</li>
<li>Data Pipeline will <strong>provision</strong> and <strong>terminate</strong> resources as, and when, required.</li>
<li>A lot of its functionality has been replaced by <strong>Lambda</strong>.</li>
<li><strong>A Pipeline consists of:</strong><ul>
<li>Datanode</li>
<li>Activity</li>
<li>Precondition</li>
<li>Schedule</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Migrations &amp; Networking<ul>
<li>CIDR Reservations<ul>
<li>When you create a subnet, you specify the CIDR block for the subnet. The allowed block size is between a <strong>/28</strong> netmask and <strong>/16</strong> netmask.</li>
<li>If you create more than one subnet in a VPC, the CIDR blocks of the subnets must <strong>not</strong> overlap.</li>
<li>The first four IP addresses and the last IP address in each subnet CIDR block are not available for you to use, and cannot be assigned to an instance. For example, in a subnet with CIDR block 10.0.0.0/24, the following <strong>5 IP addresses are reserved</strong>:<ul>
<li>10.0.0.0: Network address.</li>
<li>10.0.0.1: Reserved by AWS for the VPC router.</li>
<li>10.0.0.2: Reserved by AWS for mapping to the Amazon-Provided DNS.</li>
<li>10.0.0.3: Reserved by AWS for future use.</li>
<li>10.0.0.255: Network broadcast address. AWS do not support broadcast in a VPC; therefore they reserve this address.</li>
</ul>
</li>
</ul>
</li>
<li>VPN To Direct Connect Migrations<ul>
<li>Most organizations will have a site-to-site VPN tunnel from their location to AWS. As traffic begins to become heavier, organizations will opt to use <strong>Direct Connect</strong>.</li>
<li>Once Direct Connect is installed, you should configure it so that your VPN connection &amp; your Direct Connect connection are within the same BGP community. You then <strong>configure</strong> BGP so that your VPN connection has a higher cost than the Direct Connect connection.</li>
</ul>
</li>
<li>Exam Tips - Network &amp; Migrations<ul>
<li><strong>CIDR blocks</strong> can be between /16 - /28</li>
<li>AWS reserve <strong>5</strong> IP addresses per CIDR block.</li>
<li>You can migrate from a VPN connection to a Direct Connect connection by using <strong>BGP</strong>. Simply ensure that the VPN connection has a higher BGP cost than the Direct Connect connection.</li>
</ul>
</li>
</ul>
</li>
<li>Domain 8 - Summary<ul>
<li>Domain 8 - Cloud Migration &amp; Hybrid Architecture<ul>
<li><strong>Domain 8.0: Cloud Migration and Hybrid Architecture</strong><ul>
<li>8.1 Plan and execute for applications migrations</li>
<li>8.2 Demonstrate ability to design hybrid cloud architectures</li>
<li>Worth 10%</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips - VMware<ul>
<li>vCenter has a plug-in that it enables you to migrate VMware VMs to Amazon EC2 and manage AWS resources from within vCenter.</li>
<li><strong>Use Cases Include:</strong><ul>
<li>Migrate VMware VMs to Amazon EC2</li>
<li>Reach New Geographies from vCenter</li>
<li>Self-Server AWS Portal within vCenter</li>
<li>Leverage vCenter Experience While Getting Started with AWS</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips - Storage Gateway Migrations<ul>
<li>You can use <strong>Storage Gateway</strong> to migrate your existing VMs to AWS.</li>
<li>You need to ensure that the snapshots are <strong>consistent</strong>. The best way to do this is to your VM offline and then do the snapshot.</li>
</ul>
</li>
<li>Exam Tips - Data Pipeline<ul>
<li><strong>AWS Data Pipeline</strong> is a web service that helps you reliably process and move data between different AWS compute and storage services.</li>
<li>Can be integrated with <strong>on-premise environments</strong>.</li>
<li>Can be <strong>scheduled</strong>.</li>
<li>Data Pipeline will <strong>provision</strong> and <strong>terminate</strong> resources as, and when, required.</li>
<li>A lot of its functionality has been replaced by <strong>Lambda</strong>.</li>
<li><strong>A Pipeline consists of:</strong><ul>
<li>Datanode</li>
<li>Activity</li>
<li>Precondition</li>
<li>Schedule</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips - Network &amp; Migrations<ul>
<li><strong>CIDR blocks</strong> can be between /16 - /28</li>
<li>AWS reserve <strong>5</strong> IP addresses per CIDR block.</li>
<li>You can migrate from a VPN connection to a Direct Connect connection by using <strong>BGP</strong>. Simply ensure that the VPN connection has a higher BGP cost than the Direct Connect connection.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Additional-Content"><a href="#Additional-Content" class="headerlink" title="Additional Content"></a>Additional Content</h2><ul>
<li><p>CloudFront</p>
<ul>
<li>CloudFront Origin Access Identity (OAI)<ul>
<li>used to restrict access to S3 content <a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html" target="_blank" rel="external">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></li>
<li>注意点: 做实验时，如果设置好后，发现不能通过cloudFront的节点来访问S3的内容。要注意看一下浏览器的url，当你通过cloudfront节点访问S3资源的时，有没有被redirect到S3的url去，如果有，就说明cloudfront设置还没有完全生效，隔一段时间后再试就会发现没有问题了<ul>
<li>官方论坛里面提及了，在CloudFront使用了新建立的bucket时会可能出现有这个问题，因为新Bucket的domian还没有完全扩散出去 <a href="https://forums.aws.amazon.com/message.jspa?messageID=677452" target="_blank" rel="external">Cloudfront domain redirects to S3 Origin URL</a> </li>
<li>详细的操作步骤 – <a href="https://stackoverflow.com/a/42285049" target="_blank" rel="external">AWS CloudFront access denied to S3 bucket</a></li>
<li><a href="https://acloud.guru/forums/aws-certified-developer-associate/discussion/-KcC5f1fw3l4-tclsXje/cloudfront_redirecting_to_orig" target="_blank" rel="external">CloudFront redirecting to origin?</a></li>
</ul>
</li>
</ul>
</li>
<li>Custom SSL Certificate<ul>
<li>use SSL Certificate stored in AWS Certificate Manager (ACM) in the US East(N. Virginia) Region</li>
<li>use a certificate stored in IAM.<ul>
<li><a href="https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-custom-certificate/" target="_blank" rel="external">How do I ensure that the certificate I upload for use with Amazon CloudFront is accessible from the CloudFront console?</a></li>
</ul>
</li>
</ul>
</li>
<li>Custom SSL Options for Amazon CloudFront - <a href="https://aws.amazon.com/cloudfront/custom-ssl-domains/" target="_blank" rel="external">https://aws.amazon.com/cloudfront/custom-ssl-domains/</a><ul>
<li>SNI Custom SSL - SNI (Server Name Indication)</li>
<li>Dedicated IP Custom SSL</li>
</ul>
</li>
<li>Why set TTL to 0<ul>
<li><a href="https://aws.amazon.com/blogs/aws/amazon-cloudfront-support-for-dynamic-content/" target="_blank" rel="external">https://aws.amazon.com/blogs/aws/amazon-cloudfront-support-for-dynamic-content/</a></li>
<li>In many cases, dynamic content is either not cacheable or cacheable for a very short period of time, perhaps just a few seconds. In the past, CloudFront’s minimum TTL was 60 minutes since all content was considered static. The new minimum TTL value is 0 seconds. If you set the TTL for a particular origin to 0, CloudFront will still cache the content from that origin. It will then make a GET request with an If-Modified-Since header, thereby giving the origin a chance to signal that CloudFront can continue to use the cached content if it hasn’t changed at the origin.</li>
</ul>
</li>
</ul>
</li>
<li><p>RedShift</p>
<ul>
<li>RedShift workload Management(WLM)<ul>
<li>enables users to manage priorities flexibly within workloads so that short, fast-running queries won’t get stuck in queues behind long-running queries.</li>
</ul>
</li>
<li>Copying Snapshots to Another Region – <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html" target="_blank" rel="external">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html</a><ul>
<li>You can configure Amazon Redshift to automatically copy snapshots (automated or manual) for a cluster to another region</li>
</ul>
</li>
</ul>
</li>
<li>NAT<ul>
<li>High Available NAT<ul>
<li>Good blog on how to do it here: <a href="https://aws.amazon.com/cn/articles/high-availability-for-amazon-vpc-nat-instances-an-example/" target="_blank" rel="external">https://aws.amazon.com/cn/articles/high-availability-for-amazon-vpc-nat-instances-an-example/</a></li>
</ul>
</li>
</ul>
</li>
<li><p>WAF</p>
<ul>
<li>WAF sandwich<ul>
<li><a href="https://www.cloudaxis.com/2016/11/21/waf-sandwich/" target="_blank" rel="external">WAF Sandwich</a><br><img src="/images/AWS/CSAP/CSAP_WAF_Sandwich.png" alt="CSAP_WAF_Sandwich.png"></li>
</ul>
</li>
</ul>
</li>
<li><p>Multicast</p>
<ul>
<li>Multicast is not available on AWS</li>
<li>You can create a virtual overlay network that runs on the OS level of the instance <a href="https://aws.amazon.com/articles/overlay-multicast-in-amazon-virtual-private-cloud/" target="_blank" rel="external">https://aws.amazon.com/articles/overlay-multicast-in-amazon-virtual-private-cloud/</a></li>
</ul>
</li>
<li>EC2<ul>
<li>RAID <a href="https://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/raid-config.html" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/raid-config.html</a><ul>
<li>RAID 0 - 增加吞吐量但是不提供冗余</li>
<li>RAID 1 - 增加冗余，但是不增加吞吐量</li>
</ul>
</li>
<li>ec2:RunInstances<ul>
<li>Policy中如果RunInstances的Resource只有“arn:aws:ec2:us-east-1:accountid:instance/*”的话，是不能获得新建Instance的权限的</li>
<li>因为新建Instance需要涉及到很多资源, 还需要”key-pair/<em>“,”security-group/</em>“,volume, ami,subnet, network-interface等等</li>
</ul>
</li>
<li>How to increase throughput of the instance bandwidth<ul>
<li>Enhanced networking uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types. SR-IOV is a method of device virtualization that provides higher I/O performance and lower CPU utilization when compared to traditional virtualized network interfaces. Enhanced networking provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies.</li>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html" target="_blank" rel="external">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html</a></li>
</ul>
</li>
</ul>
</li>
<li>Identity Broker<ul>
<li>Assume Role - AssumeRole<ul>
<li><a href="https://docs.aws.amazon.com/zh_cn/STS/latest/APIReference/API_AssumeRole.html" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/STS/latest/APIReference/API_AssumeRole.html</a></li>
</ul>
</li>
<li>GetFederationToken - <ul>
<li><a href="https://docs.aws.amazon.com/zh_cn/STS/latest/APIReference/API_GetFederationToken.html" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/STS/latest/APIReference/API_GetFederationToken.html</a></li>
</ul>
</li>
</ul>
</li>
<li>CloudWatch<ul>
<li>DashBoard<ul>
<li>You can monitor AWS resources in multiple regions using a single CloudWatch dashboard.</li>
<li>select the region and add the metric to dashboard, it can be shown in other region</li>
</ul>
</li>
</ul>
</li>
<li>IAM<ul>
<li>SAML<ul>
<li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html" target="_blank" rel="external">Enabling SAML 2.0 Federated Users to Access the AWS Management Console</a></li>
<li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html" target="_blank" rel="external">About SAML 2.0-based Federation</a></li>
</ul>
</li>
<li>Revoking IAM Role Temporary Security Credential<ul>
<li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_revoke-sessions.html" target="_blank" rel="external">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_revoke-sessions.html</a></li>
</ul>
</li>
</ul>
</li>
<li><p>IPSec</p>
<ul>
<li>advantage of an IPSec<ul>
<li>Data encryption across the internet</li>
<li>Protection of data in transit over the Internet</li>
<li>Peer identity authentication between source and destination (in AWS that is the VPN gateway and customer gateway)</li>
<li>Data integrity protection across the Internet</li>
</ul>
</li>
</ul>
</li>
<li><p>Kinesis</p>
<ul>
<li>What can I do with Amazon Kinesis Data Streams - <a href="https://aws.amazon.com/kinesis/data-streams/faqs/" target="_blank" rel="external">https://aws.amazon.com/kinesis/data-streams/faqs/</a><ul>
<li><strong>Accelerated log and data feed intake</strong>: Instead of waiting to batch up the data, you can have your data producers push data to an Amazon Kinesis data stream as soon as the data is produced, preventing data loss in case of data producer failures. For example, system and application logs can be continuously added to a data stream and be available for processing within seconds. </li>
<li><strong>Real-time metrics and reporting</strong>: You can extract metrics and generate reports from Amazon Kinesis data stream data in real-time. For example, your Amazon Kinesis Application can work on metrics and reporting for system and application logs as the data is streaming in, rather than wait to receive data batches.</li>
<li><strong>Real-time data analytics</strong>: With Amazon Kinesis Data Streams, you can run real-time streaming data analytics. For example, you can add clickstreams to your Amazon Kinesis data stream and have your Amazon Kinesis Application run analytics in real-time, enabling you to gain insights out of your data at a scale of minutes instead of hours or days. </li>
<li><strong>Complex stream processing</strong>: You can create Directed Acyclic Graphs (DAGs) of Amazon Kinesis Applications and data streams. In this scenario, one or more Amazon Kinesis Applications can add data to another Amazon Kinesis data stream for further processing, enabling successive stages of stream processing.</li>
</ul>
</li>
</ul>
</li>
<li><p>OpsWorks</p>
<ul>
<li>AWS OpsWorks Stacks Lifecycle Events<ul>
<li>Setup</li>
<li>Configure</li>
<li>Deploy</li>
<li>Undeploy</li>
<li>Shutdown</li>
</ul>
</li>
</ul>
</li>
<li><p>Direct Connect</p>
<ul>
<li>AWS re:Invent video about DX in 2016 and 2017<ul>
<li><a href="https://www.youtube.com/watch?v=eNxPhHTN8gY" target="_blank" rel="external">https://www.youtube.com/watch?v=eNxPhHTN8gY</a> (2017)</li>
<li><a href="https://www.youtube.com/watch?v=Qep11X1r1QA" target="_blank" rel="external">https://www.youtube.com/watch?v=Qep11X1r1QA</a> (2016)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Whitepaper"><a href="#Whitepaper" class="headerlink" title="Whitepaper"></a>Whitepaper</h2><ul>
<li><a href="https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf" target="_blank" rel="external">https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf</a><ul>
<li><a href="https://aws.amazon.com/blogs/security/how-to-configure-rate-based-blacklisting-with-aws-waf-and-aws-lambda/" target="_blank" rel="external">https://aws.amazon.com/blogs/security/how-to-configure-rate-based-blacklisting-with-aws-waf-and-aws-lambda/</a> </li>
<li><a href="https://aws.amazon.com/blogs/security/how-to-automatically-update-your-security-groups-for-amazon-cloudfront-and-aws-waf-by-using-aws-lambda/" target="_blank" rel="external">https://aws.amazon.com/blogs/security/how-to-automatically-update-your-security-groups-for-amazon-cloudfront-and-aws-waf-by-using-aws-lambda/</a></li>
<li><a href="https://aws.amazon.com/blogs/aws/subscribe-to-aws-public-ip-address-changes-via-amazon-sns/" target="_blank" rel="external">Subscribe to AWS Public IP Address Changes via Amazon SNS</a><ul>
<li>AmazonIpSpaceChanged的Topic是在us-ease-1中的，所以订阅这个Topic的时候，必须切换到N.Virginia才能订阅， 就像Billing Alarm只能在N.Virginia中设置一样</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS Certified </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Solutions Architect </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[A Simple ELB Access Log Parser By Ruby]]></title>
      <url>/2017/12/26/a-simple-elb-access-log-parser-by-ruby/</url>
      <content type="html"><![CDATA[<p>工作时候，偶尔会有手动检查ELB Access Log的时候，下载下来的access log是空格隔开的，人眼将数值对应到字段名非常累。</p>
<p>就顺手写了个Ruby的小程序来解析access log的内容，转换到csv格式。再使用Excel打开csv文件来进行查看。</p>
<h3 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h3><p>读取access log日志，转换为csv。</p>
<p>主要用于临时检查小数据量access log来定位问题。如果是基于access log来进行数据分析，还是需要结合RedShift来进行。</p>
<a id="more"></a>
<h3 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h3><p>参数–src_access_log来指定input的access log文件，参数–dest_csv_file来指定output的csv文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ruby aws_elb_access_log_parser.rb --src_access_log source_log --dest_csv_file dest_csv</div></pre></td></tr></table></figure></p>
<h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><p>Gist地址: <a href="https://gist.github.com/jibing57/ea180bfc3f7cb96e4a1fa67aa7a7c0c2" target="_blank" rel="external">https://gist.github.com/jibing57/ea180bfc3f7cb96e4a1fa67aa7a7c0c2</a></p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">require</span> <span class="string">'csv'</span></div><div class="line"><span class="keyword">require</span> <span class="string">'optparse'</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AWSELBAccessLogParser</span></span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">()</span></span></div><div class="line">    @@elb_access_log_format=<span class="string">%Q(timestamp elb client:port backend:port request_processing_time backend_processing_time response_processing_time elb_status_code backend_status_code received_bytes sent_bytes "request" "user_agent" ssl_cipher ssl_protocol)</span></div><div class="line">    <span class="comment"># puts "elb_access_log_format is #&#123;elb_access_log_format.split(" ")&#125;"</span></div><div class="line"></div><div class="line">    @@line_regex = <span class="regexp">/</span></div><div class="line"><span class="regexp">      (?&lt;timestamp&gt;[^ ]*)                                                                    # timestamp</span></div><div class="line"><span class="regexp">      \s+(?&lt;elb&gt;[^ ]*)                                                                       # elb</span></div><div class="line"><span class="regexp">      \s+(?&lt;client&gt;[^ ]*):(?&lt;client_port&gt;[0-9]*)                                             # client:port</span></div><div class="line"><span class="regexp">      \s+(?&lt;backend&gt;[^ ]*):(?&lt;backend_port&gt;[0-9]*)                                           # backend:port</span></div><div class="line"><span class="regexp">      \s+(?&lt;request_processing_time&gt;[-.0-9]*)                                                # request_processing_time value: 0.000056 or -1</span></div><div class="line"><span class="regexp">      \s+(?&lt;backend_processing_time&gt;[-.0-9]*)                                                # backend_processing_time value: 0.093779 or -1</span></div><div class="line"><span class="regexp">      \s+(?&lt;response_processing_time&gt;[-.0-9]*)                                               # response_processing_time value: 0.000049 or -1</span></div><div class="line"><span class="regexp">      \s+(?&lt;elb_status_code&gt;-|[0-9]*)                                                        # elb_status_code</span></div><div class="line"><span class="regexp">      \s+(?&lt;backend_status_code&gt;-|[0-9]*)                                                    # backend_status_code</span></div><div class="line"><span class="regexp">      \s+(?&lt;received_bytes&gt;[-0-9]*)                                                          # received_bytes</span></div><div class="line"><span class="regexp">      \s+(?&lt;sent_bytes&gt;[-0-9]*)                                                              # sent_bytes</span></div><div class="line"><span class="regexp">      # \s+\"(?&lt;request_method&gt;[^ ]*)\s+(?&lt;request_uri&gt;[^ ]*)\s+(?&lt;request_version&gt;- |[^ ]*)\" # request section</span></div><div class="line"><span class="regexp">      \s+\"(?&lt;request&gt;[^ ]*\s+[^ ]*\s+[^ ]*)\"                                               # entire request</span></div><div class="line"><span class="regexp">      \s+\"(?&lt;user_agent&gt;[^ ]*.*[^ ]*)\"                                                  # entire user_agent</span></div><div class="line"><span class="regexp">      \s+(?&lt;ssl_cipher&gt;[^ ]*)                                                                # ssl_cipher</span></div><div class="line"><span class="regexp">      \s+(?&lt;ssl_protocol&gt;[^ ]*)                                                              # ssl_protocol</span></div><div class="line"><span class="regexp">    /x</span></div><div class="line">  <span class="keyword">end</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parse_line</span><span class="params">(line)</span></span></div><div class="line">    <span class="keyword">return</span> <span class="literal">nil</span> <span class="keyword">if</span> line.<span class="literal">nil</span>?</div><div class="line">    line.match(@@line_regex)</div><div class="line">  <span class="keyword">end</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parse_log_to_csv</span><span class="params">(src_file, dst_file)</span></span></div><div class="line">    <span class="keyword">if</span> src_file.<span class="literal">nil</span>? <span class="keyword">or</span> dst_file.<span class="literal">nil</span>?</div><div class="line">      puts <span class="string">"please entry the right src_file and dst_file"</span></div><div class="line">      <span class="keyword">return</span> <span class="literal">false</span></div><div class="line">    <span class="keyword">end</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> !File.readable?(src_file)</div><div class="line">      puts <span class="string">"src_file[<span class="subst">#&#123;src_file&#125;</span>] is not readable"</span></div><div class="line">      <span class="keyword">return</span> <span class="literal">false</span></div><div class="line">    <span class="keyword">end</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> !File.writable?(File.dirname(dst_file))</div><div class="line">      puts <span class="string">"dst_file[<span class="subst">#&#123;dst_file&#125;</span>] is not writable"</span></div><div class="line">      <span class="keyword">return</span> <span class="literal">false</span></div><div class="line">    <span class="keyword">end</span></div><div class="line"></div><div class="line">    <span class="comment"># output fields name to dst_file</span></div><div class="line">    CSV.open(dst_file, <span class="string">"w"</span>) <span class="keyword">do</span> <span class="params">|data|</span></div><div class="line">      first_line = File.open(src_file, <span class="string">"r"</span>) &#123;<span class="params">|f|</span> f.readline&#125;</div><div class="line">      puts <span class="string">"first_line of file[<span class="subst">#&#123;src_file&#125;</span>] is <span class="subst">#&#123;first_line&#125;</span>"</span></div><div class="line">      parts = <span class="keyword">self</span>.parse_line(first_line)</div><div class="line">      data &lt;&lt; parts.names</div><div class="line">    <span class="keyword">end</span></div><div class="line"></div><div class="line">    <span class="comment"># parse the log file and store to dest csv file</span></div><div class="line">    CSV.open(dst_file, <span class="string">"a+"</span>) <span class="keyword">do</span> <span class="params">|data|</span></div><div class="line">      File.open(src_file, <span class="string">"r"</span>).each <span class="keyword">do</span> <span class="params">|line|</span></div><div class="line">        parts = parse_line(line)</div><div class="line">        <span class="keyword">if</span> parts == <span class="literal">nil</span></div><div class="line">          puts <span class="string">"Error -- Can't parse line [<span class="subst">#&#123;line&#125;</span>]"</span></div><div class="line">          <span class="keyword">next</span></div><div class="line">        <span class="keyword">end</span></div><div class="line">        line_csv_array=[]</div><div class="line">        parts.names.each &#123; <span class="params">|filed_name|</span> line_csv_array.push(parts[filed_name]) &#125;</div><div class="line">        <span class="comment"># puts line_csv_array.inspect</span></div><div class="line">        data &lt;&lt; line_csv_array</div><div class="line">      <span class="keyword">end</span></div><div class="line">    <span class="keyword">end</span></div><div class="line">  <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="comment"># Parse the command line</span></div><div class="line">src_access_log=<span class="string">""</span></div><div class="line">dest_csv_file=<span class="string">""</span></div><div class="line"></div><div class="line">options = &#123;&#125;</div><div class="line"><span class="keyword">begin</span></div><div class="line">  opts = OptionParser.new</div><div class="line">  opts.banner = <span class="string">"Usage: <span class="subst">#&#123;$PROGRAM_NAME&#125;</span> [options] ..."</span></div><div class="line">  opts.separator <span class="string">''</span></div><div class="line">  opts.separator <span class="string">'Options:'</span></div><div class="line">  opts.on(<span class="string">'-s src_access_log'</span>,</div><div class="line">          <span class="string">'--src_access_log src_access_log'</span>,</div><div class="line">          String,</div><div class="line">          <span class="string">'Set source access log file'</span>) &#123;<span class="params">|key|</span> options[<span class="symbol">:src_access_log</span>] = key&#125;</div><div class="line">  opts.on(<span class="string">'-d dest_csv_file'</span>,</div><div class="line">          <span class="string">'--dest_csv_file dest_csv_file'</span>,</div><div class="line">          String,</div><div class="line">          <span class="string">'set output csv file name'</span>) &#123;<span class="params">|key|</span> options[<span class="symbol">:dest_csv_file</span>] = key&#125;</div><div class="line">  opts.on(<span class="string">'-h'</span>, <span class="string">'--help'</span>, <span class="string">'Show this message'</span>) <span class="keyword">do</span></div><div class="line">    puts opts</div><div class="line">    exit</div><div class="line">  <span class="keyword">end</span></div><div class="line"><span class="keyword">rescue</span> OptionParser::ParseError</div><div class="line">  puts <span class="string">"Oops... <span class="subst">#&#123;$!&#125;</span>"</span></div><div class="line">  puts opts</div><div class="line">  exit</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="keyword">begin</span></div><div class="line">  opts.parse!</div><div class="line">  mandatory = [<span class="symbol">:src_access_log</span>, <span class="symbol">:dest_csv_file</span>]      <span class="comment"># Enforce the presence of</span></div><div class="line">  missing = mandatory.select&#123; <span class="params">|param|</span> options[param].<span class="literal">nil</span>? &#125;</div><div class="line">  <span class="keyword">if</span> <span class="keyword">not</span> missing.empty?</div><div class="line">    puts <span class="string">"Missing options: <span class="subst">#&#123;missing.join(<span class="string">', '</span>)&#125;</span>"</span></div><div class="line">    puts opts</div><div class="line">    exit -<span class="number">1</span></div><div class="line">  <span class="keyword">end</span></div><div class="line"><span class="keyword">rescue</span> OptionParser::InvalidOption, OptionParser::MissingArgument</div><div class="line">  puts $!.to_s</div><div class="line">  puts opts<span class="comment">#</span></div><div class="line">  exit -<span class="number">1</span></div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"></div><div class="line">AWSELBAccessLogParser.new().parse_log_to_csv(options[<span class="symbol">:src_access_log</span>], options[<span class="symbol">:dest_csv_file</span>])</div></pre></td></tr></table></figure>
<h3 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h3><ul>
<li><a href="/2017/12/22/what-is-Access-Logs-of-Classic-Load-Balancer/" title="什么是传统负载均衡器的访问日志">什么是传统负载均衡器的访问日志</a></li>
<li><a href="/2017/12/25/shell-gist-to-get-http-5XX-code-from-ELB-access-log/" title="shell gist to get http 5XX code from ELB access log">shell gist to get http 5XX code from ELB access log</a>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Elastic Load Balancing </tag>
            
            <tag> Ruby </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[shell gist to get http 5XX code from ELB access log]]></title>
      <url>/2017/12/25/shell-gist-to-get-http-5XX-code-from-ELB-access-log/</url>
      <content type="html"><![CDATA[<p>一个简陋的只是够用的shell小脚本，用来从S3中下载前一天的ELB access log，筛选出http code为5XX的日志，然后发送给某个指定的email地址。</p>
<h3 id="变量设置"><a href="#变量设置" class="headerlink" title="变量设置"></a>变量设置</h3><p>设置下MAIL_TO和S3_BASE_PREFIX这两个变量，然后在有AWS CLI访问环境的机器上执行即可。</p>
<ul>
<li>MAIL_TO 如果有多个地址，可用空格隔开</li>
<li>S3_BASE_PREFIX设置到Access log日志格式之前的prefix就行。</li>
</ul>
<p>比如完整的access log地址是: <code>s3://carl-elb-logs/AWSLogs/888888888888/elasticloadbalancing/ap-northeast-2/2017/12/24/704017765382_elasticloadbalancing_ap-northeast-2_ELB-carl_20171224T0120Z_13.125.86.136_5s9i5ef2.log</code></p>
<p>那么S3_BASE_PREFIX设置为<code>s3://carl-elb-logs/AWSLogs/888888888888/elasticloadbalancing/ap-northeast-2</code>即可<br><a id="more"></a></p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>Gist地址: <a href="https://gist.github.com/jibing57/fd241ab78d0243252a4b19ba19f69fe8" target="_blank" rel="external">https://gist.github.com/jibing57/fd241ab78d0243252a4b19ba19f69fe8</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"></div><div class="line"><span class="comment">#############################################</span></div><div class="line"><span class="comment">###### Global Variable  #########</span></div><div class="line"><span class="comment">#############################################</span></div><div class="line"></div><div class="line">MAIL_TO=<span class="string">""</span></div><div class="line">S3_BASE_PREFIX=<span class="string">""</span></div><div class="line"></div><div class="line">BASE_WORK_DIR=`<span class="built_in">cd</span> $(dirname <span class="variable">$0</span>); <span class="built_in">pwd</span> `</div><div class="line"></div><div class="line"><span class="comment">#############################################</span></div><div class="line"><span class="comment">###### Common Function #########</span></div><div class="line"><span class="comment">#############################################</span></div><div class="line"></div><div class="line"><span class="keyword">function</span> <span class="built_in">log</span>()</div><div class="line">&#123;</div><div class="line">    timer=`date <span class="string">"+%Y-%m-%d %H:%M:%S"</span>`</div><div class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$timer</span> -- <span class="variable">$1</span>"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">function</span> err_log()</div><div class="line">&#123;</div><div class="line">    <span class="built_in">log</span> <span class="string">"[Error]: <span class="variable">$1</span>"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">function</span> create_dir()</div><div class="line">&#123;</div><div class="line">    dir_name=<span class="variable">$1</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> [ ! -d <span class="variable">$dir_name</span> ]; <span class="keyword">then</span></div><div class="line">        <span class="built_in">log</span> <span class="string">"dir [<span class="variable">$dir_name</span>] is not existed, should create it"</span></div><div class="line">        mkdir -p <span class="variable">$dir_name</span></div><div class="line">    <span class="keyword">fi</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> [ ! -d <span class="variable">$dir_name</span> ]; <span class="keyword">then</span></div><div class="line">        err_log <span class="string">"dir [<span class="variable">$dir_name</span>] is not existed, and can't create it"</span></div><div class="line">        <span class="built_in">exit</span> -1</div><div class="line">    <span class="keyword">fi</span></div><div class="line"></div><div class="line">    <span class="built_in">return</span> 0</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">#############################################</span></div><div class="line"><span class="comment">###### Main Process #########</span></div><div class="line"><span class="comment">#############################################</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$MAIL_TO</span>"</span> == <span class="string">""</span> ]; <span class="keyword">then</span></div><div class="line">    <span class="built_in">log</span> <span class="string">"Error: MAIL_TO is empty"</span></div><div class="line">    <span class="built_in">exit</span> 1</div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$S3_BASE_PREFIX</span>"</span> == <span class="string">""</span> ]; <span class="keyword">then</span></div><div class="line">    <span class="built_in">log</span> <span class="string">"Error: S3_BASE_PREFIX is empty"</span></div><div class="line">    <span class="built_in">exit</span> 1</div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line">is_mac=<span class="literal">false</span></div><div class="line">os_name=$(uname -s)</div><div class="line"></div><div class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$os_name</span>"</span> == <span class="string">"Linux"</span> ]]; <span class="keyword">then</span></div><div class="line">    <span class="comment">#statements</span></div><div class="line">    is_mac=<span class="literal">false</span></div><div class="line"><span class="keyword">elif</span> [[ <span class="string">"<span class="variable">$os_name</span>"</span> == <span class="string">"Darwin"</span> ]]; <span class="keyword">then</span></div><div class="line">    is_mac=<span class="literal">true</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> [[ <span class="variable">$is_mac</span> == <span class="literal">true</span> ]]; <span class="keyword">then</span></div><div class="line">    YEAR=`date -v -1d +%Y`</div><div class="line">    MONTH=`date -v -1d +%m`</div><div class="line">    DAY=`date -v -1d +%d`</div><div class="line"><span class="keyword">else</span></div><div class="line">    YEAR=`date +<span class="string">"%Y"</span> -d <span class="string">"1 day ago"</span>`</div><div class="line">    MONTH=`date +<span class="string">"%m"</span> -d <span class="string">"1 day ago"</span>`</div><div class="line">    DAY=`date +<span class="string">"%d"</span> -d <span class="string">"1 day ago"</span>`</div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="built_in">log</span> <span class="string">"=== Try to process access log of ELB on YEAR=<span class="variable">$YEAR</span>, MONTH=<span class="variable">$MONTH</span>, DAY=<span class="variable">$DAY</span>"</span></div><div class="line"></div><div class="line">DATE_PATH=<span class="string">"<span class="variable">$YEAR</span>/<span class="variable">$MONTH</span>/<span class="variable">$DAY</span>"</span></div><div class="line"></div><div class="line"><span class="built_in">cd</span> <span class="variable">$BASE_WORK_DIR</span></div><div class="line">RESULT_DIR=<span class="variable">$BASE_WORK_DIR</span>/result/</div><div class="line">WORK_DIR=<span class="string">"<span class="variable">$BASE_WORK_DIR</span>/<span class="variable">$DATE_PATH</span>/"</span></div><div class="line"><span class="built_in">log</span> <span class="string">"=== try to create dir <span class="variable">$WORK_DIR</span>"</span></div><div class="line">create_dir <span class="variable">$WORK_DIR</span></div><div class="line"><span class="built_in">log</span> <span class="string">"=== try to create dir <span class="variable">$RESULT_DIR</span>"</span></div><div class="line">create_dir <span class="variable">$RESULT_DIR</span></div><div class="line"><span class="built_in">cd</span> <span class="variable">$WORK_DIR</span></div><div class="line"><span class="built_in">log</span> <span class="string">"=== process work dir is <span class="variable">$WORK_DIR</span>, current dir is `pwd`"</span></div><div class="line"></div><div class="line"><span class="built_in">log</span> <span class="string">"===== Download s3 file start ======="</span></div><div class="line"><span class="comment"># download s3 file to local dir</span></div><div class="line"><span class="built_in">log</span> <span class="string">"aws s3 sync <span class="variable">$S3_BASE_PREFIX</span>/<span class="variable">$DATE_PATH</span>/ ./"</span></div><div class="line">aws s3 sync <span class="variable">$S3_BASE_PREFIX</span>/<span class="variable">$DATE_PATH</span>/ ./</div><div class="line"><span class="built_in">log</span> <span class="string">"===== Download s3 file end ======="</span></div><div class="line"></div><div class="line"><span class="built_in">log</span> <span class="string">"===== grep 5XX start ======="</span></div><div class="line">DATE_LOG_NAME=<span class="string">"<span class="variable">$&#123;YEAR&#125;</span>_<span class="variable">$&#123;MONTH&#125;</span>_<span class="variable">$&#123;DAY&#125;</span>.log"</span></div><div class="line">RESULT_FILE=<span class="string">"<span class="variable">$RESULT_DIR</span>/<span class="variable">$&#123;DATE_LOG_NAME&#125;</span>"</span></div><div class="line">grep <span class="string">"5[0-9]\&#123;2\&#125; 5[0-9]\&#123;2\&#125;"</span> *.<span class="built_in">log</span> &gt; <span class="variable">$RESULT_FILE</span></div><div class="line"><span class="built_in">cd</span> <span class="variable">$BASE_WORK_DIR</span></div><div class="line">rm -r <span class="variable">$WORK_DIR</span>/*.<span class="built_in">log</span></div><div class="line"><span class="built_in">log</span> <span class="string">"===== grep 5XX end ======="</span></div><div class="line"></div><div class="line"><span class="built_in">log</span> <span class="string">"=== File on result_dir[<span class="variable">$&#123;RESULT_DIR&#125;</span>] is <span class="variable">$(ls $&#123;RESULT_DIR&#125;/*$&#123;DATE_LOG_NAME&#125;)</span>"</span></div><div class="line"></div><div class="line"><span class="comment">#############################################</span></div><div class="line"><span class="comment">###### Send Mail #########</span></div><div class="line"><span class="comment">#############################################</span></div><div class="line"></div><div class="line">subject=<span class="string">"Report of Http 5XX in ELB Access log on <span class="variable">$&#123;YEAR&#125;</span>/<span class="variable">$&#123;MONTH&#125;</span>/<span class="variable">$&#123;DAY&#125;</span>"</span></div><div class="line">MAIL_ATTACHED_OPTIONS=<span class="string">""</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> result_file <span class="keyword">in</span> `ls <span class="variable">$&#123;RESULT_DIR&#125;</span>/<span class="variable">$&#123;DATE_LOG_NAME&#125;</span>`</div><div class="line"><span class="keyword">do</span></div><div class="line">    MAIL_ATTACHED_OPTIONS=<span class="string">" <span class="variable">$&#123;MAIL_ATTACHED_OPTIONS&#125;</span> -a <span class="variable">$&#123;result_file&#125;</span>"</span></div><div class="line"><span class="keyword">done</span></div><div class="line"></div><div class="line"><span class="built_in">log</span> <span class="string">"MAIL_ATTACHED_OPTIONS is [<span class="variable">$&#123;MAIL_ATTACHED_OPTIONS&#125;</span>]"</span></div><div class="line"></div><div class="line"><span class="built_in">log</span> <span class="string">"=== send report to email <span class="variable">$MAIL_TO</span>"</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> [[ <span class="variable">$is_mac</span> == <span class="literal">true</span> ]]; <span class="keyword">then</span></div><div class="line">    <span class="built_in">echo</span> -e <span class="string">"Attached is the http 5XX log on <span class="variable">$&#123;YEAR&#125;</span>/<span class="variable">$&#123;MONTH&#125;</span>/<span class="variable">$&#123;DAY&#125;</span>"</span> | mail -s <span class="string">"<span class="variable">$&#123;subject&#125;</span>"</span> <span class="variable">$MAIL_TO</span></div><div class="line"><span class="keyword">else</span></div><div class="line">    <span class="built_in">echo</span> -e <span class="string">"Attached is the http 5XX log on <span class="variable">$&#123;YEAR&#125;</span>/<span class="variable">$&#123;MONTH&#125;</span>/<span class="variable">$&#123;DAY&#125;</span>"</span> | mail -s <span class="string">"<span class="variable">$&#123;subject&#125;</span>"</span> <span class="variable">$MAIL_ATTACHED_OPTIONS</span> <span class="variable">$MAIL_TO</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure>
<h3 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h3><ul>
<li><a href="/2017/12/22/what-is-Access-Logs-of-Classic-Load-Balancer/" title="什么是传统负载均衡器的访问日志">什么是传统负载均衡器的访问日志</a></li>
<li><a href="/2017/12/26/a-simple-elb-access-log-parser-by-ruby/" title="A Simple ELB Access Log Parser By Ruby">A Simple ELB Access Log Parser By Ruby</a>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Shell </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Elastic Load Balancing </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[在Freemind主题中开启图片阴影]]></title>
      <url>/2017/12/25/how-to-add-shadow-of-images-in-freemind/</url>
      <content type="html"><![CDATA[<h3 id="添加图片阴影"><a href="#添加图片阴影" class="headerlink" title="添加图片阴影"></a>添加图片阴影</h3><p><a href="https://github.com/wzpan/hexo-theme-freemind" target="_blank" rel="external">hexo-theme-freemind</a>主题中，默认没有开启图片阴影。当图片的背景和文章的背景是同色的时候，有时会造成混淆，看不清楚哪部分是图片，哪部分是文章。</p>
<p>打开源码看了一下，发现其实<a href="https://github.com/wzpan/hexo-theme-freemind" target="_blank" rel="external">hexo-theme-freemind</a>本身就已经支持图片shadow了。只是默认没有开启，而且也没有在README.md中提及。</p>
<p>在<code>hexo-theme-freemind/layout/_partial/article.ejs</code>文件中有如下代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&lt;% if (item.shadow ) &#123; %&gt;</div><div class="line">   &lt;style type=&quot;text/css&quot;&gt;</div><div class="line">           img, video &#123;</div><div class="line">                -webkit-box-shadow:0 0 10px rgba(0, 0, 0, .5);</div><div class="line">                -moz-box-shadow:0 0 10px rgba(0, 0, 0, .5);</div><div class="line">                box-shadow:0 0 10px rgba(0, 0, 0, .5);</div><div class="line">           &#125;</div><div class="line">   &lt;/style&gt;</div><div class="line">&lt;% &#125; %&gt;</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>因此，只要在每个Post的front-matter中，添加<code>shadow: true</code>,就可以开启图片阴影了, 如果觉得默认的阴影不明显，也可以修改上面的数值，使得阴影看上去更明显一点。</p>
<h3 id="修改post和draft脚手架"><a href="#修改post和draft脚手架" class="headerlink" title="修改post和draft脚手架"></a>修改post和draft脚手架</h3><p>最后添加<code>shadow: true</code>到hexo的<code>scaffolds/post.md</code>和<code>scaffolds/draft.md</code>中，这样在生成post或者draft的时候就可以自动添加上图片阴影了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">title: &#123;&#123; title &#125;&#125;</div><div class="line">date: &#123;&#123; date &#125;&#125;</div><div class="line">categories:</div><div class="line">tags:</div><div class="line">description:</div><div class="line">feature:</div><div class="line">toc: true</div><div class="line">shadow: true</div><div class="line">---</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Blog </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[什么是传统负载均衡器的访问日志]]></title>
      <url>/2017/12/22/what-is-Access-Logs-of-Classic-Load-Balancer/</url>
      <content type="html"><![CDATA[<h3 id="什么是传统负载均衡器的Access-Logs"><a href="#什么是传统负载均衡器的Access-Logs" class="headerlink" title="什么是传统负载均衡器的Access Logs"></a>什么是传统负载均衡器的Access Logs</h3><p>Elastic Load Balancing 提供了Access Logs(访问日志)，Access Logs可记录下发送到负载均衡器的请求的详细信息。每个日志都包含固定格式的信息 (例如，收到请求的时间、客户端的 IP 地址、延迟、请求路径和服务器响应)。可以使用这些日志分析流量模式和进行Debug。</p>
<p>Access Logs是ELB的一项可选功能，默认情况下是Disable的。启用后，ELB会将logs存储到指定的某个S3 Bucket中。</p>
<h3 id="收费"><a href="#收费" class="headerlink" title="收费"></a>收费</h3><p>ELB的Access Logs本身是不需要额外的费用的，从ELB传输到S3的流量是免费的，但是S3的存储费用是需要支付的。<br><a id="more"></a></p>
<h3 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a>可靠性</h3><p>关于可靠性，官网文档中原话是”Elastic Load Balancing 将尽力记录请求。我们建议您使用访问日志来了解请求性质，而不是作为所有请求的完整描述”。</p>
<p>说明Access Logs不保证记录全部的请求，只能是作为流量分析的一个参考数据。</p>
<h3 id="如何设置"><a href="#如何设置" class="headerlink" title="如何设置"></a>如何设置</h3><p>首先需要一个S3 Bucket，并且配置为允许Elastic Load Balancing 的账户ID对Bucket进行写操作。其次配置ELB开启Access Logs并写入设置好的S3 Bucket中</p>
<h4 id="设置S3"><a href="#设置S3" class="headerlink" title="设置S3"></a>设置S3</h4><ol>
<li><p>选择或创建一个S3 Bucket，切换到”Permission”的tab，选择”Bucket Policy”<br><img src="/images/AWS/ELB/elb_access_log_s3_bucket_setting.png" alt="elb_access_log_s3_bucket_setting.png"></p>
</li>
<li><p>添加Bucket Policy，此处测试用的Bucket carl-elb-logs是ap-northease-2(首尔)的Region，因此对应的ELB账户ID应该设置为600734575887</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"Version"</span>: <span class="string">"2012-10-17"</span>,</div><div class="line">    <span class="attr">"Statement"</span>: [</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"Action"</span>: [</div><div class="line">                <span class="string">"s3:PutObject"</span></div><div class="line">            ],</div><div class="line">            <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</div><div class="line">            <span class="attr">"Resource"</span>: <span class="string">"arn:aws:s3:::carl-elb-logs/*"</span>,</div><div class="line">            <span class="attr">"Principal"</span>: &#123;</div><div class="line">                <span class="attr">"AWS"</span>: [</div><div class="line">                    <span class="string">"600734575887"</span></div><div class="line">                ]</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>每个Region的ELB账户ID是不一样的,如果是中国北京区，那就应该是638102146993，每个Region的ELB账户ID如下:<br><img src="/images/AWS/ELB/elb_access_log_elb_account_id_per_region.png" alt="elb_access_log_elb_account_id_per_region.png"></p>
</li>
</ol>
<h4 id="设置ELB-Access-Logs"><a href="#设置ELB-Access-Logs" class="headerlink" title="设置ELB Access Logs"></a>设置ELB Access Logs</h4><ol>
<li><p>在EC2 Console中，在左侧栏中找到Load Balancer, 然后选中要设置的ELB名字，此处为ELB-carl,切换到Description Tab。<br><img src="/images/AWS/ELB/elb_access_log_description_tab.png" alt="elb_access_log_description_tab.png"></p>
</li>
<li><p>在Attributes的Access logs下，点击Configure Access Logs<br><img src="/images/AWS/ELB/elb_access_log_elb_attributes_access_logs.png" alt="elb_access_log_elb_attributes_access_logs.png"></p>
</li>
<li><p>在弹出的Configure Access Logs页面中</p>
<ul>
<li>选中Enable access logs</li>
<li>Interval可选60 minutes或5 minutes<ul>
<li>一般生产环境中选择60 minutes，减少S3中Access log文件的数量。</li>
<li>测试的时候选择5 minutes, 可以更快的查看到最新的access日志。</li>
</ul>
</li>
<li>S3 location中选择需要保存的S3 Bucket和prefix路径，此处因为是测试，就直接保存在S3 carl-elb-logs的根目录下。</li>
<li>如果prefix还没有创建，可以选择”Create this location for me”来自动让ELB创建对应的prefix<br><img src="/images/AWS/ELB/elb_access_log_elb_configure_access_logs.png" alt="elb_access_log_elb_configure_access_logs.png"></li>
</ul>
</li>
<li><p>如果设置正确，就可以在步骤3中设置的S3 Bucket和prefix下看到一个名为AWSLogs/account_id/ELBAccessLogTestFile的测试文件，如果有这个文件，说明ELB和S3的设置正确了。<br><img src="/images/AWS/ELB/elb_access_log_s3_test_file.png" alt="elb_access_log_s3_test_file.png"></p>
</li>
<li><p>请求ELB的DNS，5分钟后，就可以在S3中看到access log了。</p>
</li>
<li><p>ELB的access log在S3中的文件名采用以下的格式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bucket[/prefix]/AWSLogs/aws-account-id/elasticloadbalancing/region/yyyy/mm/dd/aws-account-id_elasticloadbalancing_region_load-balancer-name_end-time_ip-address_random-string.log</div></pre></td></tr></table></figure>
</li>
<li><p>access logs的文件内容采用如下的格式,包含了用户访问的数据和服务端响应的数据，可以根据某些字段来制作自己想要的数据报告。比如可以统计各个链接的访问数量，每个时间段内的访问数，统计各个HTTP Code的相应百分比，筛选5XX错误等</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">timestamp elb client:port backend:port request_processing_time backend_processing_time response_processing_time elb_status_code backend_status_code received_bytes sent_bytes &quot;request&quot; &quot;user_agent&quot; ssl_cipher ssl_protocol</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="access-log日志格式解析"><a href="#access-log日志格式解析" class="headerlink" title="access log日志格式解析"></a>access log日志格式解析</h3><p>Access log有TCP和HTTP，此处详细说明一下HTTP类别的日志格式</p>
<p>某条日志内容如下:</p>
<p><strong>2017-12-24T09:13:07.059734Z ELB-carl 117.81.114.77:7169 172.31.13.115:80 0.000045 0.01808 0.000028 200 200 0 71015 “GET <a href="http://elb-carl-861166859.ap-northeast-2.elb.amazonaws.com:80/test.php" target="_blank" rel="external">http://elb-carl-861166859.ap-northeast-2.elb.amazonaws.com:80/test.php</a> HTTP/1.10” “Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36” - -</strong></p>
<p>按照<a href="http://docs.aws.amazon.com/zh_cn/elasticloadbalancing/latest/classic/access-log-collection.html" target="_blank" rel="external">官网</a>中文件格式的解析说明，每个字段分别的意思如下:</p>
<ul>
<li>2017-12-24T09:13:07.059734Z 代表的是timestamp</li>
<li>ELB-carl 代表的是elb，就是elb的名字</li>
<li>117.81.114.77代表的是client，就是客户端IP</li>
<li>7169代表的是client的port，就是客户端源端口</li>
<li>172.31.13.115代表的是backend，就是后端EC2的内网IP</li>
<li>80代表的是backend的port，就是后端EC2的端口</li>
<li>0.000045代表的是request_processing_time， 代表的是从ELB收到请求一直到将请求发送到后端EC2所用的总时间</li>
<li>0.01808代表的是backend_processing_time，代表的是从ELB将请求发送到后端EC2到该EC2开始发送响应标头所用的总时间</li>
<li>0.000028代表的是response_processing_time， 代表的是从ELB收到来自后端EC2的响应标头到开始向client发送响应所用的总时间。此时间包括在ELB上的排队时间以及从ELB到client的连接获取时间</li>
<li>第一个200代表的是elb_status_code, 表示来自ELB的响应的状态代码</li>
<li>第二个200代表的是backend_status_code， 表示来自后端EC2的响应的状态代码</li>
<li>0 代表的是received_bytes，表示从client (申请方) 接收的请求大小 (以字节为单位), 对于HTTP来说，包含请求正文，但不包括Header</li>
<li>71015 代表的是sent_bytes， 表示发送到client (申请方) 的响应的大小 (以字节为单位), 对于HTTP来说，包含响应正文，但不包括Header。</li>
<li>“GET <a href="http://elb-carl-861166859.ap-northeast-2.elb.amazonaws.com:80/test.php" target="_blank" rel="external">http://elb-carl-861166859.ap-northeast-2.elb.amazonaws.com:80/test.php</a> HTTP/1.10”代表的是request，表示来自client的请求</li>
<li>“Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36”代表的是user_agent,就是浏览器的agent信息。</li>
<li>第一个“-”代表的是ssl_cipher，是ssl的cipher，如果是HTTP请求，就是“-”，如果是HTTPS的，那可能就是ECDHE-RSA-AES128-SHA</li>
<li>第二个“-”代表的是ssl_protocol，是ssl的协议。如果是HTTP请求，就是“-”，如果是HTTPS的，那可能是TLSv1或者TLSv1.2等。</li>
</ul>
<h3 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h3><ul>
<li><a href="/2017/12/25/shell-gist-to-get-http-5XX-code-from-ELB-access-log/" title="shell gist to get http 5XX code from ELB access log">shell gist to get http 5XX code from ELB access log</a></li>
<li><a href="/2017/12/26/a-simple-elb-access-log-parser-by-ruby/" title="A Simple ELB Access Log Parser By Ruby">A Simple ELB Access Log Parser By Ruby</a>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html" target="_blank" rel="external">Access Logs for Your Classic Load Balancer</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Elastic Load Balancing </tag>
            
            <tag> S3 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何在Elastic Beanstalk的Rails环境中配置staging.log为可下载]]></title>
      <url>/2017/12/20/How-to-add-staging-log-to-EB-logs/</url>
      <content type="html"><![CDATA[<p>新建了一个EB环境，来跑一个staging模式的rails。EB环境变量设置好后，部署成功。一系列测试都没问题，结果却在一个不起眼的地方出了点幺蛾子。</p>
<p>Rails代码中，没有特殊配置config.logger,因此staging模式时，默认的输出日志为logs/staging.log, 但是在EB的Web console中尝试获取Full Logs后，在BundleLogs中发现没有staging.log。</p>
<p>赶紧调查试验了一下，并将调查结果记录如下。</p>
<a id="more"></a>
<h4 id="调查EB-logs日志"><a href="#调查EB-logs日志" class="headerlink" title="调查EB logs日志"></a>调查EB logs日志</h4><ol>
<li>登陆EB中的EC2 Instance，试图寻找到为何staging.log没有在BundleLogs中。</li>
<li><p>切换到BundleLogs打包下载的源目录/var/app/support/logs/, 发现只有development.log和production.log的软连接, 而没有staging.log</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[ec2-user@ip-172-31-14-22 logs]$ ll</div><div class="line">总用量 8</div><div class="line">-rw-r--r-- 1 root   root      0 12月 20 10:01 access.log</div><div class="line">lrwxrwxrwx 1 root   root     36 12月 20 06:21 development.log -&gt; /var/app/current/<span class="built_in">log</span>/development.log</div><div class="line">drwxr-xr-x 2 webapp webapp 4096 12月 20 10:01 healthd</div><div class="line">-rw-r--r-- 1 webapp webapp    0 12月 20 10:01 passenger.log</div><div class="line">lrwxrwxrwx 1 root   root     35 12月 20 06:21 production.log -&gt; /var/app/current/<span class="built_in">log</span>/production.log</div><div class="line">drwxr-xr-x 2 root   root   4096 12月 20 10:01 rotated</div><div class="line">[ec2-user@ip-172-31-14-22 logs]$</div></pre></td></tr></table></figure>
</li>
<li><p>在/opt/elasticbeanstalk/目录下遍寻生成development.log和production.log的脚本。最终寻得是在/opt/elasticbeanstalk/hooks/preinit/24_rails_support.sh中设置了development.log和production.log的软连接。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">[ec2-user@ip-172-31-14-22 elasticbeanstalk]$ cat hooks/preinit/24_rails_support.sh</div><div class="line"><span class="meta">#!/usr/bin/env bash</span></div><div class="line"><span class="comment">#==============================================================================</span></div><div class="line"><span class="comment"># Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Licensed under the Amazon Software License (the "License"). You may not use</span></div><div class="line"><span class="comment"># this file except in compliance with the License. A copy of the License is</span></div><div class="line"><span class="comment"># located at</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment">#       https://aws.amazon.com/asl/</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># or in the "license" file accompanying this file. This file is distributed on</span></div><div class="line"><span class="comment"># an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, express or</span></div><div class="line"><span class="comment"># implied. See the License for the specific language governing permissions</span></div><div class="line"><span class="comment"># and limitations under the License.</span></div><div class="line"><span class="comment">#==============================================================================</span></div><div class="line"></div><div class="line"><span class="built_in">set</span> -xe</div><div class="line"></div><div class="line">EB_APP_DEPLOY_DIR=$(/opt/elasticbeanstalk/bin/get-config container -k app_deploy_dir)</div><div class="line">EB_APP_LOG_DIR=$(/opt/elasticbeanstalk/bin/get-config container -k app_log_dir)</div><div class="line">EB_APP_USER=$(/opt/elasticbeanstalk/bin/get-config container -k app_user)</div><div class="line"></div><div class="line"><span class="comment"># For builtin Rails logging support</span></div><div class="line">ln -sf <span class="variable">$EB_APP_DEPLOY_DIR</span>/<span class="built_in">log</span>/production.log <span class="variable">$EB_APP_LOG_DIR</span>/production.log</div><div class="line">ln -sf <span class="variable">$EB_APP_DEPLOY_DIR</span>/<span class="built_in">log</span>/development.log <span class="variable">$EB_APP_LOG_DIR</span>/development.log</div><div class="line">[ec2-user@ip-172-31-14-22 elasticbeanstalk]$</div></pre></td></tr></table></figure>
</li>
<li><p>既然EB默认没有建立staging.log的软连接，那就自己动手建一个。在代码根目录的.ebextensions下添加名为10-add-staging-log-link.config的文件，写入如下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">commands:</div><div class="line"></div><div class="line">  01_link_staging_log:</div><div class="line">    command: &apos;EB_APP_DEPLOY_DIR=$(/opt/elasticbeanstalk/bin/get-config container -k app_deploy_dir); EB_APP_LOG_DIR=$(/opt/elasticbeanstalk/bin/get-config container -k app_log_dir);  sudo ln -sf $EB_APP_DEPLOY_DIR/log/staging.log $EB_APP_LOG_DIR/staging.log&apos;</div></pre></td></tr></table></figure>
</li>
<li><p>重新发布代码到EB, 发布完成后，可以看到在目录目录/var/app/support/logs/下，staging.log已经建立了对应的软连接了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[ec2-user@ip-172-31-47-16 logs]$ ll</div><div class="line">总用量 16</div><div class="line">-rw-r--r-- 1 root   root    150 12月 21 06:32 access.log</div><div class="line">lrwxrwxrwx 1 root   root     36 12月 21 06:16 development.log -&gt; /var/app/current/<span class="built_in">log</span>/development.log</div><div class="line">drwxr-xr-x 2 webapp webapp 4096 12月 21 06:32 healthd</div><div class="line">-rw-r--r-- 1 webapp webapp 3499 12月 21 06:32 passenger.log</div><div class="line">lrwxrwxrwx 1 root   root     35 12月 21 06:16 production.log -&gt; /var/app/current/<span class="built_in">log</span>/production.log</div><div class="line">drwxr-xr-x 2 root   root   4096 12月 21 06:16 rotated</div><div class="line">lrwxrwxrwx 1 root   root     32 12月 21 06:17 staging.log -&gt; /var/app/current/<span class="built_in">log</span>/staging.log</div><div class="line">[ec2-user@ip-172-31-47-16 logs]$</div></pre></td></tr></table></figure>
</li>
<li><p>在EB的Web Console中点击Full Logs下载BundleLogs,解压后可以看到/var/app/support/logs/下有staging.log了，rotated目录下也有rotate的staging的log</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">[carlshen@carl var]$ tree</div><div class="line">.</div><div class="line">├── app</div><div class="line">│   └── support</div><div class="line">│       └── logs</div><div class="line">│           ├── access.log</div><div class="line">│           ├── passenger.log</div><div class="line">│           ├── rotated</div><div class="line">│           │   ├── access.log1513753261.gz</div><div class="line">│           │   ├── access.log1513756861.gz</div><div class="line">│           │   ├── access.log1513760461.gz</div><div class="line">│           │   ├── access.log1513764061.gz</div><div class="line">│           │   ├── passenger.log1513753261.gz</div><div class="line">│           │   ├── passenger.log1513764061.gz</div><div class="line">│           │   ├── staging.log1513753261.gz</div><div class="line">│           │   ├── staging.log1513756861.gz</div><div class="line">│           │   ├── staging.log1513760461.gz</div><div class="line">│           │   └── staging.log1513764061.gz</div><div class="line">│           └── staging.log</div><div class="line">└── log</div><div class="line">    ├── cfn-hup.log</div><div class="line">    ├── cfn-init-cmd.log</div><div class="line">    ├── cfn-init.log</div><div class="line">    ├── cloud-init-output.log</div><div class="line">    ├── cloud-init.log</div><div class="line">    ├── cron</div><div class="line">    ├── eb-activity.log</div><div class="line">    ├── eb-cfn-init-call.log</div><div class="line">    ├── eb-cfn-init.log</div><div class="line">    ├── eb-commandprocessor.log</div><div class="line">    ├── eb-publish-logs.log</div><div class="line">    ├── eb-tools.log</div><div class="line">    ├── healthd</div><div class="line">    │   └── daemon.log</div><div class="line">    ├── messages</div><div class="line">    └── yum.log</div><div class="line"></div><div class="line">6 directories, 28 files</div><div class="line">[carlshen@carl var]$</div></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Elastic Beanstalk </tag>
            
            <tag> Ruby </tag>
            
            <tag> Rails </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[S3 RRS价格注意点]]></title>
      <url>/2017/12/20/expired-of-S3-RRS/</url>
      <content type="html"><![CDATA[<h3 id="S3-存储类型"><a href="#S3-存储类型" class="headerlink" title="S3 存储类型"></a>S3 存储类型</h3><p>S3 总共提供三种存储类型</p>
<ul>
<li>标准(Standard)</li>
<li>标准-低频率访问(Standard - Infrequent Access)</li>
<li>低冗余存储(Reduced Redundancy Storage)<a id="more"></a>
</li>
</ul>
<h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p>S3 RSS只提供99.99%的持久性，并且只能容灾一个存储设备出错。而Standard的持久性是11个9,并且允许坏两个存储设备还是正常工作。<br><img src="/images/AWS/S3/s3_difference_between_storage_type.png" alt="s3_difference_between_storage_type"></p>
<h3 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h3><p>三者之间的基本用途:</p>
<ul>
<li>标准类型是通用的类型，是S3的默认存储类型。</li>
<li>标准-低频率访问的存储费用比标准类型的要便宜不少,但是请求S3 Object的请求价格要比标准的高不少。根据2017-12-20全球区的价格，标准-低频率访问类型的存储价格是标准类型的存储价格的一半多一点，但是请求价格比标准类型的要贵上一倍。</li>
<li>低冗余存储的设计初衷是用来存储一些可以再生成的对象。比如可以通过原始图片生成的各种尺寸的缩略图等。RRS的存储费用比Standard的要便宜，但是降低了持久性(只提供99.99%的持久性)。</li>
</ul>
<h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><p>在初始设计中，S3 RRS的价格是低于S3 Standard的，但在2016-12-01全球区的S3 Standard类型降价后，在<a href="https://aws.amazon.com/cn/s3/pricing/" target="_blank" rel="external">S3官网</a>上可以查看到，一些Region中,S3 Standard的价格比RRS的价格反而要便宜,比如N.Viginia, S3 Standard价格为$0.023/GB, S3 RRS的价格反而是$0.0240/GB, 出现了倒挂的情况。 在一些老的文章中，还会按照以前的价格体系来推荐使用RRS类型来节省费用，但在新的价格体系下，在很多Region中，这种Best Practise已经不成立了, 直接单纯使用S3 Standard反而会更省钱。</p>
<p>出现这种价格上的倒挂，个人猜测AWS在用价格的手段在事实上渐渐地废弃S3 RRS。而且在<a href="https://amazonaws-china.com/s3/storage-classes/" target="_blank" rel="external">最新的S3存储类型的页面</a>上，已经没有RRS相关的介绍了。可能S3的价格已经降的足够便宜了，而且维护的存储类型越多,增加S3新Feature的时候复杂度就越高, 所以AWS在打算废弃S3 RRS了。</p>
<p>不过在AWS北京区中, S3 RRS的价格还是比S3 Standard便宜不少的, 宁夏区就便宜的不明显了。<br><img src="/images/AWS/S3/s3_price_in_cn.png" alt="s3_price_in_cn"></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="AWS Storage Update – S3 &amp; Glacier Price Reductions + Additional Retrieval Options for Glacier">AWS Storage Update – S3 &amp; Glacier Price Reductions + Additional Retrieval Options for Glacier</a></li>
<li><a href="https://aws.amazon.com/cn/s3/pricing/" target="_blank" rel="external">Amazon S3 定价</a></li>
<li><a href="https://aws.amazon.com/cn/s3/reduced-redundancy/?nc1=h_ls" target="_blank" rel="external">Amazon S3 低冗余存储</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> S3 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深挖AWS S3的权限管理(实验篇)]]></title>
      <url>/2017/12/19/deep-dig-on-s3-bucket-permission-exam/</url>
      <content type="html"><![CDATA[<p>在<a href="/2017/12/14/deep-dig-on-s3-bucket-permission/" title="深挖AWS S3的权限管理">深挖AWS S3的权限管理</a> 一文中介绍了AWS S3权限管理的三种方法。此处再通过几个实验来看下三种方法的使用方法。</p>
<hr>
<h3 id="实验前准备"><a href="#实验前准备" class="headerlink" title="实验前准备"></a>实验前准备</h3><h4 id="实验用相关账号介绍"><a href="#实验用相关账号介绍" class="headerlink" title="实验用相关账号介绍"></a>实验用相关账号介绍</h4><ul>
<li>IAM User carl.shen - carl.shen有AdministratorAccess权限，用来上传图片到S3</li>
<li>IAM User carl - 测试IAM User一号，初始没有任何权限</li>
<li>IAM User shen - 测试IAM User二号，初始没有任何权限</li>
<li>IAM Group S3Exam - 用来给IAM User carl和shen统一赋权限</li>
</ul>
<p>建好三个IAM账号，下载各自的Access key ID和Secret Access Key, 并设置好AWS CLI。AWS CLI多profile的设置方法可以参照 <a href="/2017/10/24/how-to-use-aws-cli-with-multi-user/" title="如何在aws cli中使用多个配置文件">如何在aws cli中使用多个配置文件</a><br><a id="more"></a></p>
<p>建立好IAM Group S3Exam，将carl和shen添加进S3Exam中。</p>
<p><img src="/images/AWS/S3/s3_permission_exam_create_IAM.png" alt="s3_permission_exam_create_IAM"></p>
<h4 id="实验用Bucket介绍"><a href="#实验用Bucket介绍" class="headerlink" title="实验用Bucket介绍"></a>实验用Bucket介绍</h4><ul>
<li>carl-test-at-seoul - 测试Bucket 1，初始没有任何Bucket Policy</li>
<li>carl-test-at-seoul-2 - 测试Bucket 2，初始没有任何Bucket Policy</li>
</ul>
<h4 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h4><ul>
<li>AWS CLI</li>
<li>Chrome浏览器 - 用来匿名访问S3 Object的url</li>
</ul>
<h4 id="测试图片"><a href="#测试图片" class="headerlink" title="测试图片"></a>测试图片</h4><p>两张测试图片:<br><img src="/images/AWS/S3/fox.jpg" alt="fox"><br><img src="/images/AWS/S3/steve.jpeg" alt="steve"></p>
<h4 id="实验项目"><a href="#实验项目" class="headerlink" title="实验项目"></a>实验项目</h4><ol>
<li>测试Default Deny</li>
<li>测试Object ACL public-read</li>
<li>测试Bucket Policy Explicit Allow override Default Deny</li>
<li>测试Bucket Policy Explicit Deny override Explicit Allow</li>
<li>测试Bucket Policy 禁止某个IAM访问</li>
<li>测试IAM Policy</li>
<li>测试Bucket Policy不能跨Bucket授权</li>
<li>测试IAM Policy授权多个Bucket的操作</li>
</ol>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="实验1-–-测试Default-Deny"><a href="#实验1-–-测试Default-Deny" class="headerlink" title="实验1 – 测试Default Deny"></a>实验1 – 测试Default Deny</h4><p>步骤: Bucket carl-test-at-seoul没有Bucket Policy，carl.shen cli传图片进S3，测试chrome和IAM访问</p>
<ol>
<li><p>使用AWS CLI用carl.shen账号上传图片</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ aws --profile carlshen s3 cp photo_used_for_test/fox.jpg s3://carl-test-at-seoul/</div><div class="line">upload: photo_used_for_test/fox.jpg to s3://carl-test-at-seoul/fox.jpg</div><div class="line">$ aws --profile carlshen s3 ls s3://carl-test-at-seoul/</div><div class="line">2017-12-18 14:49:30      11432 fox.jpg</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
<li><p>使用Chrome访问fox.jpg的public url, 会提示AccessDenied, 因为上传到S3的fox.jpg， 默认是access deny的。<br><img src="/images/AWS/S3/s3_permission_exam_access_deny_by_default.png" alt="s3_permission_exam_access_deny_by_default"></p>
</li>
<li><p>同样，IAM User carl和shen也都没有权限获取fox.jpg</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># IAM User carl没有访问fox.jpg的权限</div><div class="line">$ aws --profile carl s3 cp s3://carl-test-at-seoul/fox.jpg ./download_from_s3/</div><div class="line">fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden</div><div class="line"></div><div class="line"># IAM User shen没有访问fox.jpg的权限</div><div class="line">$ aws --profile shen s3 cp s3://carl-test-at-seoul/fox.jpg ./download_from_s3/</div><div class="line">fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="实验2-–-测试Object-ACL-public-read"><a href="#实验2-–-测试Object-ACL-public-read" class="headerlink" title="实验2 – 测试Object ACL public-read"></a>实验2 – 测试Object ACL public-read</h4><p>步骤: Bucket carl-test-at-seoul没有Bucket Policy，carl.shen cli传图片进S3, 设置public-read acl，测试chrome和IAM访问</p>
<ol>
<li><p>使用AWS CLI以carl.shen账号上传图片steve.jpeg，并设置图片的Object ACL为public-read</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ aws --profile carlshen s3 cp photo_used_for_test/steve.jpeg s3://carl-test-at-seoul/ --acl public-read</div><div class="line">upload: photo_used_for_test/steve.jpeg to s3://carl-test-at-seoul/steve.jpeg</div><div class="line">$ aws --profile carlshen s3 ls s3://carl-test-at-seoul/</div><div class="line">2017-12-18 14:49:30      11432 fox.jpg</div><div class="line">2017-12-18 15:02:02       9619 steve.jpeg</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
<li><p>使用Chrome访问steve.jpeg的public url, steve.jpeg可以被Chrome访问到。因为此时steve.jpeg的Object ACL是public-read，而explicit allow will override the default deny<br><img src="/images/AWS/S3/s3_permission_exam_access_allow_by_acl_public_read.png" alt="s3_permission_exam_access_allow_by_acl_public_read"></p>
</li>
<li><p>使用IAM User carl和shen也都可以获取到steve.jpeg</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># IAM User carl有权限访问steve.jpeg</div><div class="line">$ aws --profile carl s3 cp s3://carl-test-at-seoul/steve.jpeg ./download_from_s3/</div><div class="line">download: s3://carl-test-at-seoul/steve.jpeg to download_from_s3/steve.jpeg</div><div class="line"></div><div class="line"># IAM User shen有权限访问steve.jpeg</div><div class="line">$ aws --profile shen s3 cp s3://carl-test-at-seoul/steve.jpeg ./download_from_s3/</div><div class="line">download: s3://carl-test-at-seoul/steve.jpeg to download_from_s3/steve.jpeg</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="实验3-–-测试Bucket-Policy-Explicit-Allow-override-Default-Deny"><a href="#实验3-–-测试Bucket-Policy-Explicit-Allow-override-Default-Deny" class="headerlink" title="实验3 – 测试Bucket Policy Explicit Allow override Default Deny"></a>实验3 – 测试Bucket Policy Explicit Allow override Default Deny</h4><p>步骤: 设置Bucket carl-test-at-seoul的Policy, 设为public to read, 测试chrome和IAM访问</p>
<ol>
<li><p>设置Bucket carl-test-at-seoul的Policy，使得carl-test-at-seoul都public to read.</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"Version"</span>: <span class="string">"2012-10-17"</span>,</div><div class="line">    <span class="attr">"Id"</span>: <span class="string">"Policy1512465896017"</span>,</div><div class="line">    <span class="attr">"Statement"</span>: [</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"Sid"</span>: <span class="string">"Stmt1512465891857"</span>,</div><div class="line">            <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</div><div class="line">            <span class="attr">"Principal"</span>: <span class="string">"*"</span>,</div><div class="line">            <span class="attr">"Action"</span>: <span class="string">"s3:GetObject"</span>,</div><div class="line">            <span class="attr">"Resource"</span>: <span class="string">"arn:aws:s3:::carl-test-at-seoul/*"</span></div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>使用Chrome访问原先没权限的访问的fox.jpg的url, fox.jpg可以被访问到，因为Bucket Policy的explict allow会覆盖default deny.<br><img src="/images/AWS/S3/s3_permission_exam_access_allow_by_bucket_policy.png" alt="s3_permission_exam_access_allow_by_bucket_policy"></p>
</li>
<li><p>此时，carl和shen也可以访问到fox.jpg</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># IAM User carl有权限访问fox.jpg</div><div class="line">$ aws --profile carl s3 cp s3://carl-test-at-seoul/fox.jpg ./download_from_s3/</div><div class="line">download: s3://carl-test-at-seoul/fox.jpg to download_from_s3/fox.jpg</div><div class="line"></div><div class="line"># IAM User shen有权限访问fox.jpg</div><div class="line">$ aws --profile shen s3 cp s3://carl-test-at-seoul/fox.jpg ./download_from_s3/</div><div class="line">download: s3://carl-test-at-seoul/fox.jpg to download_from_s3/fox.jpg</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="实验4-–-测试Bucket-Policy-Explicit-Deny-override-Explicit-Allow"><a href="#实验4-–-测试Bucket-Policy-Explicit-Deny-override-Explicit-Allow" class="headerlink" title="实验4 –  测试Bucket Policy Explicit Deny override Explicit Allow"></a>实验4 –  测试Bucket Policy Explicit Deny override Explicit Allow</h4><p>步骤: Bucket carl-test-at-seoul设置Policy to deny，再测试chrome和IAM访问</p>
<ol>
<li><p>修改Bucket carl-test-at-seoul的Policy, Deny s3:GetObject的访问。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</div><div class="line">    &quot;Id&quot;: &quot;Policy1512465896017&quot;,</div><div class="line">    &quot;Statement&quot;: [</div><div class="line">        &#123;</div><div class="line">            &quot;Sid&quot;: &quot;Stmt1512465891857&quot;,</div><div class="line">            &quot;Effect&quot;: &quot;Deny&quot;,</div><div class="line">            &quot;Principal&quot;: &quot;*&quot;,</div><div class="line">            &quot;Action&quot;: &quot;s3:GetObject&quot;,</div><div class="line">            &quot;Resource&quot;: &quot;arn:aws:s3:::carl-test-at-seoul/*&quot;</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>使用Chrome访问fox.jpg和steve.jpeg, fox.jpg和steve.jpeg都无法被访问。即使steve.jpeg是ACL public-read的，但因为Bucket Policy中显示声明了deny，而exclipit deny会override explicity allow, 所以steve.jpeg也被禁止访问。<br><img src="/images/AWS/S3/s3_permission_exam_access_deny_by_exclipit_deny_fox.png" alt="s3_permission_exam_access_deny_by_exclipit_deny_fox"><br><img src="/images/AWS/S3/s3_permission_exam_access_deny_by_exclipit_deny_steve.png" alt="s3_permission_exam_access_deny_by_exclipit_deny_steve"></p>
</li>
<li><p>IAM carl和shen也无法访问到fox.jpg和steve.jpeg</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># IAM User carl没权限访问fox.jpg和steve.jpeg</span></div><div class="line">$ aws --profile carl s3 cp s3://carl-test-at-seoul/fox.jpg ./download_from_s3/</div><div class="line">fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden</div><div class="line">$ aws --profile carl s3 cp s3://carl-test-at-seoul/steve.jpeg ./download_from_s3/</div><div class="line">fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden</div><div class="line"></div><div class="line"><span class="comment"># IAM User shen没权限访问fox.jpg和steve.jpeg</span></div><div class="line">$ aws --profile shen s3 cp s3://carl-test-at-seoul/fox.jpg ./download_from_s3/</div><div class="line">fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden</div><div class="line">$ aws --profile shen s3 cp s3://carl-test-at-seoul/steve.jpeg ./download_from_s3/</div><div class="line">fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="实验5-–-测试Bucket-Policy-禁止某个IAM访问"><a href="#实验5-–-测试Bucket-Policy-禁止某个IAM访问" class="headerlink" title="实验5 – 测试Bucket Policy 禁止某个IAM访问"></a>实验5 – 测试Bucket Policy 禁止某个IAM访问</h4><p>步骤: 修改Bucket Policy为禁止carl访问，此时chrome和shen还是可以访问的。</p>
<ol>
<li><p>修改Bucket carl-test-at-seoul的Policy, 禁止IAM User carl的s3:GetObject的操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</div><div class="line">    &quot;Id&quot;: &quot;Policy1512465896017&quot;,</div><div class="line">    &quot;Statement&quot;: [</div><div class="line">        &#123;</div><div class="line">            &quot;Sid&quot;: &quot;Stmt1512465891857&quot;,</div><div class="line">            &quot;Effect&quot;: &quot;Deny&quot;,</div><div class="line">            &quot;Principal&quot;: &#123;</div><div class="line">                &quot;AWS&quot;: &quot;arn:aws:iam::XXXXXXXXXXXX:user/carl&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;Action&quot;: &quot;s3:GetObject&quot;,</div><div class="line">            &quot;Resource&quot;: &quot;arn:aws:s3:::carl-test-at-seoul/*&quot;</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>使用Chrome访问fox.jpg和steve.jpeg。Buckt Policy被修改为禁止IAM carl访问Object，但该Policy的实施对象是IAM User carl，所以对Chrome的匿名访问不起作用，因此从Chrome上访问时，带有public-read的steve.jpeg可以访问到，没有public-read ACL的fox.jpg还是无法访问到。<br><img src="/images/AWS/S3/s3_permission_exam_access_allow_by_acl_public_read_without_deny.png" alt="s3_permission_exam_access_allow_by_acl_public_read_without_deny"><br><img src="/images/AWS/S3/s3_permission_exam_access_deny_by_default_without_exclipit_allow.png" alt="s3_permission_exam_access_deny_by_default_without_exclipit_allow"></p>
</li>
<li><p>IAM User carl被Bucket Policy禁止访问fox.jpg和steve.jpeg, 而IAM User shen不受Bucket Policy影响，从而可以访问steve.jpeg,但因为没有explicit allow的规则而无法访问fox.jpg</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># IAM User carl被禁止get carl-test-at-seoul的Object</div><div class="line">$ aws --profile carl s3 cp s3://carl-test-at-seoul/fox.jpg ./download_from_s3/</div><div class="line">fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden</div><div class="line">$ aws --profile carl s3 cp s3://carl-test-at-seoul/steve.jpeg ./download_from_s3/</div><div class="line">fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden</div><div class="line"></div><div class="line"># IAM User carl不能访问fox.jpg,但能访问steve.jpeg</div><div class="line">$ aws --profile shen s3 cp s3://carl-test-at-seoul/fox.jpg ./download_from_s3/</div><div class="line">fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden</div><div class="line">$ aws --profile shen s3 cp s3://carl-test-at-seoul/steve.jpeg ./download_from_s3/</div><div class="line">download: s3://carl-test-at-seoul/steve.jpeg to download_from_s3/steve.jpeg</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="实验6-–-测试IAM-Policy"><a href="#实验6-–-测试IAM-Policy" class="headerlink" title="实验6 – 测试IAM Policy"></a>实验6 – 测试IAM Policy</h4><p>步骤: 修改IAM Group Policy为允许访问carl-test-at-seoul的全部资源，但因为Bucket Policy中是禁止carl访问资源的，因此carl还是无法访问bucket中的内容</p>
<ol>
<li><p>将carl和shen所在的Group S3Exam的Policy设为如下,即允许任意S3操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</div><div class="line">    &quot;Statement&quot;: [</div><div class="line">        &#123;</div><div class="line">            &quot;Effect&quot;: &quot;Allow&quot;,</div><div class="line">            &quot;Action&quot;: &quot;s3:*&quot;,</div><div class="line">            &quot;Resource&quot;: [</div><div class="line">                &quot;arn:aws:s3:::carl-test-at-seoul&quot;,</div><div class="line">                &quot;arn:aws:s3:::carl-test-at-seoul/*&quot;</div><div class="line">            ]</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>修改S3Exam的Policy后，carl和shen自动就继承了S3Exam的权限</p>
</li>
<li><p>shen可以访问carl-test-at-seoul的全部资源了，包括没有public-read ACL的fox.jpg。carl因为Bucket Policy中还是禁止他访问的，因此还是没有权限获取carl-test-at-seoul中的任意Object</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># IAM User carl还是没有访问carl-test-at-seoul的权限</div><div class="line">$ aws --profile carl s3 cp s3://carl-test-at-seoul/fox.jpg ./download_from_s3/</div><div class="line">fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden</div><div class="line">$ aws --profile carl s3 cp s3://carl-test-at-seoul/steve.jpeg ./download_from_s3/</div><div class="line">fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden</div><div class="line"></div><div class="line"># IAM User shen获取了carl-test-at-seoul的全部权限，因此可以访问fox.jpg</div><div class="line">$ aws --profile shen s3 cp s3://carl-test-at-seoul/fox.jpg ./download_from_s3/</div><div class="line">download: s3://carl-test-at-seoul/fox.jpg to download_from_s3/fox.jpg</div><div class="line">$ aws --profile shen s3 cp s3://carl-test-at-seoul/steve.jpeg ./download_from_s3/</div><div class="line">download: s3://carl-test-at-seoul/steve.jpeg to download_from_s3/steve.jpeg</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="实验7-–-测试Bucket-Policy不能跨Bucket授权"><a href="#实验7-–-测试Bucket-Policy不能跨Bucket授权" class="headerlink" title="实验7 – 测试Bucket Policy不能跨Bucket授权"></a>实验7 – 测试Bucket Policy不能跨Bucket授权</h4><p>步骤: Bucket carl-test-at-seoul添加对Bucket carl-test-at-seoul-2的访问，会报错</p>
<ol>
<li><p>修改Bucket carl-test-at-seoul的Policy为如下:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="string">"Version"</span>: <span class="string">"2012-10-17"</span>,</div><div class="line">    <span class="string">"Id"</span>: <span class="string">"Policy1512465896017"</span>,</div><div class="line">    <span class="string">"Statement"</span>: [</div><div class="line">        &#123;</div><div class="line">            <span class="string">"Sid"</span>: <span class="string">"Stmt1512465891857"</span>,</div><div class="line">            <span class="string">"Effect"</span>: <span class="string">"Deny"</span>,</div><div class="line">            <span class="string">"Principal"</span>: <span class="string">"*"</span>,</div><div class="line">            <span class="string">"Action"</span>: <span class="string">"s3:GetObject"</span>,</div><div class="line">            <span class="string">"Resource"</span>: <span class="string">"arn:aws:s3:::carl-test-at-seoul-2/*"</span></div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>会提示”Error Policy has invalid resource”, 因为无法在当前Bucket的Policy中添加对其他Bucket的访问权限。<br><img src="/images/AWS/S3/s3_permission_exam_cannot_create_policy_to_other_bucket.png" alt="s3_permission_exam_cannot_create_policy_to_other_bucket"></p>
</li>
</ol>
<h4 id="实验8-–-测试IAM-Policy授权多个Bucket的操作"><a href="#实验8-–-测试IAM-Policy授权多个Bucket的操作" class="headerlink" title="实验8 –  测试IAM Policy授权多个Bucket的操作"></a>实验8 –  测试IAM Policy授权多个Bucket的操作</h4><p>步骤: Group Policy中添加对carl-test-at-seoul2的访问，通过carl.shen传文件到carl-test-at-seoul2后，carl和shen能够访问了，但是chrome无法访问，因为IAM Policy, 只能对IAM实体(IAM User, Group, Role)起作用，无法对匿名用户起作用</p>
<ol>
<li><p>S3Exam的Policy中只有Bucket carl-test-at-seoul时，carl和shen没有权限访问Bucket carl-test-at-seoul-2的资源。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 使用carl.shen上传图片到Bucket carl-test-at-seoul-2中</div><div class="line">$ aws --profile carlshen s3 cp photo_used_for_test/steve.jpeg s3://carl-test-at-seoul-2/</div><div class="line">upload: photo_used_for_test/steve.jpeg to s3://carl-test-at-seoul-2/steve.jpeg</div><div class="line">$ aws --profile carlshen s3 ls s3://carl-test-at-seoul-2/</div><div class="line">2017-12-19 14:15:26       9619 steve.jpeg</div><div class="line"></div><div class="line"># IAM User carl 没有权限访问Bucket carl-test-at-seoul-2的资源</div><div class="line">$ aws --profile carl s3 cp s3://carl-test-at-seoul-2/steve.jpeg ./download_from_s3/</div><div class="line">fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden</div><div class="line"># IAM User shen 没有权限访问Bucket carl-test-at-seoul-2的资源</div><div class="line">$ aws --profile shen s3 cp s3://carl-test-at-seoul-2/steve.jpeg ./download_from_s3/</div><div class="line">fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
<li><p>在S3Exam中添加对Bucket carl-test-at-seoul-2的访问权限, 修改后的Bucket Policy如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</div><div class="line">    &quot;Statement&quot;: [</div><div class="line">        &#123;</div><div class="line">            &quot;Effect&quot;: &quot;Allow&quot;,</div><div class="line">            &quot;Action&quot;: &quot;s3:*&quot;,</div><div class="line">            &quot;Resource&quot;: [</div><div class="line">                &quot;arn:aws:s3:::carl-test-at-seoul&quot;,</div><div class="line">                &quot;arn:aws:s3:::carl-test-at-seoul/*&quot;,</div><div class="line">                &quot;arn:aws:s3:::carl-test-at-seoul-2&quot;,</div><div class="line">                &quot;arn:aws:s3:::carl-test-at-seoul-2/*&quot;</div><div class="line">            ]</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>添加权限后，使用Chrome访问Bucket carl-test-at-seoul-2中的steve.jpeg时，无法访问。因为IAM Policy不对匿名访问生效<br><img src="/images/AWS/S3/s3_permission_exam_access_deny_on_bucket_2.png" alt="s3_permission_exam_access_deny_on_bucket_2"></p>
</li>
<li><p>添加权限后，carl和shen就可以访问Bucket carl-test-at-seoul-2的资源了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># carl可以访问Bucket carl-test-at-seoul-2的资源</div><div class="line">$ aws --profile carl s3 cp s3://carl-test-at-seoul-2/steve.jpeg ./download_from_s3/</div><div class="line">download: s3://carl-test-at-seoul-2/steve.jpeg to download_from_s3/steve.jpeg</div><div class="line"></div><div class="line"># shen可以访问Bucket carl-test-at-seoul-2的资源</div><div class="line">$ aws --profile shen s3 cp s3://carl-test-at-seoul-2/steve.jpeg ./download_from_s3/</div><div class="line">download: s3://carl-test-at-seoul-2/steve.jpeg to download_from_s3/steve.jpeg</div></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<hr>
<hr>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> S3 </tag>
            
            <tag> AWS CLI </tag>
            
            <tag> AWS Deep Dive </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深挖AWS S3的权限管理]]></title>
      <url>/2017/12/14/deep-dig-on-s3-bucket-permission/</url>
      <content type="html"><![CDATA[<p>AWS共有三种方式用来管理S3的访问权限</p>
<ul>
<li>IAM policy</li>
<li>Bucket Policy</li>
<li>S3 ACL</li>
</ul>
<p>此处深挖一下这三种访问控制的用法和区别</p>
<h3 id="三种访问控制的介绍"><a href="#三种访问控制的介绍" class="headerlink" title="三种访问控制的介绍"></a>三种访问控制的介绍</h3><p>S3的三种访问控制如下:</p>
<ul>
<li>IAM Policy, 是基于用户层面的控制，attach to IAM实体(User, Role或Group),规定了谁能对哪些S3做什么操作</li>
<li>Bucket Policy, 是基于Bucket层面的控制, attach to Bucket, 规定了哪些人能对我这个Bucket中的resource做什么操作。</li>
<li>S3 ACL, 是基于Bucket或单个对象的，规定了谁能对我这个Bucket或对象做什么操作。</li>
</ul>
<a id="more"></a>
<h4 id="IAM-Policy"><a href="#IAM-Policy" class="headerlink" title="IAM Policy"></a>IAM Policy</h4><p>IAM Policy 是实施在IAM层面的，规定了某个AWS IAM实体(User, Group或者Role)可以对S3所能做的操作。</p>
<p>IAM Policy由多个Statement组成，每个Statement都规定了<strong><code>谁</code>被<code>允许/禁止</code>对<code>某个AWS资源(包括但不限于S3)</code>进行<code>哪些操作</code></strong>这样的一组策略。</p>
<p>如下是一个IAM Policy的例子，在这个Policy中，允许对名为my_bucket的S3的bucket做任何s3的操作。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"Version"</span>: <span class="string">"2012-10-17"</span>,</div><div class="line">  <span class="attr">"Statement"</span>:[&#123;</div><div class="line">    <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</div><div class="line">    <span class="attr">"Action"</span>: <span class="string">"s3:*"</span>,</div><div class="line">    <span class="attr">"Resource"</span>: [<span class="string">"arn:aws:s3:::my_bucket"</span>,</div><div class="line">                 <span class="string">"arn:aws:s3:::my_bucket/*"</span>]</div><div class="line">    &#125;</div><div class="line">  ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中,</p>
<ul>
<li><code>Effect</code>定义了<code>允许/禁止</code>，</li>
<li><code>Resource</code>定义了<code>AWS资源(包括但不限于S3)</code></li>
<li><code>Action</code>定义了<code>哪些操作</code>。</li>
</ul>
<p>IAM Policy中没有代表<code>谁</code>的<code>Principal</code>,因为IAM Policy是需要Attach给某个实体的，被Attach的实体(可以是User，Group或者Role)就承担了<code>谁</code>的这个属性，所以IAM Policy中是没有<code>Principal</code>的。</p>
<p>IAM Policy可以定义某个User，Group或者Role对所有AWS Resource的访问控制。针对S3的控制只是IAM Policy的子集而已</p>
<p>IAM Policy可以定义对多个S3 Bucket的控制,这是Bucket Policy无法做到的。</p>
<h4 id="Bucket-Policy"><a href="#Bucket-Policy" class="headerlink" title="Bucket Policy"></a>Bucket Policy</h4><p>Bucket Policy是实施在S3 Bucket层面的，只能定义本Bucket中资源的访问权限，无法定义对其他Bucket的访问权限。如果定义了其他Bucket的访问权限，保存Policy的时候会报错。</p>
<p>Bucket Policy定义了<code>谁</code>可以被<code>允许/禁止</code>对<code>本bucket</code>进行<code>哪些操作</code>.</p>
<p>如下是一个经典的Bucket Policy, 意思是允许任何人对bucket carl-test-at-seoul中的任意Object执行s3:GetObject的操作, 也就是将Bucket中的Object设置为Public Read。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"Version"</span>: <span class="string">"2012-10-17"</span>,</div><div class="line">    <span class="attr">"Id"</span>: <span class="string">"Policy1512465896017"</span>,</div><div class="line">    <span class="attr">"Statement"</span>: [</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"Sid"</span>: <span class="string">"Stmt1512465891857"</span>,</div><div class="line">            <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</div><div class="line">            <span class="attr">"Principal"</span>: <span class="string">"*"</span>,</div><div class="line">            <span class="attr">"Action"</span>: <span class="string">"s3:GetObject"</span>,</div><div class="line">            <span class="attr">"Resource"</span>: <span class="string">"arn:aws:s3:::carl-test-at-seoul/*"</span></div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Effect,Principal,Action,Resource的含义同IAM Policy中的意思相同。</p>
<h4 id="S3-ACL"><a href="#S3-ACL" class="headerlink" title="S3 ACL"></a>S3 ACL</h4><p>S3 ACL可以实施在Bucket层面,也可以实施在Object层面。</p>
<p>AWS为ACL设置了一些预定义的组</p>
<ul>
<li>AWS account Access - 代表的是账户级别的设置，可以设置本账户和授权某个账户来访问S3的资源</li>
<li>Authenticated Users group - 代表的是AWS中任意验证过的账户，对这个组授权后，是世界上任何经过身份验证的AWS用户都可以访问你的Bucket</li>
<li>All Users group - 代表世界上的任何人</li>
<li>Log Delivery group - 针对服务器访问日志记录，用来允许AWS将某个S3的访问请求写入本Bucket</li>
</ul>
<p>针对每个组，在Bucket或者Object层面上，有如下四种访问权限可以设置</p>
<ul>
<li>List objects</li>
<li>Write objects</li>
<li>Read bucket permissions</li>
<li>Write bucket permissions</li>
</ul>
<p>使用ACL来实施权限控制已经不被推荐, 有些设计例如Authenticated Users group, 设置后就会公开给所有AWS account，现在看来是很奇怪的设计。但系统都是逐渐完善的，S3是最早的一批服务，在没有IAM的当年，可能确实是有这类的需求。现在可以使用IAM Policy和Bucket Policy来设置更细粒度的控制。</p>
<p>关于ACL的使用，有一个常用的用法，当你要设置部分Object为Public-Read的时候，通过设置基于Object的All Users group的List objects权限，会很方便。而不需要在Bucket Policy中对每个Object 进行显式的配置。在AWS SDK中，表现为设置Object的s3_permissions为public_read</p>
<h3 id="多个访问控制之间的判断规则"><a href="#多个访问控制之间的判断规则" class="headerlink" title="多个访问控制之间的判断规则"></a>多个访问控制之间的判断规则</h3><p>当用户访问或操作S3的资源时，S3会综合考虑IAM policy, S3 Bucket Policy和S3 ACL三者的设置,从而得出是否允许用户的操作。</p>
<p>总共有三种类型的Effect</p>
<ul>
<li>default Deny - 基于最小权限原则，所有的Object默认都是私有的，也就是拒绝访问的</li>
<li>explicit Deny - 显式拒绝, IAM Policy或S3 Bucket Policy中<code>Effect</code>字段中显式声明为”Deny”,注意ACL中没有显示拒绝</li>
<li>explicit Allow - 显式允许, IAM Policy或S3 Bucket policy中<code>Effect</code>字段中显式生命为”Allow”的，ACL中设置的允许权限也是显式允许。</li>
</ul>
<p>整体判定规则为:</p>
<ul>
<li>当有显式拒绝时，会被判定为拒绝访问。</li>
<li>没有显式拒绝，有显示允许时，会被判定为允许访问。</li>
<li>没有显式拒绝也没有显示允许时，就适用默认拒绝规则，会被判定为拒绝访问。<br><img src="/images/AWS/S3/s3_authz_diagram.png" alt="s3_authz_diagram"></li>
</ul>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>下面通过一个故事来大概讲解下IAM Policy，Bucket Policy和S3 ACL的使用和互相之间的作用。</p>
<p>有一位土豪(AWS Account), 拥有数不尽的家产，旗下产业不计其数，拥有万千工厂(EC2),仓库(S3)以及其他资产。为了便于管理仓库，每个仓库都是独立的房子，并且有着不同的名字和编号(S3 Bucket), 每个仓库有独立的管理员(Bucket Policy)管理。仓库里各个房间里存放着不计其数的物品(Object), 物品都是私有的(private object by default deny)。但是有些物品有对外展示的需要，因此就在仓库在第一层开了一些玻璃橱窗，用来展示一些公开的物品(ACL public-read object)。</p>
<h4 id="情形1-单独设置Object-ACL"><a href="#情形1-单独设置Object-ACL" class="headerlink" title="情形1 - 单独设置Object ACL"></a>情形1 - 单独设置Object ACL</h4><blockquote>
<p>没有IAM Policy，没有Bucket Policy, Object有部分被设置为Public-read ACL时</p>
</blockquote>
<p>一天，来了个叫小明(anonymous user)的家伙，闯进了仓库(S3), 叫嚣乎说要进入仓库A(Bucket A)来查看其中的物品(Object), 仓库A的管理员老王(Bucket Policy)一看，压根不认识小明这哥们(Bucket Policy中没有匹配的规则)，立马就把他挡在了仓库大门外(access deny),小明(anonymous user)在仓库周围晃悠了一圈，只看见了在玻璃橱窗里展示的一些物品(ACL public-read object), 其他什么也没看到，只能灰溜溜的走了。</p>
<h4 id="情形2-设置Bucket-Policy允许访问"><a href="#情形2-设置Bucket-Policy允许访问" class="headerlink" title="情形2 - 设置Bucket Policy允许访问"></a>情形2 - 设置Bucket Policy允许访问</h4><blockquote>
<p>没有IAM Policy，Bucket Policy设置为explicit allow，Object有部分被设置为Public-read ACL时</p>
</blockquote>
<p>第二天，员工小毛(IAM User)来参观仓库A, 仓库A管理员老王(Bucket Policy)一看小毛在允许访问的名单内(匹配到Bucket Policy中explicit allow的规则)，就打开大门放了小毛进仓库, 小毛进来把各个房间的物品(private object)和橱窗里的物品(ACL public-read object)都看了个遍, 临走还说没看够，明天还要再来参观。</p>
<h4 id="情形3-设置Bucket-Policy禁止访问某些资源"><a href="#情形3-设置Bucket-Policy禁止访问某些资源" class="headerlink" title="情形3 - 设置Bucket Policy禁止访问某些资源"></a>情形3 - 设置Bucket Policy禁止访问某些资源</h4><blockquote>
<p>没有IAM Policy，Bucket Policy设置部分资源为explicit allow,部分资源为explicit deny, Object有部分被设置为Public-read ACL时</p>
</blockquote>
<p>小毛参观仓库的事情被分管仓库的副总知道了，叮嘱仓库A管理员老王说，6号房间和6号橱窗的东西是贵重物品，不能给小毛参观(update Bucket Policy to add explicit deny)。</p>
<p>第三天小毛(IAM User)来仓库A参观，仓库A管理员老王(Bucket Policy)帮小毛开了门，但吩咐小毛说，从今天起，6号房间划归为贵重物品存放室了，不对你开放了(explict deny), 而且公开展示的6号橱窗里面的东西暂时也不能对你开放了，已经用布挡起来了(explict deny), 小毛进来参观了一天，除了不能参观的的6号房间(explicit deny)和被挡起来的6号橱窗(explicit deny override explicit allow)，把其他物品都参观了一遍。</p>
<h4 id="情形4-无法设置Bucket-Policy来允许-禁止访问其他Bucket"><a href="#情形4-无法设置Bucket-Policy来允许-禁止访问其他Bucket" class="headerlink" title="情形4 - 无法设置Bucket Policy来允许/禁止访问其他Bucket"></a>情形4 - 无法设置Bucket Policy来允许/禁止访问其他Bucket</h4><blockquote>
<p>Bucket Policy 无法设置其它Bucket的访问控制</p>
</blockquote>
<p>第四天，小毛(IAM User)找到仓库A管理员老王说他想参观仓库B的物品，老王说自己只管理仓库A， 仓库B的东西归老张管。小毛只能悻悻地回家，没参观成仓库B。</p>
<h4 id="情形5-设置IAM-Policy来访问多个Bucket"><a href="#情形5-设置IAM-Policy来访问多个Bucket" class="headerlink" title="情形5 - 设置IAM Policy来访问多个Bucket"></a>情形5 - 设置IAM Policy来访问多个Bucket</h4><blockquote>
<p>设置IAM Policy来允许访问多个Bucket Policy</p>
</blockquote>
<p>第五天，小毛(IAM User)发现，好朋友小强(IAM User)是土豪家(AWS Account)亲戚，便和小强找到董事长土豪，想要土豪批个通行证(IAM Policy)来参观仓库A和仓库B, 土豪二话不收就写了个允许参观仓库A和仓库B的批条(IAM Policy), 并在上面添加上了小毛和小强的名字(attach IAM Policy to IAM User), 小毛和小强拿着批条顺利的进入了仓库A和仓库B。</p>
<p>有了批条，小强就可以参观仓库A和仓库B的所有物品啦。至于小毛，因为分管仓库的副总嘱咐过，所以他还是没有权利参观仓库A的6号房间和6号橱柜(explicit deny override explicit allow)的物品，但可以被允许参观仓库B的物品了。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://aws.amazon.com/cn/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/" target="_blank" rel="external">IAM Policies and Bucket Policies and ACLs! Oh, My! (Controlling Access to S3 Resources)</a></li>
<li><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html" target="_blank" rel="external">Using Bucket Policies and User Policies</a></li>
<li><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/S3_ACLs_UsingACLs.html" target="_blank" rel="external">Managing Access with ACLs</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> S3 </tag>
            
            <tag> AWS Deep Dive </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用shell添加多行到文件的几个办法]]></title>
      <url>/2017/12/12/how-to-write-multiline-to-file-by-shell/</url>
      <content type="html"><![CDATA[<p>使用shell添加多行到某个文件的几个办法, 启动EC2设置User Data时可能会用到。</p>
<h4 id="方法一-逐行echo添加"><a href="#方法一-逐行echo添加" class="headerlink" title="方法一: 逐行echo添加"></a>方法一: 逐行echo添加</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">echo &apos;#!/bin/bash&apos; &gt; way1.sh</div><div class="line">echo &apos;export PATH=$PATH:~/bin/&apos; &gt;&gt; way1.sh</div></pre></td></tr></table></figure>
<p>查看文件内容为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ cat way1.sh</div><div class="line">#!/bin/bash</div><div class="line">export PATH=$PATH:~/bin/</div><div class="line">$</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<h4 id="方法二-echo-换行添加"><a href="#方法二-echo-换行添加" class="headerlink" title="方法二: echo 换行添加"></a>方法二: echo 换行添加</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">echo &apos;#!/bin/bash</div><div class="line">export PATH=$PATH:~/bin/&apos; &gt; way2.sh</div></pre></td></tr></table></figure>
<p>查看文件内容为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ cat way2.sh</div><div class="line">#!/bin/bash</div><div class="line">export PATH=$PATH:~/bin/</div><div class="line">$</div></pre></td></tr></table></figure></p>
<h4 id="方法三-echo转义添加"><a href="#方法三-echo转义添加" class="headerlink" title="方法三: echo转义添加"></a>方法三: echo转义添加</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 使用-e参数转义\n</div><div class="line">echo -e &apos;#!/bin/bash\nexport PATH=$PATH:~/bin/&apos; &gt;&gt; way3.sh</div></pre></td></tr></table></figure>
<p>查看文件内容为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ cat way3.sh</div><div class="line">#!/bin/bash</div><div class="line">export PATH=$PATH:~/bin/</div><div class="line">$</div></pre></td></tr></table></figure></p>
<h4 id="方法四-使用cat"><a href="#方法四-使用cat" class="headerlink" title="方法四: 使用cat"></a>方法四: 使用cat</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 注意，$PATH前面需要加\进行转义，否则会将$PATH展开</div><div class="line">cat &gt; way4.sh &lt;&lt;EOF</div><div class="line">#!/bin/bash</div><div class="line">export PATH=\$PATH:~/bin/</div><div class="line">EOF</div></pre></td></tr></table></figure>
<p>查看文件内容为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ cat way4.sh</div><div class="line">#!/bin/bash</div><div class="line">export PATH=$PATH:~/bin/</div><div class="line">$</div></pre></td></tr></table></figure></p>
<p>如果$PATH前不加转移符，会将$PATH展开<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cat &gt; way4_expand.sh &lt;&lt;EOF</div><div class="line">#!/bin/bash</div><div class="line">export PATH=$PATH:~/bin/</div><div class="line">EOF</div></pre></td></tr></table></figure></p>
<p>查看文件内容为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># 先查看PATH</div><div class="line">$ echo $PATH</div><div class="line">/home/ec2-user/.nvm/versions/node/v6.11.5/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin:/home/ec2-user/bin/</div><div class="line">$</div><div class="line"></div><div class="line"># 可以看到命令将PATH展开后再写入文件way4_expand.sh中</div><div class="line">$ cat way4_expand.sh</div><div class="line">#!/bin/bash</div><div class="line">export PATH=/home/ec2-user/.nvm/versions/node/v6.11.5/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin:/home/ec2-user/bin/:~/bin/</div><div class="line">$</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> Shell </category>
            
        </categories>
        
        
        <tags>
            
            <tag> EC2 User Data </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS Certified Sysops Administrator - Associate Road Map]]></title>
      <url>/2017/12/04/AWS-Certified-Sysops-Administrator-Associate/</url>
      <content type="html"><![CDATA[<h2 id="Official-AWS-Certification-Page"><a href="#Official-AWS-Certification-Page" class="headerlink" title="Official AWS Certification Page"></a>Official AWS Certification Page</h2><p>访问官网<a href="https://amazonaws-china.com/certification/certification-prep/" target="_blank" rel="external">AWS Certification</a></p>
<ul>
<li>参加 AWS 培训课程</li>
<li>查看考试指南和样题<ul>
<li>了解考试涉及的概念并整体了解需要学习哪些内容, <a href="http://awstrainingandcertification.s3.amazonaws.com/production/AWS_certified_sysops_associate_blueprint.pdf" target="_blank" rel="external">AWS Certified SysOps Administrator - Associate 考试指南</a> 相当于考试大纲, 必看,而且需要反复的看。因为学习过一阵后再来看Guide，会有更深的体会。</li>
<li><a href="http://awstrainingandcertification.s3.amazonaws.com/production/AWS_certified_sysops_associate_examsample.pdf" target="_blank" rel="external">考试样题</a>用于熟悉题目题型</li>
</ul>
</li>
<li>完成自主进度动手实验和备考任务<ul>
<li>官方<a href="https://www.qwiklabs.com/learning_paths/20/lab_catalogue?locale=en" target="_blank" rel="external">qwikLABS 任务</a>提供了一系列动手实验, 提供部分免费实验，但大部分实验所需的积分都需要购买。高性价比的做法是， 注册一个AWS全球账号，使用一年的免费额度来对照着实验手册来进行试验。</li>
</ul>
</li>
<li>学习 AWS 白皮书<ul>
<li>白皮书是纯英文的，而且每个白皮书篇幅都很长，读起来既费时又枯燥。但是有时间还是建议把推荐的几个都看一下。</li>
</ul>
</li>
<li>查看 AWS 常见问题</li>
<li>参加模拟考试<ul>
<li>20美刀一次，主要目的是为了让人熟悉考试时上机的流程。是否需要因人而异, 特别想先熟悉下考试流程的可以考虑参加一次。我个人觉得没有必要, 因为真实考试时，操作界面一目了然，没有磕磕绊绊的机关，省下20美刀可以去买一份课程。</li>
</ul>
</li>
<li>报名考试并获得认证<ul>
<li>登陆<a href="https://www.aws.training/certification" target="_blank" rel="external">https://www.aws.training/certification</a>注册进行考试<a id="more"></a>
</li>
</ul>
</li>
</ul>
<h2 id="考试指南"><a href="#考试指南" class="headerlink" title="考试指南"></a>考试指南</h2><p><a href="http://awstrainingandcertification.s3.amazonaws.com/production/AWS_certified_sysops_associate_blueprint.pdf" target="_blank" rel="external">AWS Certified SysOps Administrator - Associate 考试指南</a> 读三遍，读三遍，读三遍</p>
<p>各个Domain的分数比例如下:<br><img src="/images/AWS/Sysops/Domain_on_sysops_admin_associate_blueprint.jpg" alt="Domain_on_sysops_admin_associate_blueprint"></p>
<h2 id="视频学习"><a href="#视频学习" class="headerlink" title="视频学习"></a>视频学习</h2><p><a href="https://acloud.guru" target="_blank" rel="external">Acloudguru</a> 中<a href="https://acloud.guru/course/aws-certified-sysops-administrator-associate/dashboard" target="_blank" rel="external">aws-certified-sysops-administrator-associate</a>视频的学习</p>
<h3 id="要点摘录"><a href="#要点摘录" class="headerlink" title="要点摘录"></a>要点摘录</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><ul>
<li>Introduction</li>
<li>Monitoring, Metrics &amp; Analysis</li>
<li>High Availability</li>
<li>Deployment &amp; Provisioning</li>
<li>Data Management</li>
<li>OpsWorks</li>
<li>Security</li>
<li>Networking</li>
<li>VPCs</li>
</ul>
<h4 id="Monitoring-Metrics-amp-Analysis"><a href="#Monitoring-Metrics-amp-Analysis" class="headerlink" title="Monitoring, Metrics &amp; Analysis"></a>Monitoring, Metrics &amp; Analysis</h4><ul>
<li><p>CloudWatch</p>
<ul>
<li><p>Amazon CloudWatch is a monitoring service to monitor your AWS resources, as well as the applications that you run on AWS.</p>
</li>
<li><p>CloudWatch can monitor things like :</p>
<ul>
<li>Compute<ul>
<li>Autoscaling Groups</li>
<li>Elastic Load Balancers</li>
<li>Route53 Health Checks</li>
</ul>
</li>
<li>Storage &amp; Content Delivery<ul>
<li>EBS Volumes</li>
<li>Storage Gateways</li>
<li>CloudFront</li>
</ul>
</li>
<li>Database &amp; Analytics<ul>
<li>DynamoDB</li>
<li>Elasticache Nodes</li>
<li>RDS Instances</li>
<li>Elastic MapReduce Job Flows</li>
<li>Redshift</li>
</ul>
</li>
<li>Other<ul>
<li>SNS Topics</li>
<li>SQS Queues</li>
<li>Opsworks</li>
<li>CloudWatch Logs</li>
<li>Estimated Charges on your AWS Bill</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>CloudWatch and EC2</p>
<ul>
<li>Host Level Metrics Consist of:<ul>
<li>CPU</li>
<li>Network</li>
<li>Disk</li>
<li>Status Check</li>
</ul>
</li>
<li>RAM Utilization is a custom metric! By default EC2 monitoring is 5 minute intervals, unless you enable detailed monitoring which will then make it 1 minute intervals.</li>
<li><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ec2-metricscollected.html" target="_blank" rel="external">EC2 Metric</a><ul>
<li>CPUCreditUsage - Units: Count</li>
<li>CPUCreditBalance - Units: Count</li>
<li>CPUUtilization - Units: Percent</li>
<li>DiskReadOps - Units: Count</li>
<li>DiskWriteOps - Units: Count</li>
<li>DiskReadBytes - Units: Bytes</li>
<li>DiskWriteBytes - Units: Bytes</li>
<li>NetworkIn - Units: Bytes</li>
<li>NetworkOut - Units: Bytes</li>
<li>NetworkPacketsIn - Units: Count</li>
<li>NetworkPacketsOut - Units: Count</li>
<li>StatusCheckFailed - Units: Count</li>
<li>StatusCheckFailed_Instance - Units: Count</li>
<li>StatusCheckFailed_System - Units: Count</li>
</ul>
</li>
<li><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ec2-metricscollected.html" target="_blank" rel="external">EC2 Dimension</a><br>  If you’re using Detailed Monitoring, you can filter the EC2 instance data using any of the dimensions in the following table.<ul>
<li>AutoScalingGroupName</li>
<li>ImageId</li>
<li>InstanceId</li>
<li>InstanceType</li>
</ul>
</li>
<li>By Default - CloudWatch Metric stored 2 Weeks. You can retrieve data that is longer than 2 weeks using the GetMetricStatistics API or by using third party tools offered by AWS partners.</li>
<li>You can retrieve data from any terminated EC2 or ELB instance for up to 2 weeks after it’s termination.</li>
</ul>
</li>
<li><p>Metric Granularity</p>
<ul>
<li>It depends on the AWS service. Many default metrics for many default services are 1 minute, but it can be 3 or 5 minutes depending on the service.</li>
<li>Exam Tips: For custom metric the minimum granularity that you can have is 1 minute.</li>
</ul>
</li>
<li><p>CloudWatch Alarms</p>
<ul>
<li>You can create an alarm to monitor any Amazon CloudWatch metric in your account. This can include EC2 CPU Utilization, Elastic Loa Balancer Latency of even the charges on your AWS bill. You can set the appropriate thresholds in which to trigger the alarms and also set what actions should be taken if an alarm state is reached. This will be covered in a subsequent lecture.</li>
</ul>
</li>
<li><p>EC2 Status check</p>
<ul>
<li>System Status Check (Checks Host, underlying physical Host)<ul>
<li>Loss of network connectivity</li>
<li>Loss of system power</li>
<li>Software issues on the physical host</li>
<li>Hardware issues on the physical host</li>
<li>Best way to resolve issues is to stop and then start the VM again.</li>
</ul>
</li>
<li>Instance Status Check (Checks VM)<ul>
<li>Failed system status checks</li>
<li>Misconfigured networking or startup configuration</li>
<li>Exhausted memory</li>
<li>Corrupted file system</li>
<li>Incompatible kernel</li>
<li>Best way to trouble shoot is by rebooting the instance or by making modifications in your operating system.</li>
</ul>
</li>
</ul>
</li>
<li><p>Custom Metrics</p>
<ul>
<li>AWS Namespaces</li>
<li>Custom Namespaces (the custom metric is selected in this namespace)</li>
</ul>
</li>
<li><p>Monitoring EBS</p>
<ul>
<li>4 Different Types of EBS Storage<ul>
<li>General Purpose (SSD) - gp2</li>
<li>Provisioned IOPS(SSD) - io1</li>
<li>Throughput Optimized(HDD) - st1</li>
<li>Cold (HDD) - sc1</li>
</ul>
</li>
<li><p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html" target="_blank" rel="external">Compare with each volumes type</a><br>  <img src="/images/AWS/Sysops/volumes_type_of_EBS.png" alt="volumes_type_of_EBS"></p>
</li>
<li><p>IOPS &amp; Volumes</p>
<ul>
<li>General Purpose SSD volumes have a base of 3 IOPS per/GiB of volume size.<ul>
<li>Maximum volume size of 16,384 GiB</li>
<li>Maximum IOPS Size of 10,000 IOPS Total (after that you need to move to provisioned IOPS)</li>
</ul>
</li>
</ul>
</li>
<li>IOPS &amp; Volumes Examples<ul>
<li>Say we have a 1 GiB Volume. We get 3 IOPS per Gb so we have 3*1=3 IOPS</li>
<li>We can burst performance on this volume up to 3000 IOPS if we want</li>
<li>Using I/O Credits</li>
<li>The burst would be 2997 IOPS( 3000 - 3)</li>
</ul>
</li>
<li>I/O Credits<ul>
<li>When your volume requires more than the baseline performance I/O level, it simply uses I/O credits in the credit balance to burst to the required performance level, up to a maximum of 3,000 IOPS.<ul>
<li>Each volume receives an initial I/O credit balance of 5,400,000 I/O credits.</li>
<li>This is enough to sustain the maximum burst performance of 3,000 IOPS for 30 minutes.</li>
<li>When you are not going over your provisioned IO level(ie bursting) you will be earning credits.</li>
</ul>
</li>
</ul>
</li>
<li>I/O Credits - it is beyond the scope of the SysOps Associate Exam to be able to calculate this.<br><img src="/images/AWS/Sysops/IO_credits_of_EBS.jpg" alt="IO_credits_of_EBS"></li>
<li><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-initialize.html" target="_blank" rel="external">Pre-Warming EBS Volumes</a><ul>
<li>New EBS volumes receive their maximum performance the moment that they are available and do not require initialization (formerly known as pre-warming). However, storage blocks on volumes that were restored from snapshots must be initialized (pulled down from Amazon S3 and written to the volume) before you can access the block. This preliminary action takes time and can cause a significant increase in the latency of an I/O operation the first time each block is accessed. For most applications, amortizing this cost over the lifetime of the volume is acceptable. Performance is restored after the data is accessed once.</li>
<li>You can avoid this performance hit in a production environment by reading from all of the blocks on your volume before you use it; this process is called initialization. For a new volume created from a snapshot, you should read all the blocks that have data before using the volume.</li>
</ul>
</li>
<li><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html" target="_blank" rel="external">EBS CloudWatch Metrics</a><br>  <img src="/images/AWS/Sysops/ebs_cloudwatch_metric.png" alt="ebs_cloudwatch_metric"></li>
<li><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html" target="_blank" rel="external">Volume Status Checks</a><br>  <img src="/images/AWS/Sysops/volume_status_check_EBS.jpg" alt="volume_status_check_EBS"><ul>
<li>Exam Tips<ul>
<li>Degraded or Severely Degraded = Warning</li>
<li>Stalled or Not Available = Impaired</li>
</ul>
</li>
</ul>
</li>
<li><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modify-volume.html" target="_blank" rel="external">Modifying EBS Volumes</a><ul>
<li>If your Amazon EBS volume is attached to a current generation EC2 instance type, you can increase its size, change its volume type, or (for an io1 volume) adjust its IOPS performance, all without detaching it. You can apply these changes to detached volumes as well.<ul>
<li>Issue the modification command (console or command line)</li>
<li>Monitor the progress of the modification</li>
<li>If the size of the volume was modified, extend the volume’s file system to take advantage of the increased storage capacity.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Monitoring RDS</p>
<ul>
<li>Two Type of monitoring available for RDS<ul>
<li>CloudWatch - In CloudWatch you can monitor RDS by Metrics.</li>
<li>RDS itself - In RDS itself, you can monitor RDS by Events.</li>
</ul>
</li>
<li><a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Monitoring.html" target="_blank" rel="external">RDS Metrics</a><ul>
<li>BinLogDiskUsage - Units: Bytes</li>
<li>CPUUtilization - Units: Percent</li>
<li><strong>DatabaseConnections</strong> - Units: Count</li>
<li><strong>DiskQueueDepth</strong> - Units: Count</li>
<li>FreeableMemory - Units: Bytes</li>
<li><strong>FreeStorageSpace</strong> - Units: Bytes</li>
<li><strong>ReplicaLag</strong> - Units: Seconds</li>
<li>SwapUsage - Units: Bytes</li>
<li><strong>ReadIOPS</strong> - Units: Count/Second</li>
<li><strong>WriteIOPS</strong> - Units: Count/Second</li>
<li><strong>ReadLatency</strong> - Units: Seconds</li>
<li><strong>WriteLatency</strong> - Units: Seconds</li>
<li>ReadThroughput - Units: Bytes/Second</li>
<li>WriteThroughput - Units: Bytes/Second</li>
<li>NetworkReceiveThroughput - Units: Bytes/second</li>
<li>NetworkTransmitThroughput - Units: Bytes/second</li>
</ul>
</li>
<li>Have a general idea of what each metric does. You do not need to know the name of each metric, but pay attention to the ones in bold.</li>
</ul>
</li>
<li><p>Monitoring ELB</p>
<ul>
<li>ELB - Every 60 seconds (Provided there is traffic)<br>Elastic Load Balancing only reports when requests are flowing through the load balancer. If there are no requests or data for a given metric, the metric will not be reported to CloudWatch. If there are requests flowing through the load balancer, Elastic Load Balancing will measure and send metrics for that load balancer in 60 second intervals</li>
<li><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/elb-metricscollected.html" target="_blank" rel="external">ELB Metrics</a><br>  <img src="/images/AWS/Sysops/ELB_Metric.png" alt="ELB_Metric"><ul>
<li>Have a general idea of what each metric does. You do not need to know the name of each metric, but pay attention to <strong>SurgeQueueLength</strong> &amp; <strong>SpilloverCount</strong>.</li>
</ul>
</li>
</ul>
</li>
<li><p>Monitoring Elasticache</p>
<ul>
<li>Two engines<ul>
<li>Memcached</li>
<li>Redis</li>
</ul>
</li>
<li><a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/CacheMetrics.WhichShouldIMonitor.html" target="_blank" rel="external">Four important things</a><ul>
<li>CPU Utilization</li>
<li>Swap Usage</li>
<li>Evictions</li>
<li>Concurrent Connections</li>
</ul>
</li>
<li>CPU Utilization<ul>
<li>Memcached<ul>
<li>Multi-threaded</li>
<li>Can handle loads of up to 90%. If it exceeds 90% add more nodes to the cluster</li>
</ul>
</li>
<li>Redis<ul>
<li>Not multi-threaded. To determine the point in which to scale, take 90 and divide by the number of cores</li>
<li>For example, suppose you are using a cache.m1.xlarge node, which has four cores. In this case, the threshold for CPU Utilization would be (90/4), or 22.5%</li>
</ul>
</li>
<li>You will not have to calculate Redis CPU Utilization in the exam.</li>
</ul>
</li>
<li>Swap Usage<ul>
<li>Put simply, swap usage is simply the amount of the Swap file that is used. The Swap File (or Paging File) is the amount of disk storage space reserved on disk if your computer runs out of ram. Typically the size of the swap file = the size of the RAM. So if you have 4Gb of RAM, you will have a 4 GB Swap File.</li>
<li>Memcached<ul>
<li>Should be around 0 most of the time and should not exceed 50Mb.</li>
<li>If this exceeds 50Mb you should increase the memcached_connections_overhead parameter.</li>
<li>The memcached_connections_overhead defines the amount of memory to be reserved for memcached connections and other miscellaneous overhead.</li>
<li>Learn More: <a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/ParameterGroups.Memcached.html" target="_blank" rel="external">https://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/ParameterGroups.Memcached.html</a></li>
</ul>
</li>
<li>Redis<ul>
<li>No SwapUsage metric, instead use reserved-memory</li>
</ul>
</li>
</ul>
</li>
<li>Evictions<ul>
<li>Think of evictions like tenants in an apartment building. There are a number of empty apartments that slowly fill up with tenants. Eventually the apartment block is full, however more tenants need to be added.</li>
<li>An Eviction occurs when a new item is added and an old item must be removed due to lack of free space in the system.</li>
<li>Memcached<ul>
<li>There is no recommended setting. Choose a threshold based off your application</li>
<li>Either Scale up (ie increase the memory of existing nodes) OR</li>
<li>Scale Out (add more nodes)</li>
</ul>
</li>
<li>Redis<ul>
<li>There is no recommended setting. Choose a threshold based off your application.</li>
<li>scale your cluster up by using a larger node type.</li>
</ul>
</li>
<li>This can be an exam question. Remember the different approaches between Memcached &amp; Redis</li>
</ul>
</li>
<li>Concurrent Connections<ul>
<li>Memcached &amp; Redis<ul>
<li>There is no recommended setting. Choose a threshold based off your application</li>
<li>If there is a large and sustained spike in the number of concurrent connections this can either mean a large traffic spike OR your application is not releasing connections as it should be</li>
</ul>
</li>
<li>This can be an exam question. Remember to set an alarm on the number of concurrent connections for elasticache.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Centralized Monitoring<br>  Most enterprises have monitoring solutions such as Zennos, Nimsoft,Splunk, IBM Tivoli, HP Operations Manager etc. These usually involve installing agents on the servers to be monitored and then allowing these agents to report metrics back to the centralized monitoring server.</p>
<ul>
<li>Protocol<ul>
<li>Depends on what is being monitored. Most basic monitoring is going to use ICMP</li>
<li>Could be SQL (1433) or MySQL (3306)</li>
<li>Exam Tip: This can come up in several areas of the exam. This could include either inside your own VPC or a VPC that is connected to your on premise data center. Remember to allow ICMP/Specific Ports to either a specific IP address or a specific range of IP Addresses.</li>
</ul>
</li>
</ul>
</li>
<li><p>AWS Organizations &amp; Consolidated Billing</p>
<ul>
<li>AWS Organizations<br>  AWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage.<ul>
<li>Available in two feature sets:<ul>
<li>Consolidated Billing</li>
<li>ALL Features</li>
</ul>
</li>
<li>consist of<ul>
<li>Root</li>
<li>Organization Unit (OU)</li>
<li>AWS Account</li>
</ul>
</li>
</ul>
</li>
<li>Consolidated Billing<ul>
<li>Accounts<ul>
<li>Paying Account (Paying account is independent. Cannot access resources of the other accounts)</li>
<li>Linked Accounts (All linked accounts are independent)</li>
</ul>
</li>
<li>Advantages<ul>
<li>One bill per AWS account</li>
<li>Very easy to track charges and allocate costs</li>
<li>Volume pricing discount</li>
</ul>
</li>
<li>S3 pricing</li>
<li>Reserved EC2 Instances</li>
<li>Best Practices<ul>
<li>Always enable multi-factor authentication on root account.</li>
<li>Always use a strong and complex password on root account.</li>
<li>Paying account should be used for billing purposes only. Do not deploy resources in to paying account.</li>
</ul>
</li>
<li>Notes<ul>
<li>Linked Accounts<ul>
<li>20 linked accounts only</li>
<li>To add more visit <a href="https://aws-portal.amazon.com/gp/aws/html-forms-controller/contactus/aws-account-and-billing" target="_blank" rel="external">https://aws-portal.amazon.com/gp/aws/html-forms-controller/contactus/aws-account-and-billing</a></li>
</ul>
</li>
<li>Billing Alerts<ul>
<li>When monitoring is enabled on the paying account the billing data for all linked accounts is included</li>
<li>You can still create billing alerts per individual account</li>
</ul>
</li>
<li>CloudTrail<ul>
<li>Per AWS Account and is enabled per region.</li>
<li>Can consolidate logs using an S3 bucket.<ol>
<li>Turn on CloudTrail in the paying account</li>
<li>Create a bucket policy that allows cross account access</li>
<li>Turn on CloudTrail in the other accounts and use the bucket in the paying account</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips:<ul>
<li>Consolidated billing allows you to get volume discounts on all your accounts.</li>
<li>Unused reserved instances for EC2 are applied across the group.</li>
<li>CloudTrail is on a per account and per region basis but can be aggregate in to a single bucket in the paying account.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>EC2 Cost Optimization<ul>
<li>EC2 Instance Types<ul>
<li>On Demand</li>
<li>Reserved</li>
<li>Spot Price</li>
</ul>
</li>
<li>Spot Instances<br>  Spot Instances allow you to name your own price for Amazon EC2 computing capacity. You simply bid on spare Amazon EC2 instances and these will run automatically whenever your bid exceeds the current Spot Price, which varies in real-time based on supply and demand. If your bid goes below the spot price after these instances are provisioned, your instances will automatically be terminated.</li>
<li>On Demand<ul>
<li>On-Demand Instances let you pay for compute capacity by the hour with no long-term commitments or upfront payments. You can increase or decrease your compute capacity depending on the demands of your application and only pay the specified hourly rate for the instances you use. Amazon EC2 always strives to have enough On-Demand capacity available to meet your needs, but during periods of very high demand, it is possible that you might not be able to launch specific On-Demand instance types in specific Availability Zones for short periods of time.</li>
</ul>
</li>
<li>Reserved<ul>
<li>Reserved Instances provide you with a significant discount (up to 75%) compared to On-Demand Instance pricing. You are assured that your Reserved Instance will always be available for the operating system (e.g. Linux/UNIX or Windows) and Availability Zone in which you purchased it. For applications that have steady state needs, Reserved Instances can provide significant savings compared to using On-Demand Instances. Functionally, Reserved Instances and On-Demand Instances perform identically.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="High-Availability"><a href="#High-Availability" class="headerlink" title="High Availability"></a>High Availability</h4><ul>
<li>Elasticity and Scalability<ul>
<li>What is Elasticity<ul>
<li>Think of elasticity as a rubber band. Elasticity allows you to stretch out and retract back your infrastructure, based on your demand.</li>
<li>Under this model you only pay for what you need.</li>
<li>Elasticity is used during a short time period, such as hours or days.</li>
</ul>
</li>
<li>What is Scalability<ul>
<li>Scalability is used to talk about building out the infrastructure to meet your demands long term.</li>
<li>Scalability is used over longer time periods, such as weeks, days, months and years.</li>
</ul>
</li>
<li>AWS Services - Scalability vs Elasticity<ul>
<li>EC2<ul>
<li>Scalability - Increase Instance Sizes as required, using reserved instances</li>
<li>Elasticity - Increase the number of EC2 instances, based on autoscaling</li>
</ul>
</li>
<li>DynamoDB<ul>
<li>Scalability - Unlimited amount of storage</li>
<li>Elasticity - Increase additional IOPS for additional spikes in traffic. Decrease that IOPS after the spike.</li>
</ul>
</li>
<li>RDS<ul>
<li>Scalability - Increase instance size, eg from small to medium</li>
<li>Elasticity - not very elastic, can’t scale RDS based on demand</li>
</ul>
</li>
<li>Automated Elasticity + Scalability<br>  <img src="/images/AWS/Sysops/aws_automated_elasticity_scalability.jpg" alt="aws_automated_elasticity_scalability"></li>
</ul>
</li>
</ul>
</li>
<li>Scale Up or Scale Up<ul>
<li>Scaling Up - 向上升级<ul>
<li>Traditional IT - increases the number of processors, the number of RAM or the amount of storage.</li>
<li>EC2 - increase th instance type from say T1.micro to T2.small, T2.medium etc</li>
</ul>
</li>
<li>Scaling Out - 横向扩展<ul>
<li>Traditionally - adding more resources (such as webservers)</li>
<li>EC2 - adding additional EC2 Instances and using autoscaling.</li>
</ul>
</li>
<li>In the exam<ul>
<li>Eliminate the obviously incorrect answers</li>
<li>ask yourself where the bottle neck is? Is it network related? If so it’s probably a scale up answer.</li>
<li>Is the problem in relation to not having enough resources (ie you can’t increase the instance size further)? If so, it’s probably a scale out answer.</li>
<li>Remember elasticity. Scaling out, you can scale back. Scaling up is easy, scaling down is not so easy.</li>
</ul>
</li>
</ul>
</li>
<li>RDS Multi-AZ Failover<ul>
<li>Introduction<ul>
<li>Multi-AZ deployments for the MySQL, Oracle and PostgreSQL engines utilize synchronous physical replication to keep data on the standby up-to-date with the primary.</li>
<li>Mylti-AZ deployments for the SQLServer engine use synchronous logical replication to achieve the same result, employing SQL Server-native Mirroring technology.</li>
<li>Both approaches safeguard your data in the event of a DB Instance failure or loss of an Availability Zone.</li>
</ul>
</li>
<li>RDS Multi-AZ Failover Advantages<ul>
<li>High availability</li>
<li>Backups are taken from secondary which avoids I/O suspension to the primary</li>
<li>Restore’s are taken from secondary which avoids I/O suspension to the primary</li>
<li>Exam Tip: You can <strong>force</strong> a failover from one AZ to another by rebooting your instance. This can be done through the AWS Management console or by using RebootDbInstance API call.</li>
</ul>
</li>
<li>RDS Multi-AZ Failover is not a scaling solution</li>
<li>Read Replica’s are used to scale</li>
</ul>
</li>
<li>RDS Read Replicas<ul>
<li>What are Read Replica<ul>
<li>Read Replicas make it easy to take advantage of supported engine’s built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB Instance for read-heave database workloads.</li>
<li>Read only copies of your database.</li>
<li>You can create a Read Replica with a few clicks in the AWS Management Console or using the CreateDBInstanceReadReplica API. Once the Read Replica is created, database updates on the source DB Instance will be replicated using a supported engine’s native, asynchronous replication. You can create multiple Read Replicas for a given source DB Instance and distribute your application’s read traffic amongst them.</li>
</ul>
</li>
<li>When would you use read replica’s<ul>
<li>Scaling beyond the compute or I/O capacity of a single DB Instance for read-heavy database workloads. This excess read traffic can be directed to one or more Read Replicas</li>
<li>Serving read traffic while the source DB Instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your Read Replica</li>
<li>Business reporting or data warehousing scenarios; you may want business reporting queries to run against a Read Replica, rather than your primary, production DB Instance.</li>
</ul>
</li>
<li>Supported Versions<ul>
<li>MySQL</li>
<li>PostgreSQL</li>
<li>MariaDB<ul>
<li>For all 3 Amazon uses these engines native asynchronous replication to update the read replica</li>
</ul>
</li>
<li>Aurora<ul>
<li>Aurora employees an SSD-backed virtualized storage layer purpose-built for database workloads. Amazon Aurora replica share the same underlying storage as the source instance, lowering costs and avoiding the need to copy data to the replica nodes.</li>
</ul>
</li>
</ul>
</li>
<li>Creating Read Replicas<ul>
<li>When creating a new Read Replica, AWS will take a snapshot of your database.</li>
<li>If Multi-AZ is not enabled:<ul>
<li>This snapshot will be of your primary database and can cause brief I/O suspension for around 1 minute.</li>
</ul>
</li>
<li>If Multi-AZ is enabled:<ul>
<li>The snapshot will be of your secondary database and you will not experience any performance hits on your primary database.</li>
</ul>
</li>
</ul>
</li>
<li>Connecting to Read Replica<ul>
<li>When a new read replica is created you will be able to connect to it using a new end point DNS address.</li>
</ul>
</li>
<li>Read Replica’s Can Be Promoted<ul>
<li>You can promote a read replica to it’s own standalone database. Doing this will break the replication link between the primary and the secondary.</li>
</ul>
</li>
<li>Exam Tips - 1<ul>
<li>You can have up to 5 read replicas for MySQL, PostgreSQL &amp; MariaDB</li>
<li>You can have read replicas in different Regions for all engines</li>
<li>Replication is Asynchronous</li>
<li>Read Replica’s can be built off Multi-AZ’s databases</li>
<li>But Read Replica’s themselves cannot be Multi-AZ currently</li>
<li>You can have Read Replica’s of Read Replica’s beware of latency</li>
<li>DB Snapshots and Automated backups cannot be taken of read replicas</li>
<li>Key Metric to look for is Replica Lag</li>
<li><strong>Know the difference between read replicas and multi-AZ</strong></li>
</ul>
</li>
<li>Exam Tips - 2<ul>
<li>If you can’t create a Read Replica, you most likely have disabled Database backups. Modify the database and turn them on.</li>
<li>You can create read replicas of read replicas in multiple Regions</li>
<li>You can either modify the database itself or create a new database from a snapshot</li>
<li>Endpoints will NOT change if you modify a database, they will change if you create a new database from a snap or if you create a read replica</li>
<li>You can manually fail over a Multi-AZ database from one AZ to another by rebooting it.</li>
</ul>
</li>
</ul>
</li>
<li><p>Bastion Hosts &amp; High Availability</p>
<ul>
<li>What is a Bastion Host<ul>
<li>A bastion host is a security measure that you can implement which acts as a gateway between you and your EC2 instances. The bastion host helps to reduce attack vectors on your infrastructure and means that you only have to harden 1 or 2 EC2 instances, rather than your entire fleet.</li>
</ul>
</li>
</ul>
</li>
<li><p>Troubleshooting Autoscaling</p>
<ul>
<li>Instances not launching in to Autoscaling Groups<br>  Below is a list of things to look for if your instances are not launching in to an autoscaling group:<ul>
<li>Associate Key Pair does not exist</li>
<li>Security group does not exist</li>
<li>Autoscaling config is not working correctly</li>
<li>Autoscaling group not found</li>
<li>Instance type specified is not supported in the AZ</li>
<li>AZ is no longer supported</li>
<li>Invalid EBS device mapping</li>
<li>Autoscaling service is not enabled on your account</li>
<li>Attempting to attach and EBS block device to an instance-store AMI</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Deployment-and-Provisioning"><a href="#Deployment-and-Provisioning" class="headerlink" title="Deployment and Provisioning"></a>Deployment and Provisioning</h4><ul>
<li><p>Services with root/admin access to Operating System</p>
<ul>
<li>This is a very popular question in both the Solutions Architect Associate Exam &amp; the SysOps Administrator Associate Exam<ul>
<li>Elastic Beanstalk</li>
<li>Elastic MapReduce</li>
<li>OpsWork</li>
<li>EC2</li>
</ul>
</li>
<li>Exam Tips: There’s a good change this will be worth 1 point in your exam, so remember these services. Remember that you do not have access to RDS, DynamoDB, S3 or Glacier</li>
</ul>
</li>
<li><p>Elastic Load Balancer Configurations</p>
<ul>
<li>Exam Tips<ul>
<li>You can use Elastic Load Balancers to load balance across different availability zones within the same region, but not to different regions (or different VPC’s) themselves.</li>
<li>An ELB and a NAT are different things entirely.</li>
</ul>
</li>
<li>Two types of ELB<ul>
<li>External Elastic Load Balancers (with external DNS names)</li>
<li>Internal Elastic Load Balancers (with internal DNS names)</li>
</ul>
</li>
<li>Health Check - Recap<br>  <img src="/images/AWS/Sysops/EB_healthy_check_recap.jpg" alt="pic_need_to_place"></li>
<li>Sticky Sessions<br>  By default, a load balancer routes each request independently to the application instance with the smallest load.<br>  However, you can use the sticky session feature (also known as session affinity), which enables the load balancer to lock a user down to a specific web server (EC2 instance).<br>  This ensures that all requests from the user during the session are always sent to the same server.<br>  The key to managing sticky sessions is to determine how long your load balancer should consistently route the user’s request to the same application instance.<ul>
<li>Sticky Session Types<ul>
<li>Duration Based Session Stickiness</li>
<li>Application-Controlled Session Stickiness</li>
</ul>
</li>
<li>Edit in the [Port Configuration] setting in the [Description] tab.</li>
<li>Duration Based<br>  Most commonly used. The load balancer itself creates the session cookie.<br>  When the load balancer receives a request, it first checks to see if this cookie is present in the request. If so, the request is sent to the application instance specified in the cookie. If there is no cookie, the load balancer chooses an application instance based on the existing load balancing algorithm and adds a new cookie in to the response.<br>  The stickiness policy configuration defines a cookie expiration, which established the duration of validity for each cookie. The cookie is automatically updated after its duration expires.<br>  If an application instance fails or becomes unhealthy, the load balancer stops routing request to that instance, instead chooses a new instance based on the existing load balancing algorithm. The request is routed to the new instance as if there is no cookie and the session is no longer sticky.</li>
<li>Application Controlled<br>  The load balancer uses a special cookie to associate the session with the instance that handled the initial request, but follows the lifetime of the application cookie specified in the policy configuration. The load balancer only inserts a new stickiness cookie if the application response includes a new application cookie. The load balancer stickiness cookie does not update with each request. If the application cookie is explicitly removed or expires, the session stops being sticky until a new application cookie is issued.<br>  If an instance fails or becomes unhealthy, the load balancer stops routing requests to that instance, and chooses a new healthy instance based on the existing load balancing algorithm. The load balancer treats the session as now “stuck” to the new healthy instance, and continues routing requests to that instance even if the failed instance comes back. However, it is up to the new application instance whether and how to respond to a session which it has not previously seen.</li>
</ul>
</li>
</ul>
</li>
<li><p>Pre-warming the Elastic Load Balancer</p>
<ul>
<li>What is ‘Pre-warming’ for ELB<ul>
<li>AWS Staff can pre-configure the load balancer to have the appropriate level of capacity based on expected traffic.</li>
<li>This is used in certain scenarios, such as when flash traffic is expected, or in the case where a load test cannot be configured to gradually increase traffic.</li>
<li>This can be done by contacting AWS staff prior to the expected event. You will need to know the start and end dates of your expected flash traffic or test, the expected request rate per second and the total size of the typical request/response that you will be experiencing.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Data-Management"><a href="#Data-Management" class="headerlink" title="Data Management"></a>Data Management</h4><ul>
<li><p>Disaster Recovery</p>
<ul>
<li>Back Up &amp; Disaster Recovery is quite a key component of the SysOps Exam. It’s important to understand the key concepts well. You can also want to read the following white paper: <a href="https://media.amazonwebservices.com/AWS_Disaster_Recovery.pdf" target="_blank" rel="external">https://media.amazonwebservices.com/AWS_Disaster_Recovery.pdf</a></li>
<li>What is Disaster Recovery<ul>
<li>Disaster recovery (DR) is about preparing for and recovering from a disaster. Any event that has a negative impact on a company’s business continuity or finances could be termed a disaster. This includes hardware or software failure, a network outage, a power outage, physical damage to a building like fire or flooding, human error, or some other significant event.</li>
</ul>
</li>
<li>Traditional Approaches to DR<ul>
<li>A traditional approach to DR usually involves an N+1 approach and has different levels of off-site duplication of data and infrastructure.<ul>
<li>Facilities to house the infrastructure, including power and cooling</li>
<li>Security to ensure the physical protection of assets</li>
<li>Suitable capacity to scale the environment</li>
<li>Support for repairing, replacing, and refreshing the infrastructure</li>
<li>Contractual agreements with an Internet service provider (ISP) to provide Internet connectivity that can sustain bandwidth utilization for the environment under a full load</li>
<li>Network infrastructure such as firewalls, routers, switches, and load balancers</li>
<li>Enough server capacity to run all mission-critical services, including storage appliances for the supporting data, and servers to run applications and backend services such as user authentication, Domain Name System (DNS)</li>
<li>Dynamic Host Configuration Protocol (DHCP), monitoring, and alerting</li>
</ul>
</li>
</ul>
</li>
<li>Why use aws for DR<ul>
<li>Only minimum hardware is required for ‘data replication’</li>
<li>Allows you to be flexible depending on what your disaster is and how to recover from it</li>
<li>Open cost model (pay as you use) rather than heavy investment upfront. Scaling is quick and easy</li>
<li>Automate disaster recovery deployment</li>
</ul>
</li>
<li>What Services<ul>
<li>Regions</li>
<li>Storage</li>
<li>S3 - 99.999999999% durability and Cross Region Replication</li>
<li>Glacier</li>
<li>Elastic Block Store (EBS)</li>
<li>Direct Connect</li>
<li>AWS Storage Gateway</li>
<li>Gateway-cached volumes - store primary data and cache most recently used data locally.</li>
<li>Gateway-stored volumes - store entire dataset on site and asynchronously replicate data back to S3</li>
<li>Gateway-virtual tape library - Store your virtual tapes in either S3 or Glacier</li>
<li>Compute<ul>
<li>EC2</li>
<li>EC2 VM Import Connector - Virtual appliance which allows you to import virtual machine images from your existing environment to Amazon EC2 instances.</li>
</ul>
</li>
<li>Networking<ul>
<li>Route53</li>
<li>Elastic Load Balancing</li>
<li>Amazon Virtual Private Cloud (VPC)</li>
<li>Amazon Direct Connect</li>
</ul>
</li>
<li>Database<ul>
<li>RDS</li>
<li>DynamoDB</li>
<li>Redshift</li>
</ul>
</li>
<li>Orchestration<ul>
<li>CloudFormation</li>
<li>ElasticBeanstalk</li>
<li>OpsWork</li>
</ul>
</li>
<li>Lambda</li>
</ul>
</li>
<li>RTO vs RPO<ul>
<li>Recovery Time Objective (RTO)<ul>
<li>RTO is the length of time from which you can recover from a disaster. It is measured from when the disaster first occurred as to when you have fully recovered from it.</li>
</ul>
</li>
<li>Recovery Point Objective (RPO)<ul>
<li>RPO is the amount of data your organization is prepared to lose in the event of a disaster (1 days worth of emails, 5 hours of online transaction records etc, 24 hours of backup etc)</li>
</ul>
</li>
<li>Typically the lower RTO &amp; RPO threshold, the more costly it solution will be.</li>
</ul>
</li>
<li>DR Scenarios<ul>
<li>Four Scenarios<ul>
<li>Backup &amp; Restore</li>
<li>Pilot Light</li>
<li>Warm Standby</li>
<li>Multi Site</li>
</ul>
</li>
<li>Backup &amp; Restore<ul>
<li>In most traditional environments, data is backed up to tape and sent off-site regularly. If you use this method, it can take a long time to restore your system in the event of a disruption or disaster. Amazon S3 is an ideal destination for backup data that might be needed quickly to perform a restore. Transferring data to and from Amazon S3 is typically done through the network, and is therefore accessible from any location.</li>
<li>You can use AWS Import/Export to transfer very large data sets by shipping storage devices directly to AWS. For longer-term data storage where retrieval times of several hours are adequate, there is Amazon Glacier, which has the same durability model as Amazon S3. . Amazon Glacier and Amazon S3 can be used in conjunction to produce a tiered backup solution.</li>
<li>Data Backup Options to Amazon S3 from On-Site Infrastructure or from AWS.<br>  <img src="/images/AWS/Sysops/data_backup_options_to_S3_from_on_site_infrastructure_from_aws.jpg" alt="data_backup_options_to_S3_from_on_site_infrastructure_from_aws"></li>
<li>Restoring a System from Amazon S3 Backups to Amazon EC2<br>  <img src="/images/AWS/Sysops/restoring_a_system_from_amazon_s3_backups_to_amazon_ec2.jpg" alt="restoring_a_system_from_amazon_s3_backups_to_amazon_ec2"></li>
<li>Key steps for backup &amp; restore<ul>
<li>Select an appropriate tool or method to back up your data into AWS.</li>
<li>Ensure that you have an appropriate retention policy for this data.</li>
<li>Ensure that appropriate security measures are in place for this data, including encryption and access policies.</li>
<li>Regularly test the recovery of this data and the restoration of your system.</li>
</ul>
</li>
</ul>
</li>
<li>Pilot Light<ul>
<li>The term pilot light is often used to describe a DR scenario in which a minimal version of an environment is always running in the cloud. The idea of the pilot light is an analogy that comes from the gas heater. In a gas heater, a small flame that’s always on can quickly ignite the entire furnace to heat up a house.</li>
<li>This scenario is similar to a backup-and-restore scenario. For example, with AWS you can maintain a pilot light by configuring and running the most critical core elements of your system in AWS. When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core.</li>
<li>Infrastructure elements for the pilot light itself typically include your database servers, which would replicate data to Amazon EC2 or Amazon RDS. Depending on the system, there might be other critical data outside of the database that needs to be replicated to AWS. This is the critical core of the system (the pilot light) around which all other infrastructure pieces in AWS (the rest of the furnace) can quickly be provisioned to restore the complete system.</li>
<li>To provision the remainder of the infrastructure to restore business-critical services, you would typically have some preconfigured servers bundled as Amazon Machine Images (AMIs), which are ready to be started up at a moment’s notice.  When starting recovery, instances from these AMIs come up quickly with their pre-defined role (for example, Web or App Server) within the deployment around the pilot light.</li>
<li>From a networking point of view, you have two main options for provisioning:<ul>
<li>Use pre-allocated elastic IP address and associate them with your instances when invoking DR. You can also use pre-allocated elastic network interfaces (ENIs) with pre-allocated Mac Addresses for applications with special licensing requirements</li>
<li>Use Elastic Load Balancing (ELB) to distribute traffic to multiple instances. You would then update your DNS records to point at your Amazon EC2 instance or point to your load balancer using a CNAME</li>
</ul>
</li>
<li>Preparation phase<ul>
<li>The Preparation Phase of the Pilot Light Scenario<br>  <img src="/images/AWS/Sysops/the_preparation_phase_of_the_pilot_light_scenario.jpg" alt="the_preparation_phase_of_the_pilot_light_scenario"></li>
<li>Key steps for preparation:<ul>
<li>Set up Amazon EC2 instances to replicate or mirror data.</li>
<li>Ensure that you have all supporting custom software packages available in AWS.</li>
<li>Create and maintain AMIs of key servers where fast recovery is required.</li>
<li>Regularly run these servers, test them, and apply any software updates and configuration changes.</li>
<li>Consider automating the provisioning of AWS resources</li>
</ul>
</li>
</ul>
</li>
<li>Recovery phase<ul>
<li>The Recovery Phase of the Pilot Light Scenario<br>  <img src="/images/AWS/Sysops/the_recovery_phase_of_the_pilot_light_scenario.jpg" alt="the_recovery_phase_of_the_pilot_light_scenario"></li>
<li>Key steps for recovery:<ul>
<li>Start your application Amazon EC2 instances from your custom AMIs.</li>
<li>Resize existing database/data store instances to process the increased traffic.</li>
<li>Add additional database/data store instances to give the DR site resilience in the data tier; if you are using Amazon RDS, turn on Multi-AZ to improve resilience.</li>
<li>Change DNS to point at the Amazon EC2 servers.</li>
<li>Install and configure any non-AMI based systems, ideally in an automated way.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Warm Standby<ul>
<li>The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud. A warm standby solution extends the pilot light elements and preparation. It further decreases the recovery time because some services are always running. By identifying your business-critical systems, you can fully duplicate these systems on AWS and have them always on.</li>
<li>These servers can be running on a minimum-sized fleet of Amazon EC2 instances on the smallest sizes possible. This solution is not scaled to take a full-production load, but it is fully functional. It can be used for non-production work, such as testing, quality assurance, and internal use.</li>
<li>In a disaster, the system is scaled up quickly to handle the production load. In AWS, this can be done by adding more instances to the load balancer and by resizing the small capacity servers to run on larger Amazon EC2 instance types.</li>
<li>Horizontal scaling is preferred over vertical scaling</li>
<li>Preparation phase<ul>
<li>The Preparation Phase of the Warm Standby Scenario<br>  <img src="/images/AWS/Sysops/the_preparation_phase_of_the_warm_standby_scenario.jpg" alt="the_preparation_phase_of_the_warm_standby_scenario"></li>
<li>Key steps for preparation:<ul>
<li>Set up Amazon EC2 instances to replicate or mirror data.</li>
<li>Create and maintain AMIs.</li>
<li>Run your application using a minimal footprint of Amazon EC2 instances or AWS infrastructure.</li>
<li>Patch and update software and configuration files in line with your live environment.</li>
</ul>
</li>
</ul>
</li>
<li>Recovery phase<ul>
<li>The Recovery Phase of the Warm Standby Scenario<br>  <img src="/images/AWS/Sysops/the_recovery_phase_of_the_warm_standby_scnario.jpg" alt="the_recovery_phase_of_the_warm_standby_scnario"></li>
<li>Key steps for recovery:<ul>
<li>Increase the size of the Amazon EC2 fleets in service with the load balancer (horizontal scaling).</li>
<li>Start applications on larger Amazon EC2 instance types as needed (vertical scaling).</li>
<li>Either manually change the DNS records, or use Amazon Route 53 automated health checks so that all traffic is routed to the AWS environment.</li>
<li>Consider using Auto Scaling to right-size the fleet or accommodate the increased load.</li>
<li>Add resilience or scale up your database.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Multi Site<ul>
<li>A multi-site solution runs in AWS as well as on your existing on-site infrastructure, in an active-active configuration. The data replication method that you employ will be determined by the recovery point that you choose.</li>
<li>You can use Route53 to root traffic to both sites either symmetrically or asymmetrically.</li>
<li>In an on-site disaster situation, you can adjust the DNS weighting and send all traffic to the AWS servers. The capacity of the AWS service can be rapidly increased to handle the full production load. You can use Amazon EC2 Auto Scaling to automate this process. You might need some application logic to detect the failure of the primary database services and cut over to the parallel database services running in AWS.</li>
<li>Preparation phase<ul>
<li>The Preparation Phase of the Multi-Site Scenario<br>  <img src="/images/AWS/Sysops/the_preparation_phase_of_the_multi_site_scenario.jpg" alt="the_preparation_phase_of_the_multi_site_scenario"></li>
<li>Key steps for preparation:<ul>
<li>Set up your AWS environment to duplicate your production environment.</li>
<li>Set up DNS weighting, or similar traffic routing technology, to distribute incoming requests to both sites.  Configure automated failover to re-route traffic away from the affected site.</li>
</ul>
</li>
</ul>
</li>
<li>Recovery phase<ul>
<li>The Recovery Phase of the Multi-Site Scenario Involving On-Site and AWS Infrastructure.<br>  <img src="/images/AWS/Sysops/the_recovery_phase_of_the_multi_site_scenario_involving_on_site_and_aws_infrastructure.jpg" alt="the_recovery_phase_of_the_multi_site_scenario_involving_on_site_and_aws_infrastructure"></li>
<li>Key steps for recovery:<ul>
<li>Either manually or by using DNS failover, change the DNS weighting so that all requests are sent to the AWS site.</li>
<li>Have application logic for failover to use the local AWS database servers for all queries.</li>
<li>Consider using Auto Scaling to automatically right-size the AWS fleet.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Failing Back<ul>
<li>Backup and restore<ol>
<li>Freeze data changes to the DR site</li>
<li>Take a backup</li>
<li>Restore the backup to the primary site</li>
<li>Re-point users to the primary site</li>
<li>Unfreeze the changes</li>
</ol>
</li>
<li>Pilot light, warm standby, and multi-site:<ol>
<li>Establish reverse mirroring/replication from the DR site back to the primary site, once the primary site has caught up with the changes.</li>
<li>Freeze data changes to the DR site</li>
<li>Re-point users to the primary site</li>
<li>Unfreeze the changes</li>
</ol>
</li>
</ul>
</li>
<li>Exam Tips<br>  Pay particular attention to the Pilot Light Scenario and specifically the fact that you can have Elastic Network Interfaces (ENI’s) with preconfigured MAC addresses.</li>
</ul>
</li>
</ul>
</li>
<li><p>AWS Services and Automated Backups</p>
<ul>
<li>Services that have Automated Backup<ul>
<li>RDS</li>
<li>Elasticache (Redis only)</li>
<li>Redshift</li>
</ul>
</li>
<li>Services that do not have Automated Backup<ul>
<li>EC2</li>
</ul>
</li>
<li>RDS Automatd Backups<ul>
<li>For MySQL you need innoDB (transactional engine)</li>
<li>There is a performance hit if Multi-AZ is not enabled</li>
<li>If you delete an instance, then ALL automated backups are deleted</li>
<li>However, manual DB snapshots will NOT be deleted</li>
<li>All stored on S3</li>
<li>When you do a restore, you can change the engine type (SQL Standard to SQL Enterprise for example). Provided you have enough storage space</li>
</ul>
</li>
<li>Elasticache Backups<ul>
<li>Available for Redis Cache Cluster only</li>
<li>The entire cluster is snapshotted</li>
<li>Snapshot will degrade performance</li>
<li>Therefore only set your snapshot window during the least busy part of the day</li>
<li>Stored on S3</li>
</ul>
</li>
<li>Redshift Backups<ul>
<li>Stored on S3</li>
<li>By default, Amazon Redshift enables automated backups of your data warehouse cluster with a 1-day retention period</li>
<li>Amazon Redshift only backs up data that has changed so most snapshots only use up a small amount of your free backup storage</li>
</ul>
</li>
<li>EC2<ul>
<li>No automated backups</li>
<li>Backups degrade you performance, schedule these times wisely</li>
<li>Snapshots are stored in S3</li>
<li>Can create automated backups using either the command line interface or Python</li>
<li>They are incremental:<ul>
<li>Snapshots only store incremental changes since last snapshot</li>
<li>Only charged for incremental storage</li>
<li>Each snapshot still contains the base snapshot data</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>EC2 Type - EBS vs Instance Store</p>
<ul>
<li>History<ul>
<li>When EC2 was first launched, all AMI’s were backed by Amazon’s Instance Store. Instance store is known as ephemeral storage, which simply means non-persistence or temporary storage.</li>
<li>Later on AWS launched EBS, Elastic Block Storage, which allows users to have data persistence and to save their data permanently.</li>
</ul>
</li>
<li>Confusion<ul>
<li>There is a lot of confusion between instance store volumes and EBS volumes in the AWS community and you need to have a good understanding of the differences between the two. Let’s start with volumes. There are two types of volumes:<ul>
<li>Root Volume (this is where your operating system is installed)</li>
<li>Additional Volumes (this can be your D:\ E:\ F:\ or /dev/sdb, /dev/sdc, /dev/sdd etc)</li>
</ul>
</li>
</ul>
</li>
<li>Route Volume Sizes<ul>
<li>Root device volumes can either be EBS volumes or Instance Store volumes</li>
<li>An Instance store root device volume’s maximum size is 10Gb</li>
<li>EBS root device volume can be up to 1 or 2Tb depending on the OS</li>
</ul>
</li>
<li>Terminating an Instance - EBS<ul>
<li>EC2 Instances can be terminated:<ul>
<li>EBS root device volumes are terminated by DEFAULT when the EC2 instance is terminated. You can stop this b unselecting the “Delete on Termination” option when creating the instance or by setting the deleteontermination flag to false using the command line</li>
<li>Other EBS volumes attached to the instance are preserved however, if you delete the instance</li>
<li>Instance store device root volumes are terminated by DEFAULT when the EC2 instance is terminated. You cannot stop this</li>
<li>Other instance store volumes will be deleted on termination automatically</li>
<li>Other EBS volumes attached to the EC2 instance will persist automatically</li>
</ul>
</li>
</ul>
</li>
<li>Stopping an Instance<ul>
<li>EBS backed instances can be stopped</li>
<li>Instance Store backed instances CANNOT be stopped. Only rebooted or terminated</li>
</ul>
</li>
<li>Instance Store Data<ul>
<li>The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists. However, data on instance store volumes is lost under the following circumstances:<ul>
<li>Failure of an underlying drive</li>
<li>Stopping an Amazon EBS-backed instance</li>
<li>Terminating an instance</li>
</ul>
</li>
</ul>
</li>
<li>Instance Store<ul>
<li>Therefore, do not rely on instance store volumes for valuable, long-term data. Instead, keep your data safe by using a replication strategy across multiple instances, storing data in Amazon S3, or using Amazon EBS volumes.</li>
</ul>
</li>
<li>Comparison EBS vs Instance Store<br>  <img src="/images/AWS/Sysops/comparison_EBS_InstanceStore.jpg" alt="comparison_EBS_InstanceStore"></li>
<li>Exam Tips<ul>
<li>‘Delete on Termination’ is the default for all EBS root device volumes. You can set this to false however but only at instance creation time</li>
<li>Additional volumes will persist automatically. You need to delete these manually when you delete an instance</li>
<li>Instance Store is known as ephemeral storage, meaning that data will not persist after an instance is deleted. You cannot set this to false, data will always be deleted when that instance disappears</li>
</ul>
</li>
</ul>
</li>
<li><p>Upgrading EBS volume types - Exam tips</p>
<ul>
<li>EBS volumes can be changed on the fly (except for magnetic standard).</li>
<li>Best practice to stop the EC2 instance and then change the volume</li>
<li>You can change volume types by taking a snapshot and then using the snapshot to create a new volume</li>
<li>If you change a volume on the fly you must wait for 6 hours before making another change</li>
<li>You can scale EBS Volumes up only</li>
<li>Volumes must be in the same AZ as the EC2 instances.</li>
</ul>
</li>
<li><p>Storing Log Files &amp; Backups</p>
<ul>
<li>Centralized Monitoring &amp; Logging<ul>
<li>You can monitor your environment in a number of ways:<ul>
<li>Using a third party, centralized monitoring platform, such as Zennos, Splunk, RSyslog, Kiwi etc. These logs can then be stored on S3.</li>
<li>Using CloudWatch Logs (relatively new service)</li>
<li>Utilizing Access Logging on S3 (S3 only)</li>
</ul>
</li>
</ul>
</li>
<li>Suggested to use S3<ul>
<li>The best place to store your logs is probably S3:<ul>
<li>99.999999999% durability</li>
<li>Life cycle management</li>
<li>Archive off to Glacier</li>
<li>Effectively allows you to Tier your back up solution</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="OpsWorks"><a href="#OpsWorks" class="headerlink" title="OpsWorks"></a>OpsWorks</h4><ul>
<li><p>What is OpsWorks</p>
<ul>
<li>Cloud-based applications usually require a group of related resources—application servers, database servers, and so on—that must be created and managed collectively. This collection of instances is called a stack. A simple application stack might look something like the following.<br><img src="/images/AWS/Sysops/what_is_opswork.png" alt="what_is_opswork"></li>
<li>AWS OpsWorks Stacks provides a simple and straightforward way to create and manage stacks and their associated applications and resources</li>
<li>Amazon Definition:<ul>
<li>AWS OpsWorks is an application management service that helps you automate operational tasks like code deployment, software configurations, package installations, database setups, and server scaling using Chef. OpsWorks gives you the flexibility to define your application architecture and resource configuration and handles the provisioning and management of your AWS resources for you. OpsWorks includes automation to scale your application based on time or load, monitoring to help you troubleshoot and take automated action based on the state of your resources, and permissions and policy management to make management of multi-user environments easier.</li>
</ul>
</li>
</ul>
</li>
<li><p>What is Chef</p>
<ul>
<li>Chef turns infrastructure into code. With Chef, you can automate how you build, deploy, and manage your infrastructure. Your infrastructure becomes as versionable, testable, and repeatable as application code.</li>
<li>Chef server stores your recipes as well as other configuration data. The Chef client is installed on each server, virtual machine, container, or networking device you manage - we’ll call these nodes. The client periodically polls Chef server latest policy and state of your network. If anything on the node is out of date, the client brings it up to date.</li>
</ul>
</li>
<li><p>What is OpsWorks</p>
<ul>
<li>A GUI to deploy and configure your infrastructure quickly. OpsWorks consists of two elements, Stacks and Layers.</li>
<li>A stack is a container (or group) of resources such as ELBS, EC2 instances, RDS instances etc.</li>
<li>A layer exists within a stack and consists of things like a web application layer. An application processing layer or a Database layer.</li>
<li>When you create a layer, rather than going and configuring everything manually (like installing Apache, PHP etc) OpsWorks takes care of this for you.</li>
</ul>
</li>
<li><p>Layers</p>
<ul>
<li>1 or more layers in the stack</li>
<li>An instance must be assigned to at least 1 layer</li>
<li>Which chef layers run, are determined by the layer the instance belongs to</li>
<li>Preconfigured Layers include:<ul>
<li>Applications</li>
<li>Databases</li>
<li>Load Balancers</li>
<li>Caching</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h4><ul>
<li>Security Token Service (STS)<ul>
<li>what is Security Token Service (STS)<ul>
<li>Grants users limited and temporary access to AWS resources. Users can come from three sources:<ul>
<li>Federation (typically Active Directory)<ul>
<li>Uses Security Assertion Markup Language (SAML)</li>
<li>Grants temporary access based off the users Active Directory credentials. Does not need to be a user in IAM</li>
<li>Single sign on allows users to log in to AWS console without assigning IAM credentials</li>
</ul>
</li>
<li>Federation with Mobile Apps<ul>
<li>Use Facebook/Amazon/Google or other OpenID providers to log in.</li>
</ul>
</li>
<li>Cross Account Access<ul>
<li>Let’s users from one AWS account access resources in another</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Understanding Key Terms<ul>
<li>Federation: combining or joining a list of users in one domain (such as IAM) with a list of users in another domain (such as Active Directory, Facebook etc)</li>
<li>Identity Broker: a service that allows you to take an identity from point A and join it (federate it) to point B</li>
<li>Identity Store - Services like Active Directory, Facebook, Google etc</li>
<li>Identities - a user of a service like Facebook etc.</li>
</ul>
</li>
<li>Scenario<ul>
<li>AWS blog [<a href="https://aws.amazon.com/cn/blogs/aws/aws-identity-and-access-management-now-with-identity-federation/" target="_blank" rel="external">AWS Identity and Access Management – Now With Identity Federation</a>]</li>
<li>You are hosting a company website on some EC2 web servers in your VPC. Users of the website must log in to the site which then authenticates against the companies active directory servers which are based on site at the companies head quarters. Your VPC is connected to your company HQ via a secure IPSEC VPN. Once logged in the user can only have access to their own S3 bucket. How do you set this up?<br>  <img src="/images/AWS/Sysops/sts_scenario.jpg" alt="sts_scenario"></li>
<li>Steps of scenario<ul>
<li>Employee enters their username and password</li>
<li>The application calls an Identity Broker. The broker captures the username and password.</li>
<li>The Identity Broker uses the organization’s LDAP directory to validate the employee’s identity.</li>
<li>The Identity Broker calls the new GetFederationToken function using IAM credentials. The call must include an IAM policy and a duration (1 to 36 hours), along with a policy that  specifies the permissions to be granted to the temporary security credentials.</li>
<li>The Security Token Service confirms that the policy of the IAM user making the call to GetFederationToken gives permission to create new tokens and then returns four values to the application: An access key, a secret access key, a token, and a duration (the token’s lifetime).</li>
<li>The Identity Broker returns the temporary security credentials to the reporting application.</li>
<li>The data storage application uses the temporary security credentials (including the token) to make requests to Amazon S3.</li>
<li>Amazon S3 uses IAM to verify that the credentials allow the requested operation on the given S3 bucket and key</li>
<li>IAM provides S3 with the go-ahead to perform the requested operation.</li>
</ul>
</li>
<li>In the Exam<ul>
<li>Develop an Identity Broker to communicate with LDAP and AWS STS</li>
<li>Identity Broker always authenticates with LDAP first, Then with AWS STS</li>
<li>Application then gets temporary access to AWS resources</li>
</ul>
</li>
</ul>
</li>
<li>Scenario 2<ul>
<li>steps of scenario<ul>
<li>Develop an Identity Broker to communicate with LDAP and AWS STS</li>
<li>Identity Broker always authenticates with LDAP first, gets an IAM Role associate with a user</li>
<li>Application then authenticates with STS and assumes that IAM Role</li>
<li>Application uses that IAM role to interact with S3</li>
</ul>
</li>
<li>In the Exam<ul>
<li>Develop an Identity Broker to communicate with LDAP and AWS STS</li>
<li>Identity Broker always authenticates with LDAP first, Then with AWS STS</li>
<li>Application then gets temporary access to AWS resources</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Overview of Security Processes</p>
<ul>
<li><a href="https://d0.awsstatic.com/whitepapers/aws-security-whitepaper.pdf" target="_blank" rel="external">aws-security-whitepaper</a></li>
<li><p>Shared Security Model</p>
<ul>
<li>AWS is responsible for securing the underlying infrastructure that supports the cloud. You’re responsible for anything you put on the cloud or connect to the cloud.<br>  <img src="/images/AWS/Sysops/aws_shared_security_responsibility_model.jpg" alt="aws_shared_security_responsibility_model"></li>
</ul>
</li>
<li><p>AWS Security Responsibilities</p>
<ul>
<li>Amazon Web Services is responsible for protecting the global infrastructure that runs all of the services offered in the AWS cloud. This infrastructure is comprised of the hardware, software, networking, and facilities that run AWS services.</li>
<li>AWS is responsible for the security configuration of its products that are considered managed services. Examples of these types of services include Amazon DynamoDB, Amazon RDS, Amazon Redshift, Amazon Elastic MapReduce, Amazon WorkSpaces</li>
</ul>
</li>
<li>Customer Security Responsibilities<ul>
<li>IAAS - —such as Amazon EC2, Amazon VPC, and Amazon S3—are completely under your control and require you to perform all of the necessary security configuration and management tasks.</li>
<li>Managed Services at AWS is responsible for patching, antivirus etc, however you are responsible for account management and user access. Its recommended that MFA be implemented, communicate to these services using SSL/TLS and that API/user activity logging be setup with CloudTrail.</li>
</ul>
</li>
<li>Storage Decommissioning<ul>
<li>When a storage device has reached the end of its useful life, AWS procedures include a decommissioning process that is designed to prevent customer data from being exposed to unauthorized individuals. AWS uses the techniques detailed in DoD 5220.22-M (“National Industrial Security Program Operating Manual “) or NIST 800-88 (“Guidelines for Media Sanitization”) to destroy data as part of the decommissioning process. All decommissioned magnetic storage devices are degaussed and physically destroyed in accordance with industry-standard practices.</li>
</ul>
</li>
<li>Network Security<ul>
<li>Transmission Protection<ul>
<li>You can connect to an AWS access point via HTTP or HTTPS using Secure Sockets Layer (SSL), a cryptographic protocol that is designed to protect against eavesdropping, tampering, and message forgery</li>
<li>For customers who require additional layers of network security, AWS offers the Amazon Virtual Private Cloud (VPC), which provides a private subnet within the AWS cloud, and the ability to use an IPsec Virtual Private Network (VPN) device to provide an encrypted tunnel between the Amazon VPC and your data center.</li>
</ul>
</li>
<li>Amazon Corporate Segregation<ul>
<li>Logically, the AWS Production network is segregated from the Amazon Corporate network by means of a complex set of network security / segregation devices.</li>
</ul>
</li>
</ul>
</li>
<li>Network Monitoring &amp; Protection<ul>
<li>Types<ul>
<li>DDos</li>
<li>Man in the middle attacks (MITM)</li>
<li>Ip Spoofing</li>
<li>Port Scanning</li>
<li>Packet Sniffing by other tenants</li>
</ul>
</li>
<li>Ip Spoofing<ul>
<li>The AWS-controlled, host-based firewall infrastructure will not permit an instance to send traffic with a source IP or MAC address other than its own.</li>
</ul>
</li>
<li>Port Scanning<ul>
<li>Unauthorized port scans by Amazon EC2 customers are a violation of the AWS Acceptable Use Policy. . You may request permission to conduct vulnerability scans as required to meet your specific compliance requirements. These scans must be limited to your own instances and must not violate the AWS Acceptable Use Policy. <strong>You must request a vulnerability scan in advance</strong>.</li>
</ul>
</li>
</ul>
</li>
<li>AWS Credentials<br>  <img src="/images/AWS/Sysops/aws_security_credentials.jpg" alt="aws_security_credentials"></li>
<li>AWS Trusted Advisor<ul>
<li>Trusted Advisor inspects your AWS environment and makes recommendations when opportunities may exist to save money, improve system performance, or close security gaps. It provides alerts on several of the most common security misconfigurations that can occur, including leaving certain ports open that make you vulnerable to hacking and unauthorized access, neglecting to create IAM accounts for your internal users, allowing public access to Amazon S3 buckets, not turning on user activity logging (AWS CloudTrail), or not using MFA on your root AWS Account.</li>
</ul>
</li>
<li>Instance Isolation<ul>
<li>Different instances running on the same physical machine are isolated from each other via the Xen hypervisor. Amazon is active in the Xen community, which provides awareness of the latest developments. In addition, the AWS firewall resides within the hypervisor layer, between the physical network interface and the instance’s virtual interface. All packets must pass through this layer, thus an instance’s neighbors have no more access to that instance than any other host on the Internet and can be treated as if they are on separate physical hosts. The physical RAM is separated using similar mechanisms.</li>
<li>Customer instances have no access to raw disk devices, but instead are presented with virtualized disks. The AWS proprietary disk virtualization layer automatically resets every block of storage used by the customer, so that one customer’s data is never unintentionally exposed to another. In addition, memory allocated to guests is scrubbed (set to zero) by the hypervisor when it is unallocated to a guest. The memory is not returned to the pool of free memory available for new allocations until the memory scrubbing is complete.<br><img src="/images/AWS/Sysops/Amazon_EC2_Multiple_Layers_of_Security.jpg" alt="Amazon_EC2_Multiple_Layers_of_Security"></li>
</ul>
</li>
<li>Other Considerations<ul>
<li><strong>Guest Operating System</strong> -  Virtual instances are completely controlled by you, the customer. You have full root access or administrative control over accounts, services, and applications. AWS does not have any access rights to your instances or the guest OS.</li>
<li><strong>Firewall</strong> -  Amazon EC2 provides a complete firewall solution; this mandatory inbound firewall is configured in a default deny-all mode and Amazon EC2 customers must explicitly open the ports needed to allow inbound traffic.</li>
<li><strong>Elastic Block Storage(EBS) Security</strong> - Encryption of sensitive data is generally a good security practice, and AWS provides the ability to encrypt EBS volumes and their snapshots with AES-256. The encryption occurs on the servers that host the EC2 instances, providing encryption of data as it moves between EC2 instances and EBS storage. In order to be able to do this efficiently and with low latency, the EBS encryption feature is only available on EC2’s more powerful instance types (e.g., M3, C3, R3, G2).</li>
<li><strong>Elastic Load Balancing</strong> - SSL Termination on the load balancer is supported.</li>
<li>Direct Connect<ul>
<li>Bypass Internet service providers in your network path. You can procure rack space within the facility housing the AWS Direct Connect location and deploy your equipment nearby. Once deployed, you can connect this equipment to AWS Direct Connect using a cross-connect.</li>
<li>Using industry standard 802.1q VLANs, the dedicated connection can be partitioned into multiple virtual interfaces. This allows you to use the same connection to access public resources such as objects stored in Amazon S3 using public IP address space, and private resources such as Amazon EC2 instances running within an Amazon VPC using private IP space, while maintaining network separation between the public and private environments.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>AWS &amp; IT Audits</p>
<ul>
<li>Compliance<ul>
<li>SOC 1/SSAE 16/ISAE 3402 (formerly SAS 70 Type II)</li>
<li>SOC2</li>
<li>SOC3</li>
<li>FISMA, DIACAP, and FedRAMP</li>
<li>PCI DSS Level 1</li>
<li>ISO 27001</li>
<li>ISO 9001</li>
<li>ITAR</li>
<li>FIPS 140-2</li>
<li>Several industry-specific standards:<ul>
<li>HIPAA</li>
<li>Cloud Security Alliance (CSA)</li>
<li>Motion Picture Association of America (MPAA)</li>
</ul>
</li>
</ul>
</li>
<li>Auditing on AWS<ul>
<li>your organization may undergo an audit. This cloud be for PCI Compliance, ISO 27001, SOC etc. There is a level of shared responsibility in regards to audits:<ul>
<li>AWS Provides - their annual certifications and reports (ISO 27001, PCI-DSS certificates etc). Amazon are responsible the global infrastructure including all hardware, data centers, physical security etc</li>
<li>Customer provides - everything they have put on AWS, such as EC2 instances, RDS instances, Applications, Assets in S3 etc. Essentially the organizations AWS assets (this can include the data itself)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h4><ul>
<li>Route53<ul>
<li>ELB’s do not have pre-defined IPv4 addresses, you resolve to them using a DNS name.</li>
<li>Understand the difference between an Alias Record and a CNAME</li>
<li>Given the choice, always choose an Alias Record over a CNAME.</li>
<li>Remember the different routing policies and their use cases.<ul>
<li>Simple</li>
<li>Weighted</li>
<li>Latency</li>
<li>Failover</li>
<li>Geolocation</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="VPC’s"><a href="#VPC’s" class="headerlink" title="VPC’s"></a>VPC’s</h4><ul>
<li>Basic Info<ul>
<li>Think of a VPC as a logical datacenter in AWS</li>
<li>Consists of IGW’s (Or Virtual Private Gateways), Route Tables, Network Access Control Lists, Subnets, Security Groups</li>
<li>1 Subnet = 1 Availability Zone</li>
<li>Security Groups are Stateful, Network Access Control Lists are Stateless.</li>
<li>Can Peer VPCs both in the same account and with other AWS accounts.</li>
<li>No Transitive Peering</li>
<li>Custom VPC network block size has to be between a /16 netmask and /28 netmask.</li>
</ul>
</li>
</ul>
<ul>
<li>What can you do with a VPC<ul>
<li>Launch instances into a subnet of your choosing</li>
<li>Assign custom IP address ranges in each subnet</li>
<li>Configure route tables between subnets</li>
<li>Create internet gateway and attach it to our VPC</li>
<li>Much better security control over your AWS resources</li>
<li>Instance security groups</li>
<li>Subnet network access control lists (ACLS)</li>
</ul>
</li>
</ul>
<ul>
<li><p>Default VPC vs Custom VPC</p>
<ul>
<li>Default VPC is user friendly, allowing you to immediately deploy instances</li>
<li>All Subnets in default VPC have a route out to the internet.</li>
<li>Each EC2 instance has both a public and private IP address</li>
<li>If you delete the default VPC the only way to get it back is to contact AWS.</li>
</ul>
</li>
<li><p>VPC peering</p>
<ul>
<li>Allows you to connect one VPC with another via a direct network route using private IP addresses.</li>
<li>Instances behave as if they were on the same private network</li>
<li>You can peer VPC’s with other AWS accounts as well as with other VPCs in the same account.</li>
<li>Peering is in a star configuration, ie 1 central VPC peers with 4 others, <strong>NO TRANSITIVE PEERING!!!</strong></li>
</ul>
</li>
<li><p>Create VPC</p>
<ul>
<li>things automatically created<ul>
<li>Route tables</li>
<li>Network ACLs</li>
<li>Security Groups</li>
<li>DHCP options set</li>
</ul>
</li>
<li>things are not automatically created<ul>
<li>Internet Gateways</li>
<li>Subnets</li>
</ul>
</li>
</ul>
</li>
<li><p>VPC Subnet</p>
<ul>
<li>There are 5 IP address reserved in each subnet by AWS, take CIDR block 10.0.0.0/24 as example<ul>
<li>10.0.0.0 Network address</li>
<li>10.0.0.1 Reserved by AWS for the VPC router</li>
<li>10.0.0.2 Reserved by AWS for DNS</li>
<li>10.0.0.3 Reserved by AWS for future use.</li>
<li>10.0.0.255 Network broadcast address, we do not support broadcast in a VPC, therefore we reserve this address.</li>
</ul>
</li>
</ul>
</li>
<li><p>NAT instances</p>
<ul>
<li>When creating a NAT instance, Disable Source/Destination Check on the Instance</li>
<li>NAT instance must be in a public subnet</li>
<li>Must have an elastic IP address to work</li>
<li>There must be a route out of the private subnet to the NAT instance, in order for this to work</li>
<li>The amount of traffic that NAT instances supports, depends on the instance size. If you are bottlenecking, increase the instance size</li>
<li>You can create high availability using Autoscaling Groups, multiple subnets in different AZ’s and a script to automate failover</li>
<li>Behind a Security Group.</li>
</ul>
</li>
<li><p>NAT Gateways</p>
<ul>
<li>Very new</li>
<li>Preferred by the enterprise</li>
<li>Scale automatically up to 10Gbps</li>
<li>No need to patch</li>
<li>Not associated with security groups</li>
<li>Automatically assigned a public ip address</li>
<li>Remember to update your route tables.</li>
<li>No need to disable Source/Destination Checks.</li>
</ul>
</li>
<li><p>NAT instances vs NAT Gateways</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Attribute</th>
<th style="text-align:left">NAT gateway</th>
<th style="text-align:left">NAT instance</th>
</tr>
</thead>
<tbody>
<tr>
<td>Availability</td>
<td style="text-align:left">Highly available. NAT gateways in each Availability Zone are implemented with redundancy. Create a NAT gateway in each Availability Zone to ensure zone-independent architecture.</td>
<td style="text-align:left">Use a script to manage failover between instances.</td>
</tr>
<tr>
<td>Bandwidth</td>
<td style="text-align:left">Supports bursts of up to 10Gbps.</td>
<td style="text-align:left">Depends on the bandwidth of the instance type.</td>
</tr>
<tr>
<td>Maintenance</td>
<td style="text-align:left">Managed by AWS.You do not need to perform any maintenance.</td>
<td style="text-align:left">Managed by you, for example, by installing software updates or operating system patches on the instance.</td>
</tr>
<tr>
<td>Performance</td>
<td style="text-align:left">Software is optimized for handling NAT traffic.</td>
<td style="text-align:left">A generic Amazon Linux AMI that’s configured to perform NAT.</td>
</tr>
<tr>
<td>Cost</td>
<td style="text-align:left">Charged depending on the number of NAT gateways you use, duration of usage, and amount of data that you send through the NAT gateways.</td>
<td style="text-align:left">Charged depending on the number of NAT instances that you use, duration of usage, and instance type and size.</td>
</tr>
<tr>
<td>Type and size</td>
<td style="text-align:left">Uniform offering; you don’t need to decide on the type or size.</td>
<td style="text-align:left">Choose a suitable instance type and size, according to your predicted workload.</td>
</tr>
<tr>
<td>Public IP addresses</td>
<td style="text-align:left">Choose the Elastic IP address to associate with a NAT gateway at creation.</td>
<td style="text-align:left">Use an Elastic IP address or a public IP address with a NAT instance. You can change the public IP address at any time by associating a new Elastic IP address with the instance.</td>
</tr>
<tr>
<td>Private IP addresses</td>
<td style="text-align:left">Automatically selected from the subnet’s IP address range when you create the gateway.</td>
<td style="text-align:left">Assign a specific private IP address from the subnet’s IP address range when you launch the instance.</td>
</tr>
<tr>
<td>Security groups</td>
<td style="text-align:left">Cannot be associated with a NAT gateway. You can associate security groups with your resources behind the NAT gateway to control inbound and outbound traffic.</td>
<td style="text-align:left">Associate with your NAT instance and the resources behind your NAT instance to control inbound and outbound traffic.</td>
</tr>
<tr>
<td>Network ACLs</td>
<td style="text-align:left">Use a network ACL to control the traffic to and from the subnet in which your NAT gateway resides.</td>
<td style="text-align:left">Use a network ACL to control the traffic to and from the subnet in which your NAT instance resides.</td>
</tr>
<tr>
<td>Flow logs</td>
<td style="text-align:left">Use flow logs to capture the traffic.</td>
<td style="text-align:left">Use flow logs to capture the traffic.</td>
</tr>
<tr>
<td>Port forwarding</td>
<td style="text-align:left">Not supported.</td>
<td style="text-align:left">Manually customize the configuration to support port forwarding.</td>
</tr>
<tr>
<td>Bastion servers</td>
<td style="text-align:left">Not supported.</td>
<td style="text-align:left">Use as a bastion server.</td>
</tr>
<tr>
<td>Traffic metrics</td>
<td style="text-align:left">Not supported.</td>
<td style="text-align:left">View CloudWatch metrics.</td>
</tr>
<tr>
<td>Timeout behavior</td>
<td style="text-align:left">When a connection times out, a NAT gateway returns an RST packet to any resources behind the NAT gateway that attempt to continue the connection (it does not send a FIN packet).</td>
<td style="text-align:left">When a connection times out, a NAT instance sends a FIN packet to resources behind the NAT instance to close the connection.</td>
</tr>
<tr>
<td>IP fragmentation</td>
<td style="text-align:left">Supports forwarding of IP fragmented packets for the UDP protocol. Does not support fragmentation for the TCP and ICMP protocols. Fragmented packets for these protocols will get dropped.</td>
<td style="text-align:left">Supports reassembly of IP fragmented packets for the UDP, TCP, and ICMP protocols.</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Network ACL’s</p>
<ul>
<li>Your VPC automatically comes a default network ACL and by default it allows all outbound and inbound traffic.</li>
<li>You can create a custom network ACL. By default, each custom network ACL denies all inbound and outbound traffic until you add rules.</li>
<li>Each subnet in your VPC must be associated with a network ACL. If you don’t explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL.</li>
<li>You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed.</li>
<li>A network ACl contains a numbered list of rules that is evaluated in order, starting with the lowest numbered rule.</li>
<li>A network ACl has separate inbound and outbound rules, and each rule can either allow or deny traffic.</li>
<li>Network ACLs are stateless responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa)</li>
<li>Block IP Addresses using network ACL’s not Security Groups</li>
</ul>
</li>
<li><p>Security Group vs Network ACL</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Security Group</th>
<th style="text-align:left">Network ACL</th>
</tr>
</thead>
<tbody>
<tr>
<td>operates at the instance level (first layer of defense)</td>
<td style="text-align:left">Operates at the subnet level (second layer of defense)</td>
</tr>
<tr>
<td>Supports allow rules only</td>
<td style="text-align:left">Supports allow rules and deny rules</td>
</tr>
<tr>
<td>Is stateful: Return traffic is automatically allowed, regardless of any rules</td>
<td style="text-align:left">Is stateless: Return traffic must be explicitly allowed by rules</td>
</tr>
<tr>
<td>We evaluate all rules before deciding whether to allow traffic</td>
<td style="text-align:left">We process rules in number order when deciding whether to allow traffic</td>
</tr>
<tr>
<td>Applies to an instance only if someone specifies the security group when launching the instance, or associates the security group with the instance later on</td>
<td style="text-align:left">Automatically applies to all instances in the subnets it’s associated with (backup layer of defense, so you don’t have to rely on someone specifying the security group)</td>
</tr>
</tbody>
</table>
<ul>
<li><p>NAT vs Bastions</p>
<ul>
<li>A NAT is used to provide internet traffic to EC2 instances in private subnets</li>
<li>A Bastion is used to securely administer EC2 instance (using SSH or RDP) in private subnets. In Australia we call them jump boxes.</li>
</ul>
</li>
<li><p>Resilient Architecture</p>
<ul>
<li>If you want resiliency, always have 2 public subnets and 2 private subnets. Make sure each subnet is in different availability zones.</li>
<li>With ELB’s make sure they are in 2 public subnets in 2 different availability zones.</li>
<li>With Bastion hosts, put them behind an autoscaling group with a minimum size of 2. Use Route53 (either round robin or using a health check) to automatically fail over.</li>
<li>NAT instances are tricky to make resilient. You need 1 in each public subnet, each with their own public IP address, and you need to write a script to fail between the two. Instead where possible, use NAT gateways.</li>
</ul>
</li>
<li><p>VPC Flow Logs</p>
<ul>
<li>You can monitor network traffic within your custom VPC’s using VPC Flow Logs.</li>
</ul>
</li>
<li><p>VPC limit</p>
<ul>
<li>Currently you can create 200 subnets per VPC by default</li>
</ul>
</li>
<li><p>Direct Connect</p>
<ul>
<li>What is Direct Connect <a href="https://aws.amazon.com/directconnect/?nc1=h_ls" target="_blank" rel="external">https://aws.amazon.com/directconnect/?nc1=h_ls</a><ul>
<li>AWS Direct Connect makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.</li>
<li>AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. This allows you to use the same connection to access public resources such as objects stored in Amazon S3 using public IP address space, and private resources such as Amazon EC2 instances running within an Amazon Virtual Private Cloud (VPC) using private IP space, while maintaining network separation between the public and private environments. Virtual interfaces can be reconfigured at any time to meet your changing needs.</li>
</ul>
</li>
<li>Advantage of Direct Connect over VPN<ul>
<li><strong>Bandwidth &amp; a more consistent network experience!</strong></li>
<li>A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC.</li>
</ul>
</li>
</ul>
</li>
<li><p>Network Bottlenecks</p>
<ul>
<li>Each Instance type have the ma bandwidth and throughput.</li>
</ul>
</li>
</ul>
<h2 id="专项问题点"><a href="#专项问题点" class="headerlink" title="专项问题点"></a>专项问题点</h2><ul>
<li><p>ELB Connection draining</p>
<ul>
<li>Min: 1s, Max: 3600, default: 300s</li>
<li>helps the user to stop sending new requests traffic from the load balancer to the EC2 instance when the instance is being deregistered while continuing in-flight requests?</li>
</ul>
</li>
<li><p>AutoScaling Group Lifecycle</p>
<ul>
<li><a href="https://docs.aws.amazon.com/autoscaling/latest/userguide/AutoScalingGroupLifecycle.html" target="_blank" rel="external">Auto Scaling Lifecycle</a><br><img src="/images/AWS/Sysops/autoscaling_group_lifecycle.jpg" alt="autoscaling_group_lifecycle"></li>
<li><a href="https://docs.aws.amazon.com/autoscaling/latest/userguide/as-maintain-instance-levels.html" target="_blank" rel="external">Maintaining the Number of Instances in Your Auto Scaling Group</a></li>
</ul>
</li>
<li>CloudWatch 能监控的Metrics <a href="https://aws.amazon.com/cloudwatch/faqs/" target="_blank" rel="external">https://aws.amazon.com/cloudwatch/faqs/</a><ul>
<li>Amazon EC2 instances</li>
<li>EBS volumes</li>
<li>Elastic Load Balancers</li>
<li>Auto Scaling groups</li>
<li>EMR job flows</li>
<li>RDS DB instances</li>
<li>DynamoDB tables</li>
<li>ElastiCache clusters</li>
<li>RedShift clusters</li>
<li>OpsWorks stacks</li>
<li>Route 53 health checks</li>
<li>SNS topics</li>
<li>SQS queues</li>
<li>SWF workflows</li>
<li>and Storage Gateways</li>
</ul>
</li>
<li><p>不提供Detail monitoring的service, 最细颗粒是5 mins的service</p>
<ul>
<li>EMR</li>
<li>SNS</li>
</ul>
</li>
<li><p>CloudWatch 免费detail monitoring的几个service <a href="https://aws.amazon.com/cloudwatch/details/?nc1=h_ls" target="_blank" rel="external">https://aws.amazon.com/cloudwatch/details/?nc1=h_ls</a></p>
<ul>
<li><strong>Auto Scaling groups</strong>: seven pre-selected metrics at one-minute frequency, optional and for no additional charge.</li>
<li><strong>Elastic Load Balancers</strong>: thirteen pre-selected metrics at one-minute frequency, for no additional charge.</li>
<li><strong>Amazon Route 53 health checks</strong>: One pre-selected metric at one-minute frequency, for no additional charge</li>
<li><strong>Amazon EBS PIOPS (SSD) volumes</strong>: ten pre-selected metrics at one-minute frequency, for no additional charge</li>
<li><strong>Amazon CloudFront</strong>: six pre-selected metrics at one-minute frequency, for no additional charge</li>
<li><strong>Amazon ElastiCache nodes</strong>: thirty-nine pre-selected metrics at one-minute frequency, for no additional charge.</li>
<li><strong>Amazon RDS DB instances</strong>: fourteen pre-selected metrics at one-minute frequency, for no additional charge.</li>
<li><strong>Amazon Redshift</strong>: Sixteen pre-selected metrics at one-minute frequency, for no additional charge</li>
<li><strong>AWS Opsworks</strong>: fifteen pre-selected metrics at one-minute frequency, for no additional charge.</li>
<li><strong>Amazon CloudWatch Logs</strong>: six pre-selected metrics at one-minute frequency, for no additional charge</li>
</ul>
</li>
<li><p>CloudWatch 提供的几种统计值</p>
<ul>
<li>Average</li>
<li>Minimum</li>
<li>Maximum</li>
<li>Sum</li>
<li>Data Samples</li>
<li>p99</li>
<li>p95</li>
<li>p90</li>
<li>p50</li>
<li>p10</li>
</ul>
</li>
<li><p>CloudWatch AWS Namespaces</p>
<ul>
<li>There is no CloudTrail</li>
<li><a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/aws-namespaces.html" target="_blank" rel="external">All Cloudwatch Namespaces</a></li>
</ul>
</li>
<li><p>CloudWatch PutMetricData</p>
<ul>
<li>Each put-metric-data request is limited to 8KB in size for HTTP GET requests and is limited to 40 KB in size for HTTP POST requests.</li>
</ul>
</li>
<li><p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-ssl-security-policy.html" target="_blank" rel="external">SSL Negotiation Configurations for Classic Load Balancers</a></p>
<ul>
<li>Security Policies<ul>
<li>Predefined Security Policies</li>
<li>Custom Security Policies</li>
</ul>
</li>
<li>SSL Protocols (no TLS 1.3)<ul>
<li>supported<ul>
<li>TLS 1.2</li>
<li>TLS 1.1</li>
<li>TLS 1.0</li>
<li>SSL 3.0</li>
</ul>
</li>
<li>deprecated SSL Protocol<ul>
<li>SSL 2.0</li>
</ul>
</li>
</ul>
</li>
<li>The Server Order Preference is not supported in the Security Policy ELBSecurity Policy-2011-08 security policy.</li>
</ul>
</li>
<li><p>ELB sticky session algorithm</p>
<ul>
<li>This is how the ELB algorithm works in general when Cookie is not present<ul>
<li>First it checks the cookie is present in the service request</li>
<li>Since the cookie is not found in the request it will then decide which instance the service request should be routed to.</li>
<li>Finally the cookie is inserted in the response</li>
</ul>
</li>
<li>This is how the ELB algorithm works in general when Cookie is present<ul>
<li>First it checks the cookie is present in the service request</li>
<li>Since the cookie is found in the request it will then decide which instance the service request should be routed to based on the already present cookie.</li>
<li>Finally the cookie is inserted in the response</li>
</ul>
</li>
</ul>
</li>
<li><p>ELB access log file format</p>
<ul>
<li>format: bucket[/prefix]/AWSLogs/aws-account-id/elasticloadbalancing/region/yyyy/mm/dd/aws-account-id_elasticloadbalancing_region_load-balancer-id_end-time_ip-address_random-string.log.gz</li>
<li>sample: /AWSLogs/921187888888/elasticloadbalancing/us-west-2/2017/07/16/921187888888_elasticloadbalancing_us-west-2_awseb-e-u-AWSEBLoa-1KH856XXXXXXX_20170716T0000Z_35.161.113.177_1rhwm1ze.log</li>
</ul>
</li>
<li><p>IAM user name rule</p>
<ul>
<li>User names can be a combination of up to 64 letters, digits, and these characters: plus (+), equal (=), comma (,), period (.), at sign (@), and hyphen (-). Names must be unique within an account. They are not distinguished by case</li>
</ul>
</li>
<li><p>上传到CloudWatch的数据的时间戳</p>
<ul>
<li>past two week ~ two hours in the future : Each metric data point must be marked with a time stamp. The time stamp can be up to two weeks in the past and up to two hours into the future</li>
</ul>
</li>
<li><p>根据Wizard创建 VPC with Public an Private Subnets时</p>
<ul>
<li>默认会创建一个NAT Gateway，可以选择创建Nat Instance，并选择对应的Instance type和Key pair</li>
<li>创建两个Route Table<ul>
<li>面向private子网，0.0.0.0/0指向Nat Instance的是Main Route Table</li>
<li>面向public，0.0.0.0/0指向Internet Gateway的是Custom Route Table</li>
</ul>
</li>
<li>创建后不能直接删除，需要删除Public子网中的Nat Instance后才能删除VPC</li>
</ul>
</li>
<li><p>ACL rule priority</p>
<ul>
<li>Rule number. Rules are evaluated starting with the lowest numbered rule. As soon as a rule matches traffic, it’s applied regardless of any higher-numbered rule that may contradict it.</li>
</ul>
</li>
<li><p>VPC Dedicated Tenancy</p>
<ul>
<li>VPC 是非Dedicated的，建立EC2的时候是可以选Shared, Dedicated, Dedicated Host的</li>
<li>VPC建立的时候就是建立的Dedicated VPC时，建立在此VPC中的EC2，是不能选Shared的，只有Dedicated和Dedicated Host</li>
<li>在启动实例后，要想更改其租赁属性，有一定限制。<ul>
<li>在启动实例后，不能将其租赁属性从 default 改为 dedicated 或 host。</li>
<li>在启动实例后，不能将其租赁属性从 dedicated 或 host 改为 default。</li>
<li>在启动实例后，可以将其租赁属性从 dedicated 改为 host，或从 host 改为 dedicated</li>
</ul>
</li>
<li>You cannot change the tenancy of a default instance after you’ve launched it.</li>
</ul>
</li>
<li><p>VPC Peering Limitations</p>
<ul>
<li>You cannot create a VPC peering connection between VPCs that have matching or overlapping IPv4 or IPv6 CIDR blocks. Amazon always assigns your VPC a unique IPv6 CIDR block. If your IPv6 CIDR blocks are unique but your IPv4 blocks are not, you cannot create the peering connection.</li>
<li>You cannot create a VPC peering connection between VPCs in different regions.</li>
<li>You have a limit on the number active and pending VPC peering connections that you can have per VPC.</li>
<li>VPC peering does not support transitive peering relationships; in a VPC peering connection, your VPC does not have access to any other VPCs that the peer VPC may be peered with. This includes VPC peering connections that are established entirely within your own AWS account.</li>
<li>You cannot have more than one VPC peering connection between the same two VPCs at the same time.</li>
<li>A placement group can span peered VPCs; however, you do not get full-bisection bandwidth between instances in peered VPCs.</li>
<li>Unicast reverse path forwarding in VPC peering connections is not supported.</li>
<li>You can enable resources on either side of a VPC peering connection to communicate with each other over IPv6; however, IPv6 communication is not automatic. You must associate an IPv6 CIDR block with each VPC, enable the instances in the VPCs for IPv6 communication, and add routes to your route tables that route IPv6 traffic intended for the peer VPC to the VPC peering connection.</li>
</ul>
</li>
<li><p>IAM Limitation</p>
<ul>
<li>Groups in an AWS account - 300</li>
<li>Roles in an AWS account - 1000</li>
<li>Customer managed policies in an AWS account - 1500</li>
<li>Users in an AWS account - 5000</li>
<li>Groups an IAM user can be a member of - 10</li>
</ul>
</li>
<li><p>EC2 Termination Protection 不会保护来自instance内部的shutdown命令导致的terminal - <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#Using_ChangingDisableAPITermination" target="_blank" rel="external">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#Using_ChangingDisableAPITermination</a></p>
<ul>
<li>The DisableApiTermination attribute does not prevent you from terminating an instance by initiating shutdown from the instance (using an operating system command for system shutdown) when the InstanceInitiatedShutdownBehavior attribute is set.</li>
</ul>
</li>
<li><p>CloudWatch can monitoring AutoScaling Metric</p>
<ul>
<li>可以直接在CloudWatch中查看AutoScaling中EC2的整体指标，包括CPU，Network In/Out等</li>
</ul>
</li>
<li><p>EBS limitation</p>
<ul>
<li>Number of EBS Volumes - 5000</li>
<li>Number of EBS snapshots - 10000</li>
</ul>
</li>
<li><p>VPC Limitation</p>
<ul>
<li>VPCs per region - 5</li>
<li>Subnets per VPC - 100</li>
</ul>
</li>
<li><p>Bucket Policy 优先级</p>
<ul>
<li>The explicited deny permission will override the allow</li>
</ul>
</li>
<li><p>关于Redis中出现Eviction时的对应方法:</p>
<ul>
<li>官网说是直接scale up by using a larger node type</li>
<li>AcloudGuru上有讨论说处理方法是Only Scale out</li>
<li>各个<ul>
<li><a href="http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/CacheMetrics.WhichShouldIMonitor.html" target="_blank" rel="external">Which Metrics Should I Monitor</a></li>
<li><a href="https://acloud.guru/forums/aws-certified-sysops-administrator-associate/discussion/-Kc_XDW2XzOR32IFCY9f/redis-_scale_up_or_scale_out_-" target="_blank" rel="external">Redis- Scale up or scale out - question about lecture</a></li>
<li><a href="https://acloud.guru/forums/aws-certified-sysops-administrator-associate/discussion/-KDdkFs7QIlvu1npl2kR/redis-evictions-correction" target="_blank" rel="external">Redis Evictions Correction</a></li>
<li><a href="https://acloud.guru/forums/aws-certified-sysops-administrator-associate/discussion/-KNcfMRPnKdJinvljh8W/redis-evictions" target="_blank" rel="external">Redis Evictions</a></li>
<li><a href="https://acloud.guru/forums/aws-certified-sysops-administrator-associate/discussion/-K_D1AlOj9xH30fa0tl0/why-are-the-quizzes-so-messed-up-it-is-very-confusing" target="_blank" rel="external">Why are the quizzes so messed up? It is very confusing</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Doc"><a href="#Doc" class="headerlink" title="Doc"></a>Doc</h2><h3 id="User-Guide"><a href="#User-Guide" class="headerlink" title="User Guide"></a>User Guide</h3><ul>
<li><a href="http://docs.aws.amazon.com/opsworks/latest/userguide/welcome.html" target="_blank" rel="external">AWS Opsworks User Guide</a></li>
</ul>
<h3 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h3><ul>
<li><a href="https://aws.amazon.com/cn/elasticache/faqs/?nc1=h_ls" target="_blank" rel="external">Elasticache FAQ</a></li>
<li><a href="https://amazonaws-china.com/lambda/faqs/" target="_blank" rel="external">Lambda FAQ</a></li>
<li><a href="https://amazonaws-china.com/api-gateway/faqs/" target="_blank" rel="external">API Gateway FAQ</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS Certified </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Sysops </tag>
            
            <tag> CloudWatch </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[在AWS RDS中为Postgresql开启慢查询日志]]></title>
      <url>/2017/11/29/how-to-enable-slow-query-on-aws-rds-postgresql/</url>
      <content type="html"><![CDATA[<p>sql遇到性能问题，就需要开启慢查询日志，将执行时间超过某个限定值的sql输出到日志，给后续开发人员分析。下面是AWS RDS中Postgresql开启慢查询日志的方法。包含</p>
<ul>
<li>RDS设置</li>
<li>下载RDS log的方法</li>
<li>一些注意点</li>
</ul>
<h3 id="RDS设置步骤"><a href="#RDS设置步骤" class="headerlink" title="RDS设置步骤"></a>RDS设置步骤</h3><ol>
<li>登陆aws console，切换到RDS<br><img src="/images/AWS/RDSSlowQuery/switch_to_rds.png" alt="switch_to_rds.png"><a id="more"></a></li>
<li>在左边菜单栏中，找到”Parameter Groups”<br><img src="/images/AWS/RDSSlowQuery/parameter_groups.png" alt="parameter_groups.png"></li>
<li>选中要开启慢查询日志的PostgreSQL使用的参数组, 此处是myparametergroup, 点击”Edit Parameters”修改配置<br><img src="/images/AWS/RDSSlowQuery/select_parameter_group.png" alt="select_parameter_group.png"></li>
<li>修改<code>log_min_duration_statement</code>为超限的毫秒数，超过这个数值，PostgreSQL就会记录下相关的log。日志格式参见后续说明。<br><img src="/images/AWS/RDSSlowQuery/set_log_min_duration_statement.png" alt="set_log_min_duration_statement.png"></li>
<li>如果原来开启了<code>log_statement</code>和<code>log_duration</code>的，需要将<code>log_statement</code>设为默认的none，将<code>log_duration</code>设为0, 否则输出的慢查询日志的sql和执行时间就不在同一行，不便于观察。</li>
<li>因为<code>log_min_duration_statement</code>是动态参数的(修改页面上的Apply Type属性是Dynamic)，因此修改设置后不需要重启，RDS会自动load新的设置。<br><img src="/images/AWS/RDSSlowQuery/load_parameter_group.png" alt="load_parameter_group.png"></li>
<li><strong>注意:</strong> 如果RDS原先使用的是default的参数组，那么RDS换为自己定义的参数组的时候，RDS会重启</li>
</ol>
<h3 id="下载日志的方法"><a href="#下载日志的方法" class="headerlink" title="下载日志的方法"></a>下载日志的方法</h3><h4 id="设置IAM权限来允许下载RDS-logs"><a href="#设置IAM权限来允许下载RDS-logs" class="headerlink" title="设置IAM权限来允许下载RDS logs"></a>设置IAM权限来允许下载RDS logs</h4><p>将如下Policy 添加到IAM User或者Role中，就可以使用API或者CLI来下载RDS的log了</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"Version"</span>: <span class="string">"2012-10-17"</span>,</div><div class="line">    <span class="attr">"Statement"</span>: [</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"Action"</span>: [</div><div class="line">                <span class="string">"rds:Describe*"</span>,</div><div class="line">                <span class="string">"rds:DownloadDBLogFilePortion"</span></div><div class="line">            ],</div><div class="line">            <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</div><div class="line">            <span class="attr">"Resource"</span>: <span class="string">"*"</span></div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="aws-cli下载文件的命令"><a href="#aws-cli下载文件的命令" class="headerlink" title="aws cli下载文件的命令"></a>aws cli下载文件的命令</h4><p><code>aws rds download-db-log-file-portion help</code>中有下载完整log的用法</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">aws rds download-db-log-file-portion --db-instance-identifier myinstance --<span class="built_in">log</span>-file-name log.txt --starting-token 0 --output text &gt; full.txt</div></pre></td></tr></table></figure>
<p>如下是下载名为mydb的Instance中某个指定日志的示例:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">[ec2-user@ip-172-31-13-115 rds_log]$ aws rds describe-db-log-files --db-instance-identifier=mydb</div><div class="line">&#123;</div><div class="line">    <span class="string">"DescribeDBLogFiles"</span>: [</div><div class="line">        &#123;</div><div class="line">            <span class="string">"LastWritten"</span>: 1511938537000,</div><div class="line">            <span class="string">"LogFileName"</span>: <span class="string">"error/postgres.log"</span>,</div><div class="line">            <span class="string">"Size"</span>: 307</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="string">"LastWritten"</span>: 1511938540000,</div><div class="line">            <span class="string">"LogFileName"</span>: <span class="string">"error/postgresql.log.2017-11-29-06"</span>,</div><div class="line">            <span class="string">"Size"</span>: 1051</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="string">"LastWritten"</span>: 1511942140000,</div><div class="line">            <span class="string">"LogFileName"</span>: <span class="string">"error/postgresql.log.2017-11-29-07"</span>,</div><div class="line">            <span class="string">"Size"</span>: 4032</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="string">"LastWritten"</span>: 1511942441000,</div><div class="line">            <span class="string">"LogFileName"</span>: <span class="string">"error/postgresql.log.2017-11-29-08"</span>,</div><div class="line">            <span class="string">"Size"</span>: 336</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div><div class="line">[ec2-user@ip-172-31-13-115 rds_log]$ aws rds download-db-log-file-portion --db-instance-identifier mydb --<span class="built_in">log</span>-file-name error/postgresql.log.2017-11-29-07 --starting-token 0 --output text &gt; postgresql.log.2017-11-29-07</div><div class="line">[ec2-user@ip-172-31-13-115 rds_log]$ ll</div><div class="line">总用量 4</div><div class="line">-rw-rw-r-- 1 ec2-user ec2-user 4033 11月 29 08:02 postgresql.log.2017-11-29-07</div><div class="line">[ec2-user@ip-172-31-13-115 rds_log]$</div></pre></td></tr></table></figure>
<p>注意下载下来文件的大小，会比<code>describe-db-log-files</code>显示的文件大小大1 Byte, 因为下载下来的文件末尾会额外多一个空行</p>
<h4 id="Ruby-SDK-下载log的一个例子"><a href="#Ruby-SDK-下载log的一个例子" class="headerlink" title="Ruby SDK 下载log的一个例子"></a>Ruby SDK 下载log的一个例子</h4><p>参见<a href="https://gist.github.com/ruckus/d30531c543d677eb3acb" target="_blank" rel="external">GithubGist</a></p>
<p>摘录关键点如下:</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">additional_data_pending = <span class="literal">true</span></div><div class="line">File.open(out_log_file, <span class="string">"wb+"</span>) <span class="keyword">do</span> <span class="params">|file|</span></div><div class="line">  <span class="keyword">while</span> additional_data_pending <span class="keyword">do</span></div><div class="line">    out = rds.download_db_log_file_portion(opts)</div><div class="line">    file.write(out[<span class="symbol">:log_file_data</span>])</div><div class="line">    <span class="comment">#puts out[:marker]</span></div><div class="line">    opts[<span class="symbol">:marker</span>] = out[<span class="symbol">:marker</span>]</div><div class="line">    additional_data_pending = out[<span class="symbol">:additional_data_pending</span>]</div><div class="line">  <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><h4 id="log-min-duration-statement和log-statement同时使用的格式"><a href="#log-min-duration-statement和log-statement同时使用的格式" class="headerlink" title="log_min_duration_statement和log_statement同时使用的格式"></a>log_min_duration_statement和log_statement同时使用的格式</h4><p>按照<a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.PostgreSQL.html" target="_blank" rel="external">官方文档</a>说明,开启了<code>log_statement</code>和<code>log_duration</code>后，error log中输出的日志格式如下.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">2013-11-05 16:51:10 UTC:[local]:master@postgres:[9193]:LOG:  statement: SELECT c2.relname, i.indisprimary, i.indisunique, i.indisclustered, i.indisvalid, pg_catalog.pg_get_indexdef(i.indexrelid, 0, true),</div><div class="line">	  pg_catalog.pg_get_constraintdef(con.oid, true), contype, condeferrable, condeferred, c2.reltablespace</div><div class="line">	FROM pg_catalog.pg_class c, pg_catalog.pg_class c2, pg_catalog.pg_index i</div><div class="line">	  LEFT JOIN pg_catalog.pg_constraint con ON (conrelid = i.indrelid AND conindid = i.indexrelid AND contype IN (&apos;p&apos;,&apos;u&apos;,&apos;x&apos;))</div><div class="line">	WHERE c.oid = &apos;1255&apos; AND c.oid = i.indrelid AND i.indexrelid = c2.oid</div><div class="line">	ORDER BY i.indisprimary DESC, i.indisunique DESC, c2.relname;</div><div class="line">2013-11-05 16:51:10 UTC:[local]:master@postgres:[9193]:LOG:  duration: 3.367 ms</div><div class="line">2013-11-05 16:51:10 UTC:[local]:master@postgres:[9193]:LOG:  statement: SELECT c.oid::pg_catalog.regclass FROM pg_catalog.pg_class c, pg_catalog.pg_inherits i WHERE c.oid=i.inhparent AND i.inhrelid = &apos;1255&apos; ORDER BY inhseqno;</div><div class="line">2013-11-05 16:51:10 UTC:[local]:master@postgres:[9193]:LOG:  duration: 1.002 ms</div><div class="line">2013-11-05 16:51:10 UTC:[local]:master@postgres:[9193]:LOG:  statement: SELECT c.oid::pg_catalog.regclass FROM pg_catalog.pg_class c, pg_catalog.pg_inherits i WHERE c.oid=i.inhrelid AND i.inhparent = &apos;1255&apos; ORDER BY c.oid::pg_catalog.regclass::pg_catalog.text;</div><div class="line">2013-11-05 16:51:18 UTC:[local]:master@postgres:[9193]:LOG:  statement: select proname from pg_proc;</div><div class="line">2013-11-05 16:51:18 UTC:[local]:master@postgres:[9193]:LOG:  duration: 3.469 ms</div></pre></td></tr></table></figure></p>
<p>如果关闭了<code>log_statement</code>和<code>log_duration</code>, 只开启了<code>log_min_duration_statement</code>时，输出的日志格式中，duration和sql在同一行，比较便于阅读。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2017-11-26 08:01:32 UTC:172.31.13.115(40782):user@mydb:[4234]:LOG: duration: 1326.449 ms execute &lt;unnamed&gt;: SELECT COUNT(&quot;location_properties&quot;.&quot;id&quot;) FROM &quot;location_properties&quot; WHERE (location_record_id = 175034)</div></pre></td></tr></table></figure>
<h4 id="RDS-Parameter-Groups动态和静态参数的描述"><a href="#RDS-Parameter-Groups动态和静态参数的描述" class="headerlink" title="RDS Parameter Groups动态和静态参数的描述"></a>RDS Parameter Groups动态和静态参数的描述</h4><p>官方文档<a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html" target="_blank" rel="external">Working with DB Parameter Groups</a>中对修改RDS动态参数和静态参数的描述</p>
<blockquote>
<p>When you change a dynamic parameter and save the DB parameter group, the change is applied immediately regardless of the Apply Immediately setting. When you change a static parameter and save the DB parameter group, the parameter change will take effect after you manually reboot the DB instance.</p>
</blockquote>
<p>总结起来就是:</p>
<ul>
<li>修改动态参数, RDS会立即apply</li>
<li>修改静态参数, 只能reboot后才能生效</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="http://www.getfareye.com/blog/amazon-rds-postgres-activate-slow-query-logs" target="_blank" rel="external">Amazon RDS + Postgres – Activate slow query logs</a></li>
<li><a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.PostgreSQL.html" target="_blank" rel="external">PostgreSQL Database Log Files</a></li>
<li><a href="https://gist.github.com/ruckus/d30531c543d677eb3acb" target="_blank" rel="external">rds_download_logfiles.rb</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> RDS </tag>
            
            <tag> Ruby </tag>
            
            <tag> AWS CLI </tag>
            
            <tag> AWS SDK </tag>
            
            <tag> PostgreSQL </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[创建AWS Account后的基本账号设置]]></title>
      <url>/2017/10/31/account-setting-by-aws/</url>
      <content type="html"><![CDATA[<p>记录创建好AWS账号后的几个基本设置, 大概介绍如下几点:</p>
<ul>
<li>查看是否满足Free Tier Usage</li>
<li>创建IAM账号</li>
<li>为IAM账号启用Billing</li>
<li>如何提交Case</li>
<li>切换Console语言</li>
</ul>
<a id="more"></a>
<h3 id="查看是否满足Free-Tier-Usage"><a href="#查看是否满足Free-Tier-Usage" class="headerlink" title="查看是否满足Free Tier Usage"></a>查看是否满足Free Tier Usage</h3><p>根据官方文档 <a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/free-tier-eligibility.html" target="_blank" rel="external">Eligibility for the Free Tier</a> 所描述，在 <a href="https://console.aws.amazon.com/billing/home#/" target="_blank" rel="external">Billing and Cost Management console</a> 页面中的<strong>Alerts &amp; Notifications</strong>中看到 <strong>You are eligible for the AWS Free Usage Tier. See the Getting Started Guide AWS Free Usage Tier to learn how to get started with the free usage tier</strong> 的描述，就说明该账号是符合Free Tier条件的。</p>
<p><img src="/images/AWS/Startup/free-tier-usage-prompt.jpg" alt="Free Tier Usage Prompt"></p>
<h3 id="创建IAM用户"><a href="#创建IAM用户" class="headerlink" title="创建IAM用户"></a>创建IAM用户</h3><p>AWS 的root账号就是注册时使用的email账号，因为该账号拥有全部的操作权限。经常登录相对来说更容易泄露账号和密码，根据AWS的最佳实践的要求，不建议日常登陆来进行操作，强烈推荐创建IAM用户来进行日常的操作。</p>
<p>如果是为了练习AWS各个Feature用的，建议就直接创建拥有Admin权限的IAM用户。 进入<a href="https://console.aws.amazon.com/iam/home?#/home" target="_blank" rel="external">IAM Consoe</a>页面</p>
<h4 id="创建Groups"><a href="#创建Groups" class="headerlink" title="创建Groups"></a>创建Groups</h4><p>依次点击侧边栏[Groups]-&gt;[Create New Group], 跳出[Create New Group Wizard]页面, 在后续页面上依次操作如下:</p>
<ol>
<li>[Step 1 : Group Name] – 输入Group名字，可以输入 Admin</li>
<li>[Step 2 : Attach Policy] – 在Filter输入框中输入admin, 然后选择AdministratorAccess</li>
<li>[Step 3 : Review] – 点击”Create Group”</li>
</ol>
<h4 id="创建Users"><a href="#创建Users" class="headerlink" title="创建Users"></a>创建Users</h4><p>依次点击侧边栏[Users]-&gt;[Add user],  跳出[Add User]页面，后续页面依次操作如下:</p>
<ol>
<li><p>[Details] 页面</p>
<ul>
<li>输入User name</li>
<li>Access type勾选 AWS Management Console access</li>
<li>Console password 中设置密码</li>
<li>因为是练习使用， 不勾选 Require password reset 前面的checkbox</li>
</ul>
</li>
<li><p>[Permissions] 页面 – 直接勾选前面创建好的Admin组</p>
</li>
<li>[Review] 页面 – 确认信息是否都正确， 然后点击Create User</li>
<li>[Complete] – 记录类似 <code>https://xxxxxxxxxxxx.signin.aws.amazon.com/console</code>的登陆链接, 其中xxxxxxxxxxxx应该是一个12位的数字, 每个AWS账号都有自己独立的一串12位的数字</li>
</ol>
<p>至此，一个有Admin操作权限的用户就生成好了。</p>
<h3 id="为IAM账号启用Billing"><a href="#为IAM账号启用Billing" class="headerlink" title="为IAM账号启用Billing"></a>为IAM账号启用Billing</h3><p>为了安全起见，AWS IAM用户默认是没有Billing操作功能的，但作为使用Free Tier的练习者，每天使用后免不了都会去Billing页面瞅瞅是否有超额使用导致了费用。每次切换root account会很麻烦，因此为IAM用户启用Billing权限，是很有必要的。</p>
<p>操作步骤:</p>
<ol>
<li>使用root account访问 <a href="https://console.aws.amazon.com/billing/home?#/account" target="_blank" rel="external">Billing页面</a></li>
<li>找到<code>IAM User and Role Access to Billing Information</code>一栏</li>
<li>点击右边的<code>Edit</code>按钮，在展开的页面中，勾选<code>Activate IAM Access</code>前面的勾选框，然后点击<code>Update</code>按钮</li>
</ol>
<p>此时，退出root account或者换个浏览器，使用创建用户时候获取的<code>https://xxxxxxxxxxxx.signin.aws.amazon.com/console</code>登陆页面，输入IAM账号密码，然后访问<a href="https://console.aws.amazon.com/billing/home?#/" target="_blank" rel="external">Billing页面</a>, 就可以看到账单信息了。</p>
<p>如果没有开启IAM用户的Billing权限，访问<a href="https://console.aws.amazon.com/billing/home?#/" target="_blank" rel="external">Billing页面</a>, 就会报告<code>You Need Permissions</code>的错误，提示没有权限</p>
<p><img src="/images/AWS/Startup/No_Permission_To_Access_Billing.jpg" alt="No Permission To Access Billing"></p>
<h3 id="如何提交Case"><a href="#如何提交Case" class="headerlink" title="如何提交Case"></a>如何提交Case</h3><p>AWS的使用，碰到问题，第一要诀就是Google，第二要诀就是查官方Doc，最后一个途径就是向AWS Support提交Case。</p>
<p>提交Case的几个通用的场景:</p>
<ul>
<li>要提高AWS服务的各个Limit，比如EC2 Instance的Limit，Elastic IP的Limit等</li>
<li>个人账号和账单有关的一些问题</li>
<li>技术问题, 但技术支持需要Developer及以上Level才可用，免费的Basic Support Plan不可用</li>
</ul>
<p>登陆Console后，依次点击右上角的[Support]-&gt;[Support Center], 进入<code>Support Center</code>页面。</p>
<p>提交一个Case的要点:</p>
<ol>
<li>点击左侧[Create Case], 进入<code>Create Case</code>页面</li>
<li>Regarding中选择是<code>Account and Billing Support</code>还是<code>Service Limit Increase</code></li>
<li>填写相关的<code>Category</code>, <code>Description</code>等信息</li>
<li>AWS全球账号中<code>Support Language</code>目前可选English或者日语</li>
<li>最后选择是Web中Comment的方式回复还是Phone直接打电话沟通</li>
</ol>
<p>提交Case后，每一个Case都会列在Case History中。</p>
<p>另外，因为AWS的Suport资源会首先满足付费的Support Plan，因此Basic Support Plan的提问，经常是要过一两天甚至更久后才会有回复。</p>
<h3 id="切换Console语言"><a href="#切换Console语言" class="headerlink" title="切换Console语言"></a>切换Console语言</h3><p>AW Console是可以切换显示语言的，点击左下角<code>Feedback</code>右侧的带一个地球小图标的语言选择栏就可以切换显示语言了。</p>
<p>但注意的是，不同的Service可能会有不同的语言选项， 比如<a href="https://console.aws.amazon.com/billing/home?#/" target="_blank" rel="external">Billing页面</a>支持包括中文在内的8种语言，<a href="https://console.aws.amazon.com/iam/home" target="_blank" rel="external">IAM</a>只支持中文，英语，法语，日语这四种语言，而<a href="https://console.aws.amazon.com/support/home" target="_blank" rel="external">Support Center</a>就只支持日语和英语两种语言了。</p>
<p>不过建议还是保持默认的English，有利于尽快的熟悉各种AWS的专用名词。毕竟到时候出了问题上网找答案时，能找到的大多数有用的回答，都是English。</p>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> IAM </tag>
            
            <tag> AWS Free Tier </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[绑定万网域名到github pages]]></title>
      <url>/2017/10/31/hexo-bind-domain-to-github-pages/</url>
      <content type="html"><![CDATA[<blockquote>
<p>此处介绍如何将万网的域名绑定到github pages</p>
</blockquote>
<h3 id="万网设置"><a href="#万网设置" class="headerlink" title="万网设置"></a>万网设置</h3><p>进入aliyun控制的云解析DNS控制台，选中所要解析的域名，此处是jibing57.com, 点击右侧解析按钮。</p>
<ul>
<li>记录类型选择 CNAME</li>
<li>主机记录填写 www</li>
<li>记录值填入github pages的域名，此处是jibing57.github.io<a id="more"></a>
</li>
</ul>
<p><img src="/images/Hexo/add_dns_record_on_wanwang.jpg" alt="add_dns_record_on_wanwang"></p>
<h3 id="Hexo-设置"><a href="#Hexo-设置" class="headerlink" title="Hexo 设置"></a>Hexo 设置</h3><p>在source目录下添加CNAME文件，输入所要绑定的域名, 此处是<code>www.jibing57.com</code>, 注意不需要http。提交CNAME到git。</p>
<p>使用<code>hexo deploy</code>发布到github pages，访问<a href="http://www.jibing57.com">www.jibing57.com</a>就可以访问到github pages的内容了。</p>
]]></content>
      
        <categories>
            
            <category> Blog </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Github </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hexo引用站内文章]]></title>
      <url>/2017/10/30/how-to-use-post-link-on-hexo/</url>
      <content type="html"><![CDATA[<p>写文章的时候，经常需要引用站内的其他文章，此时可以使用Hexo内置的<a href="https://hexo.io/zh-cn/docs/tag-plugins.html" target="_blank" rel="external">标签插件</a>（Tag Plugins）中的<code>post_link</code>来实现。</p>
<a id="more"></a>
<p>用法:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;% post_link slug [title] %&#125;</div></pre></td></tr></table></figure>
<p>其中slug就是_posts文件夹下需要引用的文章的markdown文件的名字，title可以指定引用的文章需要显示的名字。</p>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>例如，<a href="/2017/07/17/hexo-github-blog/" title="使用Hexo搭建Blog">使用Hexo搭建Blog</a>这篇文章的markdown名字为hexo-github-blog.md</p>
<p>那么在需要引用的markown源文件中，输入如下标记的时候</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">- &#123;% post_link hexo-github-blog %&#125;</div><div class="line">- &#123;% post_link hexo-github-blog 显示的名字 %&#125;</div></pre></td></tr></table></figure>
<p>页面显示效果如下:</p>
<ul>
<li><a href="/2017/07/17/hexo-github-blog/" title="使用Hexo搭建Blog">使用Hexo搭建Blog</a></li>
<li><a href="/2017/07/17/hexo-github-blog/" title="显示的名字">显示的名字</a>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Blog </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Paperclip destroy的callback中attachment的各个attribute为nil的问题调查]]></title>
      <url>/2017/10/29/ruby-paperclip-destroy-callback-nil/</url>
      <content type="html"><![CDATA[<h3 id="问题点"><a href="#问题点" class="headerlink" title="问题点"></a>问题点</h3><p>Rails的Image的models中，使用了paperclip这个Gem来处理图片。近期需要添加一个功能，删除image后，需要向某个email地址发送一封邮件，告之某个图片已经被删除了。</p>
<p>实际操作中，发现在无论函数是定义在before_destroy或after_destroy的callback中，attachment_file_name,attachment_file_size, attachment_content_type, attachment_updated_at的属性，取出来都是nil。</p>
<p>调查了一下，现将结果汇总如下:<br><a id="more"></a></p>
<h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>有两个办法可以解决这个问题:</p>
<h4 id="方法一"><a href="#方法一" class="headerlink" title="方法一:"></a>方法一:</h4><p>将自己的before_destroy置于 has_attached_file 之前，这样就能够在执行Paperclip的 before_destroy之前执行你的before_destroy的方法，此时相关字段还没有设置为nil，还能访问到.</p>
<h4 id="方法二"><a href="#方法二" class="headerlink" title="方法二:"></a>方法二:</h4><p>将自己的before_destory置于 has_attached_file 之后，此时Paperclip的before_destroy会先于自己写的before_destroy之前调用，会将相关字段设置为nil。<br>此时，只能依靠activerecord的_was方法来获取修改前的值, 比如获取attachment_file_name的话，就调用attachment_file_name_was</p>
<h4 id="适用情况"><a href="#适用情况" class="headerlink" title="适用情况"></a>适用情况</h4><p>如果要before_destroy中callback中获取以上字段的值，上述两个方法都可行</p>
<p>如果是after_destroy的话，就只能依靠上述的方法二来获取attachment_file_name之类的值</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="http://www.tikalk.com/use-beforedestroy-model-paperclip/" target="_blank" rel="external">use-beforedestroy-model-paperclip</a></li>
<li><a href="https://github.com/thoughtbot/paperclip/issues/2088" target="_blank" rel="external">Github - paperclip_issue_2088</a></li>
<li><a href="https://stackoverflow.com/questions/6578302/unable-to-access-attached-file-data-in-before-destroy-while-using-paperclip" target="_blank" rel="external">Stackoverflow - unable-to-access-attached-file-data-in-before-destroy-while-using-paperclip</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Code </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ruby </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何在aws cli中使用多个配置文件]]></title>
      <url>/2017/10/24/how-to-use-aws-cli-with-multi-user/</url>
      <content type="html"><![CDATA[<p><code>aws cli</code>使用中，可能会有在多个IAM账户中进行切换的需求，手动切换<code>~/.aws/</code>目录下的<code>config</code>和<code>credentials</code>是十分费力的事情。还好<code>aws cli</code>本身就可以支持多个aws credentials</p>
<h3 id="配置多个profile"><a href="#配置多个profile" class="headerlink" title="配置多个profile"></a>配置多个profile</h3><p><code>aws configure</code>时，加上<code>--profile</code>参数来命名不同的账户, 依次输入access id, access key, region和output format。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ aws configure --profile user1</div><div class="line">$ aws configure --profile user2</div></pre></td></tr></table></figure>
<p>此时生成的<code>config</code>和<code>credentials</code>文件中，会使用账户名来分割不同的配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[carlshen@carl-macpro-lan ~]$ cat ~/.aws/config</div><div class="line">[profile user1]</div><div class="line">output = json</div><div class="line">region = us-west-2</div><div class="line">[profile user2]</div><div class="line">output = json</div><div class="line">region = ap-northeast-2</div><div class="line">[carlshen@carl-macpro-lan ~]$</div><div class="line">[carlshen@carl-macpro-lan ~]$ cat ~/.aws/credentials</div><div class="line">[user1]</div><div class="line">aws_access_key_id = AKIAXXXXXXXXXXXXXXXX</div><div class="line">aws_secret_access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</div><div class="line">[user2]</div><div class="line">aws_access_key_id = AKIAXXXXXXXXXXXXXXXX</div><div class="line">aws_secret_access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</div><div class="line">[carlshen@carl-macpro-lan ~]$</div></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="使用多个profile"><a href="#使用多个profile" class="headerlink" title="使用多个profile"></a>使用多个profile</h3><h4 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h4><p>使用的时候，在命令后面加上参数<code>--profile user_name</code>即可使用user_name对应的profile</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ aws s3 ls --profile user_name</div></pre></td></tr></table></figure>
<p>如下命令使用user2的profile来查看S3下的bucket list</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[carlshen@carl-macpro-lan ~]$ aws s3 ls --profile user2</div><div class="line">2017-10-24 11:18:38 carl-test-at-seoul</div><div class="line">[carlshen@carl-macpro-lan ~]$</div></pre></td></tr></table></figure>
<h4 id="简化"><a href="#简化" class="headerlink" title="简化"></a>简化</h4><p>每次输入<code>--profile user_name</code>是很繁琐的事情，在Mac或者Linux下，可以使用alias来简化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ alias aws_user_name=&apos;aws --profile user_name&apos;</div></pre></td></tr></table></figure>
<p>这样，每次使用的时候，直接使用<code>aws_user_name</code>来使用user_name的profile来运行aws命令</p>
<p>以下命令设置aws_user2为使用user2的profile来运行aws命名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[carlshen@carl-macpro-lan ~]$ alias aws_user2=&apos;aws --profile user2&apos;</div><div class="line">[carlshen@carl-macpro-lan ~]$ aws_user2 s3 ls</div><div class="line">2017-10-24 11:18:38 carl-test-at-seoul</div><div class="line">[carlshen@carl-macpro-lan ~]$</div></pre></td></tr></table></figure>
<p>添加到~/.bashrc中使得alias永久生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ echo &quot;alias aws_user2=&apos;aws --profile user2&apos;&quot; &gt;&gt; ~/.bashrc</div></pre></td></tr></table></figure>
<h4 id="设置默认profile"><a href="#设置默认profile" class="headerlink" title="设置默认profile"></a>设置默认profile</h4><p>如果有一个账号是使用的比较频繁的，而不想每次都使用alias的方式来运行aws，那么也可以设置环境变量<code>AWS_DEFAULT_PROFILE</code>为频繁使用的账号名，此时输入aws时候，会自动使用指定的账号配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ export AWS_DEFAULT_PROFILE=user2</div></pre></td></tr></table></figure>
<p>运行结果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">## 没有设置AWS_DEFAULT_PROFILE时</div><div class="line">[carlshen@carl-macpro-lan ~]$ echo $AWS_DEFAULT_PROFILE</div><div class="line"></div><div class="line">[carlshen@carl-macpro-lan ~]$ aws s3 ls</div><div class="line">Unable to locate credentials. You can configure credentials by running &quot;aws configure&quot;.</div><div class="line">[carlshen@carl-macpro-lan ~]$</div><div class="line"></div><div class="line">## 设置了AWS_DEFAULT_PROFILE为user2后，aws默认就会使用user2的profile</div><div class="line">[carlshen@carl-macpro-lan ~]$ export AWS_DEFAULT_PROFILE=user2</div><div class="line">[carlshen@carl-macpro-lan ~]$ aws s3 ls</div><div class="line">2017-10-24 11:18:38 carl-test-at-seoul</div><div class="line">[carlshen@carl-macpro-lan ~]$</div></pre></td></tr></table></figure>
<p>添加到~/.bashrc中来使AWS_DEFAULT_PROFILE永久生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ echo &quot;export AWS_DEFAULT_PROFILE=user2&quot; &gt;&gt; ~/.bashrc</div></pre></td></tr></table></figure>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>配置 AWS CLI - <a href="http://docs.aws.amazon.com/zh_cn/cli/latest/userguide/cli-chap-getting-started.html" target="_blank" rel="external">http://docs.aws.amazon.com/zh_cn/cli/latest/userguide/cli-chap-getting-started.html</a></li>
</ol>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AWS CLI </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用aws codecommit作为私有的git远程服务器]]></title>
      <url>/2017/10/24/how-to-use-aws-codecommit/</url>
      <content type="html"><![CDATA[<p>博客的hexo的代码，一直是保存在本地的。上次电脑花了一次屏后，感觉到保存在本地实在是不够安全。因此考虑寻觅一个远端的私密git库，存起来。</p>
<p>开始寻寻觅觅合适的仓库.</p>
<ul>
<li>Github?,私有库是收费的。但为了这些哪天就不更新的markdown，每月7刀开个Developer的Plan，感觉不划算。</li>
<li>自建Gitlab？嫌麻烦麻烦。</li>
<li>oschina的私有库？不想用。</li>
<li>。。。。。。</li>
</ul>
<p>好吧，我承认我就是想尝试用一下aws的codecommit。</p>
<p>关于CodeCommit的免费额度，<a href="https://aws.amazon.com/cn/codecommit/pricing/" target="_blank" rel="external">官网</a>介绍:</p>
<ul>
<li>最初5位活动用户<ul>
<li>无限存储库</li>
<li>50GB的月存储量</li>
<li>每月10000个git请求</li>
</ul>
</li>
</ul>
<p>托管我一个小博客，妥妥的够了。毕竟除了我，没人还会来关心这点markdown文件, 5位用户免费足够了。至于50GB的月存储量么，除非把看过的电影都commit进git来，要不然应该是足够了。</p>
<a id="more"></a>
<h3 id="创建Repository"><a href="#创建Repository" class="headerlink" title="创建Repository"></a>创建Repository</h3><ol>
<li><p>在aws console的Services中，找到CodeCommit<br><img src="/images/AWS/CodeCommit/find_codecommit.jpg" alt="find_codecommit"></p>
</li>
<li><p>在CodeCommit页面中点击Create, 打开新建Repository的页面，在<strong>Repository Name</strong>中填入仓库的名字，<strong>Description</strong>中填写仓库的描述, 然后点击<strong>Create repository</strong>创建仓库。<br><img src="/images/AWS/CodeCommit/codecommit_create_repository.jpg" alt="codecommit_create_repository"></p>
</li>
</ol>
<h3 id="配置ssh-key"><a href="#配置ssh-key" class="headerlink" title="配置ssh key"></a>配置ssh key</h3><ol>
<li><p>CodeCommit有两种访问方式，分别是ssh和https模式。我习惯使用ssh方式。</p>
</li>
<li><p>首先，需要在IAM User中添加SSH keys, 用来访问CodeCommit。</p>
<ul>
<li>打开IAM，切换到User界面，</li>
<li>在<code>Security credentials</code>的tab下，找到<code>SSH keys for AWS CodeCommit</code>一栏</li>
<li>点击下面<code>Upload SSH public key</code>按钮</li>
<li>在打开的上传key的页面中输入常用的key pair的public key, 然后点击<code>Upload SSH public Key</code>的按钮<br><img src="/images/AWS/CodeCommit/upload_ssh_for_codecommit.jpg" alt="upload_ssh_for_codecommit.jpg"></li>
</ul>
</li>
<li><p>上传完毕后，就会生成一个新的Entry，复制或保存此处SSH key ID的值。<br><img src="/images/AWS/CodeCommit/publickey_of_IAM_used_for_codecommit.jpg" alt="publickey_of_IAM_used_for_codecommit.jpg"></p>
</li>
<li><p>配置本地~/.ssh/config, 添加有关CodeCommit的Host条目, <code>IdentityFile</code>设置为private key, 并保存。如果本地还没有~/.ssh/config文件，则创建，并在保存后使用命令<code>chmod 600 ~/.ssh/config</code>将访问属性修改为600.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Host git-codecommit.*.amazonaws.com</div><div class="line">  User APKAIHTDFHHOYTHJYI3Q</div><div class="line">  IdentityFile ~/.ssh/id_rsa</div></pre></td></tr></table></figure>
</li>
<li><p>打开codeCommit的repository, 点击<code>Clone URL</code>，选择SSH来获取仓库的ssh地址, 我此处的地址是 ssh://git-codecommit.ap-northeast-2.amazonaws.com/v1/repos/my_blog<br><img src="/images/AWS/CodeCommit/get_ssh_url_of_codecommit.jpg" alt="get_ssh_url_of_codecommit"></p>
</li>
<li><p>找个临时目录，使用git clone命令来测试是否可以正常访问新建的仓库</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$ git clone ssh://git-codecommit.ap-northeast-2.amazonaws.com/v1/repos/my_blog</div><div class="line">Cloning into &apos;my_blog&apos;...</div><div class="line">The authenticity of host &apos;git-codecommit.ap-northeast-2.amazonaws.com (52.95.194.93)&apos; can&apos;t be established.</div><div class="line">RSA key fingerprint is 9f:68:48:9b:5f:fc:96:69:39:45:58:87:95:b3:69:ed.</div><div class="line">Are you sure you want to continue connecting (yes/no)? yes</div><div class="line">Warning: Permanently added &apos;git-codecommit.ap-northeast-2.amazonaws.com,52.95.194.93&apos; (RSA) to the list of known hosts.</div><div class="line">warning: You appear to have cloned an empty repository.</div><div class="line">Checking connectivity... done.</div><div class="line">$ ll</div><div class="line">total 0</div><div class="line">drwxr-xr-x  3 carlshen  staff  102 10 24 21:32 my_blog</div><div class="line">$</div></pre></td></tr></table></figure>
<h3 id="push已有的repository到CodeCommit"><a href="#push已有的repository到CodeCommit" class="headerlink" title="push已有的repository到CodeCommit"></a>push已有的repository到CodeCommit</h3><p>刚建立的repository是空的，我们可以clone下来，然后逐次添加文件，也可以将已经存在的git reposigory push到CodeCommit上的空仓库中。</p>
<p>切换到已有的git 仓库中，然后使用如下命令将git仓库push到CodeCommit中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git push https://git-codecommit.us-east-2.amazonaws.com/v1/repos/MyFirstRepo --all</div></pre></td></tr></table></figure></p>
<p>如下是将本地的my_blog推送到远端的步骤:</p>
<ol>
<li><p>本地的remote为空</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ git remote -v</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
<li><p>推送本地的所有代码到CodeCommit上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">$ git push ssh://git-codecommit.ap-northeast-2.amazonaws.com/v1/repos/my_blog --all</div><div class="line">Warning: Permanently added the RSA host key for IP address &apos;52.95.194.107&apos; to the list of known hosts.</div><div class="line">Counting objects: 490, done.</div><div class="line">Delta compression using up to 8 threads.</div><div class="line">Compressing objects: 100% (463/463), done.</div><div class="line">Writing objects: 100% (490/490), 6.71 MiB | 441.00 KiB/s, done.</div><div class="line">Total 490 (delta 214), reused 0 (delta 0)</div><div class="line">remote: processing To ssh://git-codecommit.ap-northeast-2.amazonaws.com/v1/repos/my_blog</div><div class="line"> * [new branch]      master -&gt; master</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
<li><p>将CodeCommit上的仓库设置为远端的origin</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ git remote add origin ssh://git-codecommit.ap-northeast-2.amazonaws.com/v1/repos/my_blog</div><div class="line">$ git remote -v</div><div class="line">origin	ssh://git-codecommit.ap-northeast-2.amazonaws.com/v1/repos/my_blog (fetch)</div><div class="line">origin	ssh://git-codecommit.ap-northeast-2.amazonaws.com/v1/repos/my_blog (push)</div></pre></td></tr></table></figure>
</li>
<li><p>关联本地和远端的master分支</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ git branch --set-upstream-to=origin/master master</div><div class="line">Branch master set up to track remote branch master from origin.</div><div class="line">$ git pull</div><div class="line">Already up-to-date.</div><div class="line">$</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>将 Git 存储库迁移到 AWS CodeCommit - <a href="https://docs.aws.amazon.com/zh_cn/codecommit/latest/userguide/how-to-migrate-repository-existing.html" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/codecommit/latest/userguide/how-to-migrate-repository-existing.html</a></li>
<li>在 Linux, macOS, or Unix 上设置到 AWS CodeCommit 存储库的 SSH 连接的步骤 - <a href="https://docs.aws.amazon.com/zh_cn/codecommit/latest/userguide/setting-up-ssh-unixes.html#setting-up-ssh-unixes-keys-unixes" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/codecommit/latest/userguide/setting-up-ssh-unixes.html#setting-up-ssh-unixes-keys-unixes</a></li>
</ol>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Git </tag>
            
            <tag> CodeCommit </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Linux 和 Mac下date命令的基本用法]]></title>
      <url>/2017/08/03/date-command-on-Linux-and-Mac/</url>
      <content type="html"><![CDATA[<p>Mac下的date命令是BSD系的, Linux下date命令是GNU系的，两者的用法有一些区别，罗列如下:</p>
<h3 id="共同点"><a href="#共同点" class="headerlink" title="共同点"></a>共同点</h3><p>基本的时间格式的缩写是相同的，规则如下:</p>
<ul>
<li>%Y 表示四位数形式的年份, 比如2017</li>
<li>%m 表示带前导0的月份，比如02,12</li>
<li>%d 表示带前导0的日子， 比如 02，28</li>
<li>%H 表示带前导0的24小时， 比如 01, 23</li>
<li>%M 表示带前导0的分钟数， 比如 05, 22</li>
<li>%S 表示带前导0的秒数， 比如 06，45</li>
<li>%s 表示距离格林威治时间(1970年1月1日0点)的秒数</li>
</ul>
<a id="more"></a>
<p>运行结果:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># Macos</div><div class="line">$ date</div><div class="line">2017年 8月 4日 星期五 14时31分00秒 CST</div><div class="line">$ date +%Y%m%d%H%M%S</div><div class="line">20170804143107</div><div class="line">$ date +%Y%m%d%H%M%S</div><div class="line">20170804143112</div><div class="line">$</div><div class="line"></div><div class="line"># Linux(Centos)</div><div class="line">$ date</div><div class="line">2017年 08月 04日 星期五 06:31:33 UTC</div><div class="line">$ date +%Y%m%d%H%M%S</div><div class="line">20170804063135</div><div class="line">$ date +%s</div><div class="line">1501828299</div><div class="line">$</div></pre></td></tr></table></figure>
<h3 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h3><p>获取前一天和获取后一天的写法不同</p>
<p>Macos 下通过使用<code>-v</code>参数时间，<code>-v-1d</code>代表前一天， <code>-v-1y</code>代表上一年<br>Linux 下通过<code>--date</code>参数实现, <code>--date=&#39;-1 day&#39;</code>代表前一天， <code>--date=&#39;-1 year&#39;</code>代表上一年</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># Macos</div><div class="line"></div><div class="line">$ date -v-1d -v-1y +%Y-%m-%d</div><div class="line">2016-08-03</div><div class="line">$</div><div class="line"></div><div class="line"># Linux</div><div class="line"></div><div class="line">$ date +%Y-%m-%d --date=&apos;-1 day -1 year&apos;</div><div class="line">2016-08-03</div><div class="line">$</div></pre></td></tr></table></figure>
<h3 id="检查平台来决定如何使用date"><a href="#检查平台来决定如何使用date" class="headerlink" title="检查平台来决定如何使用date"></a>检查平台来决定如何使用date</h3><p>可以使用<code>uname -s</code>的输出来判定是哪个平台，Linux下命令输出是<code>Linux</code>, Macos下命令输出是<code>Darwin</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line"></div><div class="line">os=$(uname -s)</div><div class="line">if [[ &quot;$os&quot; == &quot;Linux&quot; ]]; then</div><div class="line">    date +%Y-%m-%d --date=&apos;-1 day -1 year&apos;</div><div class="line">elif [[ &quot;$os&quot; == &quot;Darwin&quot; ]]; then</div><div class="line">    date -v-1d -v-1y +%Y-%m-%d</div><div class="line">else</div><div class="line">    echo &quot;unknown OS&quot;</div><div class="line">    exit 1</div><div class="line">fi</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Shell </category>
            
        </categories>
        
        
        <tags>
            
            <tag> MacOS </tag>
            
            <tag> Linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Macos中如何语音朗读文字]]></title>
      <url>/2017/07/25/how-to-read-text-on-macos/</url>
      <content type="html"><![CDATA[<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>Macos上看到大段大段的英文，有时候除了看以外，还想边听边看</p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="Macos-自带speech"><a href="#Macos-自带speech" class="headerlink" title="Macos 自带speech"></a>Macos 自带speech</h3><p>Macos 自带了文本至语音的功能, 开启方法如下:</p>
<ol>
<li>打开[系统偏好设置] -&gt; [听写与语音] -&gt; 切换至[文本至语音]</li>
<li>可以选择系统嗓音和朗读速率</li>
<li>可以设置快捷键，默认为Option + Esc, 选择文字后按快捷键开启，再次按快捷键关闭</li>
</ol>
<p>或选中文字，右键菜单选择[语音]-&gt;[开始讲话]</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://support.apple.com/kb/PH14230?locale=en_US" target="_blank" rel="external">OS X Mavericks: Hear your Mac speak text</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> MacOS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS相关的一些有用的网址]]></title>
      <url>/2017/07/25/useful-website-of-aws/</url>
      <content type="html"><![CDATA[<h3 id="cloudping-info"><a href="#cloudping-info" class="headerlink" title="cloudping.info"></a>cloudping.info</h3><p><a href="http://www.cloudping.info/" target="_blank" rel="external">clougping.info</a>是一个可以检测当前浏览器到AWS各个Region的延迟的网站, 可以用来评估访问哪个Region更快一点。在建立测试服务的时候十分有用。</p>
<p>单次的测试结果不一定准确，建议多试几次后再选取平均延迟低的结果。</p>
<ul>
<li>国内测试的结果:<br><img src="/images/AWS/Tools/cloudping_on_cn.jpg" alt="cloudping_on_cn"></li>
</ul>
<p>在国内使用AWS全球账号时，在韩国首尔Region建测试服务延迟会小一点。</p>
<a id="more"></a>
<h3 id="open-guides-og-aws"><a href="#open-guides-og-aws" class="headerlink" title="open-guides og-aws"></a>open-guides og-aws</h3><p>AWS拥有非常多的Service，每个Service都有着厚厚的UserGuide, <a href="https://github.com/open-guides/og-aws" target="_blank" rel="external">og-aws</a> 所做的，就是从AWS文档中提炼出各个Service的一些要点，再结合实际使用中各个工程师的经验, 汇总成了一份Guide。通读一遍就可以对整个AWS的服务有个整体的了解，可以结合实际操作经验隔段时间就来读一遍。</p>
<p>内容包含如下:</p>
<ul>
<li>AWS以及其拥有的所有Service的简介</li>
<li>和其他云服务的比较</li>
<li>各个AWS Service的Basics, Tips, 还有Gotchas and Limitations</li>
</ul>
<h3 id="AWS-词汇表"><a href="#AWS-词汇表" class="headerlink" title="AWS 词汇表"></a>AWS 词汇表</h3><ul>
<li>中文版 <a href="https://docs.aws.amazon.com/zh_cn/general/latest/gr/glos-chap.html" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/general/latest/gr/glos-chap.html</a></li>
<li>英文版 <a href="https://docs.aws.amazon.com/general/latest/gr/glos-chap.html" target="_blank" rel="external">https://docs.aws.amazon.com/general/latest/gr/glos-chap.html</a></li>
</ul>
<h2 id="还没调查的"><a href="#还没调查的" class="headerlink" title="还没调查的"></a>还没调查的</h2><p><a href="https://gist.github.com/leonardofed/bbf6459ad154ad5215d354f3825435dc" target="_blank" rel="external">https://gist.github.com/leonardofed/bbf6459ad154ad5215d354f3825435dc</a></p>
]]></content>
      
        <categories>
            
            <category> AWS </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AWS Tools </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS Certified Developer - Associate Road Map]]></title>
      <url>/2017/07/24/AWS-Certified-Developer/</url>
      <content type="html"><![CDATA[<h2 id="Official-AWS-Centification-Page"><a href="#Official-AWS-Centification-Page" class="headerlink" title="Official AWS Centification Page"></a>Official AWS Centification Page</h2><p>访问官网<a href="https://amazonaws-china.com/certification/certification-prep/" target="_blank" rel="external">AWS Centification</a></p>
<ul>
<li>参加 AWS 培训课程</li>
<li>查看考试指南和样题<ul>
<li>了解考试涉及的概念并整体了解需要学习哪些内容, <a href="http://awstrainingandcertification.s3.amazonaws.com/production/AWS_certified_developer_associate_blueprint.pdf" target="_blank" rel="external">AWS Certified Developer – Associate 考试指南</a> 相当于考试大纲, 必看,而且需要反复的看。因为学习过一阵后再来看Guide，会有更深的体会。</li>
<li><a href="https://d0.awsstatic-china.com/training-and-certification/docs/AWS_certified_developer_associate_examsample.pdf" target="_blank" rel="external">考试样题</a>用于熟悉题目题型</li>
</ul>
</li>
<li>完成自主进度动手实验和备考任务<ul>
<li>官方<a href="https://www.qwiklabs.com/learning_paths/20/lab_catalogue?locale=en" target="_blank" rel="external">qwikLABS 任务</a>提供了一系列动手实验, 提供部分免费实验，但大部分实验所需的积分都需要购买。高性价比的做法是， 注册一个AWS全球账号，使用一年的免费额度来对照着实验手册来进行试验。</li>
</ul>
</li>
<li>学习 AWS 白皮书<ul>
<li>白皮书是纯英文的，而且每个白皮书篇幅都很长，读起来既费时又枯燥。但是有时间还是建议把推荐的几个都看一下。</li>
</ul>
</li>
<li>查看 AWS 常见问题<ul>
<li>官网推荐的FAQ都建议看完，另外<a href="https://amazonaws-china.com/cn/dynamodb/faqs/" target="_blank" rel="external">DynamoDB FAQ</a>这个必须要看。</li>
</ul>
</li>
<li>参加模拟考试<ul>
<li>20美刀一次，主要目的是为了让人熟悉考试时上机的流程。是否需要因人而异, 特别想先熟悉下考试流程的可以考虑参加一次。我个人觉得没有必要, 因为真实考试时，操作界面一目了然，没有磕磕绊绊的机关，省下20美刀可以去买一份课程。</li>
</ul>
</li>
<li>报名考试并获得认证<ul>
<li>登陆<a href="https://www.aws.training/certification" target="_blank" rel="external">https://www.aws.training/certification</a>注册进行考试</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="考试指南"><a href="#考试指南" class="headerlink" title="考试指南"></a>考试指南</h2><p><a href="http://awstrainingandcertification.s3.amazonaws.com/production/AWS_certified_developer_associate_blueprint.pdf" target="_blank" rel="external">AWS Certified Developer – Associate 考试指南</a> 读三遍，读三遍，读三遍</p>
<p>各个Domain的分数比例如下:<br><img src="/images/AWS/Developer/Domain_on_developer_associate_blueprint.jpg" alt="Domain_on_developer_associate_blueprint"></p>
<p>Domain 3 Deployment and Security 中3.2 中最后一句[CIA and AAA models, ingress vs. egress filtering, and which AWS services and features fit]中的CIA和AAA的解释如下:</p>
<p>CIA are the fundamentals of Information Security</p>
<ul>
<li>Confidentiality 机密性 (generally encryption)</li>
<li>Integrity 完整性 (the accuracy of a message or server…i.e. hash value)</li>
<li>Availability 可用性 (availability of a service)</li>
</ul>
<p>AAA</p>
<ul>
<li>Authentication</li>
<li>Authorization</li>
<li>Accounting</li>
</ul>
<p>参考自: <a href="https://acloud.guru/forums/aws-certified-developer-associate/discussion/-KTdRPtz4PF2rLHO1_tD/what-is-cia-and-aaa-models-ingress-vs-egress-filtering-and-which-aws-services-an" target="_blank" rel="external">Acloudguru discussion</a></p>
<h2 id="视频学习"><a href="#视频学习" class="headerlink" title="视频学习"></a>视频学习</h2><p><a href="https://acloud.guru" target="_blank" rel="external">Acloudguru</a> 中<a href="https://acloud.guru/course/aws-certified-developer-associate/dashboard" target="_blank" rel="external">aws-centified-developer-associate</a>视频的学习，大体内容和Centified Solutions Architect-Associate的大同小异，多了DynamoDB的部分</p>
<h3 id="要点摘录"><a href="#要点摘录" class="headerlink" title="要点摘录"></a>要点摘录</h3><h4 id="IAM"><a href="#IAM" class="headerlink" title="IAM"></a>IAM</h4><ul>
<li><p>IAM give</p>
<ul>
<li>Centralised control of your AWS account</li>
<li>Shared Access to your AWS account</li>
<li>Granular Permissions</li>
<li>Identity Federation (including Active Directory, Facebook, Linkedin etc)</li>
<li>Multifactor Authentication</li>
<li>Provide temporary access for users/devices and services where necessary</li>
<li>Allows you to set up your own password rotation policy</li>
<li>Integrates with many different AWS services</li>
<li>Supports PCI DSS Compliance</li>
</ul>
</li>
<li><p>IAM consists of the following:</p>
<ul>
<li>Users - End Users (think people)</li>
<li>Groups (A collection of users under one set of permissions. A way to group our users and apply polices to them collectively)</li>
<li>Roles - You create roles and can then assign them to AWS resources</li>
<li>Policy Documents - A document that defines one (or more permissions) - <a href="https://awspolicygen.s3.amazonaws.com/policygen.html" target="_blank" rel="external">IAM Online Policy Generator</a><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</div><div class="line">    &quot;Statement&quot;: [</div><div class="line">        &#123;</div><div class="line">            &quot;Effect&quot;: &quot;Allow&quot;,</div><div class="line">            &quot;Action&quot;: &quot;*&quot;,</div><div class="line">            &quot;Resource&quot;: &quot;*&quot;</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>IAM is universal, It does not apply to regions at this time.</p>
</li>
<li>The “root account” is simply the account created when first setup your AWS account. It has complete Admin access.</li>
<li>New Users have NO permissions when first created.</li>
<li>New Users are assigned <strong>Access Key ID &amp; Secret Access Keys</strong> when first created.</li>
<li>There are not the same as a password, and you cannot use the <strong>Access key ID &amp; Secret Access Key</strong> to Login in to the console. You can use this to access AWS via the APIs and Command Line however.</li>
<li>You only get to view these once. If you lose them, you have to regenerate them. So save them in a secure location.</li>
<li>Always setup Multifactor Authentication on your root account.</li>
<li>You can create and customise your own password rotation policies.</li>
<li>Determining whether a request is allowed or denied – <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html" target="_blank" rel="external">http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a><ul>
<li>Start with default deny, and evaluate all applicable policies</li>
<li>If there is explicit deny, result is deny</li>
<li>If there is explicit allow, result is allow<br><img src="/images/AWS/Developer/reference_policies_evaluation_logic.jpg" alt="reference policies evaluation logic"></li>
</ul>
</li>
</ul>
<h4 id="STS"><a href="#STS" class="headerlink" title="STS"></a>STS</h4><ul>
<li>In The Exam<ul>
<li>Develop and Identity Broker to communicate with LDAP and AWS STS</li>
<li>Identity Broker always authenticates with LDAP first, THEN with AWS STS</li>
<li>Application then gets temporary access to AWS resource</li>
</ul>
</li>
</ul>
<h4 id="EC2"><a href="#EC2" class="headerlink" title="EC2"></a>EC2</h4><ul>
<li><p>know the differences between:</p>
<ul>
<li>On Demand - allow you to pay a fixed rate by the hour with no commitment</li>
<li>Spot - enable you to bi whatever price you want for instance capacity, providing for even greater savings if your applications have flexible start and end times.<ul>
<li>If you terminate the instance, you pay for the hour</li>
<li>If AWS terminates the spot instance, you get the hour it was terminated in for free.</li>
</ul>
</li>
<li>Reserved - provide you with a capacity reservation, and offer a significant discount on the hourly charge for an instance. 1 Year or 3 Year Terms</li>
<li>Dedicated Hosts - Physical EC2 server dedicated for your use. Dedicated Hosts can help you reduce costs by allowing you to use your existing server-bound software licenses.</li>
</ul>
</li>
<li><p>EC2 Instance Types</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Family</th>
<th style="text-align:center">Speciality</th>
<th style="text-align:right">Use case</th>
</tr>
</thead>
<tbody>
<tr>
<td>D2</td>
<td style="text-align:center">Dense Storage</td>
<td style="text-align:right">Fileservers/Data Wareshousing/Hadoop</td>
</tr>
<tr>
<td>R4</td>
<td style="text-align:center">Memory Optimized</td>
<td style="text-align:right">Memory Intensive Apps/DBs</td>
</tr>
<tr>
<td>M4</td>
<td style="text-align:center">General Purpose</td>
<td style="text-align:right">Application Servers</td>
</tr>
<tr>
<td>C4</td>
<td style="text-align:center">Compute Optimized</td>
<td style="text-align:right">CPU Intensive Apps/DBs</td>
</tr>
<tr>
<td>G2</td>
<td style="text-align:center">Graphics Intensive</td>
<td style="text-align:right">Video Encoding/3D Application Streaming</td>
</tr>
<tr>
<td>I2</td>
<td style="text-align:center">High Speed Storage</td>
<td style="text-align:right">NoSQL DBs, Data Warehousing etc</td>
</tr>
<tr>
<td>F1</td>
<td style="text-align:center">Field Programmable Gate Array</td>
<td style="text-align:right">Hardware acceleration for your code</td>
</tr>
<tr>
<td>T2</td>
<td style="text-align:center">Lowest Cost, General Purpose</td>
<td style="text-align:right">Web Servers/Small DBs</td>
</tr>
<tr>
<td>P2</td>
<td style="text-align:center">Graphics/General Purpose GPU</td>
<td style="text-align:right">Machine Learning, Bit Coin Mining etc</td>
</tr>
<tr>
<td>X1</td>
<td style="text-align:center">Memory Optimize</td>
<td style="text-align:right">SAP HANA/Apache Spark etc</td>
</tr>
</tbody>
</table>
<ul>
<li><p>How to remember Instance type</p>
<ul>
<li>D for Density</li>
<li>R for RAM</li>
<li>M - main choice for general purpose apps</li>
<li>C for Compute</li>
<li>G - Graphics</li>
<li>I for IOPS</li>
<li>F for FPGA</li>
<li>T cheap general purpose (think T2 micro)</li>
<li>P - Graphics (think Pics)</li>
<li>X - Extreme Memory</li>
<li><strong>DR Mc GIFT PX</strong></li>
</ul>
</li>
<li><p>EBS Consists of:</p>
<ul>
<li>SSD, General Purpose - GP2 - (Up to 10,000 IOPS)<ul>
<li>General purpose, balances both price and performance.</li>
<li>Ratio of 3 IOPS per GB with up to 10000 IOPS and the ability to burst up to 3000 IOPS for extended periods of time for volumes under 1Gib.</li>
</ul>
</li>
<li>SSD, Provisioned IOPS - IO1 - (More than 10,000 IOPS)<ul>
<li>Designed for I/O intensive applications such as large relational or NoSQL databases.</li>
<li>Use if you need more than 10000 IOPS</li>
<li>Can provision up to 20000 IOPS per volume.</li>
</ul>
</li>
<li>HDD, Throughput Optimized - ST1 - frequently accessed workloads<ul>
<li>Big data</li>
<li>Data warehouse</li>
<li>Log processing</li>
<li>Cannot be a boot volume</li>
</ul>
</li>
<li>HDD, Cold - SC1 - less frequently accessed data.<ul>
<li>Lowest Cost Storage for infrequently accessed workloads</li>
<li>File Server</li>
<li>Cannot be a boot volume</li>
</ul>
</li>
<li>HDD, Magnetic - Standard - cheap, infrequently accessed storage<ul>
<li>Lowest cost per gigabyte of all EBS volume types that is <strong>bootable</strong>. Magnetic volumes are ideal for workloads where data is accessed infrequently, and applications where the lowest storage cost is important.</li>
</ul>
</li>
<li>You cannot mount 1 EBS volume to multiple EC2 instances, instead use EFS.</li>
</ul>
</li>
<li><p>EC2 Lab Exam Tips</p>
<ul>
<li>Termination Protection is turned off by default, you must turn it on.</li>
<li>On an EBS-backed instance, the default action is for the root EBS volume to be deleted when the instance is terminated.</li>
<li>EBS Root Volumes of your DEFAULT AMI’s cannot be encrypted. you need a third party tool(such as bit locker etc) to encrypt the root volume or this can be done when creating AMI’s in the AWS console or using the API.</li>
<li>Additional volumes can be encrypted.</li>
</ul>
</li>
<li><p>Upgrading EBS Volume Types</p>
<ul>
<li>EBS Volumes can be changed on the fly (except for magnetic standard)</li>
<li>Best practice to stop the EC2 instance and then change the volume</li>
<li>You can change volume types by taking a snapshot and then using the snapshot to create a new volume</li>
<li>If you change a volume on the fly you must wait for 6 hours before making another change</li>
<li>You can scale EBS Volumes up only</li>
<li>Volumes must be in the same AZ as the EC2 instances</li>
</ul>
</li>
<li><p>Security Group</p>
<ul>
<li>All Inbound Traffic is Blocked By Default</li>
<li>All Outbound Traffic is Allowed</li>
<li>Changes to Security Groups take effect immediately</li>
<li>You can have any number of EC2 instances within a security group.</li>
<li>You can have multiple security groups attached to EC2 Instances</li>
<li>Security Groups are <strong>STATEFUL</strong><ul>
<li>If you create an inbound rule allowing traffic in, that traffic is automatically allowed back out again.</li>
</ul>
</li>
<li>You cannot block specific IP addresses using Security Groups, instead use Network Access Control Lists.</li>
<li>You can specify allow rules, but not deny rules.</li>
</ul>
</li>
<li><p>Volumes vs Snapshots</p>
<ul>
<li>Volumes exist on EBS<ul>
<li>Virtual Hard Disk</li>
</ul>
</li>
<li>Snapshots exist on S3</li>
<li>You can take a snapshot of a volume, this will store that volume on S3.</li>
<li>Snapshots are point in time copies of Volumes.</li>
<li>Snapshots are incremental, this means that only the blocks that have changed since your last snapshot are moved to S3.</li>
<li>If this is your first snapshot, it may take some time to create.</li>
</ul>
</li>
<li><p>Volumes vs Snapshots - Security</p>
<ul>
<li>Snapshots of encrypted volumes are encrypted automatically.</li>
<li>Volumes restored from encrypted snapshots are encrypted automatically.</li>
<li>You can share snapshots, but only if they are unencrypted.<ul>
<li>These snapshots can be shared with other AWS accounts of made public.</li>
</ul>
</li>
</ul>
</li>
<li><p>Snapshots of Root Device Volumes</p>
<ul>
<li>To Create a snapshot for Amazon EBS volumes that serve as root devices, you should stop the instance before taking the snapshot.</li>
</ul>
</li>
<li><p>EBS vs Instance Store</p>
<ul>
<li>Instance Store Volumes are sometimes called Ephemeral Storage.</li>
<li>Instance store volumes cannot be stopped. If the underlying host fails, you will lose your data.</li>
<li>EBS backed instances can be stopped. You will not lose the data on this instance if it is stopped.</li>
<li>You can reboot both, you will not lose your data.</li>
<li>By default, both ROOT volumes will be deleted on termination, however with EBS volumes, you can tell AWS to keep the root device volume.</li>
</ul>
</li>
<li><p>How can I take a Snapshot of a RAID Array?</p>
<ul>
<li>Problem - Take a snapshot, the snapshot excludes data held in the cache by applications and the OS. This tends not to matter on a single volume, however using multiple volumes in a RAID array, this can be a problem due to interdependencies of the array.</li>
<li>Solution - Take an application consistent snapshot.<ul>
<li>Stop the application from writing to disk.</li>
<li>Flush all caches to the disk.</li>
<li>How can we do this?<ul>
<li>Freeze the file system</li>
<li>Unmount the RAID Array</li>
<li>Shutting down the associated EC2 instance.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Amazon Machine Images</p>
<ul>
<li>AMI’s are regional. You can only launch an AMI from the region in which it is stored. However you can copy AMI’s to other regions using the console, command line or the Amazon EC2 API.</li>
</ul>
</li>
<li><p>CloudWatch and CloudTrail</p>
<ul>
<li>Standard Monitoring = 5 Minutes</li>
<li>Detailed Monitoring = 1 Minute</li>
<li>CloudWatch is for performance monitoring</li>
<li>CloudTrail is for auditing</li>
<li>CloudWatch<ul>
<li>Dashboards - Creates awesome dashboards to see what is happening with your AWS environment.</li>
<li>Alarms - Allows you to set Alarms that notify you when particular thresholds are hit.</li>
<li>Events - CloudWatch Events helps you to respond to state changes in your AWS resources.</li>
<li>Logs - CloudWatch Logs helps you to aggregate, monitor, and store logs.</li>
</ul>
</li>
</ul>
</li>
<li><p>EC2 Role</p>
<ul>
<li>Roles are more secure than storing your access key and secret access key on individual EC2 instances.</li>
<li>Roles are easier to manage</li>
<li>Roles can only be assigned when that EC2 instance is being provisioned.</li>
<li>Roles are universal, you can use them in any region.</li>
</ul>
</li>
<li><p>Instance Meta-data</p>
<ul>
<li>Used to get information about an instance(such as public ip)</li>
<li>curl <a href="http://169.254.169.254/latest/meta-data/" target="_blank" rel="external">http://169.254.169.254/latest/meta-data/</a></li>
<li>No such thing as user-data for an instance</li>
</ul>
</li>
<li><p>EFS Features</p>
<ul>
<li>Supports the Network File System version 4(NFSv4) protocol</li>
<li>You only pay for the storage you use (no pre-provisioning required)</li>
<li>Can scale up to the petabytes</li>
<li>Can support thousands of concurrent NFS connections</li>
<li>Data is stored across multiple AZ’s within a region</li>
<li>Read After Write Consistency</li>
<li><strong>Great use cases for a file server</strong>. You can apply both file level and directory level permissions within EFS.</li>
</ul>
</li>
<li><p>EC2 CLI Command</p>
<ul>
<li>aws ec2 describe-instances</li>
<li>aws ec2 describe-images</li>
<li>aws ec2 run-instances</li>
<li>Do not confuse <strong>start-instances</strong> with <strong>run-instances</strong><ul>
<li><strong>start-instances</strong> starts an stopped instance</li>
<li><strong>run-instances</strong> is used to create a new instance</li>
</ul>
</li>
</ul>
</li>
<li><p>Lambda</p>
<ul>
<li>AWS Lambda is a compute service where you can upload your code and create a Lambda function. AWS Lambda takes care of provisioning and managing the servers that you use to run the code. You don’t have to worry about operating systems, patching, scaling, etc. You can use Lambda in the following ways.<ul>
<li>As an event-driven compute service where AWS Lambda runs your code in response to events. These events could be changes to data in an Amazon S3 bucket or an Amazon DynamoDB table.</li>
<li>As a compute service to run your code in response to HTTP requests using Amazon API Gateway or API calls made using AWS SDKs.</li>
</ul>
</li>
<li>Language supported<ul>
<li>Node.js</li>
<li>Java</li>
<li>Python</li>
<li>C#</li>
</ul>
</li>
<li>Lambda Priced<ul>
<li>Number of requests<ul>
<li>First 1 million requests are free. $0.20 per 1 million requests thereafter.</li>
</ul>
</li>
<li>Duration<ul>
<li>Duration is calculated from the time you code begins executing until it returns or otherwise terminates, rounded up to the nearest 100ms. The price depends on the amount of memory you allocate to your function. You are charged $0.00001667 for every GB-second used.</li>
</ul>
</li>
</ul>
</li>
<li>Why is Lambda Cool<ul>
<li>No Servers!</li>
<li>Continuous Scaling</li>
<li>Super super super cheap!</li>
</ul>
</li>
<li>The default timeout of Lambda Function is 3 second, maximum time is 300 second (5 mins), minimum time is 1 second.</li>
<li>Lambda code(and any dependent libraries) as a Zip and upload to console, Uploads must be no larger than 50MB(compressed).</li>
<li>Compute resource : You can set your memory in 64MB increments from 128MB to 1.5GB</li>
</ul>
</li>
<li><p>Elastic Load Balancers</p>
<ul>
<li>Instances monitored by ELB are reported as:<ul>
<li>InService</li>
<li>OutOfService</li>
</ul>
</li>
<li>Health Checks check the instance health by talking to it</li>
<li>Have their own DNS name. You are never given an IP address.</li>
<li>Classic Load Balancer FAQ</li>
</ul>
</li>
<li><p>SDK Tips</p>
<ul>
<li>Available SDK<ul>
<li><a href="https://aws.amazon.com/tools/" target="_blank" rel="external">https://aws.amazon.com/tools/</a></li>
<li>Android, iOS, JavaScript(Browser)</li>
<li>Java</li>
<li>.Net</li>
<li>Node.js</li>
<li>PHP</li>
<li>Python</li>
<li>Ruby</li>
<li>Go</li>
<li>C++</li>
</ul>
</li>
<li>Default Region - US-EAST-1<ul>
<li>Some have default regions(Java)</li>
<li>Some do not (Node.js)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="S3"><a href="#S3" class="headerlink" title="S3"></a>S3</h4><ul>
<li><p>Exam Tips</p>
<ul>
<li>Remember that S3 is Object based i.e. allows you to upload files.</li>
<li>File can be from 0 Bytes to 5TB.</li>
<li>There is unlimited storage.</li>
<li>Files are stored in Buckets.</li>
<li>S3 is a universal namespace, that is, names must be unique globally.</li>
<li><a href="https://s3-eu-west-1.amazonaws.com/acloudguru" target="_blank" rel="external">https://s3-eu-west-1.amazonaws.com/acloudguru</a></li>
<li>Read after Write consistency for PUTS of new Objects</li>
<li>Eventual Consistency for overwrite PUTS and DELETS (can take some time to propagate)</li>
<li>Object based storage only ( for files)</li>
<li><strong>Not suitable to install an operating system on.</strong></li>
<li>You can insert a presigned url into a webpage to download private data directly from S3.</li>
</ul>
</li>
<li><p>S3 object consist of:</p>
<ul>
<li>Key (This is simply the name of the object)</li>
<li>Value (This is simply the data and is made up of a sequence of bytes)</li>
<li>Version ID (Important for versioning)</li>
<li>Metadata (Data about the data you are storing)</li>
<li>Subresources<ul>
<li>Access control lists</li>
<li>Torrent</li>
</ul>
</li>
</ul>
</li>
<li><p>S3 The Basics</p>
<ul>
<li>Built for 99.99 availability for the S3 platform.</li>
<li>Amazon Guarantee 99.9% availability</li>
<li>Amazon Guarantees 99.999999999% durability for S3 information. ( Remember 11x9’s)</li>
<li>Tiered Storage Available</li>
<li>Lifecycle Management</li>
<li>Versioning</li>
<li>Encryption</li>
<li>Secure your data using Access Control Lists and Bucket Policies</li>
</ul>
</li>
<li><p>S3 Storage Classes/Tiers</p>
<ul>
<li>S3 (durable, immediately available, frequently accessed)<ul>
<li>99.99% availability, 99.999999999% durability, stored redundantly across multiple devices in multiple facilities and is designed to sustain the loss of 2 facilities concurrently.</li>
</ul>
</li>
<li>S3 - IA (durable, immediately available, infrequently accessed)<ul>
<li>For data that is accessed less frequently, but requires rapid access when needed. Lower fee than S3, but you are charged a retrieval fee.</li>
</ul>
</li>
<li>S3 - Reduced Redundancy Storage (data that is easily reproducible, such as thumb nails etc).<ul>
<li>Designed to provide 99.99% durability and 99.99% availability of objects over a given year.</li>
</ul>
</li>
<li>Glacier - Archived data, where you can wait 3 -5 hours before accessing.</li>
<li>S3 Storage Tier<br><img src="/images/AWS/Developer/s3_storage_tier.jpg" alt="s3_storage_tier"></li>
<li>S3 vs Glacier<br><img src="/images/AWS/Developer/s3_vs_glacier.jpg" alt="s3_vs_glacier"></li>
</ul>
</li>
<li><p>S3 Charges</p>
<ul>
<li>Storage</li>
<li>Requests</li>
<li>Storage Management Pricing</li>
<li>Data Transfer Pricing</li>
<li>Transfer Acceleration</li>
</ul>
</li>
<li><p>S3 - Versioning</p>
<ul>
<li>Stores all versions of an object (including all writes and even if you delete an object)</li>
<li>Great backup tool.</li>
<li>Once enabled, Versioning cannot be disabled, only suspended.</li>
<li>Integrates with Lifecycle rules.</li>
<li>Versioning’s MFA Delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security.</li>
<li>Cross Region Replication, requires versioning enabled on the source bucket.</li>
<li>Any objects uploaded prior to versioning will have the version ID as NULL</li>
</ul>
</li>
<li><p>Cross Region Replication</p>
<ul>
<li>Versioning must be enabled on both the source and destination buckets.</li>
<li>Regions must be unique.</li>
<li>Files in an existing bucket are not replicated automatically. All subsequent updated files will be replicated automatically.</li>
<li>You cannot replicate to multiple buckets of use daisy chaining (at this time).</li>
<li>When delete object, the delete markers are replicated.</li>
<li>Deleting individual versions or delete markers will not be replicated.</li>
</ul>
</li>
<li><p>Lifecycle Management</p>
<ul>
<li>Can be used in conjunction with versioning.</li>
<li>Can be applied to current versions and previous versions.</li>
<li>Following actions can now be done:<ul>
<li>Transition to the Standard - Infrequent Access Storage Class(128Kb and 30 days after the creation date).</li>
<li>Archive to the Glacier Storage Class (30 days after IA, if relevant)</li>
<li>Permanently Delete</li>
</ul>
</li>
</ul>
</li>
<li><p>CloudFront</p>
<ul>
<li>Edge Location - This is the location where content will be cached. This is separate to an AWS Region/AZ</li>
<li>Origin - This is the origin of all the files that the CDN will distribute. This can be either an S3 Bucket, an EC2 Instance, an Elastic Load Balancer or Route53.</li>
<li>Distribution - This is the name given the CDN which consists of a collection of Edge Locations.<ul>
<li>Web Distribution - Typically used for Websites.</li>
<li>RTMP - Used for Media Streaming.</li>
</ul>
</li>
<li>Edge locations are not just READ only, you can write to them too. (ie put an object on to them)</li>
<li>Objects are cached for the life of the TTL (Time To Live)</li>
<li>You can clear cached objects, but you will be charged.</li>
<li>Restrict viewer access by signed URL or Signed Cookies</li>
<li>Restrict content based on geo location(whitelist and blacklist)</li>
</ul>
</li>
<li><p>Securing your buckets</p>
<ul>
<li>By default, all newly created buckets are PRIVATE</li>
<li>You can setup access control to your buckets using:<ul>
<li>Bucket Policies</li>
<li>Access Control Lists</li>
</ul>
</li>
<li>S3 buckets can be configured to create access logs which log all requests made to the S3 bucket. This can be done to another bucket.</li>
</ul>
</li>
<li><p>Encryption</p>
<ul>
<li>In Transit:<ul>
<li>SSL/TLS</li>
</ul>
</li>
<li>At Rest<ul>
<li>Server Side Encryption<ul>
<li>S3 Managed Keys - SSE-S3</li>
<li>AWS Key Management Service, Managed Keys - SSE-KMS</li>
<li>Server Side Encryption With Customer Provided Keys - SSE-C</li>
</ul>
</li>
<li>Client Side Encryption</li>
</ul>
</li>
</ul>
</li>
<li><p>Storage Gateway</p>
<ul>
<li>File Gateway - For flat files, stored directly on S3.<ul>
<li>NFS</li>
<li>Unlimited amount of storage. However maximal file size is 5TB.</li>
</ul>
</li>
<li>Volume Gateway<ul>
<li>Stored Volumes - Entire Dataset is stored on site and is asynchronously backed up to S3.<ul>
<li>iSCSI based block storage</li>
<li>Each Volume can store up to 16TB in Size.</li>
<li>32 Volumes supported. 512TB of data can be stored (32*16)</li>
</ul>
</li>
<li>Cached Volumes - Entire Dataset is stored on S3 and the most frequently accessed data is cache on site.<ul>
<li>iSCSI based block storage</li>
<li>Each Volume can store up to 32TB in Size.</li>
<li>32 Volumes supported. 1PB of data can be stored(32*32)</li>
</ul>
</li>
</ul>
</li>
<li>Gateway Virtual Tape Library (VTL)<ul>
<li>Used for backup and uses popular backup applications like NetBackup, Backup Exec, Veam etc<ul>
<li>iSCSI based virtual tape solution</li>
<li>Virtual Tape Library (S3) 1500 virtual tapes (1PB)</li>
<li>Virtual Tape Shelf (Glacier) unlimited tapes.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Storage Gateway - General Facts</p>
<ul>
<li>Can be deployed on-premise, or as an EC2 instance.</li>
<li>Can schedule snapshots.</li>
<li>You can use Storage Gateway with Direct Connect.</li>
<li>You can implement bandwidth throttling.</li>
<li>On-Premise needs with either Vmware’s ESXi or Hyper-V.</li>
<li>Hardware Requirements:<ul>
<li>4 or 8vCPUs</li>
<li>7.5 GB of RAM</li>
<li>75 GB for installation of VM image and system data</li>
</ul>
</li>
</ul>
</li>
<li><p>Storage Gateway - Storage Requirements</p>
<ul>
<li>For gateway-cached volume configuration, you will need storage for the local cache and an upload buffer.</li>
<li>For gateway-stored volume configuration, you will need storage for your entire dataset and an upload buffer. Gateway-stored volumes can range from 1GiB to 1 TB. Each gateway configured for gateway-stored volumes can support up to 12 volumes and a total volume storage of 16TB.</li>
<li>For gateway-VTL configuration, you will need storage for the local cache and an upload buffer.</li>
</ul>
</li>
<li><p>Storage Gateway - Networking Requirements</p>
<ul>
<li>Open port 443 on your firewalls.</li>
<li>Internally, you will need to allow port 80 (activation only), port 3260 (by local systems to connect to iSCSI targets exposed by the gateway) and port UDP 53 (DNS)</li>
</ul>
</li>
<li><p>Storage Gateway - Encryption</p>
<ul>
<li>Data in transit is secured using SSL</li>
<li>Data at rest can be encrypted using AES-256</li>
</ul>
</li>
<li><p>Gateway-Cached and Gateway-Stored Volumes</p>
<ul>
<li>You can take point-in-time, incremental snapshots of your volume and store them in Amazon S3 in the form of Amazon EBS snapshots.</li>
<li>Snapshots can be initiated on a scheduled or ad-hoc basis.</li>
<li>Gateway Stored Snapshots<ul>
<li>If your volume data is stored on-premises, snapshots provide durable, off-site backups in Amazon S3.</li>
<li>You can create a new Gateway-Stored volume from a snapshot in the event you need to recover a backup.</li>
<li>You can also use a snapshot of your Gateway-Stored volume as the starting point for a new Amazon EBS volume which you can then attach to an Amazon EC2 instance.</li>
</ul>
</li>
<li>Gateway Cached Snapshots<ul>
<li>Snapshots can be used to preserve versions of your data, allowing you to revert to a prior version when required or to repurpose a point-in-time version as a new Gateway-Cached volume.</li>
</ul>
</li>
</ul>
</li>
<li><p>Gateway-Virtual Tape Library Retrieval<br>  The virtual tape containing your data must be stored in a Virtual Tape Library before it can be accessed. Access to virtual tapes in your Virtual Tape Library is <strong>instantaneous</strong>.</p>
<p>  If the virtual tape containing your data is in your Virtual Tape Shelf, you must first retrieve the virtual tape from your Virtual Tape Shelf. It takes about <strong>24 Hours</strong> for the retrieved virtual tape to be available in the selected Virtual Tape Library.</p>
</li>
<li><p>Gateway-Virtual Tape Library Supports</p>
<ul>
<li>Symantec NetBackup version 7.x</li>
<li>Symantec Backup Exec 2012</li>
<li>Symantec Backup Exec 2014</li>
<li>Symantec Backup Exec 15</li>
<li>Microsoft System Center 2012 R2 Data Protection Manager</li>
<li>Veeam Backup &amp; Replication V7</li>
<li>Veeam Backup &amp; Replication V8</li>
<li>Dell NetVault Backup 10.0</li>
</ul>
</li>
<li><p>Storage Gateway Exam Tips</p>
<ul>
<li>Know the four different Storage Gateway Types:<ul>
<li>File Gateway</li>
<li>Volume Gateway<ul>
<li>Cached - OLD NAME (Gateway-Cached Volumes)</li>
<li>Stored - OLD NAME (Gateway-Stored Volumes)</li>
</ul>
</li>
<li>Tape Gateway - OLD NAME (Gateway-Virtual Tape Library)</li>
</ul>
</li>
<li>Remember that access to virtual tapes in your virtual tape library are instantaneous. If your tape is in the virtual tape shelf(glacier) it can take 24 hours to get back to your virtual tape library.</li>
<li>Encrypted using SSL for transit and is encrypted at rest in Amazon S3 using AES-256.</li>
<li>Gateway-Stored Volumes - stores data as Amazon EBS Snapshots in S3.</li>
<li>Snapshot can be scheduled.</li>
<li>Bandwidth can be throttled (good for remote sites)</li>
<li>You need a storage gateway in each site if using multiple locations.</li>
</ul>
</li>
<li><p>Snowball</p>
<ul>
<li>Types<ul>
<li>Snowball</li>
<li>Snowball Edge</li>
<li>Snowmobile</li>
</ul>
</li>
<li>Understand what Snowball is</li>
<li>Understand what Import Export is</li>
<li>Snowball Can<ul>
<li>Import to S3</li>
<li>Export from S3</li>
</ul>
</li>
</ul>
</li>
<li><p>Import/Export</p>
<ul>
<li>Import/Export Disk<ul>
<li>Import to S3, EBS, Glacier</li>
<li>export from S3</li>
</ul>
</li>
<li>Import/Export Snowball<ul>
<li>Import to S3</li>
<li>Export to S3</li>
</ul>
</li>
</ul>
</li>
<li><p>S3 Transfer Acceleration</p>
<ul>
<li>You can speed up transfers to S3 using S3 transfer acceleration. This costs extra, and has the greatest impact on people who are in for away location.</li>
</ul>
</li>
</ul>
<ul>
<li><p>S3 static Websites</p>
<ul>
<li>You can use S3 to host static websites</li>
<li>Serverless</li>
<li>Very cheap, scales automatically.</li>
<li>STATIC only, cannot host dynamic sites.</li>
<li>Website url example: <a href="http://examplebucket.s3-website-us-west-2.amazonaws.com/" target="_blank" rel="external">http://examplebucket.s3-website-us-west-2.amazonaws.com/</a></li>
</ul>
</li>
<li><p>S3 CORS</p>
<ul>
<li>Cross Origin Resource Sharing</li>
<li>Need to enable it on the resources bucket and state the URL for the origin that will be calling the bucket.</li>
<li><a href="http://mybucketname.s3-website.en-west-2.amazonaws.com" target="_blank" rel="external">http://mybucketname.s3-website.en-west-2.amazonaws.com</a></li>
<li><a href="https://s3.eu-west-2.amazonaws.com/mybucketname" target="_blank" rel="external">https://s3.eu-west-2.amazonaws.com/mybucketname</a></li>
</ul>
</li>
<li><p>S3 multipart upload advantages</p>
<ul>
<li>Improved throughput - You can upload parts in parallel to improve throughput.</li>
<li>Quick recovery from any network issues - Smaller part size minimizes the impact of restarting a failed upload due to a network error.</li>
<li>Pause and resume object uploads - You can upload object parts over time. Once you initiate a multipart upload there is no expiry; you must explicitly complete or abort the multipart upload.</li>
<li>Begin an upload before you know the final object size - You can upload an object as you are creating it.</li>
</ul>
</li>
<li><p>Last Few Tips</p>
<ul>
<li>Write to S3 - HTTP 200 code for a successful write.</li>
<li>You can load files to S3 much faster by enabling multipart upload.</li>
<li>Read the S3 FAQ before taking the exam. It comes up A LOT!</li>
<li>S3 bucket name rules<ul>
<li>Bucket names must be at least 3 and no more than 63 characters long</li>
<li>Bucket names must be a series of one or more labels. Adjacent labels are separated by a single period (.). Bucket names can contain lowercase letters, numbers, and hyphens. Each label must start and end with a lowercase letter or a number (可以有多个lable，每个lable使用.分割，每个lable中只能包含小写字母，数字和连字符-， lable首尾必须要是小写字母或者数字)</li>
<li>Bucket names must not be formatted as an IP address (e.g., 192.168.5.4).</li>
<li>When using virtual hosted–style buckets with SSL, the SSL wildcard certificate only matches buckets that do not contain periods. To work around this, use HTTP or write your own certificate verification logic. We recommend that you do not use periods (“.”) in bucket names.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Database"><a href="#Database" class="headerlink" title="Database"></a>Database</h4><ul>
<li><p>Database component</p>
<ul>
<li>Database</li>
<li>Tables</li>
<li>Row</li>
<li>Fields(Columns)</li>
</ul>
</li>
<li><p>RDS Types</p>
<ul>
<li>SQL Server</li>
<li>Oracle</li>
<li>MySQL Server</li>
<li>PostgreSQL</li>
<li>Aurora</li>
<li>MariaDB</li>
</ul>
</li>
<li><p>Non Relational Databases</p>
<ul>
<li>Database<ul>
<li>Collection        = Table</li>
<li>Document          = Row</li>
<li>Key Value Pairs   = Fields</li>
</ul>
</li>
</ul>
</li>
<li><p>Data Warehousing</p>
<ul>
<li>Used for business intelligence. Tools like Cognos, Jaspersoft, SQL Server Reporting Services, Oracle Hyperion, SAP NetWeaver.</li>
<li>Used to pull in very large and complex data sets. Usually used by management to do queries on data ( such as current performance vs targets etc)</li>
</ul>
</li>
<li><p>OLTP vs OLAP</p>
<ul>
<li>Online Transaction Processing (OLTP) differs from Online Analytics Processing (OLAP) in terms of the types of queries run.</li>
<li>OLTP Example:<br>  Order number 2120212<br>  Pulls up a row of data such as Name, Date, Address to Deliver to, Delivery Status etc.</li>
<li><p>OLAP transaction Example:<br>  Net profit for EMEA and Pacific for the Digital Radio Product.<br>  Pulls in large numbers of records</p>
<p>  Sum of Radios Sold in EMEA<br>  Sum of Radios Sold in Pacific<br>  Unit Cost of Radio in each region<br>  Sales price of each radio<br>  Sales price - unit cost.</p>
<p>  Data Warehousing databases use different type of architecture both from a database perspective and infrastructure layer.</p>
</li>
</ul>
</li>
<li><p>Elasticache</p>
<ul>
<li>Elastic Cache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based databases.</li>
<li>ElastiCache supports two open-source in-memory caching engines:<ul>
<li>Memcached</li>
<li>Redis</li>
</ul>
</li>
</ul>
</li>
<li><p>DMS</p>
<ul>
<li>Announced at re:Invent 2015, DMS stands for Database Migration Service. Allows you to migrate your production database to AWS. Once the migration has started, AWS manages all the complexities of the migration process like data type transformation, compression, and parallel transfer ( for faster data transfer) while ensuring that data changes to the source database that occur during the migration process are automatically replicated to the target.</li>
<li>AWS schema conversion tool automatically converts the source database schema and a majority of the custom code, including views, stored procedures, and functions, to a format compatible with the target database.</li>
</ul>
</li>
<li><p>Database Summary</p>
<ul>
<li>RDS -OLTP<ul>
<li>SQL</li>
<li>MySQL</li>
<li>PostgreSQL</li>
<li>Oracle</li>
<li>Aurora</li>
<li>MariaDB</li>
</ul>
</li>
<li>DynamoDB - No SQL</li>
<li>Redshift - OLAP</li>
<li>Elasticache - In Memory Caching.<ul>
<li>Memcached</li>
<li>Redis</li>
</ul>
</li>
<li>DMS</li>
</ul>
</li>
</ul>
<h4 id="DynamoDB"><a href="#DynamoDB" class="headerlink" title="DynamoDB"></a>DynamoDB</h4><ul>
<li>Quick Facts about DynameDB<ul>
<li>Stored on SSD Storage</li>
<li>Spread Across 3 geographically distinct data centers</li>
<li>Eventual Consistent Reads (Default)<ul>
<li>Consistency across all copies of data is usually reached within a second. Repeating a read after a short time should return the updated data. (Best Read Performance)</li>
</ul>
</li>
<li>Strongly Consistent Reads<ul>
<li>A strongly consistent read returns a result that reflects all writes that received a successful response prior to the read.</li>
</ul>
</li>
</ul>
</li>
<li><p>The Basic</p>
<ul>
<li>Tables</li>
<li>Items ( Think a row of data in table)</li>
<li>Attributes (Think of a column of data in a table)</li>
</ul>
</li>
<li><p>Pricing</p>
<ul>
<li>Provisioned Throughput Capacity<ul>
<li>Write Throughput $0.0065 per hour for every 10 units</li>
<li>Read Throughput $0.0065 per hour for every 50 units</li>
</ul>
</li>
<li>First 25 GB stored per month is free</li>
<li>Storage costs of $0.25 GB per month there after.</li>
</ul>
</li>
<li><p>Primary Keys</p>
<ul>
<li>Two Types of Primary Keys Available<ul>
<li>Single Attribute (think unique ID)<ul>
<li>Partition Key (Hash Key) composed of one attribute.</li>
</ul>
</li>
<li>Composite (think unique ID and a data range)<ul>
<li>Partition Key &amp; Sort Key (Hash &amp; Range) composed of two attributes.</li>
</ul>
</li>
</ul>
</li>
<li>Partition Key and Sort Key<ul>
<li>DynamoDB uses the partition key’s value as input to an internal hash function. The output from the hash function determines the partition (this is simply the physical location in which the data is stored)</li>
<li>Two items can have the same partition key, but they <strong>must have a different sort key</strong>.</li>
<li>All items with the same partition key are stored together, in sorted order by sort key value.</li>
</ul>
</li>
</ul>
</li>
<li><p>DynamoDB - Indexes</p>
<ul>
<li>Local Secondary Index<ul>
<li>Has the SAME Partition key, different sort key.</li>
<li>Can ONLY be created when creating a table. They cannot be removed or modified later.</li>
</ul>
</li>
<li>Global Secondary Index<ul>
<li>Has DIFFERENT Partition key and different sort key.</li>
<li>Can be created at table creation or added LATER.</li>
</ul>
</li>
</ul>
</li>
<li><p>DynamoDB Streams<br>Used to capture any kind of modification of the DynamoDB tables.</p>
<ul>
<li>If a new item is added to the table, the stream captures an image of the entire item, including all of its attributes.</li>
<li>If an item is updated, the stream captures the “before” and “after” image of any attributes that were modified in the item.</li>
<li>If an item is deleted from the table, the stream captures an image of the entire item before it was deleted</li>
</ul>
</li>
<li><p>Query &amp; Scans</p>
<ul>
<li>Query<ul>
<li>A Query operation finds items in a table using only primary key attribute values. You must provide a partition key attribute name and a distinct value to search for.</li>
<li>You can optionally provide a sort key attribute name and value, and use a comparison operator to refine the search results.</li>
<li>A Scan operation examines every item in the table. By default, a Scan returns all of the data attributes for every item; however, you can use the <strong>ProjectionExpression</strong> parameter so that the Scan only returns some of the attributes, rather than all of them.</li>
<li>Query results are always sorted by the sort key. If the data type of the sort key is a number, the results are returned in numeric order; otherwise, the results are returned in order of ASCII character code values. By default, the sort order is ascending. To reverse the order, set the <strong>ScanIndexForward</strong> parameter to false.</li>
<li>By Default is eventually consistent but can be changed to be strongly consistent.</li>
</ul>
</li>
<li>Scan<ul>
<li>A Scan operation examines every item in the table. By default, a Scan returns all of the data attributes for every item; however, you can use the <strong>ProjectionExpression</strong> parameter so that the Scan only returns some of the attributes, rather than all of them.</li>
</ul>
</li>
<li>Try to use a query operation over a Scan operation as it is more efficient.</li>
</ul>
</li>
<li><p>DynamoDB Provisioned Throughput</p>
<ul>
<li>One read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size</li>
<li>One write capacity unit represents one write per second for an item up to 1 KB in size</li>
<li><strong>400 HTTP Status Code - ProvisionedThroughputExceededException</strong> indicated You exceeded your maximum allowed provisioned throughput for a table or for one or more global secondary indexes.</li>
<li>Example</li>
</ul>
</li>
<li><p>Step taken to authenticate</p>
<ul>
<li>User Authenticates with ID provider (such as Facebook)</li>
<li>They are passed a Token by their ID provider.</li>
<li>Your code calls <strong>AssumeRoleWithWebIdentity</strong> API and provides the providers token and specifies the ARN for the IAM Role</li>
<li>App can now access Dynamodb from between 15 minutes to 1 hour (default is 1 hour)</li>
</ul>
</li>
<li><p>Conditional Writes<br>IF item = $10 then update to $12</p>
<ul>
<li>The conditional writes are idempotent</li>
<li>You can send the same conditional write request multiple times, but it will have no further effect on the item after the first time DynamoDB performs the specified update.</li>
</ul>
</li>
<li><p>Atomic Counters</p>
<ul>
<li>DynamoDB supports atomic counters</li>
<li>use the <strong>UpdateItem</strong> operation to increment or decrement the value of an existing attribute without interfering with other write requests.</li>
<li>All write requests are applied in the order in which they were received</li>
</ul>
</li>
<li><p>Batch Operations<br>If you application needs to read multiple items, you can use the <strong>BatchGetItem</strong> API. A single <strong>BatchGetItem</strong> request can retrieve up to 16 MB of data, which can contain as many as 100 items. In addition, a single BatchGetItem request can retrieve items from multiple tables.</p>
</li>
<li><p>DynamoDB的操作</p>
<ul>
<li>DynamoDB的插入操作<ul>
<li>PutItem</li>
<li>BatchWriteItem (<strong>记住并没有BatchPutItem这种操作</strong>)</li>
</ul>
</li>
<li>DynamoDB的检索操作<ul>
<li>GetItem</li>
<li>BatchGetItem</li>
<li>Query API</li>
</ul>
</li>
</ul>
</li>
<li><p>Dynamodb limit</p>
<ul>
<li>maximum 5 local secondary index per table</li>
<li>maximum 5 global secondary index per table</li>
<li>default provision throughput (Account can increase them by contacting AWS)<ul>
<li>US East (N. Virginia) Region<ul>
<li>Per table – 40,000 read capacity units and 40,000 write capacity units</li>
<li>Per account – 80,000 read capacity units and 80,000 write capacity units</li>
</ul>
</li>
<li>All Other Regions<ul>
<li>Per table – 10,000 read capacity units and 10,000 write capacity units</li>
<li>Per account – 20,000 read capacity units and 20,000 write capacity units</li>
</ul>
</li>
</ul>
</li>
<li>No table size limit</li>
<li>There is an initial limit of 256 tables per region.</li>
<li>Maximum item size is 400KB(包括属性名称和属性值)</li>
<li>No limit on the number of attributes of a item, but the size of item can’t exceed 400KB</li>
<li>BatchGetItem - A single BatchGetItem operation can retrieve a maximum of 100 items. The total size of all the items retrieved cannot exceed 16 MB</li>
<li>Query - The result set from a Query is limited to 1 MB per call</li>
<li>Scan - The result set from a Scan is limited to 1 MB per call</li>
<li>The smallest Reserved Capacity offering is 100 Capacity units(reads or writes)</li>
</ul>
</li>
<li><p><strong>Read The FAQ!!!</strong></p>
</li>
</ul>
<h4 id="SQS"><a href="#SQS" class="headerlink" title="SQS"></a>SQS</h4><ul>
<li>SQS usage example<ul>
<li>Asynchronously pulls the task messages from the queue</li>
<li>Retrieves the named file</li>
<li>Processes the conversion</li>
<li>Writes the image back to Amazon S3</li>
<li>Writes a “task complete” message to another queue</li>
<li>Deletes the original task message</li>
<li>Checks for more messages in the worker queue</li>
</ul>
</li>
</ul>
<ul>
<li><p>SQS Tips</p>
<ul>
<li>Does not offer FIFO</li>
<li>12 hours visibility time out</li>
<li>Amazon SQS is engineered to provide “at least once” delivery of all messages in its queues. Although most of the time each message will be delivered to your application exactly once, you should design your system so that processing a message more than once does not create any errors or inconsistencies.</li>
<li>256kb message size now available</li>
<li>Billed at 64kb “Chunks”</li>
<li>A 256kb message will be 4*64kb “chunks”</li>
<li>You can create any number of message queues.</li>
</ul>
</li>
<li><p>SQS Pricing</p>
<ul>
<li>First 1 million Amazon SQS requests per month are free</li>
<li>$0.05 per 1 million Amazon SQS Requests per month thereafter ($0.00000050 per SQS Request)</li>
<li>A single request can have from 1 to 10 messages, up to a maximum total payload of 256KB.</li>
<li>Each 64KB ‘chunk’ of payload is billed as 1 request. For example, a single API call with a 256KB payload will be billed as four requests.</li>
</ul>
</li>
<li><p>SQS Delivery</p>
<ul>
<li>SQS messages can be delivered multiple times and in any order.</li>
</ul>
</li>
<li><p>SQS Default Visibility Time Out</p>
<ul>
<li>Default Visibility Time Out is 30 Seconds</li>
<li>Maximum Time Out is 12 Hours</li>
<li>When you receive a message from a queue and begin processing it, you may find the visibility timeout for the queue is insufficient to fully process and delete that message. To give yourself more time to process the message, you can extend its visibility timeout by using the <strong>ChangeMessageVisibility</strong> action to specify a new timeout value. Amazon SQS restarts the timeout period using the new value.</li>
</ul>
</li>
<li><p>SQS Long Polling</p>
<ul>
<li>SQS long polling is a way to retrieve messages from your SQS queues. While the traditional SQS short polling returns immediately, even if the queue being polled is empty, SQS long polling doesn’t return a response until a message arrives in the queue, or the long poll times out. SQS long polling makes it easy and inexpensive to retrieve messages from your SQS queue as soon as they are available.</li>
<li>Maximum Long Poll Time Out = 20 seconds</li>
<li>队列属性<strong>ReceiveMessageWaitTimeSeconds</strong>设置为1~20的数字，在队列中启动长轮询</li>
<li>单个ReveiveMessge的请求中将WaitTimeSeconds设置为1~20的数字</li>
</ul>
</li>
<li><p>SQS Fanning Out</p>
<ul>
<li>Create an SNS topic first using SNS. Then create and subscribe multiple SQS queues to the SNS topic.</li>
<li>Now whenever a message is sent to the SNS topic, the message will be fanned out to the SQS queues, i.e. SNS will deliver the message to all the SQS queues that are subscribed to the topic.</li>
</ul>
</li>
</ul>
<h4 id="SNS"><a href="#SNS" class="headerlink" title="SNS"></a>SNS</h4><ul>
<li><p>SNS Benefits</p>
<ul>
<li>Instantaneous, push-based delivery (no polling)</li>
<li>Simple APIs and easy integration with applications</li>
<li>Flexible message delivery over multiple transport protocols</li>
<li>Inexpensive, pay-as-you-go model with no up-front costs</li>
<li>Web-based AWS Management Console offers the simplicity of a point-and-click interface</li>
</ul>
</li>
<li><p>SNS vs SQS</p>
<ul>
<li>Both Messaging Services in AWS</li>
<li>SNS - Push</li>
<li>SQS - Polls (Pulls)</li>
</ul>
</li>
<li><p>SNS Pricing</p>
<ul>
<li>Users pay $0.50 per 1 million Amazon SNS Requests</li>
<li>$0.06 per 100,000 Notification deliveries over HTTP</li>
<li>$0.75 per 100 Notification deliveries over SMS</li>
<li>$2.00 per 100,000 Notification deliveries over Email</li>
</ul>
</li>
<li><p>SNS Summary</p>
<ul>
<li>Instantaneous, push-bashed delivery (no polling)</li>
<li>Protocols include:<ul>
<li>HTTP</li>
<li>HTTPS</li>
<li>Email</li>
<li>Email-JSON</li>
<li>Amazon SQS</li>
<li>Application</li>
<li>AWS Lambda</li>
<li>SMS</li>
</ul>
</li>
<li>Messages can be customized for each protocol</li>
</ul>
</li>
<li><p>Valid arguments for an SNS Publish request</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">POST / HTTP/1.1</div><div class="line">x-amz-sns-message-type: SubscriptionConfirmation</div><div class="line">x-amz-sns-message-id: 165545c9-2a5c-472c-8df2-7ff2be2b3b1b</div><div class="line">x-amz-sns-topic-arn: arn:aws:sns:us-west-2:123456789012:MyTopic</div><div class="line">Content-Length: 1336</div><div class="line">Content-Type: text/plain; charset=UTF-8</div><div class="line">Host: myhost.example.com</div><div class="line">Connection: Keep-Alive</div><div class="line">User-Agent: Amazon Simple Notification Service Agent</div><div class="line"></div><div class="line">&#123;</div><div class="line">  &quot;Type&quot; : &quot;SubscriptionConfirmation&quot;,</div><div class="line">  &quot;MessageId&quot; : &quot;165545c9-2a5c-472c-8df2-7ff2be2b3b1b&quot;,</div><div class="line">  &quot;Token&quot; : &quot;2336412f37fb687f5d51e6e241d09c805a5a57b30d712f794cc5f6a988666d92768dd60a747ba6f3beb71854e285d6ad02428b09ceece29417f1f02d609c582afbacc99c583a916b9981dd2728f4ae6fdb82efd087cc3b7849e05798d2d2785c03b0879594eeac82c01f235d0e717736&quot;,</div><div class="line">  &quot;TopicArn&quot; : &quot;arn:aws:sns:us-west-2:123456789012:MyTopic&quot;,</div><div class="line">  &quot;Message&quot; : &quot;You have chosen to subscribe to the topic arn:aws:sns:us-west-2:123456789012:MyTopic.\nTo confirm the subscription, visit the SubscribeURL included in this message.&quot;,</div><div class="line">  &quot;SubscribeURL&quot; : &quot;https://sns.us-west-2.amazonaws.com/?Action=ConfirmSubscription&amp;TopicArn=arn:aws:sns:us-west-2:123456789012:MyTopic&amp;Token=2336412f37fb687f5d51e6e241d09c805a5a57b30d712f794cc5f6a988666d92768dd60a747ba6f3beb71854e285d6ad02428b09ceece29417f1f02d609c582afbacc99c583a916b9981dd2728f4ae6fdb82efd087cc3b7849e05798d2d2785c03b0879594eeac82c01f235d0e717736&quot;,</div><div class="line">  &quot;Timestamp&quot; : &quot;2012-04-26T20:45:04.751Z&quot;,</div><div class="line">  &quot;SignatureVersion&quot; : &quot;1&quot;,</div><div class="line">  &quot;Signature&quot; : &quot;EXAMPLEpH+DcEwjAPg8O9mY8dReBSwksfg2S7WKQcikcNKWLQjwu6A4VbeS0QHVCkhRS7fUQvi2egU3N858fiTDN6bkkOxYDVrY0Ad8L10Hs3zH81mtnPk5uvvolIC1CXGu43obcgFxeL3khZl8IKvO61GWB6jI9b5+gLPoBc1Q=&quot;,</div><div class="line">  &quot;SigningCertURL&quot; : &quot;https://sns.us-west-2.amazonaws.com/SimpleNotificationService-f3ecfb7224c7233fe7bb5f59f96de52f.pem&quot;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
</li>
<li><p>SNS limit</p>
<ul>
<li>Topic names are limited to 256 characters.</li>
<li>Token included in the confirmation message sent to end-points on a subscription request are valid for 3 days.</li>
</ul>
</li>
</ul>
<h4 id="SWF"><a href="#SWF" class="headerlink" title="SWF"></a>SWF</h4><ul>
<li>SWF worker<ul>
<li>Workers are programs that interact with Amazon SWF to get tasks, process received tasks, and return the results.</li>
</ul>
</li>
<li>SWF Decider<ul>
<li>The Decider is a program that controls the coordination of tasks, i.e. their ordering, concurrency, and scheduling according to the application logic.</li>
</ul>
</li>
<li>SWF Workers &amp; Deciders<ul>
<li>The workers and the decider can run on cloud infrastructure, such as Amazon EC2, or on machines behind firewalls. Amazon SWF brokers the interactions between workers and the decider. It allows the decider to get consistent views into the progress of tasks and to initiate new tasks in an ongoing manner. At the same time, Amazon SWF stores tasks, assigns them to workers when they are ready, and monitors their progress. It ensures that a task is assigned only once and is never duplicated. Since Amazon SWF maintains the application’s state durably, workers and deciders don’t have to keep track of execution state. They can run independently and scale quickly.</li>
</ul>
</li>
<li>SWF Domains<ul>
<li>Your workflow and activity types and the workflow execution itself are all scoped to a domain. Domains isolate a set of types, executions, and task lists from others within the same account. You can register a domain by using the AWS Management Console or by using the RegisterDomain action in the Amazon SWF API.</li>
<li>The parameter are specified in JavaScript Object Notation(JSON) format.<br><a href="https://swf.us-east-1.amazonaws.com" target="_blank" rel="external">https://swf.us-east-1.amazonaws.com</a><br>RetisterDomain<br>{<br>  “name” : “1234567”,<br>  “description” : “music”,<br>  “workflowExecutionRetentionPeriodInDays” : “60”<br>}</li>
</ul>
</li>
<li>Maximum WorkFlow can be 1 year and the value is always measured in seconds.</li>
<li><p>SWF vs SQS</p>
<ul>
<li>Amazon SWF presents a task-oriented API, whereas Amazon SQS offers a message-oriented API.</li>
<li>Amazon SWF ensures that a task is assigned only once and is never duplicated. With Amazon SQS, you need to handle duplicated messages and may also need to ensure that a message is processed only once.</li>
<li>Amazon SWF keeps track of all the tasks and events in an application. With Amazon SQS, you need to implement your own application-level tracking especially if your application uses multiple queues.</li>
</ul>
</li>
<li><p>SWF limit</p>
<ul>
<li>maximum number of SWF domains is 100 (includes both registered and deprecated domains)</li>
<li>Maximum workflow and activity types - 10,000 each per domain (includes both registered and deprecated types)</li>
<li>Maximum request size is 1 MB per request (including the request header and all other associated request data.)</li>
<li>Maximum open workflow executions - 100,000 per domain (includes child workflow executions)</li>
<li>Maximum workflow execution time - 1 year</li>
<li>You can only have a maximum of 1,000 open activity tasks per workflow execution.</li>
</ul>
</li>
</ul>
<h4 id="Elastic-Beanstalk"><a href="#Elastic-Beanstalk" class="headerlink" title="Elastic Beanstalk"></a>Elastic Beanstalk</h4><ul>
<li>Supported Application<ul>
<li>Java</li>
<li>.NET</li>
<li>PHP</li>
<li>Node.js</li>
<li>Python</li>
<li>Ruby</li>
<li>Go</li>
<li>Docker</li>
</ul>
</li>
<li>Its uses ASG, ELB, EC2, RDS, SNS and S3 to provision things.</li>
<li>Environment Tier - WebServer, Worker</li>
<li>Preconfigured docker:<ul>
<li>Glassfish</li>
<li>Python</li>
<li>Go</li>
</ul>
</li>
<li>Environment URL - has to be unique</li>
<li>Console Item<ul>
<li>Dashboard</li>
<li>Configuration<ul>
<li>Scaling</li>
<li>Instances(DIRTMCG instance types, key pair)</li>
<li>Notifications</li>
<li>Software configuration</li>
<li>Updates and Deployments</li>
<li>Health</li>
<li>Managed Updates</li>
<li>Networking tier(ELB, VPC)</li>
<li>Data tier(RDS)</li>
</ul>
</li>
<li>Logs</li>
<li>Health</li>
<li>Monitoring</li>
<li>Alarms</li>
<li>Managed Updates</li>
<li>Events</li>
<li>Tags</li>
</ul>
</li>
</ul>
<h4 id="Route53-DNS"><a href="#Route53-DNS" class="headerlink" title="Route53 DNS"></a>Route53 DNS</h4><ul>
<li>ELB’s do not have pre-defined IPv4 addresses, you resolve to them using a DNS name.</li>
<li>Understand the difference between an Alias Record and a CNAME</li>
<li>Given the choice, always choose an Alias Record over a CNAME.</li>
<li>Remember the different routing policies and their use cases.<ul>
<li>Simple</li>
<li>Weighted</li>
<li>Latency</li>
<li>Failover</li>
<li>Geolocation</li>
</ul>
</li>
</ul>
<h4 id="VPC"><a href="#VPC" class="headerlink" title="VPC"></a>VPC</h4><ul>
<li>Basic Info<ul>
<li>Think of a VPC as a logical datacenter in AWS</li>
<li>Consists of IGW’s (Or Virtual Private Gateways), Route Tables, Network Access Control Lists, Subnets, Security Groups</li>
<li>1 Subnet = 1 Availability Zone</li>
<li>Security Groups are Stateful, Network Access Control Lists are Stateless.</li>
<li>Can Peer VPCs both in the same account and with other AWS accounts.</li>
<li>No Transitive Peering</li>
<li>Custom VPC network block size has to be between a /16 netmask and /28 netmask.</li>
</ul>
</li>
</ul>
<ul>
<li>What can you do with a VPC<ul>
<li>Launch instances into a subnet of your choosing</li>
<li>Assign custom IP address ranges in each subnet</li>
<li>Configure route tables between subnets</li>
<li>Create internet gateway and attach it to our VPC</li>
<li>Much better security control over your AWS resources</li>
<li>Instance security groups</li>
<li>Subnet network access control lists (ACLS)</li>
</ul>
</li>
</ul>
<ul>
<li><p>Default VPC vs Custom VPC</p>
<ul>
<li>Default VPC is user friendly, allowing you to immediately deploy instances</li>
<li>All Subnets in default VPC have a route out to the internet.</li>
<li>Each EC2 instance has both a public and private IP address</li>
<li>If you delete the default VPC the only way to get it back is to contact AWS.</li>
</ul>
</li>
<li><p>VPC peering</p>
<ul>
<li>Allows you to connect one VPC with another via a direct network route using private IP addresses.</li>
<li>Instances behave as if they were on the same private network</li>
<li>You can peer VPC’s with other AWS accounts as well as with other VPCs in the same account.</li>
<li>Peering is in a star configuration, ie 1 central VPC peers with 4 others, <strong>NO TRANSITIVE PEERING!!!</strong></li>
</ul>
</li>
<li><p>Create VPC</p>
<ul>
<li>things automatically created<ul>
<li>Route tables</li>
<li>Network ACLs</li>
<li>Security Groups</li>
<li>DHCP options set</li>
</ul>
</li>
<li>things are not automatically created<ul>
<li>Internet Gateways</li>
<li>Subnets</li>
</ul>
</li>
</ul>
</li>
<li><p>VPC Subnet</p>
<ul>
<li>There are 5 IP address reserved in each subnet by AWS, take CIDR block 10.0.0.0/24 as example<ul>
<li>10.0.0.0 Network address</li>
<li>10.0.0.1 Reserved by AWS for the VPC router</li>
<li>10.0.0.2 Reserved by AWS for DNS</li>
<li>10.0.0.3 Reserved by AWS for future use.</li>
<li>10.0.0.255 Network broadcast address, we do not support broadcast in a VPC, therefore we reserve this address.</li>
</ul>
</li>
</ul>
</li>
<li><p>NAT instances</p>
<ul>
<li>When creating a NAT instance, Disable Source/Destination Check on the Instance</li>
<li>NAT instance must be in a public subnet</li>
<li>Must have an elastic IP address to work</li>
<li>There must be a route out of the private subnet to the NAT instance, in order for this to work</li>
<li>The amount of traffic that NAT instances supports, depends on the instance size. If you are bottlenecking, increase the instance size</li>
<li>You can create high availability using Autoscaling Groups, multiple subnets in different AZ’s and a script to automate failover</li>
<li>Behind a Security Group.</li>
</ul>
</li>
<li><p>NAT Gateways</p>
<ul>
<li>Very new</li>
<li>Preferred by the enterprise</li>
<li>Scale automatically up to 10Gbps</li>
<li>No need to patch</li>
<li>Not associated with security groups</li>
<li>Automatically assigned a public ip address</li>
<li>Remember to update your route tables.</li>
<li>No need to disable Source/Destination Checks.</li>
</ul>
</li>
<li><p>NAT instances vs NAT Gateways</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Attribute</th>
<th style="text-align:left">NAT gateway</th>
<th style="text-align:left">NAT instance</th>
</tr>
</thead>
<tbody>
<tr>
<td>Availability</td>
<td style="text-align:left">Highly available. NAT gateways in each Availability Zone are implemented with redundancy. Create a NAT gateway in each Availability Zone to ensure zone-independent architecture.</td>
<td style="text-align:left">Use a script to manage failover between instances.</td>
</tr>
<tr>
<td>Bandwidth</td>
<td style="text-align:left">Supports bursts of up to 10Gbps.</td>
<td style="text-align:left">Depends on the bandwidth of the instance type.</td>
</tr>
<tr>
<td>Maintenance</td>
<td style="text-align:left">Managed by AWS.You do not need to perform any maintenance.</td>
<td style="text-align:left">Managed by you, for example, by installing software updates or operating system patches on the instance.</td>
</tr>
<tr>
<td>Performance</td>
<td style="text-align:left">Software is optimized for handling NAT traffic.</td>
<td style="text-align:left">A generic Amazon Linux AMI that’s configured to perform NAT.</td>
</tr>
<tr>
<td>Cost</td>
<td style="text-align:left">Charged depending on the number of NAT gateways you use, duration of usage, and amount of data that you send through the NAT gateways.</td>
<td style="text-align:left">Charged depending on the number of NAT instances that you use, duration of usage, and instance type and size.</td>
</tr>
<tr>
<td>Type and size</td>
<td style="text-align:left">Uniform offering; you don’t need to decide on the type or size.</td>
<td style="text-align:left">Choose a suitable instance type and size, according to your predicted workload.</td>
</tr>
<tr>
<td>Public IP addresses</td>
<td style="text-align:left">Choose the Elastic IP address to associate with a NAT gateway at creation.</td>
<td style="text-align:left">Use an Elastic IP address or a public IP address with a NAT instance. You can change the public IP address at any time by associating a new Elastic IP address with the instance.</td>
</tr>
<tr>
<td>Private IP addresses</td>
<td style="text-align:left">Automatically selected from the subnet’s IP address range when you create the gateway.</td>
<td style="text-align:left">Assign a specific private IP address from the subnet’s IP address range when you launch the instance.</td>
</tr>
<tr>
<td>Security groups</td>
<td style="text-align:left">Cannot be associated with a NAT gateway. You can associate security groups with your resources behind the NAT gateway to control inbound and outbound traffic.</td>
<td style="text-align:left">Associate with your NAT instance and the resources behind your NAT instance to control inbound and outbound traffic.</td>
</tr>
<tr>
<td>Network ACLs</td>
<td style="text-align:left">Use a network ACL to control the traffic to and from the subnet in which your NAT gateway resides.</td>
<td style="text-align:left">Use a network ACL to control the traffic to and from the subnet in which your NAT instance resides.</td>
</tr>
<tr>
<td>Flow logs</td>
<td style="text-align:left">Use flow logs to capture the traffic.</td>
<td style="text-align:left">Use flow logs to capture the traffic.</td>
</tr>
<tr>
<td>Port forwarding</td>
<td style="text-align:left">Not supported.</td>
<td style="text-align:left">Manually customize the configuration to support port forwarding.</td>
</tr>
<tr>
<td>Bastion servers</td>
<td style="text-align:left">Not supported.</td>
<td style="text-align:left">Use as a bastion server.</td>
</tr>
<tr>
<td>Traffic metrics</td>
<td style="text-align:left">Not supported.</td>
<td style="text-align:left">View CloudWatch metrics.</td>
</tr>
<tr>
<td>Timeout behavior</td>
<td style="text-align:left">When a connection times out, a NAT gateway returns an RST packet to any resources behind the NAT gateway that attempt to continue the connection (it does not send a FIN packet).</td>
<td style="text-align:left">When a connection times out, a NAT instance sends a FIN packet to resources behind the NAT instance to close the connection.</td>
</tr>
<tr>
<td>IP fragmentation</td>
<td style="text-align:left">Supports forwarding of IP fragmented packets for the UDP protocol. Does not support fragmentation for the TCP and ICMP protocols. Fragmented packets for these protocols will get dropped.</td>
<td style="text-align:left">Supports reassembly of IP fragmented packets for the UDP, TCP, and ICMP protocols.</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Network ACL’s</p>
<ul>
<li>Your VPC automatically comes a default network ACL and by default it allows all outbound and inbound traffic.</li>
<li>You can create a custom network ACL. By default, each custom network ACL denies all inbound and outbound traffic until you add rules.</li>
<li>Each subnet in your VPC must be associated with a network ACL. If you don’t explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL.</li>
<li>You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed.</li>
<li>A network ACl contains a numbered list of rules that is evaluated in order, starting with the lowest numbered rule.</li>
<li>A network ACl has separate inbound and outbound rules, and each rule can either allow or deny traffic.</li>
<li>Network ACLs are stateless responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa)</li>
<li>Block IP Addresses using network ACL’s not Security Groups</li>
</ul>
</li>
<li><p>Security Group vs Network ACL</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Security Group</th>
<th style="text-align:left">Network ACL</th>
</tr>
</thead>
<tbody>
<tr>
<td>operates at the instance level (first layer of defense)</td>
<td style="text-align:left">Operates at the subnet level (second layer of defense)</td>
</tr>
<tr>
<td>Supports allow rules only</td>
<td style="text-align:left">Supports allow rules and deny rules</td>
</tr>
<tr>
<td>Is stateful: Return traffic is automatically allowed, regardless of any rules</td>
<td style="text-align:left">Is stateless: Return traffic must be explicitly allowed by rules</td>
</tr>
<tr>
<td>We evaluate all rules before deciding whether to allow traffic</td>
<td style="text-align:left">We process rules in number order when deciding whether to allow traffic</td>
</tr>
<tr>
<td>Applies to an instance only if someone specifies the security group when launching the instance, or associates the security group with the instance later on</td>
<td style="text-align:left">Automatically applies to all instances in the subnets it’s associated with (backup layer of defense, so you don’t have to rely on someone specifying the security group)</td>
</tr>
</tbody>
</table>
<ul>
<li><p>NAT vs Bastions</p>
<ul>
<li>A NAT is used to provide internet traffic to EC2 instances in private subnets</li>
<li>A Bastion is used to securely administer EC2 instance (using SSH or RDP) in private subnets. In Australia we call them jump boxes.</li>
</ul>
</li>
<li><p>Resilient Architecture</p>
<ul>
<li>If you want resiliency, always have 2 public subnets and 2 private subnets. Make sure each subnet is in different availability zones.</li>
<li>With ELB’s make sure they are in 2 public subnets in 2 different availability zones.</li>
<li>With Bastion hosts, put them behind an autoscaling group with a minimum size of 2. Use Route53 (either round robin or using a health check) to automatically fail over.</li>
<li>NAT instances are tricky to make resilient. You need 1 in each public subnet, each with their own public IP address, and you need to write a script to fail between the two. Instead where possible, use NAT gateways.</li>
</ul>
</li>
<li><p>VPC Flow Logs</p>
<ul>
<li>You can monitor network traffic within your custom VPC’s using VPC Flow Logs.</li>
</ul>
</li>
<li><p>VPC limit</p>
<ul>
<li>Currently you can create 200 subnets per VPC by default</li>
</ul>
</li>
</ul>
<h4 id="CloudFormation"><a href="#CloudFormation" class="headerlink" title="CloudFormation"></a>CloudFormation</h4><ul>
<li>Use of CFT(CloudFormation Templates), Beanstalk and AutoScaling are free but you pay for the AWS resources that these services create.</li>
<li>Fn::GetAtt - values that you can use to return result for an AWS created resource or used to display in output</li>
<li>By Default - rollback everything on error</li>
<li>Infrastructure as a code, Version controlled, declarative and flexible</li>
<li>API ListStackResources is used to list all resources that belong to a CloudFormation Stack</li>
<li><p>You can use intrinsic functions only in specific parts of a template. Currently, you can use intrinsic functions in resource properties, metadata attributes, and update policy attributes.</p>
</li>
<li><p>CloudFormation Basic</p>
<ul>
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/gettingstarted.templatebasics.html" target="_blank" rel="external">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/gettingstarted.templatebasics.html</a></li>
<li>Learn about the following about templates:<ul>
<li>Declaring resources and their properties</li>
<li><strong>Ref</strong>erencing(提取) other resources with the <strong>Ref</strong> function and resource attributes using the Fn::GetAtt function</li>
<li>Using parameters to enable users to specify values at stack creation time and using constraints to validate parameter input</li>
<li>Using mappings to determine conditional values</li>
<li>Using the Fn::Join function to construct values based on parameters, resource attributes, and other things</li>
<li>Using output values based to capture information about the stack’s resources.</li>
</ul>
</li>
</ul>
</li>
<li><p>CloudFormation intrinsic Function</p>
<ul>
<li>Fn::Base64<ul>
<li>returns the Base64 representation of the input string. this function is typically used to pass encoded data to Amazon EC2 instances by way of the UserData property.</li>
<li>JSON Format { “Fn::Base64” : valueToEncode }</li>
</ul>
</li>
<li>Fn::FindInMap<ul>
<li>returns the value corresponding to keys in a two-level map that is declared in the Mapping section.</li>
<li>JSON Format { “Fn::FindInMap” : [ “MapName”, “TopLevelKey”, “SecondLevelKey”] }</li>
</ul>
</li>
<li>Fn::GetAtt<ul>
<li>returns the value of an attribute from a resource in the template</li>
<li>JSON Format { “Fn::GetAtt” : [ “logicalNameOfResource”, “attributeName” ] }</li>
</ul>
</li>
<li>Fn::Join<ul>
<li>Fn::Join appends a set of values into a single value, separated by the specified delimiter. If a delimiter is the empty string, the set of values are concatenated with no delimiter.</li>
<li>JSON Format { “Fn::Join” : [ “delimiter”, [ comma-delimited list of values ] ] }</li>
<li>JSON example {“Fn::Join” : [ “:”, [ “a”, “b”, “c” ] ]} will returns “a:b:c”</li>
</ul>
</li>
<li>Fn::Select<ul>
<li>returns a single object from a list of objects by index</li>
<li>JSON Format { “Fn::Select” : [ index, listOfObjects ] }</li>
<li>JSON example { “Fn::Select” : [ “1”, [ “apples”, “grapes”, “oranges”, “mangoes” ] ] } will returns “grapes”</li>
</ul>
</li>
<li>Fn::Split<ul>
<li>To split a string into a list of string values so that you can select an element from the resulting string list.</li>
<li>JSON Format { “Fn::Split” : [ “delimiter”, “source string” ] }</li>
<li>JSON example { “Fn::Split” : [ “|” , “a|b|c” ] } will return [“a”, “b”, “c”]</li>
</ul>
</li>
<li><p>Fn::Sub</p>
<ul>
<li>将输入字符串中的变量替换为您指定的值</li>
<li>JSON Format  { “Fn::Sub” : [ String, { Var1Name: Var1Value, Var2Name: Var2Value } ] }</li>
<li>JSON example - 将AWS::Region和AWS::StackName替换为实际的值<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&quot;UserData&quot;: &#123; &quot;Fn::Base64&quot;: &#123; &quot;Fn::Join&quot;: [&quot;\n&quot;, [</div><div class="line">&quot;#!/bin/bash -xe&quot;,</div><div class="line">&quot;yum update -y aws-cfn-bootstrap&quot;,</div><div class="line">&#123; &quot;Fn::Sub&quot;: &quot;/opt/aws/bin/cfn-init -v --stack $&#123;AWS::StackName&#125; --resource LaunchConfig --configsets wordpress_install --region $&#123;AWS::Region&#125;&quot; &#125;,</div><div class="line">&#123; &quot;Fn::Sub&quot;: &quot;/opt/aws/bin/cfn-signal -e $? --stack $&#123;AWS::StackName&#125; --stack $&#123;AWS::StackName&#125; --resource WebServer --region $&#123;AWS::Region&#125;&quot; &#125;]]</div><div class="line">&#125;&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Ref</p>
<ul>
<li>returns the value of the specified parameter or resource.<ul>
<li>When you specify a parameter’s logical name, it returns the value of the parameter.</li>
<li>When you specify a resource’s logical name, it returns a value that you can typically use to refer to that resource, such as a physical ID.</li>
</ul>
</li>
<li>JSON Format { “Ref” : “logicalName” }</li>
</ul>
</li>
</ul>
</li>
<li><p>CloudFormation limit</p>
<ul>
<li>You can include up to 60 parameters and 60 outputs in a template.</li>
<li>There are no limit to the number of templates.</li>
<li>Each AWS CloudFormation account is limited to a maximum of 200 stacks.</li>
</ul>
</li>
<li><p>CloudFormation – Ref, Fn::Join, GetAtt, Fn::split, Fn::select and etc function</p>
</li>
</ul>
<h2 id="Doc"><a href="#Doc" class="headerlink" title="Doc"></a>Doc</h2><h3 id="DynamoDB-Doc"><a href="#DynamoDB-Doc" class="headerlink" title="DynamoDB Doc"></a>DynamoDB Doc</h3><ul>
<li><a href="https://docs.aws.amazon.com/zh_cn/amazondynamodb/latest/developerguide/Introduction.html" target="_blank" rel="external">DynamoDB Developer Guide (Html)</a></li>
<li><a href="https://docs.aws.amazon.com/zh_cn/amazondynamodb/latest/developerguide/dynamodb-dg-zh_cn.pdf" target="_blank" rel="external">DynamoDB Developer Guide (PDF)</a></li>
<li><a href="https://amazonaws-china.com/dynamodb/faqs/" target="_blank" rel="external">DynamoDB FAQ</a></li>
</ul>
<h3 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h3><ul>
<li><a href="https://amazonaws-china.com/cn/dynamodb/faqs/?nc1=h_ls" target="_blank" rel="external">https://amazonaws-china.com/cn/dynamodb/faqs/?nc1=h_ls</a></li>
<li><a href="https://amazonaws-china.com/lambda/faqs/" target="_blank" rel="external">https://amazonaws-china.com/lambda/faqs/</a></li>
<li><a href="https://amazonaws-china.com/api-gateway/faqs/" target="_blank" rel="external">https://amazonaws-china.com/api-gateway/faqs/</a></li>
</ul>
<h3 id="考点"><a href="#考点" class="headerlink" title="考点"></a>考点</h3><ul>
<li><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers.html" target="_blank" rel="external">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers.html</a></li>
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html" target="_blank" rel="external">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a><ul>
<li>Template Sections</li>
</ul>
</li>
<li><a href="https://aws.amazon.com/security/penetration-testing/" target="_blank" rel="external">https://aws.amazon.com/security/penetration-testing/</a></li>
</ul>
<h3 id="注意点摘录"><a href="#注意点摘录" class="headerlink" title="注意点摘录:"></a>注意点摘录:</h3><ul>
<li>筛选表达式(–filter-expression)在 Scan 已完成但结果尚未返回时应用。因此，无论是否存在筛选表达式，Scan 都将占用同等数量的读取容量</li>
</ul>
<h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><ul>
<li>scalar data types –&gt; 标量数据类型</li>
<li>collection data types –&gt; 集合数据类型</li>
<li>projection –&gt; 投影</li>
<li>Fine Grained Access Control (FGAC)  –&gt; 精细访问控制</li>
<li>write-through –&gt; 直写</li>
<li>optimistic – 乐观</li>
</ul>
<h2 id="外部资料"><a href="#外部资料" class="headerlink" title="外部资料"></a>外部资料</h2><ul>
<li>老外心得 <a href="https://acloud.guru/forums/aws-certified-developer-associate/discussion/-KUdI5f2LNbi4wvK7v4I/how-to-pass-aws-certified-developer-exam" target="_blank" rel="external">https://acloud.guru/forums/aws-certified-developer-associate/discussion/-KUdI5f2LNbi4wvK7v4I/how-to-pass-aws-certified-developer-exam</a></li>
<li>老外心得 <a href="https://acloud.guru/forums/aws-certified-developer-associate/discussion/-KPuWHwfTCiCJNsGzAwu/passed" target="_blank" rel="external">https://acloud.guru/forums/aws-certified-developer-associate/discussion/-KPuWHwfTCiCJNsGzAwu/passed</a></li>
<li>re:Invent videos. “Deep Dive on Amazon DynamoDB” <a href="https://www.youtube.com/watch?v=bCW3lhsJKfw" target="_blank" rel="external">https://www.youtube.com/watch?v=bCW3lhsJKfw</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> AWS Certified </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DynamoDB </tag>
            
            <tag> AWS SDK </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用Hexo搭建Blog]]></title>
      <url>/2017/07/17/hexo-github-blog/</url>
      <content type="html"><![CDATA[<h2 id="Hexo-安装"><a href="#Hexo-安装" class="headerlink" title="Hexo 安装"></a>Hexo 安装</h2><h3 id="安装前提"><a href="#安装前提" class="headerlink" title="安装前提"></a>安装前提</h3><p>安装Hexo需要依赖如下两个程序, 需要提前安装</p>
<ul>
<li>Node.js</li>
<li>git</li>
</ul>
<h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><p>Node.js和git都安装完毕后，执行如下命令安装Hexo</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install -g hexo-cli</div></pre></td></tr></table></figure>
<h3 id="初始化Blog"><a href="#初始化Blog" class="headerlink" title="初始化Blog"></a>初始化Blog</h3><p>cd到存放博客的目标目录，执行<code>hexo init</code>命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo init my_blog</div></pre></td></tr></table></figure>
<p>其中<code>my_blog</code>就是博客所在的文件夹名字。</p>
<p>注意: 最新版的<code>hexo</code>不需要切换到文件夹下敲击<code>npm install</code>了，<code>init</code>的时候会一并安装所需的npm packet。</p>
<p>进入目录，目录结构类似如下.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">$ cd my_blog/</div><div class="line">$ tree -I &quot;node_modules&quot; ./</div><div class="line">./</div><div class="line">├── _config.yml</div><div class="line">├── db.json</div><div class="line">├── package.json</div><div class="line">├── scaffolds</div><div class="line">│   ├── draft.md</div><div class="line">│   ├── page.md</div><div class="line">│   └── post.md</div><div class="line">├── source</div><div class="line">│   └── _posts</div><div class="line">│       └── hello-world.md</div><div class="line">└── themes</div></pre></td></tr></table></figure>
<p>执行下面的命令开启<code>hexo</code>服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo -s --debug</div></pre></td></tr></table></figure>
<p>访问 <code>http://0.0.0.0:4000</code>应该就能看到默认的页面了。</p>
<a id="more"></a>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="基本信息配置"><a href="#基本信息配置" class="headerlink" title="基本信息配置"></a>基本信息配置</h3><p>打开博客根目录下的<code>_config.yml</code>文件，修改<code>title</code>, <code>subtitle</code>, <code>description</code>, <code>author</code>, <code>url</code>等个人信息</p>
<p>将<code>language</code>设置为<code>default</code>.</p>
<p>配置文件中默认参数的描述可以参见官网说明 <a href="https://hexo.io/zh-cn/docs/configuration.html" target="_blank" rel="external">https://hexo.io/zh-cn/docs/configuration.html</a></p>
<h2 id="更换Theme"><a href="#更换Theme" class="headerlink" title="更换Theme"></a>更换Theme</h2><p>以下以更换<a href="(https://github.com/wzpan/hexo-theme-freemind/)">hexo-theme-freemind</a>主题为例:</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>使用如下命令安装<a href="(https://github.com/wzpan/hexo-theme-freemind/)">hexo-theme-freemind</a>主题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git clone https://github.com/wzpan/hexo-theme-freemind.git themes/freemind</div></pre></td></tr></table></figure>
<p>安装可选插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-tag-bootstrap --save</div><div class="line">$ npm install hexo-tag-bootstrap --save</div></pre></td></tr></table></figure>
<h3 id="启用freemind预定义的几个pages"><a href="#启用freemind预定义的几个pages" class="headerlink" title="启用freemind预定义的几个pages"></a>启用freemind预定义的几个pages</h3><p>Freemind 预先定义了 Categories（分类）、Tags（标签） 和 About（关于）页面，要使用它们，你需要先在博客的<code>source</code>目录中添加相应页面。</p>
<p>使用如下命令来生成几个Pages页面</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ hexo new page &quot;tags&quot;</div><div class="line">$ hexo new page &quot;categories&quot;</div><div class="line">$ hexo new page &quot;about&quot;</div></pre></td></tr></table></figure>
<p>修改生成的目录下的<code>index.md</code>文件为如下内容:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">$ cat about/index.md</div><div class="line">---</div><div class="line">title: About</div><div class="line">layout: about</div><div class="line">---</div><div class="line">$ cat categories/index.md</div><div class="line">---</div><div class="line">title: Categories</div><div class="line">layout: categories</div><div class="line">---</div><div class="line">$ cat tags/index.md</div><div class="line">---</div><div class="line">title: Tags</div><div class="line">layout: tags</div><div class="line">---</div><div class="line">$</div></pre></td></tr></table></figure>
<h3 id="启用-freemind"><a href="#启用-freemind" class="headerlink" title="启用 freemind"></a>启用 freemind</h3><p>在根目录_config.yml中，替换<code>theme</code>选项为<code>freemind</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># Extensions</div><div class="line">## Plugins: https://hexo.io/plugins/</div><div class="line">## Themes: https://hexo.io/themes/</div><div class="line"># theme: landscape</div><div class="line">theme: freemind</div></pre></td></tr></table></figure>
<h3 id="修改freemind的配置文件"><a href="#修改freemind的配置文件" class="headerlink" title="修改freemind的配置文件"></a>修改freemind的配置文件</h3><ul>
<li>修改<code>slogan</code></li>
<li>修改<code>links</code>为自己想要链接的网址</li>
<li>暂时不想开启评论，因此注释掉了<code>comment_js</code></li>
<li>修改<code>theme</code>来调整color theme, <code>freemind</code>所支持的<a href="http://www.hahack.com/hexo-theme-freemind/2016/01/30/color-themes/" target="_blank" rel="external">color theme</a></li>
</ul>
<p>freemind配置文件的详细解释参见<a href="(https://github.com/wzpan/hexo-theme-freemind/)">freemind github</a>中的Configuration章节。</p>
<h3 id="freemind-front-matter-选项"><a href="#freemind-front-matter-选项" class="headerlink" title="freemind front-matter 选项"></a>freemind front-matter 选项</h3><p>根据<a href="(https://github.com/wzpan/hexo-theme-freemind/)">Github</a>中的描述，<code>freemind</code>共提供了如下5个设置:</p>
<ul>
<li>description - a short description about the articles that will be display at the top of the post</li>
<li>feature - sets a feature image that will be show at the index page</li>
<li>toc - renders a table of contents</li>
<li>top - pin the article to top if it is set to true</li>
<li>issue_id - comment.js issue_id for explicitly point out which Github issue should be connect to your post. For most situations you don’t need it unless the post doesn’t link to the issue you want.</li>
</ul>
<p>以Hexo默认生成的<code>_posts/hello-world.md</code>的为例来展示<code>description</code>,<code>feature</code>,<code>toc</code>,<code>top</code>四个设置的显示效果。</p>
<p>首先修改后的<code>hello-world.md</code>文件头部如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">title: Hello World</div><div class="line">description: Add description of freemind to title &quot;Hello World&quot;</div><div class="line">feature: images/7217667e206c9bec45dbddb7b608bcb4.jpg</div><div class="line">toc: true</div><div class="line">top: true</div></pre></td></tr></table></figure>
<p>设置<code>description</code>和<code>toc</code>后的效果如下:</p>
<p><img src="/images/Hexo/Starting/freemind_description_toc.jpg" alt="info"></p>
<p>设置<code>feature</code>和<code>top</code>后的显示效果如下:</p>
<p><img src="/images/Hexo/Starting/freemind_top_feature.jpg" alt="info"></p>
<h3 id="添加统计"><a href="#添加统计" class="headerlink" title="添加统计"></a>添加统计</h3><p>开启<a href="https://tongji.baidu.com" target="_blank" rel="external">百度统计</a></p>
<p>freemind自带百度统计功能，在主题的_config.yml中找到Analytics, 设置baidu_tongji一栏下面的enable为true，再添加上siteid即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># Analytics</div><div class="line">google_analytics:</div><div class="line">  enable: false</div><div class="line">  siteid:</div><div class="line">baidu_tongji:</div><div class="line">  enable: true</div><div class="line">  siteid: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</div></pre></td></tr></table></figure>
<h2 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h2><h3 id="安装豆瓣插件"><a href="#安装豆瓣插件" class="headerlink" title="安装豆瓣插件"></a>安装豆瓣插件</h3><p>安装豆瓣插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm install hexo-generator-douban --save</div></pre></td></tr></table></figure>
<p>在hexo的<code>_config.yml</code>的<code>Extensions</code>设置下添加如下配置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">douban:</div><div class="line">    user: douban_id</div></pre></td></tr></table></figure>
<p>在<code>freemind</code>的<code>_config.yml</code>的<code>menu</code>项下添加豆瓣的page页面</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">- title: Douban</div><div class="line">  url: douban</div><div class="line">  intro: &quot;Douban&quot;</div><div class="line">  icon: &quot;fa fa-book&quot;</div></pre></td></tr></table></figure>
<p>重启一下就能看到主页中Douban的page了。</p>
<h3 id="安装rss插件"><a href="#安装rss插件" class="headerlink" title="安装rss插件"></a>安装rss插件</h3><p>添加<code>hexo-generator-feed</code>插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-generator-feed</div></pre></td></tr></table></figure>
<p>修改_config.yml，添加Extensions</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># Feed Atom</div><div class="line">feed:</div><div class="line">  type: atom</div><div class="line">  path: atom.xml</div><div class="line">  limit: 20</div><div class="line">  hub:</div><div class="line">  content:</div></pre></td></tr></table></figure>
<p>设置完成后，可以访问<code>http://jibing57.github.io/atom.xml</code>来检验是否成功生成。</p>
<h3 id="安装sitemap"><a href="#安装sitemap" class="headerlink" title="安装sitemap"></a>安装sitemap</h3><p>添加<code>hexo-generator-sitemap</code>插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-generator-sitemap --save</div></pre></td></tr></table></figure>
<p>修改_config.yml, 添加Extensions</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sitemap:</div><div class="line">  path: sitemap.xml</div></pre></td></tr></table></figure>
<p>设置完成后，访问<code>http://jibing57.github.io/sitemap.xml</code>来检验是否成功生成了sitemap</p>
<h3 id="安装搜索插件"><a href="#安装搜索插件" class="headerlink" title="安装搜索插件"></a>安装搜索插件</h3><p>添加<code>hexo-generator-search</code>插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-generator-search --save</div></pre></td></tr></table></figure>
<h2 id="post相关"><a href="#post相关" class="headerlink" title="post相关"></a>post相关</h2><h3 id="修改默认的post脚手架"><a href="#修改默认的post脚手架" class="headerlink" title="修改默认的post脚手架"></a>修改默认的post脚手架</h3><p>修改<code>scaffolds/post.md</code>，添加如下freemind支持的<code>Front-matter</code>, 每次<code>hexo new post xx</code>的时候，就自动会生成到新的post文件中，不用每次手动生成了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">title: &#123;&#123; title &#125;&#125;</div><div class="line">date: &#123;&#123; date &#125;&#125;</div><div class="line">categories:</div><div class="line">tags:</div><div class="line">description:</div><div class="line">feature:</div><div class="line">toc: true</div><div class="line">---</div></pre></td></tr></table></figure>
<h3 id="调整post的侧边栏"><a href="#调整post的侧边栏" class="headerlink" title="调整post的侧边栏"></a>调整post的侧边栏</h3><p>将Toc调整为显示时间之下，categories和tags之上，并且调整显示时间为精确到秒</p>
<p>修改调整 <code>themes/freemind/layout/_partial/post/meta.ejs</code>的内容如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&lt;!-- date --&gt;</div><div class="line">&lt;% if (item.date) &#123; %&gt;</div><div class="line">&lt;div class=&quot;meta-widget&quot;&gt;</div><div class="line">&lt;i class=&quot;fa fa-clock-o&quot;&gt;&lt;/i&gt;</div><div class="line">&lt;%= item.date.format(config.date_format)+&apos; &apos;+item.date.format(config.time_format) %&gt;</div><div class="line">&lt;/div&gt;</div><div class="line">&lt;% &#125; %&gt;</div><div class="line"></div><div class="line">&lt;!-- toc --&gt;</div><div class="line">&lt;div class=&quot;meta-widget&quot;&gt;</div><div class="line">&lt;% if(item.toc)&#123; %&gt;</div><div class="line">   &lt;a data-toggle=&quot;collapse&quot; data-target=&quot;#toc&quot;&gt;&lt;i class=&quot;fa fa-bars&quot;&gt;&lt;/i&gt;&lt;/a&gt;</div><div class="line">   &lt;div id=&quot;toc&quot; class=&quot;toc collapse in&quot;&gt;</div><div class="line">		&lt;%- toc(item.content, &#123;class: &quot;toc-article&quot;, list_number:false&#125;) %&gt;</div><div class="line">	&lt;/div&gt;</div><div class="line">&lt;% &#125; %&gt;</div><div class="line">&lt;/div&gt;</div></pre></td></tr></table></figure>
<h3 id="Post文章在列表中的预览"><a href="#Post文章在列表中的预览" class="headerlink" title="Post文章在列表中的预览"></a>Post文章在列表中的预览</h3><p>默认情况下，在列表预览中会将所有的文章内容都显示出来，会显得比较冗余，可以在Post文章中，添加<code>&lt;!-- more --&gt;</code>预览标签，这样列表预览中只会显示文章开头到<code>&lt;!-- more --&gt;</code>预览标签之间的文字。</p>
<h3 id="多tag"><a href="#多tag" class="headerlink" title="多tag"></a>多tag</h3><p>给文章设置多个tags的方法:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 方法1</div><div class="line">tags: [tag1,tag2,tag3]</div><div class="line"></div><div class="line">#方法2</div><div class="line">tags:</div><div class="line">  - tag1</div><div class="line">  - tag2</div><div class="line">  - tag3</div></pre></td></tr></table></figure>
<h2 id="deploy部署至Github-Pages"><a href="#deploy部署至Github-Pages" class="headerlink" title="deploy部署至Github Pages"></a>deploy部署至Github Pages</h2><h3 id="deploy至Github-Pages"><a href="#deploy至Github-Pages" class="headerlink" title="deploy至Github Pages"></a>deploy至Github Pages</h3><p>首先需要在<code>github</code>上创建<code>jibing57.github.io</code>的repository, 其中<code>jibing57</code>需要替换为自己的<code>github</code>的用户名, 还需要在<code>~/.ssh/config</code>中设置好访问<code>github</code>的私钥。</p>
<p>安装Hexo的扩展</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-deployer-git --save</div></pre></td></tr></table></figure>
<p>修改<code>_config.yml</code>中<code>deploy</code>一栏的设置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">deploy:</div><div class="line">  type: git</div><div class="line">  repo: https://github.com/jibing57/jibing57.github.io.git</div><div class="line">  branch: master</div></pre></td></tr></table></figure>
<p>使用以下命令发布到Github Pages</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo d</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Blog </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Github </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何给本地视频生成字幕]]></title>
      <url>/2017/07/17/How-to-get-subtitle-of-local-video/</url>
      <content type="html"><![CDATA[<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>适合如下场景:</p>
<ul>
<li>视频本身不带字幕</li>
<li>直接看英文视频比较吃力</li>
</ul>
<p>此时可以使用<code>autosub</code>这个工具来根据视频中的声音来生成对应的字幕，虽然生成的字幕并不总是那么正确，但当做参考还是不错的。</p>
<h2 id="autosub简介"><a href="#autosub简介" class="headerlink" title="autosub简介"></a>autosub简介</h2><p><a href="https://github.com/agermanidis/autosub" target="_blank" rel="external">autosub</a>是一款由python 2编写的通过Google Web Speech API和FFmpeg来获取视频subtitle的软件</p>
<p>Github地址: <a href="https://github.com/agermanidis/autosub" target="_blank" rel="external">https://github.com/agermanidis/autosub</a></p>
<p>Github上的Usage:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">$ autosub -h</div><div class="line">usage: autosub [-h] [-C CONCURRENCY] [-o OUTPUT] [-F FORMAT] [-S SRC_LANGUAGE]</div><div class="line">               [-D DST_LANGUAGE] [-K API_KEY] [--list-formats]</div><div class="line">               [--list-languages]</div><div class="line">               [source_path]</div><div class="line"></div><div class="line">positional arguments:</div><div class="line">  source_path           Path to the video or audio file to subtitle</div><div class="line"></div><div class="line">optional arguments:</div><div class="line">  -h, --help            show this help message and exit</div><div class="line">  -C CONCURRENCY, --concurrency CONCURRENCY</div><div class="line">                        Number of concurrent API requests to make</div><div class="line">  -o OUTPUT, --output OUTPUT</div><div class="line">                        Output path for subtitles (by default, subtitles are</div><div class="line">                        saved in the same directory and name as the source</div><div class="line">                        path)</div><div class="line">  -F FORMAT, --format FORMAT</div><div class="line">                        Destination subtitle format</div><div class="line">  -S SRC_LANGUAGE, --src-language SRC_LANGUAGE</div><div class="line">                        Language spoken in source file</div><div class="line">  -D DST_LANGUAGE, --dst-language DST_LANGUAGE</div><div class="line">                        Desired language for the subtitles</div><div class="line">  -K API_KEY, --api-key API_KEY</div><div class="line">                        The Google Translate API key to be used. (Required for</div><div class="line">                        subtitle translation)</div><div class="line">  --list-formats        List all available subtitle formats</div><div class="line">  --list-languages      List all available source/destination languages</div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="Macos-下安装方法"><a href="#Macos-下安装方法" class="headerlink" title="Macos 下安装方法"></a>Macos 下安装方法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">brew install ffmpeg</div><div class="line">pip install autosub</div></pre></td></tr></table></figure>
<h2 id="批量生成"><a href="#批量生成" class="headerlink" title="批量生成"></a>批量生成</h2><p>原程序参数貌似不支持多文件，基于自身需要，写了个粗糙的小脚本来批量转换整个目录下的视频文件。目前够用，后续如果有子目录以及除mp4的其他格式的需求，再改进</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">USAGE=&quot;USAGE: $0 dir|file&quot;</div><div class="line"></div><div class="line">if [ &quot;$1&quot; == &quot;&quot; ]; then</div><div class="line">    echo $USAGE</div><div class="line">    exit 1</div><div class="line">fi</div><div class="line"></div><div class="line"># file process</div><div class="line">if [ -f &quot;$1&quot; ]; then</div><div class="line">    echo &quot;Start to use autosub to generate subtitle for video $1&quot;</div><div class="line">    autosub &quot;$1&quot;</div><div class="line">    exit 0</div><div class="line">fi</div><div class="line"></div><div class="line"># dir process</div><div class="line">if [ ! -d &quot;$1&quot; ]; then</div><div class="line">    echo $USAGE</div><div class="line">    exit 1</div><div class="line">fi</div><div class="line"></div><div class="line">base_dir=&quot;$1&quot;</div><div class="line">echo &quot;**************************************************&quot;</div><div class="line">echo &quot;Start to use autosub to generate subtitle for video in $base_dir&quot;</div><div class="line">echo &quot;**************************************************&quot;</div><div class="line">echo &quot;cd $base_dir&quot;</div><div class="line">echo &quot;&quot;</div><div class="line">cd &quot;$base_dir&quot;</div><div class="line"></div><div class="line">find . -iname &quot;*.mp4&quot; | while read file</div><div class="line">do</div><div class="line">    echo &quot;oooooooooooooooooooooooooo&quot;</div><div class="line">    echo &quot; ==== Start to process file - $&#123;file&#125;&quot;</div><div class="line">    file_basename=`basename &quot;$file&quot; .mp4`</div><div class="line">    subtitle_file_name=&quot;$&#123;file_basename&#125;.srt&quot;</div><div class="line">    # echo &quot;subtitle_file_name is [$&#123;subtitle_file_name&#125;]&quot;</div><div class="line">    if [ -e &quot;$subtitle_file_name&quot; ]; then</div><div class="line">        echo &quot;Subtitle of $file has already existed. don&apos;t need to process again&quot;</div><div class="line">    else</div><div class="line">        autosub &quot;$file&quot;</div><div class="line">    fi</div><div class="line">    echo &quot; ==== End to process file - $&#123;file&#125;&quot;</div><div class="line">    echo &quot;oooooooooooooooooooooooooo&quot;</div><div class="line">    echo &quot;&quot;</div><div class="line">done</div><div class="line"></div><div class="line">echo &quot;&quot;</div><div class="line">echo &quot;**************************************************&quot;</div><div class="line">echo &quot;End to use autosub to generate subtitle for video in $base_dir&quot;</div><div class="line">echo &quot;**************************************************&quot;</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Tools </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Video </tag>
            
            <tag> Tips </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
