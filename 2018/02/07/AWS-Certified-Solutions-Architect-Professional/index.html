<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>AWS Certified Solutions Architect - Professional Road Map | Recording</title>
  <meta name="author" content="jibing57">
  
  <meta name="description" content="AWS, 阅读, Architecture, PostgreSQL">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="AWS Certified Solutions Architect - Professional Road Map"/>
  <meta property="og:site_name" content="Recording"/>

  
    <meta property="og:image" content="undefined"/>
  

  
  
    <link href="/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/css/themes/bootstrap.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>
  
  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-111771042-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?59d3e14cdc96877383afed646f44d216";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


</head>

 <body>  
  <nav id="main-nav" class="navbar  navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/">Recording</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		  <li>
			<a href="/douban" title="Douban">
			  <i class="fa fa-book"></i>Douban
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header ">		
			<h1 class="title "> AWS Certified Solutions Architect - Professional Road Map</h1>
		</div>		
	




   <style type="text/css">
           img, video {
                -webkit-box-shadow:0 0 10px rgba(0, 0, 0, .5);  
                -moz-box-shadow:0 0 10px rgba(0, 0, 0, .5);  
                box-shadow:0 0 10px rgba(0, 0, 0, .5);  
           }
   </style>



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <h2 id="Official-AWS-Certification-Page"><a href="#Official-AWS-Certification-Page" class="headerlink" title="Official AWS Certification Page"></a>Official AWS Certification Page</h2><p>访问官网<a href="https://amazonaws-china.com/certification/certification-prep/" target="_blank" rel="external">AWS Certification</a></p>
<ul>
<li>参加 AWS 培训课程</li>
<li>查看考试指南和样题<ul>
<li>了解考试涉及的概念并整体了解需要学习哪些内容, <a href="http://d0.awsstatic-china.com/Train%20&amp;%20Cert/docs/AWS_certified_solutions_architect_professional_blueprint.pdf" target="_blank" rel="external">AWS Certified Solutions Architect – Professional 考试指南</a> 相当于考试大纲, 必看,而且需要反复的看。因为学习过一阵后再来看Guide，会有更深的体会。</li>
<li><a href="https://d0.awsstatic-china.com/training-and-certification/docs/AWS_certified_solutions_architect_professional_examsample.pdf" target="_blank" rel="external">考试样题</a>用于熟悉题目题型。几个样题都是大段大段的文字，实际考试题目中也有字数少的题目。但题目的阅读量绝对是比Asoocaiated的大得多。</li>
</ul>
</li>
<li>练习试验<ul>
<li>注册一个AWS全球账号，使用一年的免费额度结合Blueprint中的各个内容进行试验。</li>
</ul>
</li>
<li>学习 AWS 白皮书<ul>
<li>白皮书是纯英文的，每个白皮书篇幅都很长。</li>
<li>在BluePrint的描述中，和AWS Certified Solutions Architect - Associate的白皮书列表相比，多了<strong>Defining Fault Tolerant Applications in the AWS Cloud</strong>这一份白皮书，官网找了一下，只有一个名字类似的名为<a href="https://d1.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf" target="_blank" rel="external">Building Fault-Tolerant Applications on AWS</a>的白皮书，时间还是October 2011的。不管怎样，但愿好理念永不过时，还是整个念一遍吧。</li>
</ul>
</li>
<li>查看 AWS 常见问题<ul>
<li>官网推荐的FAQ都建议看完。</li>
</ul>
</li>
<li>参加模拟考试<ul>
<li>40美刀一次。是否值得因人而异。参加Professional的考试，肯定都已经经历过AWS Certified Solutions Architect – Associate的洗礼了, 因此熟悉考试界面肯定不是决定要参加模拟考试的目的。一个合理的理由是，可以提前熟悉一下Professional考题阅读难度, 为正式考试的时候做好心理准备。网上都说AWS Certified Solutions Architect – Professional这个考试最大的难度就是要在170分钟内阅读，理解77~80道字数不算少的题，然后选出最优的答案。网上一些母语是英语的老外都感慨阅读量比较大，何况是我们这种非英语系的国家的考生了。</li>
</ul>
</li>
<li>报名考试并获得认证<ul>
<li>登陆<a href="https://www.aws.training/certification" target="_blank" rel="external">https://www.aws.training/certification</a>注册进行考试</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="考试指南"><a href="#考试指南" class="headerlink" title="考试指南"></a>考试指南</h2><p><a href="http://d0.awsstatic-china.com/Train%20&amp;%20Cert/docs/AWS_certified_solutions_architect_professional_blueprint.pdf" target="_blank" rel="external">AWS Certified Solutions Architect – Professional 考试指南</a> 读三遍，读三遍，读三遍</p>
<p>总计8个Domain, 各个Domain占的百分比如下</p>
<ul>
<li>High Availability and Business Continuity - 15%</li>
<li>Costing                                   - 5%</li>
<li>Deployment Management                     - 10%</li>
<li>Network Design                            - 10%</li>
<li>Data Storage                              - 15%</li>
<li>Security                                  - 20%</li>
<li>Scalability &amp; Elasticity                  - 15%</li>
<li>Cloud Migration &amp; Hybrid Architecture     - 10%</li>
</ul>
<h2 id="涉及到的AWS-Services"><a href="#涉及到的AWS-Services" class="headerlink" title="涉及到的AWS Services"></a>涉及到的AWS Services</h2><ul>
<li>AWS KMS</li>
<li>AWS Import/Export</li>
<li>AWS STS</li>
<li>CloudFormation</li>
<li>CloudFront</li>
<li>CloudHSM</li>
<li>CloudSearch</li>
<li>CloudWatch</li>
<li>CloudTrail</li>
<li>Data Pipeline</li>
<li>Direct Connect</li>
<li>DynamoDB</li>
<li>EBS</li>
<li>EC2</li>
<li>AutoScaling</li>
<li>ELB</li>
<li>EMR</li>
<li>ElastiCache</li>
<li>Elastic Beanstalk</li>
<li>Elastic Transcoder</li>
<li>Glacier</li>
<li>IAM</li>
<li>Kinesis</li>
<li>OpsWorks</li>
<li>RDS</li>
<li>RedShift</li>
<li>Route 53</li>
<li>S3</li>
<li>SES</li>
<li>SNS</li>
<li>SQS</li>
<li>SWF</li>
<li>Storage Gateway</li>
<li>VPC</li>
</ul>
<h2 id="视频学习"><a href="#视频学习" class="headerlink" title="视频学习"></a>视频学习</h2><p>网上有多种视频教程，当时个人觉得<a href="https://acloud.guru" target="_blank" rel="external">Acloudguru</a>的AWS Associated系列的教程非常好(三份Associated的教程都买了)，因此此次也购买了<a href="https://acloud.guru" target="_blank" rel="external">Acloudguru</a>的<a href="https://acloud.guru/learn/aws-certified-solutions-architect-professional" target="_blank" rel="external">AWS Certified Solutions Architect – Professional</a>的教程。但学下来整体感觉这个Professional的教程一般，总感觉内容和AWS Certified Solutions Architect – Associated系列的差不多。不够深入和细致。其余的教程没有学习过，因此无法做评价。</p>
<h2 id="要点摘录"><a href="#要点摘录" class="headerlink" title="要点摘录"></a>要点摘录</h2><h3 id="Domain-1-High-Availability-and-Business-Continuity-15"><a href="#Domain-1-High-Availability-and-Business-Continuity-15" class="headerlink" title="Domain 1 - High Availability and Business Continuity - 15%"></a>Domain 1 - High Availability and Business Continuity - 15%</h3><ul>
<li><strong>Disaster Recovery</strong><ul>
<li>WhitePaper : <a href="https://media.amazonwebservices.com/AWS_Disaster_Recovery.pdf" target="_blank" rel="external">https://media.amazonwebservices.com/AWS_Disaster_Recovery.pdf</a></li>
<li>What is Disaster Recovery<ul>
<li>Disaster recovery (DR) is about preparing for and recovering from a disaster. Any event that has a negative impact on a company’s business continuity or finances could be termed a disaster. This includes hardware or software failure, a network outage, a power outage, physical damage to a building like fire or flooding, human error, or some other significant event.</li>
</ul>
</li>
<li>Recovery Time Objective (RTO)<ul>
<li>Recovery Time Objective is the amount of time that it takes for your business to recover from an outage or disruption.</li>
<li>It can include the time for trying to fix the problem without a recovery, the recovery itself, testing and the communication to the users</li>
</ul>
</li>
<li>Recovery Point Objective (RPO)<ul>
<li>Recovery Point Objective (RPO) is the maximum period of time in which data might be lost from an IT service due to a major incident.</li>
<li>In other words, how much data can your organization afford to lose? An hour’s worth? A day’s worth? None at all?</li>
</ul>
</li>
<li>Traditional Approaches to DR<ul>
<li>A traditional approach to DR usually involves an N+1 approach and has different levels of off-site duplication of data and infrastructure.<ul>
<li>Facilities to house the infrastructure, including power and cooling</li>
<li>Security to ensure the physical protection of assets</li>
<li>Suitable capacity to scale the environment</li>
<li>Support for repairing, replacing, and refreshing the infrastructure</li>
<li>Contractual agreements with an Internet service provider (ISP) to provide Internet connectivity that can sustain bandwidth utilization for the environment under a full load</li>
<li>Network infrastructure such as firewalls, routers, switches, and load balancers</li>
<li>Enough server capacity to run all mission-critical services, including storage appliances for the supporting data, and servers to run applications and backend services such as user authentication, Domain Name System (DNS)</li>
<li>Dynamic Host Configuration Protocol (DHCP), monitoring, and alerting</li>
</ul>
</li>
</ul>
</li>
<li>Why use aws for DR<ul>
<li>Only minimum hardware is required for ‘data replication’</li>
<li>Allows you to be flexible depending on what your disaster is and how to recover from it</li>
<li>Open cost model (pay as you use) rather than heavy investment upfront. Scaling is quick and easy</li>
<li>Automate disaster recovery deployment</li>
</ul>
</li>
<li>What Services<ul>
<li>Regions</li>
<li>Storage<ul>
<li>S3 - 99.999999999% durability and Cross Region Replication</li>
<li>Glacier</li>
<li>Elastic Block Store (EBS)</li>
<li>Direct Connect</li>
<li>AWS Storage Gateway<ul>
<li>Gateway-cached volumes - store primary data and cache most recently used data locally.</li>
<li>Gateway-stored volumes - store entire dataset on site and asynchronously replicate data back to S3</li>
<li>Gateway-virtual tape library - Store your virtual tapes in either S3 or Glacier</li>
</ul>
</li>
</ul>
</li>
<li>Compute<ul>
<li>EC2</li>
<li>EC2 VM Import Connector - Virtual appliance which allows you to import virtual machine images from your existing environment to Amazon EC2 instances.</li>
</ul>
</li>
<li>Networking<ul>
<li>Route53</li>
<li>Elastic Load Balancing</li>
<li>Amazon Virtual Private Cloud (VPC)</li>
<li>Amazon Direct Connect</li>
</ul>
</li>
<li>Database<ul>
<li>RDS</li>
<li>DynamoDB</li>
<li>Redshift</li>
</ul>
</li>
<li>Orchestration<ul>
<li>CloudFormation</li>
<li>ElasticBeanstalk</li>
<li>OpsWork</li>
</ul>
</li>
<li>Lambda</li>
</ul>
</li>
<li>DR Scenarios<ul>
<li>Four Scenarios<ul>
<li>Backup &amp; Restore</li>
<li>Pilot Light</li>
<li>Warm Standby</li>
<li>Multi Site</li>
</ul>
</li>
<li>Backup &amp; Restore<ul>
<li>In most traditional environments, data is backed up to tape and sent off-site regularly. If you use this method, it can take a long time to restore your system in the event of a disruption or disaster. Amazon S3 is an ideal destination for backup data that might be needed quickly to perform a restore. Transferring data to and from Amazon S3 is typically done through the network, and is therefore accessible from any location.</li>
<li>You can use AWS Import/Export to transfer very large data sets by shipping storage devices directly to AWS. For longer-term data storage where retrieval times of several hours are adequate, there is Amazon Glacier, which has the same durability model as Amazon S3. . Amazon Glacier and Amazon S3 can be used in conjunction to produce a tiered backup solution.</li>
<li>Data Backup Options to Amazon S3 from On-Site Infrastructure or from AWS.<br>  <img src="/images/AWS/Sysops/data_backup_options_to_S3_from_on_site_infrastructure_from_aws.jpg" alt="data_backup_options_to_S3_from_on_site_infrastructure_from_aws"></li>
<li>Restoring a System from Amazon S3 Backups to Amazon EC2<br>  <img src="/images/AWS/Sysops/restoring_a_system_from_amazon_s3_backups_to_amazon_ec2.jpg" alt="restoring_a_system_from_amazon_s3_backups_to_amazon_ec2"></li>
<li>Key steps for backup &amp; restore<ul>
<li>Select an appropriate tool or method to back up your data into AWS.</li>
<li>Ensure that you have an appropriate retention policy for this data.</li>
<li>Ensure that appropriate security measures are in place for this data, including encryption and access policies.</li>
<li>Regularly test the recovery of this data and the restoration of your system.</li>
</ul>
</li>
</ul>
</li>
<li>Pilot Light<ul>
<li>The term pilot light is often used to describe a DR scenario in which a minimal version of an environment is always running in the cloud. The idea of the pilot light is an analogy that comes from the gas heater. In a gas heater, a small flame that’s always on can quickly ignite the entire furnace to heat up a house.</li>
<li>This scenario is similar to a backup-and-restore scenario. For example, with AWS you can maintain a pilot light by configuring and running the most critical core elements of your system in AWS. When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core.</li>
<li>Infrastructure elements for the pilot light itself typically include your database servers, which would replicate data to Amazon EC2 or Amazon RDS. Depending on the system, there might be other critical data outside of the database that needs to be replicated to AWS. This is the critical core of the system (the pilot light) around which all other infrastructure pieces in AWS (the rest of the furnace) can quickly be provisioned to restore the complete system.</li>
<li>To provision the remainder of the infrastructure to restore business-critical services, you would typically have some preconfigured servers bundled as Amazon Machine Images (AMIs), which are ready to be started up at a moment’s notice.  When starting recovery, instances from these AMIs come up quickly with their pre-defined role (for example, Web or App Server) within the deployment around the pilot light.</li>
<li>From a networking point of view, you have two main options for provisioning:<ul>
<li>Use pre-allocated elastic IP address and associate them with your instances when invoking DR. You can also use pre-allocated elastic network interfaces (ENIs) with pre-allocated Mac Addresses for applications with special licensing requirements</li>
<li>Use Elastic Load Balancing (ELB) to distribute traffic to multiple instances. You would then update your DNS records to point at your Amazon EC2 instance or point to your load balancer using a CNAME</li>
</ul>
</li>
<li>Preparation phase<ul>
<li>The Preparation Phase of the Pilot Light Scenario<br>  <img src="/images/AWS/Sysops/the_preparation_phase_of_the_pilot_light_scenario.jpg" alt="the_preparation_phase_of_the_pilot_light_scenario"></li>
<li>Key steps for preparation:<ul>
<li>Set up Amazon EC2 instances to replicate or mirror data.</li>
<li>Ensure that you have all supporting custom software packages available in AWS.</li>
<li>Create and maintain AMIs of key servers where fast recovery is required.</li>
<li>Regularly run these servers, test them, and apply any software updates and configuration changes.</li>
<li>Consider automating the provisioning of AWS resources</li>
</ul>
</li>
</ul>
</li>
<li>Recovery phase<ul>
<li>The Recovery Phase of the Pilot Light Scenario<br>  <img src="/images/AWS/Sysops/the_recovery_phase_of_the_pilot_light_scenario.jpg" alt="the_recovery_phase_of_the_pilot_light_scenario"></li>
<li>Key steps for recovery:<ul>
<li>Start your application Amazon EC2 instances from your custom AMIs.</li>
<li>Resize existing database/data store instances to process the increased traffic.</li>
<li>Add additional database/data store instances to give the DR site resilience in the data tier; if you are using Amazon RDS, turn on Multi-AZ to improve resilience.</li>
<li>Change DNS to point at the Amazon EC2 servers.</li>
<li>Install and configure any non-AMI based systems, ideally in an automated way.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Warm Standby<ul>
<li>The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud. A warm standby solution extends the pilot light elements and preparation. It further decreases the recovery time because some services are always running. By identifying your business-critical systems, you can fully duplicate these systems on AWS and have them always on.</li>
<li>These servers can be running on a minimum-sized fleet of Amazon EC2 instances on the smallest sizes possible. This solution is not scaled to take a full-production load, but it is fully functional. It can be used for non-production work, such as testing, quality assurance, and internal use.</li>
<li>In a disaster, the system is scaled up quickly to handle the production load. In AWS, this can be done by adding more instances to the load balancer and by resizing the small capacity servers to run on larger Amazon EC2 instance types.</li>
<li>Horizontal scaling is preferred over vertical scaling</li>
<li>Preparation phase<ul>
<li>The Preparation Phase of the Warm Standby Scenario<br>  <img src="/images/AWS/Sysops/the_preparation_phase_of_the_warm_standby_scenario.jpg" alt="the_preparation_phase_of_the_warm_standby_scenario"></li>
<li>Key steps for preparation:<ul>
<li>Set up Amazon EC2 instances to replicate or mirror data.</li>
<li>Create and maintain AMIs.</li>
<li>Run your application using a minimal footprint of Amazon EC2 instances or AWS infrastructure.</li>
<li>Patch and update software and configuration files in line with your live environment.</li>
</ul>
</li>
</ul>
</li>
<li>Recovery phase<ul>
<li>The Recovery Phase of the Warm Standby Scenario<br>  <img src="/images/AWS/Sysops/the_recovery_phase_of_the_warm_standby_scnario.jpg" alt="the_recovery_phase_of_the_warm_standby_scnario"></li>
<li>Key steps for recovery:<ul>
<li>Increase the size of the Amazon EC2 fleets in service with the load balancer (horizontal scaling).</li>
<li>Start applications on larger Amazon EC2 instance types as needed (vertical scaling).</li>
<li>Either manually change the DNS records, or use Amazon Route 53 automated health checks so that all traffic is routed to the AWS environment.</li>
<li>Consider using Auto Scaling to right-size the fleet or accommodate the increased load.</li>
<li>Add resilience or scale up your database.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Multi Site<ul>
<li>A multi-site solution runs in AWS as well as on your existing on-site infrastructure, in an active-active configuration. The data replication method that you employ will be determined by the recovery point that you choose.</li>
<li>You can use Route53 to root traffic to both sites either symmetrically or asymmetrically.</li>
<li>In an on-site disaster situation, you can adjust the DNS weighting and send all traffic to the AWS servers. The capacity of the AWS service can be rapidly increased to handle the full production load. You can use Amazon EC2 Auto Scaling to automate this process. You might need some application logic to detect the failure of the primary database services and cut over to the parallel database services running in AWS.</li>
<li>Preparation phase<ul>
<li>The Preparation Phase of the Multi-Site Scenario<br>  <img src="/images/AWS/Sysops/the_preparation_phase_of_the_multi_site_scenario.jpg" alt="the_preparation_phase_of_the_multi_site_scenario"></li>
<li>Key steps for preparation:<ul>
<li>Set up your AWS environment to duplicate your production environment.</li>
<li>Set up DNS weighting, or similar traffic routing technology, to distribute incoming requests to both sites.  Configure automated failover to re-route traffic away from the affected site.</li>
</ul>
</li>
</ul>
</li>
<li>Recovery phase<ul>
<li>The Recovery Phase of the Multi-Site Scenario Involving On-Site and AWS Infrastructure.<br>  <img src="/images/AWS/Sysops/the_recovery_phase_of_the_multi_site_scenario_involving_on_site_and_aws_infrastructure.jpg" alt="the_recovery_phase_of_the_multi_site_scenario_involving_on_site_and_aws_infrastructure"></li>
<li>Key steps for recovery:<ul>
<li>Either manually or by using DNS failover, change the DNS weighting so that all requests are sent to the AWS site.</li>
<li>Have application logic for failover to use the local AWS database servers for all queries.</li>
<li>Consider using Auto Scaling to automatically right-size the AWS fleet.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Failing Back<ul>
<li>Backup and restore<ol>
<li>Freeze data changes to the DR site</li>
<li>Take a backup</li>
<li>Restore the backup to the primary site</li>
<li>Re-point users to the primary site</li>
<li>Unfreeze the changes</li>
</ol>
</li>
<li>Pilot light, warm standby, and multi-site:<ol>
<li>Establish reverse mirroring/replication from the DR site back to the primary site, once the primary site has caught up with the changes.</li>
<li>Freeze data changes to the DR site</li>
<li>Re-point users to the primary site</li>
<li>Unfreeze the changes</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>You will be given different RTO’s and RPO’s and then asked which AWS services you should choose. All choices may be correct, just some are more correct than others.</li>
<li>Key AWS Back Up &amp; DR Technologies<ul>
<li>S3 provides a highly durable storage infrastructure designed for mission critical and primary data storage. Objects are redundantly stored on multiple devices across multiple facilities within a region, designed to provide a durability of 99.999999999% (11 9s)</li>
<li>Archives (think objects) are optimized for infrequent access, for which retrieval times of several hours are adequate. It can take at least <strong>3 Hours</strong> to recover a file from glacier.</li>
<li>EBS provides the ability to create point-in-time snapshots of data volumes.</li>
<li>DynamoDB offers cross region replication. You can read more about how to set it up here.<ul>
<li><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.CrossRegionRepl.html" target="_blank" rel="external">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.CrossRegionRepl.html</a></li>
</ul>
</li>
<li>RDS gives you the ability to snapshot data from one region to another, and also to have a read replica running in another region.</li>
<li>Redshift: snapshot your data warehouse to be durably stored in Amazon S3 within the same region or copied to another region.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>DR&amp;BC For Database</strong></p>
<ul>
<li>HA for Popular Database<ul>
<li>SQL Server = AlwaysOn Availability Groups, SQL Mirroring</li>
<li>MySQL = Asynchronous replication</li>
<li>Oracle = Oracle Data Guard, Oracle RAC</li>
</ul>
</li>
<li>RDS Multi-AZ Failover<ul>
<li>Automatic failover in case of<ul>
<li>Loss of availability in primary AZ</li>
<li>Loss of connectivity to primary DB</li>
<li>Storage or host failure to primary DB</li>
<li>Software patching</li>
<li>Rebooting of primary DB.</li>
</ul>
</li>
<li>MultiAZ deployments for Oracle, PostgreSQL, MySQL, and MariaDB DB instances use Amazon’s failover technology.</li>
<li>SQL Server DB instances use SQL Server Mirroring.</li>
<li>Amazon Aurora instances stores copies of the data in a DB cluster across multiple Availability Zones in a single region.</li>
<li>All approaches safeguard your data in the event of a DB instance failure or loss of an Availability Zone.</li>
</ul>
</li>
<li>Read Replica<ul>
<li>What are Read Replica<ul>
<li>Read Replicas make it easy to take advantage of supported engine’s built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB Instance for read-heave database workloads.</li>
<li>Read only copies of your database.</li>
<li>You can create a Read Replica with a few clicks in the AWS Management Console or using the CreateDBInstanceReadReplica API. Once the Read Replica is created, database updates on the source DB Instance will be replicated using a supported engine’s native, asynchronous replication. You can create multiple Read Replicas for a given source DB Instance and distribute your application’s read traffic amongst them.</li>
</ul>
</li>
<li>When would you use read replica’s<ul>
<li>Scaling beyond the compute or I/O capacity of a single DB Instance for read-heavy database workloads. This excess read traffic can be directed to one or more Read Replicas</li>
<li>Serving read traffic while the source DB Instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your Read Replica</li>
<li>Business reporting or data warehousing scenarios; you may want business reporting queries to run against a Read Replica, rather than your primary, production DB Instance.</li>
</ul>
</li>
<li>Supported Versions<ul>
<li>MySQL<ul>
<li>MySQL 5.6 (NOT 5.1 or 5.5)</li>
<li>Can use both MySQL engines (MyISAM and InnoDB) however only InnoDB is supported by AWS</li>
</ul>
</li>
<li>PostgreSQL<ul>
<li>PostgreSQL 9.3.5 or newer</li>
</ul>
</li>
<li>MariaDB<ul>
<li>All current versions</li>
<li>For all 3 Amazon uses these engines native asynchronous replication to update the read replica</li>
</ul>
</li>
<li>Oracle and MSSQL<ul>
<li>All current versions</li>
</ul>
</li>
<li>Aurora<ul>
<li>Aurora employees an SSD-backed virtualized storage layer purpose-built for database workloads. Amazon Aurora replica share the same underlying storage as the source instance, lowering costs and avoiding the need to copy data to the replica nodes.</li>
</ul>
</li>
</ul>
</li>
<li>Creating Read Replicas<ul>
<li>When creating a new Read Replica, AWS will take a snapshot of your database.</li>
<li>If Multi-AZ is not enabled:<ul>
<li>This snapshot will be of your primary database and can cause brief I/O suspension for around 1 minute.</li>
</ul>
</li>
<li>If Multi-AZ is enabled:<ul>
<li>The snapshot will be of your secondary database and you will not experience any performance hits on your primary database.</li>
</ul>
</li>
</ul>
</li>
<li>Connecting to Read Replica<ul>
<li>When a new read replica is created you will be able to connect to it using a new end point DNS address.</li>
</ul>
</li>
<li>Read Replica’s Can Be Promoted<ul>
<li>You can promote a read replica to it’s own standalone database. Doing this will break the replication link between the primary and the secondary.</li>
</ul>
</li>
<li>Multi-region Read Replicas<ul>
<li>With Amazon Relational Database Service (Amazon RDS), you can create a MySQL, PostgreSQL, or MariaDB Read Replica in a different AWS Region than the source DB instance. You create a Read Replica to do the following:<ul>
<li>Improve your disaster recovery capabilities.</li>
<li>Scale read operations into a region closer to your users.</li>
<li>Make it easier to migrate from a data center in one region to a data center in another region.</li>
</ul>
</li>
<li>You can create an <strong>Amazon Aurora DB cluster</strong> as a Read Replica in a different AWS Region than the source DB cluster. Taking this approach can improve your disaster recovery capabilities, let you scale read operations into a region that is closer to your users, and make it easier to migrate from one region to another.</li>
<li>You can create Read Replicas of both encrypted and unencrypted DB clusters. The Read Replica must be encrypted if the source DB cluster is encrypted.</li>
</ul>
</li>
</ul>
</li>
<li>Few More Exam Tips<ul>
<li>You can have up to 5 read replicas.</li>
<li>You can have read replicas in different REGIONS (with the exception of SQL Server and Oracle).</li>
<li>For Read Replicas Replication is Asynchronous only, not synchronous.</li>
<li>Read Replicas can be built off Multi-AZ’s databases.</li>
<li><strong>BUT</strong> Read Replicas themselves <strong>cannot</strong> be Multi-AZ currently.</li>
<li>You can have Read Replicas of Read Replicas, however <strong>only</strong> for MySQL 5.6 and MariaDB and this will further increase latency, PostgreSQL is not currently supported.</li>
<li>DB Snapshots and Automated backups <strong>cannot</strong> be taken of read replicas.</li>
<li><strong>Synchronous</strong> replication is used for <strong>Multi-AZ</strong></li>
<li><strong>Asynchronous</strong> replication is used for <strong>Read Replicas</strong></li>
<li>If your application does not require transaction support, Atomicity, Consistency, Isolation, Durability (ACID) compliance, joins &amp; SQL, consider using DynamoDB rather than RDS.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Storage Gateway</strong></p>
<ul>
<li>How can I backup My Data<ul>
<li>Write backup data to S3 directly, using <strong>API calls</strong>.</li>
<li>Write backup data to <strong>Storage Gateway</strong>, which then securely replicates it to S3.</li>
</ul>
</li>
<li>Three type Storage Gateway Interface<ul>
<li>File Interface<ul>
<li>NFS</li>
<li>unlimited amount of storage, However maximum file size is 5TB.</li>
</ul>
</li>
<li>Volume Interface<ul>
<li>Gateway-Cached Volumes</li>
<li>Gateway-Stored Volumes</li>
</ul>
</li>
<li>Tape Interface<ul>
<li>Gateway-Virtual Tape Library</li>
</ul>
</li>
</ul>
</li>
<li>Storage Gateway<ul>
<li>File Volumes<ul>
<li>NFS</li>
<li>Unlimited amount of storage. However maximum file size is 5TB.</li>
</ul>
</li>
<li>Volume Gateway<ul>
<li>Cached (Gateway-Cached Volumes)<ul>
<li>iSCSI based block storage</li>
<li>Each Volume can store up to 32TB in Size.</li>
<li>32 Volumes supported. 1PB of data can be stored(32*32)</li>
</ul>
</li>
<li>Stored (Gateway-Stored Volumes)<ul>
<li>iSCSI based block storage</li>
<li>Each Volume can store up to 16TB in Size.</li>
<li>32 Volumes supported. 512TB of data can be stored (32*16)</li>
</ul>
</li>
<li>Tape Gateway (Gateway-Virtual Tape Library)<ul>
<li>iSCSI based virtual tape solution</li>
<li>Virtual Tape Library (S3) 1500 virtual tapes (1PB)</li>
<li>Virtual Tape Shelf (Glacier) unlimited tapes.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Storage Gateway Introduction</p>
<ul>
<li>File Gateway<br>  File gateway provides a virtual file server, which enables you to store and retrieve <strong>Amazon S3</strong> objects through standard file storage protocols. File gateway allows your existing file-based applications or devices to use secure and durable cloud storage without needing to be modified. With file gateway, your configured S3 buckets will be available as Network File System (NFS) mount points. Your applications read and write files and directories over NFS, interfacing to the gateway as a file server.<br>  In turn, the gateway translates these file operations into object requests on your S3 buckets. Your most recently used data is cached on the gateway for low-latency access, and data transfer between your data center and AWS is fully managed and optimized by the gateway.</li>
<li>Gateway-Cached Volumes<br>  You can store your primary data in <strong>Amazon S3</strong>, and retain your frequently accessed data locally. Gateway-cached volumes provide substantial cost savings on primary storage, minimize the need to scale your storage on-premises, and retain low-latency access to your frequently accessed data.</li>
<li>Gateway-Stored Volumes<br>  In the event you need low-latency access to your entire data set, you can configure your on-premises data gateway to store your primary data locally, and asynchronously back up point-in-time snapshots of this data to <strong>Amazon S3</strong>.</li>
<li>Gateway-Virtual Tape Library<br>  You can have a limitless collection of virtual tapes. Each virtual tape can be stored in a Virtual Tape Library backed by <strong>Amazon S3</strong> or a Virtual Tape Shelf(VTS) backed by <strong>Amazon Glacier</strong>.</li>
</ul>
</li>
<li><p>Storage Gateway - General Facts</p>
<ul>
<li>Can be deployed on-premise, or as an <strong>EC2</strong> instance.</li>
<li>Can schedule snapshots.</li>
<li>You can use Storage Gateway with <strong>Direct Connect</strong>.</li>
<li>You can implement bandwidth throttling.</li>
<li>On-Premise needs with either <strong>Vmware’s ESXi</strong> or <strong>Hyper-V</strong>.</li>
<li>Hardware Requirements:<ul>
<li>4 or 8vCPUs</li>
<li>7.5 GB of RAM</li>
<li>75 GB for installation of VM image and system data</li>
</ul>
</li>
</ul>
</li>
<li><p>Storage Gateway - Storage Requirements</p>
<ul>
<li>For gateway-cached volume configuration, you will need storage for the local cache and an upload buffer.</li>
<li>For gateway-stored volume configuration, you will need storage for your entire dataset and an upload buffer. Gateway-stored volumes can range from 1GiB to 1 TB. Each gateway configured for gateway-stored volumes can support up to 12 volumes and a total volume storage of 16TB.</li>
<li>For gateway-VTL configuration, you will need storage for the local cache and an upload buffer.</li>
</ul>
</li>
<li><p>Storage Gateway - Networking Requirements</p>
<ul>
<li>Open <strong>port 443</strong> on your firewalls.</li>
<li>Internally, you will need to allow <strong>port 80</strong> (activation only), <strong>port 3260</strong> (by local systems to connect to iSCSI targets exposed by the gateway) and <strong>port UDP 53 (DNS)</strong></li>
</ul>
</li>
<li><p>Storage Gateway - Encryption</p>
<ul>
<li>Data in transit is secured using SSL</li>
<li>Data at rest can be encrypted using AES-256</li>
</ul>
</li>
<li><p>Gateway-Cached and Gateway-Stored Volumes</p>
<ul>
<li>You can take point-in-time, incremental snapshots of your volume and store them in <strong>Amazon S3</strong> in the form of <strong>Amazon EBS</strong> snapshots.</li>
<li>Snapshots can be initiated on a scheduled or ad-hoc basis.</li>
<li>Gateway Stored Snapshots<ul>
<li>If your volume data is stored on-premises, snapshots provide durable, off-site backups in <strong>Amazon S3</strong>.</li>
<li>You can create a new <strong>Gateway-Stored</strong> volume from a snapshot in the event you need to recover a backup.</li>
<li>You can also use a snapshot of your Gateway-Stored volume as the starting point for a new <strong>Amazon EBS</strong> volume which you can then attach to an <strong>Amazon EC2</strong> instance.</li>
</ul>
</li>
<li>Gateway Cached Snapshots<ul>
<li>Snapshots can be used to preserve versions of your data, allowing you to revert to a prior version when required or to repurpose a point-in-time version as a new <strong>Gateway-Cached volume</strong>.</li>
</ul>
</li>
</ul>
</li>
<li><p>Gateway-Virtual Tape Library Retrieval<br>  The virtual tape containing your data must be stored in a Virtual Tape Library before it can be accessed. Access to virtual tapes in your Virtual Tape Library is <strong>instantaneous</strong>.</p>
<p>  If the virtual tape containing your data is in your Virtual Tape Shelf, you must first retrieve the virtual tape from your Virtual Tape Shelf. It takes about <strong>24 Hours</strong> for the retrieved virtual tape to be available in the selected Virtual Tape Library.</p>
</li>
<li><p>Gateway-Virtual Tape Library Supports</p>
<ul>
<li>Symantec NetBackup version 7.x</li>
<li>Symantec Backup Exec 2012</li>
<li>Symantec Backup Exec 2014</li>
<li>Symantec Backup Exec 15</li>
<li>Microsoft System Center 2012 R2 Data Protection Manager</li>
<li>Veeam Backup &amp; Replication V7</li>
<li>Veeam Backup &amp; Replication V8</li>
<li>Dell NetVault Backup 10.0</li>
</ul>
</li>
<li><p>Storage Gateway Exam Tips</p>
<ul>
<li>Know the four different Storage Gateway Types:<ul>
<li>File Gateway</li>
<li>Volume Gateway<ul>
<li>Cached - OLD NAME (Gateway-Cached Volumes)</li>
<li>Stored - OLD NAME (Gateway-Stored Volumes)</li>
</ul>
</li>
<li>Tape Gateway - OLD NAME (Gateway-Virtual Tape Library)</li>
</ul>
</li>
<li>Remember that access to virtual tapes in your virtual tape library are instantaneous. If your tape is in the virtual tape shelf(glacier) it can take 24 hours to get back to your virtual tape library.</li>
<li>Encrypted using SSL for transit and is encrypted at rest in Amazon S3 using AES-256.</li>
<li>Gateway-Stored Volumes - stores data as Amazon EBS Snapshots in S3.</li>
<li>Snapshot can be scheduled.</li>
<li>Bandwidth can be throttled (good for remote sites)</li>
<li>You need a storage gateway in each site if using multiple locations.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Snowball (Import/Export)</strong></p>
<ul>
<li>Snowball<ul>
<li>Types<ul>
<li>Snowball</li>
<li>Snowball Edge</li>
<li>Snowmobile</li>
</ul>
</li>
<li>Understand what Snowball is</li>
<li>Understand what Import Export is</li>
<li>Snowball Can<ul>
<li>Import to S3</li>
<li>Export from S3</li>
</ul>
</li>
</ul>
</li>
<li>Import/Export<ul>
<li>Import/Export Disk<ul>
<li>Import to S3, EBS, Glacier</li>
<li>export from S3</li>
</ul>
</li>
<li>Import/Export Snowball<ul>
<li>Import to S3</li>
<li>Export to S3</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Automated Backups</strong></p>
<ul>
<li>AWS Services and Automated Backups<ul>
<li>Services that have Automated Backup<ul>
<li>RDS</li>
<li>Elasticache (Redis only)</li>
<li>Redshift</li>
</ul>
</li>
<li>Services that do not have Automated Backup<ul>
<li>EC2</li>
</ul>
</li>
<li>RDS Automated Backups<ul>
<li>For MySQL you need <strong>innoDB</strong> (transactional engine)</li>
<li>There is a performance hit if Multi-AZ is not enabled</li>
<li>If you delete an instance, then <strong>ALL</strong> automated backups are deleted</li>
<li>However, manual DB snapshots will <strong>NOT</strong> be deleted</li>
<li>All stored on S3</li>
<li>When you do a restore, you can change the engine type (<strong>SQL Standard to SQL Enterprise for example</strong>). Provided you have enough storage space</li>
</ul>
</li>
<li>Elasticache Backups<ul>
<li>Available for <strong>Redis Cache Cluster</strong> only</li>
<li>The entire cluster is snapshotted</li>
<li>Snapshot will degrade performance</li>
<li>Therefore only set your snapshot window during the least busy part of the day</li>
<li>Stored on <strong>S3</strong></li>
</ul>
</li>
<li>Redshift Backups<ul>
<li>Stored on <strong>S3</strong></li>
<li>By default, <strong>Amazon Redshift</strong> enables automated backups of your data warehouse cluster with a <strong>1-day</strong> retention period</li>
<li><strong>Amazon Redshift</strong> only backs up data that has changed so most snapshots only use up a small amount of your free backup storage</li>
</ul>
</li>
<li>EC2<ul>
<li>No automated backups</li>
<li>Backups <strong>degrade</strong> you performance, schedule these times wisely</li>
<li>Snapshots are stored in <strong>S3</strong></li>
<li>Can create automated backups using either the command line interface or Python</li>
<li>They are <strong>incremental</strong>:<ul>
<li>Snapshots only store incremental changes since last snapshot</li>
<li>Only charged for incremental storage</li>
<li>Each snapshot still contains the base snapshot data</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Domain 1 - Summary</strong></p>
<ul>
<li>Domain 1</li>
<li>Domain 1.0: <strong>High Availability</strong> and <strong>Business Continuity</strong></li>
<li>Demonstrate ability to architect the appropriate level of availability based on stakeholder requirements<ul>
<li>1.2 Demonstrate ability to implement <strong>DR</strong> for systems based on <strong>RPO</strong> and <strong>RTO</strong></li>
<li>1.3 Determine appropriate use of <strong>multi-Availability Zones</strong> vs. <strong>multi-Region architectures</strong></li>
<li>1.4 Demonstrate ability to implement self-healing capabilities</li>
<li><strong>15%</strong> of the exam</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Domain-2-Costing-5"><a href="#Domain-2-Costing-5" class="headerlink" title="Domain 2 -Costing - 5%"></a>Domain 2 -Costing - 5%</h3><ul>
<li>Cross Account Access Role &amp; Permissions<ul>
<li>Steps<ul>
<li>Identify our account numbers</li>
<li>Create a group in IAM - Dev</li>
<li>Create a user in IAM - Dev</li>
<li>Log in to Production</li>
<li>Create the “read-write-app-bucket” policy</li>
<li>Create the “UpdateApp” Cross Account Role</li>
<li>Apply the newly created policy to the role</li>
<li>Log in to the Developer Account</li>
<li>Create a new inline policy</li>
<li>Apply it to the Developer group</li>
<li>Login as John</li>
<li>Switch Accounts</li>
</ul>
</li>
</ul>
</li>
<li><p>AWS Organizations &amp; Consolidated Billing</p>
<ul>
<li>AWS Organizations<br>  <strong>AWS Organizations</strong> is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage.<ul>
<li>Available in <strong>two</strong> feature sets:<ul>
<li>Consolidated Billing</li>
<li>ALL Features</li>
</ul>
</li>
<li>consist of<ul>
<li>Root</li>
<li>Organization Unit (OU)</li>
<li>AWS Account</li>
</ul>
</li>
</ul>
</li>
<li>Consolidated Billing<ul>
<li>Accounts<ul>
<li>Paying Account (Paying account is independent. Cannot access resources of the other accounts)</li>
<li>Linked Accounts (All linked accounts are independent)</li>
</ul>
</li>
<li>Advantages<ul>
<li><strong>One bill</strong> per AWS account</li>
<li>Very easy to <strong>track</strong> charges and <strong>allocate</strong> costs</li>
<li>Volume pricing <strong>discount</strong></li>
</ul>
</li>
<li>S3 pricing</li>
<li>Reserved EC2 Instances</li>
<li>Best Practices<ul>
<li>Always <strong>enable</strong> multi-factor authentication on root account.</li>
<li>Always use a <strong>strong</strong> and <strong>complex</strong> password on root account.</li>
<li>Paying account should be used for billing purposes <strong>only</strong>. Do not deploy resources in to paying account.</li>
</ul>
</li>
<li>Notes<ul>
<li>Linked Accounts<ul>
<li><strong>20</strong> linked accounts only</li>
<li>To add more visit <a href="https://aws-portal.amazon.com/gp/aws/html-forms-controller/contactus/aws-account-and-billing" target="_blank" rel="external">https://aws-portal.amazon.com/gp/aws/html-forms-controller/contactus/aws-account-and-billing</a></li>
</ul>
</li>
<li>Billing Alerts<ul>
<li>When monitoring is enabled on the paying account the billing data for all linked accounts is included</li>
<li>You can still create billing alerts per individual account</li>
</ul>
</li>
<li>CloudTrail<ul>
<li>Per <strong>AWS Account</strong> and is enabled <strong>per region</strong>.</li>
<li>Can consolidate logs using an <strong>S3 bucket</strong>.<ol>
<li>Turn on CloudTrail in the paying account</li>
<li>Create a <strong>bucket policy</strong> that allows cross account access</li>
<li>Turn on CloudTrail in the other accounts and use the bucket in the paying account</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips:<ul>
<li>Consolidated billing allows you to get <strong>volume discounts</strong> on all your accounts.</li>
<li>Unused reserved instances for <strong>EC2</strong> are applied across the group.</li>
<li><strong>CloudTrail</strong> is on a per account and per region basis but can be aggregate in to a single bucket in the paying account.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Tagging &amp; Resource Groups</p>
<ul>
<li>What Are Tags<ul>
<li><strong>Key Value Pairs</strong> attached to AWS resources</li>
<li>Metadata <strong>(data about data)</strong></li>
<li>Tags can sometimes be <strong>inherited</strong><ul>
<li>Autoscaling, CloudFormation and Elastic Beanstalk can create other resources</li>
</ul>
</li>
</ul>
</li>
<li>What Are Resource Groups<ul>
<li>Resource groups make it easy to <strong>group your resources</strong> using the tags that are assigned to them. You can group resources that share one or more tags.</li>
<li>Resource groups contain information such as:<ul>
<li>Region</li>
<li>Name</li>
<li>Health Checks</li>
</ul>
</li>
<li>Specific information<ul>
<li>For <strong>EC2</strong> - Public &amp; Private IP Addresses</li>
<li>For <strong>ELB</strong> - Port Configurations</li>
<li>For <strong>RDS</strong> - Database Engine etc.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Reserved Instance for EC2 &amp; RDS</p>
<ul>
<li>EC2<ul>
<li>EC2 - Pricing Models<ul>
<li><strong>On Demand</strong> - allow you to pay a fixed rate by the hour with no commitment.</li>
<li><strong>Reserved</strong> - provide you with a capacity reservation, and offer a significant discount on the hourly charge for an instance 1 year or 3 year terms.</li>
<li><strong>Spot</strong> - enable you to bid whatever price you want for instance capacity, providing for even greater savings if your applications have flexible start and end times.</li>
<li><strong>Dedicated</strong> - Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that’s dedicated to a single customer. Your Dedicated Instances are physically isolated at the host hardware level from your instances that aren’t Dedicated Instances and from instances that belong to other AWS accounts.</li>
</ul>
</li>
<li>EC2 - Pricing Models detail<ul>
<li><strong>On Demand</strong><ul>
<li>Users that want the low cost and flexibility of Amazon EC2 without any up-front payment or long-term commitment</li>
<li>Applications with short term, spiky, or unpredictable workloads that cannot be interrupted.</li>
<li>Applications being developed or tested on Amazon EC2 for the first time</li>
</ul>
</li>
<li><strong>Reserved</strong><ul>
<li>Applications with steady state or predictable usage</li>
<li>Applications that require reserved capacity</li>
<li>Users able to make upfront payments to reduce their total computing costs even further.</li>
</ul>
</li>
<li><strong>Spot</strong><ul>
<li>Applications that have flexible start and end times</li>
<li>Applications that are only feasible at very low compute prices</li>
<li>Users with urgent computing needs for large amounts of additional capacity</li>
</ul>
</li>
</ul>
</li>
<li>Understanding Reserved Instances<ul>
<li>All Up Front = <strong>Largest Discount</strong> (Up to 75%)</li>
<li>Partial Up Front = <strong>Middle Discount</strong></li>
<li>No Upfront = <strong>Least Discount</strong> (Still cheaper than on demand)</li>
</ul>
</li>
<li>Different Types of RIs<ul>
<li><strong>Standard RIs</strong><ul>
<li>These provide the most significant discount <strong>(up to 75% off On-Demand)</strong> and are best suited for steady-state usage.</li>
</ul>
</li>
<li><strong>Convertible RIs</strong><ul>
<li>These provide a discount <strong>(up to 45% off On-Demand)</strong> and the capability to change the attributes of the RI as long as the exchange results in the creation of Reserved Instances of equal or greater value. Like Standard RIs, Convertible RIs are best suited for steady-state usage.</li>
</ul>
</li>
<li><strong>Scheduled RIs</strong><ul>
<li>These are available to launch within the time windows you reserve. This option allows you to match your capacity reservation to a predictable recurring schedule that only requires a fraction of a day, a week, or a month.</li>
</ul>
</li>
<li><strong>Convertible RIs</strong><ul>
<li>Change instance families, operating system, tenancy and payment option</li>
</ul>
</li>
</ul>
</li>
<li>Understanding Reserved Instances<ul>
<li>You can modify reserved instances<ul>
<li>Switching Availability Zones</li>
<li>Change the instance type within the same instance family</li>
</ul>
</li>
</ul>
</li>
<li>Change Your Reserved Instances<ul>
<li>You can change your standard reserved instances by submitting a modification request. This request will be processed providing the footprint remains the same. This is calculated by using <strong>normalization factors</strong>.</li>
</ul>
</li>
<li>Normalization Factors<ul>
<li>Each <strong>Reserved Instance</strong> has an instance size footprint, which is determined by the normalization factor of the instance type and the number of instances in the reservation.</li>
<li>A modification request is <strong>not</strong> processed if the footprint of the target configuration does not match the size of the original configuration. In the <strong>Amazon EC2</strong> console, the footprint is measured in units.</li>
<li>Example:<ul>
<li>Take the quantity of reserved instances and multiply it by the normalization factor.</li>
<li>So a quantity of 2 large instances = (2*4) = <strong>8</strong></li>
<li>This can be changed to 8 small instances <strong>(8*1)</strong> or 4 medium instances <strong>(2*4)</strong></li>
</ul>
</li>
<li><strong>Reserved Instances can only be modified if they are within the SAME family</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>RDS</p>
<ul>
<li><p>RDS - Reserved Instances</p>
<ul>
<li>Each reservation is associated with the following set of attributes:<ul>
<li>DB Engine</li>
<li>DB Instance class</li>
<li>Deployment type</li>
<li>License Model</li>
<li>Region</li>
</ul>
</li>
<li>Each reservation can only be applied to a DB Instance with the <strong>same</strong> attributes for the duration of the term. If you decide to modify any of these attributes of your running DB Instance class before the end of the reservation term, your hourly usage rates for that DB Instance will <strong>revert</strong> to on demand hourly rates.</li>
<li>If you later modify the running DB Instance’s attributes to <strong>match</strong> those of the original reservation, or <strong>create a new</strong> DB Instance with the <strong>same</strong> attributes as your original reservation, your reserved pricing will be applied to it until the end of your reservation term.</li>
<li>You can have <strong>reserved</strong> instances for <strong>RDS Multi-AZ’s</strong> as well as <strong>Read Replicas</strong>.</li>
<li>For Read Replica’s the DB instance class and region <strong>must</strong> be the same.</li>
</ul>
</li>
<li><p>RDS - Move AZs</p>
<ul>
<li>Each Reserved Instance is associated with a specific <strong>Region</strong>, which is fixed for the lifetime of the reservation and cannot be changed. Each reservation <strong>can</strong>, however, be used in any of the available AZs within the associated Region.</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Largest discount is applied for all Upfront <strong>(up to 75%)</strong></li>
<li>Contract length is <strong>1 or 3 years</strong>. The longer the contract, the more you save.</li>
<li>Three types of RIs<ul>
<li><strong>Standard, Convertible &amp; Scheduled</strong></li>
</ul>
</li>
<li>Standard RIs for EC2 can be <strong>modified</strong>, but only if they are in the <strong>same family</strong> and only if the normalization factors are equal and only for linux (excluding RedHat or SUSE Linux)<ul>
<li>Therefore it is better to purchase an RI with more <strong>capacity</strong> than you actually need, so as to allow for growth.</li>
</ul>
</li>
<li>You can <strong>switch</strong> EC2 RIs between AZs, but <strong>not</strong> between regions.</li>
<li>You can sell EC2 RIs on <a href="https://aws.amazon.com/ec2/purchasing-options/reserved-instances/marketplace/" target="_blank" rel="external">https://aws.amazon.com/ec2/purchasing-options/reserved-instances/marketplace/</a></li>
<li>You can have reserved <strong>RDS instances</strong></li>
<li>You can move AZ’s but <strong>not</strong> regions</li>
<li>Understand the different use cases of <strong>reserved, on demand</strong> &amp; <strong>spot instances</strong>.</li>
<li>Typically you only want to use on demand instances for things like <strong>autoscaling</strong>. Spot is going to be the cheapest way of doing something, but is might not be the best answer technically.</li>
<li>Read the question <strong>carefully</strong>. Is it asking you what is the most commercially feasible way of designing a solution, or is it asking for high availability with low RTO/RPOs.</li>
<li>Knowing the <strong>different</strong> instance type by name <strong>REALLY</strong> helps.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Knowing your EC2 Instance Type</p>
<ul>
<li><p>EC2 Instance Types</p>
<p>| Family | Speciality| Use case|<br>| ————- |:————-:| —–:|<br>| D2 | Dense Storage | Fileservers/Data Wareshousing/Hadoop|<br>| R4 | Memory Optimized  | Memory Intensive Apps/DBs|<br>| M4 | General Purpose| Application Servers |<br>| C4 | Compute Optimized | CPU Intensive Apps/DBs |<br>| G2 | Graphics Intensive | Video Encoding/3D Application Streaming |<br>| I2 | High Speed Storage | NoSQL DBs, Data Warehousing etc |<br>| F1 | Field Programmable Gate Array| Hardware acceleration for your code |<br>| T2 | Lowest Cost, General Purpose | Web Servers/Small DBs |<br>| P2 | Graphics/General Purpose GPU | Machine Learning, Bit Coin Mining etc |<br>| X1 | Memory Optimize | SAP HANA/Apache Spark etc |</p>
</li>
<li><p>How to remember Instance type</p>
<ul>
<li>D for Density</li>
<li>R for RAM</li>
<li>M - main choice for general purpose apps</li>
<li>C for Compute</li>
<li>G - Graphics</li>
<li>I for IOPS</li>
<li>F for FPGA</li>
<li>T cheap general purpose (think T2 micro)</li>
<li>P - Graphics (think Pics)</li>
<li>X - Extreme Memory</li>
<li><strong>DR Mc GIFT PX</strong></li>
</ul>
</li>
</ul>
</li>
<li>Domain 2 Summary<ul>
<li>Domain 2.0: Costing<ul>
<li>2.1 Demonstrate ability to make architectural decisions that minimize and optimize infrastructure cost</li>
<li>2.2 Apply the appropriate AWS account and billing set-up options based on scenario</li>
<li>2.3 Ability to compare and contrast the cost implications of different architectures</li>
<li>5% of the exam</li>
</ul>
</li>
<li>Steps To Enable Cross Account Access<ul>
<li>If you need a custom policy (such as read and write access to a custom S3 bucket) create this policy first.</li>
<li>Create role with cross account access</li>
<li>Apply the policy to that role and note down the ARN.</li>
<li>Grant access to the Role</li>
<li>Switch to the Role</li>
<li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html" target="_blank" rel="external">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></li>
<li><a href="https://aws.amazon.com/blogs/security/how-to-enable-cross-account-access-to-the-aws-management-console/" target="_blank" rel="external">https://aws.amazon.com/blogs/security/how-to-enable-cross-account-access-to-the-aws-management-console/</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Domain-3-Deployment-Management-10"><a href="#Domain-3-Deployment-Management-10" class="headerlink" title="Domain 3 - Deployment Management - 10%"></a>Domain 3 - Deployment Management - 10%</h3><ul>
<li><p>CloudFormation</p>
<ul>
<li><strong>READ</strong> the CloudFormation FAQs: <a href="https://aws.amazon.com/cloudformation/faqs/" target="_blank" rel="external">https://aws.amazon.com/cloudformation/faqs/</a> and do the deep dive lab</li>
<li>What is CloudFormation?<ul>
<li>One of the most powerful parts of AWS, <strong>CloudFormation</strong> allows you to take what was once traditional hardware infrastructure and convert it into code.</li>
<li><strong>CloudFormation</strong> gives developers an systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.</li>
<li>You don’t need to figure out the order for provisioning AWS services or the subtleties of making those dependencies work. <strong>CloudFormation</strong> takes care of this for you.</li>
<li>After the AWS resources are deployed, you can <strong>modify and update</strong> them in a controlled and predictable way, in effect applying version control to your AWS infrastructure the same way you do with your software.</li>
</ul>
</li>
<li>CloudFormation Stack vs. Template<ul>
<li>A <strong>CloudFormation Template</strong> is essentially an architectural diagram an a <strong>CloudFormation Stack</strong> is the end result of that diagram (i.e. what is actually provisioned).</li>
<li>You create, update and delete a collection of resources by creating, updating an deleting stacks using CloudFormation templates.</li>
<li>CloudFormation templates are in the <strong>JSON</strong> format or <strong>YAML</strong>.</li>
</ul>
</li>
<li>Elements Of A Template<ul>
<li><strong>Mandatory Elements</strong><ul>
<li>List of AWS Resources and their associated configuration values</li>
</ul>
</li>
<li><strong>Optional Elements</strong><ul>
<li>The template’s file format &amp; version number</li>
<li>Template Parameters<ul>
<li>The input values that are supplied at stack creation time. Limit of 60.</li>
</ul>
</li>
<li>Output Values<ul>
<li>The output values required once a stack has finished building (such as the public IP address, ELB address, etc.) Limit of 60.</li>
</ul>
</li>
<li>List of data tables<ul>
<li>Used to look up static configuration values such as AMI’s etc.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Outputting Data</p>
<ul>
<li>You can use <strong>Fn:GetAtt</strong> to output data.<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&quot;Public&quot;: &#123;</div><div class="line">&quot;Description&quot;: &quot;Public IP address of the web server&quot;,</div><div class="line">&quot;Value&quot;: &#123;</div><div class="line">&quot;Fn::GetAtt&quot;:[</div><div class="line">&quot;WebServerHost&quot;,</div><div class="line">&quot;PublicIp&quot;</div><div class="line">]</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Chef &amp; Puppet Integration</p>
<ul>
<li>Cloud Formation supports <strong>Chef &amp; Puppet Integration</strong>, meaning that you can deploy and configure right down to the application layer.</li>
<li><strong>Bootstrap scripts</strong> are also supported enabling you to install packages, files, an services on your <strong>EC2 instances</strong> by simply describing them in your CloudFormation template.</li>
</ul>
</li>
<li>Stack Creation Errors<ul>
<li>By Default, the **”automatic rollback on error” feature is enabled. This will cause all AWS resources that AWS CloudFormation created successfully for a stack up to the point where an error occurred to be deleted.</li>
<li>You will be <strong>charged</strong> for resources that are provisioned, even if there is an error.</li>
<li>CloudFormation, itself, is <strong>free</strong>.</li>
</ul>
</li>
<li>Stacks Can Wait For Applications<ul>
<li>AWS CloudFormation provides a <strong>WaitCondition</strong> resources that acts as a barrier, blocking the creation of other resources until a completion signal is received from an external source, such as your application or management system.</li>
</ul>
</li>
<li>You can Specify Deletion Policies<ul>
<li>AWS CloudFormation allows you to <strong>define deletion policies</strong> for resources in the template. You can specify that snapshots be created for Amazon EBS volumes or Amazon RDS database instances before they are deleted.</li>
<li>You can also specify that a resource should be preserved and not deleted when the stack is deleted. This is useful for preserving <strong>Amazon S3 buckets</strong> when the stack is deleted.</li>
</ul>
</li>
<li>You Can Update Your Stack After It Is Created<ul>
<li>You can use AWS CloudFormation to <strong>modify</strong> and <strong>update</strong> the resources in your existing stacks in a controlled and predictable way. By using templates to manage your stack changes, you have the ability to apply version control to your <strong>AWS infrastructure</strong>, just as you do with the software running on it.</li>
</ul>
</li>
<li><p>CloudFormation &amp; Roles</p>
<ul>
<li><p>CloudFormation can be used to create roles in <strong>IAM</strong>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">&quot;Type&quot;: &quot;AWS::IAM::Role&quot;,</div><div class="line">&quot;Properties&quot;:&#123;</div><div class="line">  &quot;AssumeRolePolicyDocument&quot;: &#123; JSON &#125;,</div><div class="line">  &quot;ManagedPolicyArns&quot;: [String,...],</div><div class="line">  &quot;Path&quot;: String,</div><div class="line">  &quot;Policies&quot;: [Policies,...]</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>CloudFormation can be used to grant EC2 instances access to roles.</p>
</li>
</ul>
</li>
<li><p>VPCs Can Be Created And Customized</p>
<ul>
<li><strong>CloudFormation</strong> supports creating VPCs, Subnets, Gateways, Route Tables and Network ACLs, as well as creating resources such as Elastic IPs, Amazon EC2 Instances, EC2 Security Groups, Auto Scaling Groups, Elastic Load Balancers, Amazon RDS Database Instances, and Amazon RDS Security Groups in a VPC.</li>
<li>You can specify IP address ranges both in terms of <strong>CIDR ranges</strong> as well as <strong>individual IP addresses</strong> for specific instances. You can specify pre-existing EIPs.  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;AWSTemplateFormatVersion&quot; : &quot;2010-09-09&quot;,</div><div class="line">    &quot;Description&quot; : &quot;Simple Standalone ENI&quot;,</div><div class="line">    &quot;Resources&quot; : &#123;</div><div class="line">        &quot;myENI&quot; : &#123;</div><div class="line">            &quot;Type&quot; : &quot;AWS::EC2::NetworkInterface&quot;,</div><div class="line">            &quot;Properties&quot; : &#123;</div><div class="line">                &quot;Tags&quot;: [&#123;&quot;Key&quot;:&quot;foo&quot;,&quot;Value&quot;:&quot;bar&quot;&#125;],</div><div class="line">                &quot;Description&quot;: &quot;A nice description.&quot;,</div><div class="line">                &quot;SourceDestCheck&quot;: &quot;false&quot;,</div><div class="line">                &quot;GroupSet&quot;: [&quot;sg-75zzz219&quot;],</div><div class="line">                &quot;SubnetId&quot;: &quot;subnet-3z648z53&quot;,</div><div class="line">                &quot;PrivateIpAddress&quot;: &quot;10.0.0.16&quot;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>VPC Peering</p>
<ul>
<li>You can create <strong>multiple</strong> VPCs inside one template.</li>
<li>You can enable <strong>VPC Peering</strong> using <strong>CloudFormation</strong>, but only within the <strong>same</strong> AWS Account.</li>
</ul>
</li>
<li>Route53 Supported<ul>
<li>You can create <strong>new</strong> hosted zones or <strong>update existing</strong> hosted zones using <strong>CloudFormation Templates</strong>.</li>
<li>This includes <strong>adding</strong> or <strong>changing</strong> items such as A records, Aliases, CNAMEs, etc.</li>
</ul>
</li>
<li>Exam Tips<ul>
<li><strong>CloudFormation</strong> - a big topic in the exam.</li>
<li>Know <strong>all</strong> the services that are supported.</li>
<li>Remember what is mandatory for a template - <strong>“Resources”</strong></li>
<li>You can create <strong>multiple VPCs</strong> inside one template.</li>
<li>You can <strong>enable VPC Peering</strong> using CloudFormation, but only within the <strong>same</strong> AWS Account.</li>
<li><strong>Chef, Puppet</strong> &amp; <strong>Bootstrap Scripts</strong> are supported.</li>
<li>You can use <strong>Fn::GetAtt</strong> to output data.</li>
<li>By default, the <strong>“automatic rollback on error”</strong> feature is <strong>enabled</strong>.</li>
<li>You are <strong>charged</strong> for errors.</li>
<li>CloudFormation is <strong>free</strong>.</li>
<li>Stacks can wait for applications to be provisioned using the <strong>“WaitCondition”</strong>.</li>
<li>Route53 is completely <strong>supported</strong>. This includes creating <strong>new</strong> hosted zones or <strong>updating</strong> existing ones.</li>
<li>You can create A Records, Aliases etc.</li>
<li>IAM Role Creation and Assignment is also <strong>supported</strong>.</li>
</ul>
</li>
<li><p>CloudFormation material</p>
<ul>
<li>CloudFormation Basic<ul>
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/gettingstarted.templatebasics.html" target="_blank" rel="external">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/gettingstarted.templatebasics.html</a></li>
<li>Learn about the following about templates:<ul>
<li>Declaring resources and their properties</li>
<li><strong>Ref</strong>erencing(提取) other resources with the <strong>Ref</strong> function and resource attributes using the Fn::GetAtt function</li>
<li>Using parameters to enable users to specify values at stack creation time and using constraints to validate parameter input</li>
<li>Using mappings to determine conditional values</li>
<li>Using the Fn::Join function to construct values based on parameters, resource attributes, and other things</li>
<li>Using output values based to capture information about the stack’s resources.</li>
</ul>
</li>
</ul>
</li>
<li><p>CloudFormation intrinsic Function</p>
<ul>
<li>Fn::Base64<ul>
<li>returns the Base64 representation of the input string. this function is typically used to pass encoded data to Amazon EC2 instances by way of the UserData property.</li>
<li>JSON Format { “Fn::Base64” : valueToEncode }</li>
</ul>
</li>
<li>Fn::FindInMap<ul>
<li>returns the value corresponding to keys in a two-level map that is declared in the Mapping section.</li>
<li>JSON Format { “Fn::FindInMap” : [ “MapName”, “TopLevelKey”, “SecondLevelKey”] }</li>
</ul>
</li>
<li>Fn::GetAtt<ul>
<li>returns the value of an attribute from a resource in the template</li>
<li>JSON Format { “Fn::GetAtt” : [ “logicalNameOfResource”, “attributeName” ] }</li>
</ul>
</li>
<li>Fn::Join<ul>
<li>Fn::Join appends a set of values into a single value, separated by the specified delimiter. If a delimiter is the empty string, the set of values are concatenated with no delimiter.</li>
<li>JSON Format { “Fn::Join” : [ “delimiter”, [ comma-delimited list of values ] ] }</li>
<li>JSON example {“Fn::Join” : [ “:”, [ “a”, “b”, “c” ] ]} will returns “a:b:c”</li>
</ul>
</li>
<li>Fn::Select<ul>
<li>returns a single object from a list of objects by index</li>
<li>JSON Format { “Fn::Select” : [ index, listOfObjects ] }</li>
<li>JSON example { “Fn::Select” : [ “1”, [ “apples”, “grapes”, “oranges”, “mangoes” ] ] } will returns “grapes”</li>
</ul>
</li>
<li>Fn::Split<ul>
<li>To split a string into a list of string values so that you can select an element from the resulting string list.</li>
<li>JSON Format { “Fn::Split” : [ “delimiter”, “source string” ] }</li>
<li>JSON example { “Fn::Split” : [ “|” , “a|b|c” ] } will return [“a”, “b”, “c”]</li>
</ul>
</li>
<li><p>Fn::Sub</p>
<ul>
<li>将输入字符串中的变量替换为您指定的值</li>
<li>JSON Format  { “Fn::Sub” : [ String, { Var1Name: Var1Value, Var2Name: Var2Value } ] }</li>
<li>JSON example - 将AWS::Region和AWS::StackName替换为实际的值<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&quot;UserData&quot;: &#123; &quot;Fn::Base64&quot;: &#123; &quot;Fn::Join&quot;: [&quot;\n&quot;, [</div><div class="line">&quot;#!/bin/bash -xe&quot;,</div><div class="line">&quot;yum update -y aws-cfn-bootstrap&quot;,</div><div class="line">&#123; &quot;Fn::Sub&quot;: &quot;/opt/aws/bin/cfn-init -v --stack $&#123;AWS::StackName&#125; --resource LaunchConfig --configsets wordpress_install --region $&#123;AWS::Region&#125;&quot; &#125;,</div><div class="line">&#123; &quot;Fn::Sub&quot;: &quot;/opt/aws/bin/cfn-signal -e $? --stack $&#123;AWS::StackName&#125; --stack $&#123;AWS::StackName&#125; --resource WebServer --region $&#123;AWS::Region&#125;&quot; &#125;]]</div><div class="line">&#125;&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Ref</p>
<ul>
<li>returns the value of the specified parameter or resource.<ul>
<li>When you specify a parameter’s logical name, it returns the value of the parameter.</li>
<li>When you specify a resource’s logical name, it returns a value that you can typically use to refer to that resource, such as a physical ID.</li>
</ul>
</li>
<li>JSON Format { “Ref” : “logicalName” }</li>
</ul>
</li>
</ul>
</li>
<li>CloudFormation limit<ul>
<li>You can include up to 60 parameters and 60 outputs in a template.</li>
<li>There are no limit to the number of templates.</li>
<li>Each AWS CloudFormation account is limited to a maximum of 200 stacks.</li>
</ul>
</li>
<li>CloudFormation – Ref, Fn::Join, GetAtt, Fn::split, Fn::select and etc function</li>
</ul>
</li>
</ul>
</li>
<li><p>Elastic Beanstalk</p>
<ul>
<li>Elastic Beanstalk<ul>
<li><strong>AWS Elastic Beanstalk</strong> makes it even easier for developers to quickly deploy and manage applications in the AWS cloud. Developers simply upload their application, and Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring.</li>
</ul>
</li>
<li>Difference Between CloudFormation &amp; Elastic Beanstalk?<ul>
<li><strong>CloudFormation</strong> supports Elastic Beanstalk, but Elastic Beanstalk will <strong>not</strong> provision CloudFormation Templates.</li>
</ul>
</li>
<li>Supported Languages * Stacks<ul>
<li>Packer Builder</li>
<li>Apache<ul>
<li>Apache Tomcat for Java applications (Java 6,7 &amp; 8)</li>
<li>Apache HTTP Server for PHP applications (PHP 5.4 - 7.0)</li>
<li>Apache HTTP Server for Python applications</li>
<li>Nginx or Apache HTTP Server for Node.js applications</li>
</ul>
</li>
<li>Docker<ul>
<li>Single Container Docker</li>
<li>Multicontainer Docker</li>
<li>Preconfigured Docker</li>
</ul>
</li>
<li>Ruby (Passenger or Puma) (Ruby 1.9 - 2.3)</li>
<li>Go 1.6</li>
<li>Java with Tomcat</li>
<li>Java SE (Java 7 &amp; 8)</li>
<li>.NET on Windows Server with IIS 7.5, 8.0 and 8.5</li>
<li>Node.js (nginx &amp; Apache)</li>
<li>Python (2.6 - 3.4)</li>
<li>PHP</li>
</ul>
</li>
<li>What Can Be Controlled With Elastic Beanstalk<ul>
<li>Access built-in <strong>Amazon CloudWatch</strong> monitoring and get notifications on application health and other important events</li>
<li>Adjust application server settings <strong>(e.g. JVM settings)</strong> and pass environment variables</li>
<li>Run other application components, such as a <strong>memory caching service</strong>, side-by-side in Amazon EC2</li>
<li>Access log files <strong>without</strong> logging in to the application servers</li>
</ul>
</li>
<li>Provisioning With Elastic Beanstalk<ul>
<li>You can simply upload your deployable code <strong>(e.g. WAR file)</strong> or push your Git repository, and AWS Elastic Beanstalk does the rest. The AWS Toolkit for <strong>Visual Studio</strong> and the AWS Toolkit for <strong>Eclipse</strong> allow you to deploy your application to AWS Elastic Beanstalk and manage it, without leaving your IDE.</li>
</ul>
</li>
<li>Updating Elastic Beanstalk<ul>
<li>You can push out <strong>updates from GIT</strong>: only the modified files are transmitted to AWS Elastic Beanstalk.</li>
<li>Elastic Beanstalk is designed to support <strong>multiple</strong> running environments such as one for integration testing, one for <strong>pre-production</strong>, and one for <strong>production</strong>. Each environment is independently configured and runs on its <strong>own</strong> separate AWS resources. Elastic Beanstalk also <strong>stores</strong> and <strong>tracks</strong> application versions over time, so an existing environment can be easily rolled back to a prior version or a new environment can be launched using an older version to try and reproduce a customer problem.</li>
</ul>
</li>
<li>How Does It Work?<ul>
<li>Elastic Beanstalk stores your application files and, <strong>optionally</strong>, your server log files in Amazon S3. If you are using the AWS Management Console, Git, the AWS Toolkit for Visual Studio, or AWS Toolkit for Eclipse, an Amazon S3 bucket will be created in your account, and the files you upload will be <strong>automatically</strong> copied from your local client to Amazon S3.</li>
<li>Optionally, you may configure Elastic Beanstalk to copy your server log files <strong>every hour</strong> to Amazon S3. You do this by editing the environment configuration settings.</li>
</ul>
</li>
<li>Can It Store Application Data Like Images?<ul>
<li>You can use Amazon S3 for <strong>application storage</strong>. The easiest way to do this is by including the <strong>AWS SDK</strong> as part of your application’s deployable file. For example, you can <strong>include</strong> the AWS SDK for Java as port of your application’s WAR file.</li>
</ul>
</li>
<li>Can Elastic Beanstalk Provision RDS Instances?<ul>
<li><strong>Yes:</strong> Elastic Beanstalk can <strong>automatically</strong> provision an Amazon RDS DB Instance. The connectivity information to the DB Instance is exposed to your application by <strong>environment variables</strong>.</li>
</ul>
</li>
<li>Elastic Beanstalk Fault Tolerance<ul>
<li>Elastic Beanstalk can be configured to be <strong>fault tolerant</strong> within a <strong>single region</strong> using <strong>multiple</strong> availability zones.</li>
</ul>
</li>
<li>Elastic Beanstalk Security<ul>
<li>By default, your application is available publicly at <strong>myapp.elasticbeanstalk.com</strong></li>
<li>Integrates with <strong>AWS VPC</strong>. You can then <strong>restrict</strong> who can access the app via <strong>white-listed IP addresses</strong> (either at the security group level or NACL level.)</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Elastic Beanstalk can provision <strong>RDS</strong> instances</li>
<li>Elastic Beanstalk supports <strong>IAM</strong></li>
<li>Elastic Beanstalk supports <strong>VPC</strong></li>
<li>You have <strong>full access</strong> to the resources under Elastic Beanstalk</li>
<li>Code is stored in <strong>S3</strong></li>
<li><strong>Multiple</strong> environments are allowed to support version control. You can <strong>roll</strong> back changes.</li>
<li><strong>Only</strong> the changes from <strong>Git repositories</strong> are replicated.</li>
<li>Amazon Linux AMI &amp; Windows 2012 R2 <strong>supported</strong>.\</li>
</ul>
</li>
</ul>
</li>
<li><p>OpsWorks</p>
<ul>
<li><p>What is OpsWorks</p>
<ul>
<li>Cloud-based applications usually require a group of related resources—application servers, database servers, and so on—that must be created and managed collectively. This collection of instances is called a stack. A simple application stack might look something like the following.<br><img src="/images/AWS/Sysops/what_is_opswork.png" alt="what_is_opswork"></li>
<li>AWS OpsWorks Stacks provides a simple and straightforward way to create and manage stacks and their associated applications and resources</li>
<li><strong>Amazon Definition</strong>:<ul>
<li>AWS OpsWorks is an application management service that helps you <strong>automate</strong> operational tasks like code deployment, software configurations, package installations, database setups, and server scaling using <strong>Chef</strong>. <strong>OpsWorks</strong> gives you the flexibility to define your <strong>application architecture</strong> and <strong>resource configuration</strong> and handles the <strong>provisioning</strong> and <strong>management</strong> of your AWS resources for you. OpsWorks includes automation to scale your application based on <strong>time</strong> or <strong>load</strong>, monitoring to help you troubleshoot and take automated action based on the state of your resources, and permissions and policy management to make management of multi-user environments <strong>easier</strong>.</li>
</ul>
</li>
</ul>
</li>
<li><p>What is Chef</p>
<ul>
<li>Chef turns <strong>infrastructure</strong> into <strong>code</strong>. With Chef, you can <strong>automate</strong> how you <strong>build</strong>, <strong>deploy</strong>, and <strong>manage</strong> your infrastructure. Your infrastructure becomes as versionable, testable, and repeatable as application code.</li>
<li>Chef server stores your <strong>recipes</strong> as well as other configuration <strong>data</strong>. The Chef client is installed on each server, virtual machine, container, or networking device you manage - we’ll call these <strong>nodes</strong>. The client periodically polls Chef server latest policy and state of your network. If anything on the node is out of date, the client brings it up to date.</li>
</ul>
</li>
<li><p>What is OpsWorks</p>
<ul>
<li>A <strong>GUI</strong> to deploy and configure your infrastructure quickly. OpsWorks consists of <strong>two</strong> elements, <strong>Stacks</strong> and <strong>Layers</strong>.</li>
<li>A stack is a <strong>container</strong> (or <strong>group</strong>) of resources such as <strong>ELBs</strong>, <strong>EC2</strong> instances, <strong>RDS</strong> instances etc.</li>
<li>A <strong>layer</strong> exists <strong>within</strong> a stack and consists of things like a <strong>web application</strong> layer. An <strong>application processing</strong> layer or a <strong>Database</strong> layer.</li>
<li>When you <strong>create</strong> a layer, rather than going and configuring everything manually (like installing Apache, PHP etc) OpsWorks takes <strong>care</strong> of this for you.</li>
</ul>
</li>
<li><p>Layers</p>
<ul>
<li><strong>1 or more</strong> layers in the stack</li>
<li>An instance must be assigned to <strong>at least</strong> 1 layer</li>
<li>Which chef layers run, are <strong>determined by</strong> the layer the instance belongs to</li>
<li>Preconfigured Layers <strong>include</strong>:<ul>
<li>Applications</li>
<li>Databases</li>
<li>Load Balancers</li>
<li>Caching</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Domain 3<ul>
<li>Domain 3.0: Deployment Management<ul>
<li>3.1 Ability to manage the lifecycle of an application on AWS</li>
<li>3.2 Demonstrate ability to implement the right architecture for development, testing, and staging environments</li>
<li>3.3 Position and select most appropriate AWS deployment mechanism based on scenario</li>
</ul>
</li>
<li>OpsWorks Exam Tips:<ul>
<li>Do the <strong>OpsWorks Lab</strong></li>
<li>You need to know the difference between:<ul>
<li>A <strong>Stack</strong></li>
<li>A <strong>Layer</strong></li>
<li>A <strong>Recipe</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Domain-4-Network-Design-10"><a href="#Domain-4-Network-Design-10" class="headerlink" title="Domain 4 - Network Design - 10%"></a>Domain 4 - Network Design - 10%</h3><ul>
<li><p>VPC</p>
<ul>
<li>What You Should Know Already<ul>
<li>What a <strong>VPC</strong> is</li>
<li>How to <strong>build</strong> your own VPC</li>
<li>How to make a subnet <strong>public</strong></li>
<li>How to make a subnet <strong>private</strong></li>
<li>What a <strong>NAT</strong> is<ul>
<li>Disable Source/Destination Checks</li>
</ul>
</li>
<li>What a <strong>route table</strong> is </li>
<li><strong>Subnets</strong> can communicate with each other by default</li>
</ul>
</li>
<li>Basic Info<ul>
<li>Think of a VPC as a logical datacenter in AWS</li>
<li>Consists of IGW’s (Or Virtual Private Gateways), Route Tables, Network Access Control Lists, Subnets, Security Groups</li>
<li>1 Subnet = 1 Availability Zone</li>
<li>Security Groups are Stateful, Network Access Control Lists are Stateless.</li>
<li>Can Peer VPCs both in the same account and with other AWS accounts.</li>
<li>No Transitive Peering</li>
<li>Custom VPC network block size has to be between a /16 netmask and /28 netmask.</li>
</ul>
</li>
<li>What can you do with a VPC<ul>
<li>Launch instances into a subnet of your choosing</li>
<li>Assign custom IP address ranges in each subnet</li>
<li>Configure route tables between subnets</li>
<li>Create internet gateway and attach it to our VPC</li>
<li>Much better security control over your AWS resources</li>
<li>Instance security groups</li>
<li>Subnet network access control lists (ACLS)</li>
</ul>
</li>
<li><p>Default VPC vs Custom VPC</p>
<ul>
<li>Default VPC is user friendly, allowing you to immediately deploy instances</li>
<li>All Subnets in default VPC have a route out to the internet.</li>
<li>Each EC2 instance has both a public and private IP address</li>
<li>If you delete the default VPC the only way to get it back is to contact AWS.</li>
</ul>
</li>
<li><p>VPC peering</p>
<ul>
<li>Allows you to connect one VPC with another via a direct network route using private IP addresses.</li>
<li>Instances behave as if they were on the same private network</li>
<li>You can peer VPC’s with other AWS accounts as well as with other VPCs in the same account.</li>
<li>Peering is in a star configuration, ie 1 central VPC peers with 4 others, <strong>NO TRANSITIVE PEERING!!!</strong></li>
</ul>
</li>
<li><p>Create VPC</p>
<ul>
<li>things automatically created<ul>
<li>Route tables</li>
<li>Network ACLs</li>
<li>Security Groups</li>
<li>DHCP options set</li>
</ul>
</li>
<li>things are not automatically created<ul>
<li>Internet Gateways</li>
<li>Subnets</li>
</ul>
</li>
</ul>
</li>
<li><p>VPC Subnet</p>
<ul>
<li>There are 5 IP address reserved in each subnet by AWS, take CIDR block 10.0.0.0/24 as example<ul>
<li>10.0.0.0 Network address</li>
<li>10.0.0.1 Reserved by AWS for the VPC router</li>
<li>10.0.0.2 Reserved by AWS for DNS</li>
<li>10.0.0.3 Reserved by AWS for future use.</li>
<li>10.0.0.255 Network broadcast address, we do not support broadcast in a VPC, therefore we reserve this address.</li>
</ul>
</li>
</ul>
</li>
<li><p>NAT instances</p>
<ul>
<li>When creating a NAT instance, Disable Source/Destination Check on the Instance</li>
<li>NAT instance must be in a public subnet</li>
<li>Must have an elastic IP address to work</li>
<li>There must be a route out of the private subnet to the NAT instance, in order for this to work</li>
<li>The amount of traffic that NAT instances supports, depends on the instance size. If you are bottlenecking, increase the instance size</li>
<li>You can create high availability using Autoscaling Groups, multiple subnets in different AZ’s and a script to automate failover</li>
<li>Behind a Security Group.</li>
</ul>
</li>
<li><p>NAT Gateways</p>
<ul>
<li>Very new</li>
<li>Preferred by the enterprise</li>
<li>Scale automatically up to 10Gbps</li>
<li>No need to patch</li>
<li>Not associated with security groups</li>
<li>Automatically assigned a public ip address</li>
<li>Remember to update your route tables.</li>
<li>No need to disable Source/Destination Checks.</li>
</ul>
</li>
<li><p>NAT instances vs NAT Gateways<br>  <img src="/images/AWS/CSAP/CSAP_comparing_of_NAT_instance_an_NAT_gateways.png" alt="CSAP_comparing_of_NAT_instance_an_NAT_gateways"></p>
</li>
<li><p>Network ACL’s</p>
<ul>
<li>Your VPC automatically comes a default network ACL and by default it allows all outbound and inbound traffic.</li>
<li>You can create a custom network ACL. By default, each custom network ACL denies all inbound and outbound traffic until you add rules.</li>
<li>Each subnet in your VPC must be associated with a network ACL. If you don’t explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL.</li>
<li>You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed.</li>
<li>A network ACL contains a numbered list of rules that is evaluated in order, starting with the lowest numbered rule.</li>
<li>A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic.</li>
<li>Network ACLs are stateless responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa)</li>
<li>Block IP Addresses using network ACL’s not Security Groups</li>
</ul>
</li>
<li><p>Security Group vs Network ACL<br>  <img src="/images/AWS/CSAP/CSAP_comparison_of_SecurityGroups_and_NetworkACLs.png" alt="CSAP_comparison_of_SecurityGroups_and_NetworkACLs"></p>
</li>
<li><p>NAT vs Bastions</p>
<ul>
<li>A NAT is used to provide internet traffic to EC2 instances in private subnets</li>
<li>A Bastion is used to securely administer EC2 instance (using SSH or RDP) in private subnets. In Australia we call them jump boxes.</li>
</ul>
</li>
<li><p>Resilient Architecture</p>
<ul>
<li>If you want resiliency, always have 2 public subnets and 2 private subnets. Make sure each subnet is in different availability zones.</li>
<li>With ELB’s make sure they are in 2 public subnets in 2 different availability zones.</li>
<li>With Bastion hosts, put them behind an autoscaling group with a minimum size of 2. Use Route53 (either round robin or using a health check) to automatically fail over.</li>
<li>NAT instances are tricky to make resilient. You need 1 in each public subnet, each with their own public IP address, and you need to write a script to fail between the two. Instead where possible, use NAT gateways.</li>
</ul>
</li>
<li><p>VPC Flow Logs</p>
<ul>
<li>You can monitor network traffic within your custom VPC’s using VPC Flow Logs.</li>
</ul>
</li>
<li><p>VPC limit</p>
<ul>
<li>Currently you can create 200 subnets per VPC by default</li>
</ul>
</li>
</ul>
</li>
<li><p>VPC Peering</p>
<ul>
<li>What is VPC Peering<ul>
<li>VPC Peering is simply a connection between two VPCs that enables you to route traffic between them using private IP addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account within a <strong>single region</strong>.</li>
<li>AWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor a VPN connection, and <strong>does not</strong> rely on a separate pieces of physical hardware. There is no single point of failure for communication or a bandwidth bottleneck.</li>
</ul>
</li>
<li>Transitive Peering NOT Supported</li>
<li>VPC Peering Limitations<ul>
<li>Soft limit of <strong>50 VPC</strong> peers per VPC, can be increased to <strong>125</strong> by request.</li>
<li>You <strong>cannot</strong> create a VPC peering connection between VPCs that have matching or overlapping CIDR blocks.</li>
<li>You <strong>cannot</strong> create a VPC peering connection between VPCs in different regions.</li>
<li>VPC peering does <strong>not</strong> support transitive peering relationships.</li>
<li>A placement group can span peered VPCs; however, you will <strong>not</strong> get full bandwidth between instances in peered VPCs.</li>
<li>Private DNS values <strong>cannot</strong> be resolved between instances in peered VPCs.</li>
</ul>
</li>
<li>Steps To Setup VPC Peering<ul>
<li>Owner of the <strong>local</strong> VPC sends a request to the owner of the second VPC to peer.</li>
<li>Owner of the <strong>second</strong> VPC has to accept.</li>
<li>Owner of the <strong>local</strong> VPC adds a route to their route table allowing their subnets to route out to the peer VPC</li>
<li>Owner of the <strong>peer</strong> VPC adds a route to their route table allowing their subnets to route back to the other VPC.</li>
<li>Security Groups in both VPCs have to <strong>both</strong> allow traffic.</li>
</ul>
</li>
<li>Troubleshooting<ul>
<li><strong>Setting up the peer:</strong><ul>
<li>Are the VPCs in the same region?</li>
<li>If you can’t create a VPC peer, check to see if the CIDR blocks are overlapping.</li>
</ul>
</li>
<li><strong>After the peer is setup:</strong><ul>
<li>Check that the relevant security groups and NACLs are allowing traffic through.</li>
<li>Check that a route has been created in BOTH VPCs routing tables.</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Remember the <strong>steps</strong> to setting up a VPC peer.</li>
<li>Remember how to <strong>troubleshoot</strong> issues with VPC peering.</li>
</ul>
</li>
</ul>
</li>
<li><p>Direct Connect</p>
<ul>
<li><p>What is Direct Connect <a href="https://aws.amazon.com/directconnect/?nc1=h_ls" target="_blank" rel="external">https://aws.amazon.com/directconnect/?nc1=h_ls</a></p>
<ul>
<li><strong>AWS Direct Connect</strong> makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.</li>
<li><strong>AWS Direct Connect</strong> lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. </li>
<li>This allows you to use the <strong>same</strong> connection to access public resources such as objects stored in Amazon S3 using public IP address space, and private resources such as Amazon EC2 instances running within an Amazon Virtual Private Cloud (<strong>VPC</strong>) using private IP space, while maintaining network separation between the public and private environments. Virtual interfaces can be reconfigured at any time to meet your changing needs.</li>
</ul>
</li>
<li><p>Direct Connect Benefits</p>
<ul>
<li>Reduce <strong>costs</strong> when using large volumes of traffic</li>
<li>Increase <strong>reliability</strong></li>
<li>Increase <strong>bandwidth</strong></li>
</ul>
</li>
<li>How Is Direct Connect Different From A VPN?<ul>
<li><strong>VPN Connections</strong> can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.</li>
<li><strong>AWS Direct Connect</strong> does not involve the Internet. Instead, it uses dedicated, private network connections between your intranet and <strong>Amazon VPC</strong>.</li>
</ul>
</li>
<li>Direct Connect Connections<ul>
<li>Available In:<ul>
<li>10 Gbps</li>
<li>1 Gbps</li>
</ul>
</li>
<li>Sub 1Gbps can be purchased through <strong>AWS Direct Connect Partners</strong></li>
<li>Uses <strong>Ethernet VLAN trunking (802.1Q)</strong></li>
<li>This dedicated connection can be partitioned in to multiple virtual interfaces (<strong>VIFs</strong>)<ul>
<li>Allows <strong>public</strong> connections to <strong>EC2</strong> or <strong>S3</strong> using public IP addresses</li>
<li>Allows <strong>private</strong> connections to <strong>VPC</strong> using internal IP addresses<br><img src="/images/AWS/CSAP/CSAP_direct_connect_diagram_1.png" alt="CSAP_direct_connect_diagram_1"><br><img src="/images/AWS/CSAP/CSAP_direct_connect_diagram_2.png" alt="CSAP_direct_connect_diagram_2"></li>
</ul>
</li>
<li>Use BGP to Fail Over Automatically from Direct Connect to Site to Site VPN<br><img src="/images/AWS/CSAP/CSAP_direct_connect_use_bgp_to_failover_automatically_from_direct_connect_to_site_to_site_VPN.png" alt="CSAP_direct_connect_use_bgp_to_failover_automatically_from_direct_connect_to_site_to_site_VPN"></li>
</ul>
</li>
<li>CGW vs. VPG<ul>
<li>When using a <strong>VPN</strong> to connect to a <strong>VPC</strong>, you need an anchor on each side of that connection. A <strong>Customer Gateway</strong> is the anchor on your side of that connection. It can be a physical or software appliance.</li>
<li>The anchor on the AWS side of the VPN connection is called a <strong>Virtual Private Gateway</strong>.</li>
</ul>
</li>
<li>Key Points To Remember<ul>
<li>If you are accessing public services using HTTPS endpoints (think DynamoDB, S3) then use <strong>public</strong> VIFs.</li>
<li>If you are accessing VPCs using private IP address ranges, then use <strong>private</strong> VIFs.</li>
<li>In the US, you only need <strong>1</strong> direct connect connection to connect in to all 4 US regions. Data transferred between regions goes over AWS’s internal lines, not the public internet.</li>
<li>Direct connect itself is not redundant. You can <strong>add</strong> redundancy by having <strong>2</strong> connections(<strong>2 routes, 2 direct connects</strong>), or by having a site-to-site VPN in place.</li>
<li>Layer 2 connections are <strong>not</strong> supported.</li>
<li>Know the difference between a <strong>Customer Gateway CGW</strong> (Customer side) and <strong>Virtual Private Gateway</strong> (AWS side.)</li>
</ul>
</li>
</ul>
</li>
<li><p>HPC &amp; Enhanced Networking</p>
<ul>
<li>HPC - Guide Lines<br>High performance compute is used by many different industries, such as the pharmaceutical or automotive industries. <strong>HPC</strong> typically involves:<ul>
<li>Batch processing with large and compute intensive workloads</li>
<li>Demands High Performance CPU, Network &amp; Storage</li>
<li>Usually Jumbo Frames are required</li>
</ul>
</li>
<li>What Are Jumbo Frames?<ul>
<li><strong>Jumbo Frames</strong> are ethernet frames with more than <strong>1500 bytes</strong> of payload. A Jumbo Frame can carry up to <strong>9000 bytes</strong> of payload.</li>
<li>Shared file systems such as <strong>Lustre</strong> and <strong>NFS</strong> use Jumbo Frames frequently. HPC applications use a lot of disk I/O and need access to a shared file system, this makes Jumbo Frames critical.</li>
</ul>
</li>
<li>SR-IOV<ul>
<li>The use of Jumbo Frames is supported on AWS through enhanced networking. Enhanced networking is available using single-root I/O virtualization (<strong>SR-IOV</strong>) on supported instance types:<ul>
<li>C3</li>
<li>C4</li>
<li>D2</li>
<li>I2</li>
<li>M4</li>
<li>R3</li>
</ul>
</li>
<li>Must be on HVM VMs, not PV</li>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html" target="_blank" rel="external">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html</a></li>
</ul>
</li>
<li>Placement Groups<br>  A placement group is a logical grouping of instances within a single <strong>Availability Zone</strong>. Using placement groups enables applications to participate in a low-latency, 10Gbps network. Placement groups are <strong>recommended</strong> for applications that benefit from low network latency, high network throughput, or both.<br>  To provide the lowest latency and the highest packet-per-second network performance for your placement group, choose an instance type that supports <strong>enhanced networking</strong>.<ul>
<li>Don’t span availability zones: <strong>1</strong> placement group = <strong>1 AZ</strong>.</li>
<li>Placement groups can span subnets, but the subnets must be in the <strong>same</strong> AZ.</li>
<li>Only certain instance types (<strong>enhanced networking instances</strong>) may be launched in to a placement group:<ul>
<li>C3</li>
<li>C4</li>
<li>D2</li>
<li>I2</li>
<li>M4</li>
<li>R3</li>
</ul>
</li>
<li>Existing instances <strong>cannot</strong> be moved in to a placement group.</li>
<li>Although launching multiple instance types into a placement group is possible, this <strong>reduces</strong> the likelihood that the required capacity will be available for your launch to succeed.</li>
<li>There may not be <strong>sufficient</strong> capacity to add extra instances later on. It is best practice to size the placement group for the peak load and launch all instances at the same time.</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Enhanced Networking is available using single root I/O virtualization (<strong>SR-IOV</strong>). This requires <strong>HVM virtualization</strong>.</li>
<li>A placement group <strong>cannot</strong> span availability zones, but it <strong>can</strong> span subnets, provided they are in the <strong>same</strong> VPC.</li>
<li>You <strong>cannot</strong> move existing instances into a placement group.</li>
<li>Try to use <strong>homogenous instance types</strong> when launching placement groups.</li>
<li>Provision your placement group for peak load. You may <strong>not</strong> be able to add instances later.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>ELB<ul>
<li>What is ELB<ul>
<li><strong>Elastic Load Balancing</strong> automatically distributes incoming application traffic across multiple Amazon EC2 instances in the cloud.</li>
<li>It enables you to achieve greater levels of <strong>fault tolerance</strong> in your applications, seamlessly providing the required amount of load balancing capacity needed to distribute application traffic.</li>
</ul>
</li>
<li>ELB Tips<ul>
<li>2 Types of Load Balancers:<ul>
<li>Classic Load Balancer</li>
<li>Application Load Balancer</li>
</ul>
</li>
</ul>
</li>
<li>Classic Load Balancer<ul>
<li><strong>High Availability</strong><ul>
<li>You can deploy Classic Load Balancers in either a single or multiple availability zones</li>
</ul>
</li>
<li><strong>Health Checks</strong><ul>
<li>You can detect the health of Amazon EC2 instances. When a health check detects an unhealthy EC2 instance, it no longer routes traffic to that instance and it will spread the load across the remaining healthy instances.</li>
</ul>
</li>
<li><strong>Security Features</strong><ul>
<li>When using a VPC, you can create and manage security groups associated with Classic Load Balancers to provide additional networking and security options. You can also create a Classic Load Balancer without a public IP addresses to serve as an internal (non-internet-facing) load balancer.</li>
</ul>
</li>
<li><strong>SSL Offloading</strong><ul>
<li>You can offload your SSL connections to your classic load balancer. Flexible cipher support allows you to control the ciphers and protocols the load balancer presents to clients.</li>
</ul>
</li>
<li><strong>Sticky Sessions</strong><ul>
<li>Classic Load Balancers support the ability to stick user sessions to specific EC2 instances using cookies. Traffic will be routed to the same instances as the user continues to access your application.</li>
</ul>
</li>
<li><strong>IPv6</strong><ul>
<li>Classic Load Balancers support the use of both the Internet Protocol version 4 and 6 (IPv4 an IPv6)</li>
</ul>
</li>
<li><strong>Layer 4 or Layer 7 Load Balancing</strong><ul>
<li>You can load balance HTTP/HTTPS applications and use layer 7-specific features, such as X-Forwarded and sticky sessions. You can also use strict layer 4 load balancing for applications that rely purely on the TCP protocol.</li>
</ul>
</li>
<li><strong>Operational Monitoring</strong><ul>
<li>Classic Load Balancer metrics, such as request count and request latency, are reported by Amazon CloudWatch.</li>
</ul>
</li>
<li><strong>Logging</strong><ul>
<li>You can use the Access Logs feature to record all requests sent to your load balancer, and store the logs in Amazon S3 for later analysis. You can use these logs to diagnose application failures and for analyzing web traffic. You can also use AWS CloudTrail to record classic load balancer API calls for your account and deliver log files to S3. The API call history enables you to perform security analysis, resource change tracking, and compliance auditing.</li>
</ul>
</li>
</ul>
</li>
<li>Application Load Balancer<ul>
<li>Content-Based Routing<ul>
<li>If your application is composed of several individual services, an Application Load Balancer can route a request to a service based on the content of the request.</li>
</ul>
</li>
<li>Host-Based Routing<ul>
<li>You can route a client request based on Host field of the HTTP header allowing you to route to multiple domains from the same load balancer.</li>
</ul>
</li>
<li>Path-based Routing<ul>
<li>You can route a client request based on the URL path of the HTTP header.</li>
<li>e.g. <a href="https://domain.com/uploads" target="_blank" rel="external">https://domain.com/uploads</a> vs <a href="https://domain.com/downloads" target="_blank" rel="external">https://domain.com/downloads</a></li>
</ul>
</li>
<li>Containerized Application Support (integrates with ECS)</li>
<li>HTTP/2 Support</li>
<li>WebSockets Support</li>
<li>Native IPv6 Support</li>
<li>Sticky Sessions</li>
<li>Health Checks</li>
<li>High Availability</li>
<li>Security Features</li>
<li>Integrates with WAF</li>
<li>Layer-7 Load Balancing</li>
<li>HTTPS Support</li>
<li>Operational Monitoring</li>
<li>Logging</li>
<li>Delete Protection</li>
<li>Request Tracing - custom identifier “X-Amzn-Trace-Id” HTTP header on all requests</li>
</ul>
</li>
<li>Classic Load Balancer Tips<ul>
<li>Know which ports ELBs support<ul>
<li><strong>[EC2-VPC]</strong> 1-65535</li>
<li><strong>[EC2-Classic]</strong> 25,80,443, 465, 587, 1024-65535</li>
</ul>
</li>
<li>You <strong>cannot</strong> assign an Elastic IP address to an Elastic Load Balancer.</li>
<li><strong>IPv4</strong> &amp; <strong>IPv6</strong> is supported.</li>
<li>You can load balance to the “<strong>Zone Apex</strong>“ of your domain name.</li>
</ul>
</li>
<li>ELB Tips<ul>
<li>You can get a <strong>history</strong> of Elastic Load Balancing API calls made on my account for security analysis and operational troubleshooting purposes by turning on CloudTrail.</li>
<li>If you have <strong>multiple</strong> SSL certificates, you should use multiple Elastic Load Balancers, unless you have a wildcard certificate.</li>
</ul>
</li>
</ul>
</li>
<li><p>Scaling NATs</p>
<ul>
<li>NATs<br>  Instances that you launch into a private subnet in a virtual private cloud (<strong>VPC</strong>) can’t communicate with the Internet. You can <strong>optionally</strong> use a network address translation (<strong>NAT</strong>) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound traffic to the Internet, but prevent the instances from receiving inbound traffic initiated by someone on the Internet.</li>
<li>NAT Bottlenecks<br>  Bottlenecks can occur when you have a single NAT that has too much traffic passing through it. There are several approaches to reducing bottlenecks:<ul>
<li><strong>Scale Up</strong><ul>
<li>Increase your instance size</li>
<li>Choose an instance family which supports Enhanced Networking</li>
</ul>
</li>
<li><strong>Scale Out</strong><ul>
<li>Add an additional NAT &amp; subnet and migrate half your workloads to the new subnet.</li>
</ul>
</li>
</ul>
</li>
<li>HA For NATs<br>  You can create <strong>HA for NAT</strong> instances, but each subnet can <strong>only</strong> route to 1 NAT at a time. You can fail over a subnet to another NAT.<br>  Good blog on how to do it here: <a href="https://aws.amazon.com/cn/articles/high-availability-for-amazon-vpc-nat-instances-an-example/" target="_blank" rel="external">https://aws.amazon.com/cn/articles/high-availability-for-amazon-vpc-nat-instances-an-example/</a></li>
<li>Blog On Scaling NATs<br>  Great Blog by Matthew Barlocker: <a href="http://nineofclouds.blogspot.com/2013/01/vpc-migration-nats-bandwidth-bottleneck.html" target="_blank" rel="external">http://nineofclouds.blogspot.com/2013/01/vpc-migration-nats-bandwidth-bottleneck.html</a></li>
</ul>
</li>
<li><p>Domain 4 Summary</p>
<ul>
<li>Domain 4<ul>
<li>Domain 4.0: Network Design for a complex large scale deployment</li>
<li>4.1 Demonstrate ability to design and implement networking features of AWS</li>
<li>4.2 Demonstrate ability to design and implement connectivity features of AWS</li>
</ul>
</li>
<li>Direct Connect<ul>
<li>If you are accessing public services using HTTPS endpoints (think DynamoDB, S3) then use <strong>public</strong> VIFs.</li>
<li>If you are accessing VPCs using private IP address ranges, then use <strong>private</strong> VIFs.</li>
<li>In the US, you only need <strong>1</strong> direct connect connection to connect in to all 4 US regions. Data transferred between regions goes over AWS’s internal lines, not the public internet.</li>
<li>Direct connect itself is not redundant. You can <strong>add</strong> redundancy by having <strong>2</strong> connections(<strong>2 routes, 2 direct connects</strong>), or by having a site-to-site VPN in place.</li>
<li>Layer 2 connections are <strong>not</strong> supported.</li>
<li>Know the difference between a <strong>Customer Gateway CGW</strong> (Customer side) and <strong>Virtual Private Gateway</strong> (AWS side.)</li>
</ul>
</li>
<li>ELB’s<ul>
<li>Know which ports ELBs support<ul>
<li><strong>[EC2-VPC]</strong> 1-65535</li>
<li><strong>[EC2-Classic]</strong> 25,80,443, 465, 587, 1024-65535</li>
</ul>
</li>
<li>You <strong>cannot</strong> assign an Elastic IP address to an Elastic Load Balancer.</li>
<li><strong>IPv4</strong> &amp; <strong>IPv6</strong> is supported (although VPC’s do <strong>not</strong> support IPv6 at this time)</li>
<li>You can load balance to the “<strong>Zone Apex</strong>“ of your domain name.</li>
<li>You can get a <strong>history</strong> of Elastic Load Balancing API calls made on my account for security analysis and operational troubleshooting purposes by turning on CloudTrail.</li>
<li>If you have <strong>multiple</strong> SSL certificates, you should use multiple Elastic Load Balancers, unless you have a wildcard certificate.</li>
</ul>
</li>
<li>HPC &amp; Enhanced Networking<ul>
<li>Enhanced Networking is available using single root I/O virtualization (<strong>SR-IOV</strong>). This requires <strong>HVM virtualization</strong>.</li>
<li>A placement group <strong>cannot</strong> span availability zones, but it <strong>can</strong> span subnets, provided they are in the <strong>same</strong> VPC.</li>
<li>You <strong>cannot</strong> move existing instances into a placement group.</li>
<li>Try to use <strong>homogenous instance types</strong> when launching placement groups.</li>
<li>Provision your placement group for peak load. You may <strong>not</strong> be able to add instances later.</li>
</ul>
</li>
<li>Scaling NATs<br>  Bottlenecks can occur when you have a single NAT that has too much traffic passing through it. There are several approaches to reducing bottlenecks:<ul>
<li><strong>Scale Up</strong><ul>
<li>Increase your instance size</li>
<li>Choose an instance family which supports Enhanced Networking</li>
</ul>
</li>
<li><strong>Scale Out</strong><ul>
<li>Add an additional NAT &amp; subnet and migrate half your workloads to the new subnet.</li>
</ul>
</li>
</ul>
</li>
<li>VPC Peering<ul>
<li>Create Step<ul>
<li>Owner of the <strong>local</strong> VPC sends a request to the owner of the second VPC to peer.</li>
<li>Owner of the <strong>second</strong> VPC has to accept.</li>
<li>Owner of the <strong>local</strong> VPC adds a route to their route table allowing their subnets to route out to the peer VPC</li>
<li>Owner of the <strong>peer</strong> VPC adds a route to their route table allowing their subnets to route back to the other VPC.</li>
<li>Security Groups in both VPCs have to <strong>both</strong> allow traffic.</li>
</ul>
</li>
<li>Troubleshooting<ul>
<li><strong>Setting up the peer:</strong><ul>
<li>Are the VPCs in the same region?</li>
<li>If you can’t create a VPC peer, check to see if the CIDR blocks are overlapping.</li>
</ul>
</li>
<li><strong>After the peer is setup:</strong><ul>
<li>Check that the relevant security groups and NACLs are allowing traffic through.</li>
<li>Check that a route has been created in BOTH VPCs routing tables.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Domain-5-Data-Storage-15"><a href="#Domain-5-Data-Storage-15" class="headerlink" title="Domain 5 Data Storage - 15%"></a>Domain 5 Data Storage - 15%</h3><ul>
<li>Optimizing S3<ul>
<li>Optimizing For Puts<ul>
<li><strong>Two different types of network scenarios:</strong><ul>
<li>Strong internet connection, with fast speeds (such as fibre.)</li>
<li>Less reliable internet connection with inconsistent network performance.</li>
</ul>
</li>
<li><strong>How can we optimize?</strong><ul>
<li>For strong networks, we want to take advantage of the network itself and make the network the bottleneck.</li>
<li>For weaker networks, we want to prevent large files having to restart their uploads.</li>
</ul>
</li>
</ul>
</li>
<li>Parallelizing For PUTs<ul>
<li>By <strong>dividing</strong> your files in to small parts and then uploading those parts simultaneously, you are “<strong>parallelizing</strong>“ your puts.</li>
<li>If 1 part fails, it can be <strong>restarted</strong> an there are fewer large restarts on an unreliable network.</li>
<li>Moves the bottleneck to the network itself, which can help <strong>increase</strong> the aggregate throughput.</li>
<li><strong>25-50 MB</strong> on high bandwidth networks, and around <strong>10 MB</strong> on mobile networks.</li>
<li>Need to find a <strong>balance</strong>. Too many increases the connection overhead, too few doesn’t give you any resiliency.</li>
</ul>
</li>
<li>Optimizing For GETs<ul>
<li><strong>Use CloudFront:</strong><ul>
<li>Multiple Endpoints Globally</li>
<li>Low Latency</li>
<li>High Transfer Speeds Available</li>
<li>Caches Objects from S3</li>
<li><strong>Two varieties:</strong><ul>
<li>RTMP</li>
<li>Web Distribution</li>
</ul>
</li>
</ul>
</li>
<li>Need to use <strong>range-based GETs</strong> to get multithreaded performance.</li>
<li>Using the Range HTTP header in a GET request, you can <strong>retrieve</strong> a specific range of bytes in an object stored in Amazon S3.</li>
<li>Allows you to send <strong>multiple</strong> GETs at once, hence parallelizing for GETs.</li>
<li><strong>Compensates</strong> for unreliable network performance.</li>
<li><strong>Maximizes</strong> bandwidth throughput.</li>
</ul>
</li>
<li>S3 is Lexicographical<ul>
<li>Introduce some randomness in to the name.</li>
<li>The more random, the better the performance</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Use <strong>parallelization</strong> to <strong>optimize</strong> both GETs &amp; PUTs</li>
<li>S3 stores data in <strong>Lexicographical</strong> order.</li>
<li>Introduce <strong>randomness</strong> to maximize performance.</li>
</ul>
</li>
</ul>
</li>
<li>S3 Best Practices<ul>
<li>Versioning - Recap<ul>
<li>Allows you to <strong>protect</strong> your files from being accidentally deleted or overwritten.<ul>
<li><strong>No</strong> penalty on performance.</li>
</ul>
</li>
<li>Every time you upload a new file, a new version is created <strong>automatically</strong>.</li>
<li>Makes it easy to <strong>retrieve</strong> deleted versions.</li>
<li>Makes it easy to <strong>roll-back</strong> to previous versions.</li>
<li>Once you turn versioning on, you <strong>cannot</strong> turn it off – you can only <strong>suspend</strong> it.</li>
<li>There are <strong>further steps</strong> you can take to secure your S3 environment.</li>
</ul>
</li>
<li>Securing S3<ul>
<li>You can use <strong>bucket policier</strong> to restrict deletes.</li>
<li>You can also use <strong>MFA Delete</strong>:<ul>
<li>You have to use two-factor authentication to <strong>delete</strong> objects permanently.</li>
<li>You have to use two-factor authentication to <strong>change</strong> the state of versioning within your bucket.</li>
</ul>
</li>
<li>Requires both your <strong>security credentials</strong>, as well as a <strong>code</strong> from an approved authentication device ( such as <strong>Google Authenticator</strong>).</li>
</ul>
</li>
<li>Securing S3 - Use Another Account<ul>
<li>Versioning will protect you on an <strong>individual</strong> file level.</li>
<li>Does <strong>not</strong> protect you against deleting a bucket.</li>
<li>Bucket deletion could be either <strong>malicious</strong> or <strong>accidental</strong>.</li>
<li><strong>Back</strong> your bucket up to another bucket owned by another account using cross account access.</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>You can <strong>secure S3</strong> by:<ul>
<li>Using Bucket Policies</li>
<li>Using MFA Delete</li>
<li>Backing Your Bucket Up To Another S3 Bucket Owned By A Separate Account</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Database Design<ul>
<li>Recommended White Paper<ul>
<li>AWS Storage Options</li>
<li><a href="https://d1.awsstatic.com/whitepapers/Storage/AWS%20Storage%20Services%20Whitepaper-v9.pdf" target="_blank" rel="external">https://d1.awsstatic.com/whitepapers/Storage/AWS%20Storage%20Services%20Whitepaper-v9.pdf</a></li>
<li>Take particular note of the Anti-Patterns for the different technologies.</li>
</ul>
</li>
<li>Multi-AZ vs Read Replicas<ul>
<li><strong>Multi-AZ</strong><ul>
<li>Used for DR only, not for scaling</li>
<li>Is synchronous replication</li>
</ul>
</li>
<li><strong>Read Replicas</strong><ul>
<li>Used for Scaling Out, not DR</li>
<li>Is Asynchronous replication</li>
</ul>
</li>
</ul>
</li>
<li>RDS Use Cases<ul>
<li><strong>Amazon RDS</strong> is ideal for existing applications that rely on MySQL, Oracle, SQL, PostgreSQL, MariaDB and Aurora.</li>
<li>Since Amazon RDS offers full compatibility and direct access to native database engines, most code, libraries, and tools designed for these databases should work <strong>unmodified</strong> with Amazon RDS.</li>
<li>Amazon RDS is also <strong>optimal</strong> for new applications with structured data that requires more sophisticated querying and joining capabilities than those provided by Amazon’s NoSQL database offering, Amazon DynamoDB.</li>
</ul>
</li>
<li>ACID? Think RDS<ul>
<li>The <strong>ACID</strong> concept is described in <strong>ISO/IEC 10026-1:1998 Section 4</strong>.</li>
<li><strong>A</strong>tomicity. In a transaction involving two or more discrete pieces of information, either all of the pieces are committed or none are.</li>
<li><strong>C</strong>onsistency. A transaction either creates a new and valid state of data, or if any failure occurs, returns all data to its state before the transaction was started.</li>
<li><strong>I</strong>solation. A transaction in process and not yet committed must remain isolated from any other transaction.</li>
<li><strong>D</strong>urability. Committed data is saved by the system such that, even in the event of a failure and system restart, the data is available in its correct state.</li>
</ul>
</li>
<li>Where Not To Use RDS<br>  <strong>Amazon RDS</strong> is a great solution for cloud-based, fully-managed relational databases; but in a number of scenarios, it may not be the right choice.<ul>
<li><strong>Index and query-focused data</strong> - Many cloud-based solutions don’t require advanced features found in a relational database, such as joins an complex transactions. If your application is more oriented toward indexing and querying data, you may find Amazon DynamoDB to be more appropriate for your needs.</li>
<li><strong>Numerous BLOBs</strong> - While all of the database engines provided by Amazon RDS support binary large objects (BLOBs), if your application makes heavy use of them (audio files, videos, images and so on), you may fin Amazon S3 to be a better choice.</li>
<li><strong>Automated scalability</strong> - Amazon RDS provides pushbutton scaling. If you need fully automated scaling, Amazon DynamoDB may be a better choice.</li>
<li><strong>Other database platforms</strong> - At this time, Amazon RDS provides MySQL, Oracle, SQL Server, PostgreSQL, MariaDB and Aurora databases. If you need another database platform (such as IBM DB2, Informix, or Sybase), you need to deploy a self-managed database on an Amazon EC2 instance by using a relational database AMI, or by installing database software on an Amazon EC2 instance.</li>
<li><strong>Complete control</strong> - If your application requires complete, OS-level control of the database server, with full root or admin login privileges (for example, to install additional third-party software on the same server), a self managed database on Amazon EC2 may be a better match.</li>
</ul>
</li>
<li>DynamoDB Use Cases<br>  <strong>Amazon DynamoDB</strong> is ideal for existing or new applications that need a flexible NoSQL database with low read and write latencies, and the ability to scale storage an throughput up or down as needed without code changes or downtime.<br>  <strong>Common use cases</strong> include: mobile apps, gaming, digital ad serving, live voting and audience interaction for live events, sensor networks, log ingestion, access control for web-based content, metadata storage for Amazon S3 objects, e-commerce shopping carts, and web session management.<br>  Many of these use cases require a highly available and scalable database because downtime or performance degradation has an immediate negative impact on an organization’s business.<br>  <strong>Need to automatically scale your database, think DynamoDB.</strong></li>
<li>Where Not To Use DynamoDB<ul>
<li><strong>Prewritten application tied to a traditional relational database</strong> - If you are attempting to port an existing application to the AWS cloud, and need to continue using a relational database.</li>
<li><strong>Joins and/or complex transactions</strong> - While many solutions are able to leverage Amazon DynamoDB to support their users, it’s possible that your application may require joins, complex transactions, and other relational infrastructure provided by traditional database platforms. If this is the case, you may want to explore Amazon RDS or Amazon EC2 with a self-managed database.</li>
<li><strong>Blob Data</strong> - If you plan on storing large (greater than 64 KB) BLOB data, such as digital video, images or music, you’ll want to consider Amazon S3. However, Amazon DynamoDB still has a role to play in this scenario, for keeping track of metadata (e.g. item name, size, date created, owner, location and so on) about your binary objects.</li>
<li><strong>Large data with low I/O rate</strong> - Amazon DynamoDB uses SSD drives and is optimized for workloads with a high I/O rate per GB stored. If you plan to store very large amounts of data that are infrequently accessed, other storage options, such as Amazon S3, may be a better choice.</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Multi-AZ = <strong>Synchronous</strong> Replication</li>
<li>Read Replica = <strong>Asynchronous</strong> Replication</li>
<li><strong>RDS</strong> is optimal for new applications with structured data that requires more sophisticated querying and joining capabilities</li>
<li><strong>ACID</strong> = <strong>RDS</strong></li>
<li>Don’t use RDS for:<ul>
<li>Index and query-focused data (<strong>DynamoDB</strong>)</li>
<li>Numerous BLOBs (<strong>S3</strong>)</li>
<li>Automated scalability (<strong>DynamoDB</strong>)</li>
<li>Other database platforms such as IBM DB2, Informix, or Sybase (<strong>EC2</strong>)</li>
<li>Complete Control (<strong>EC2</strong>)</li>
</ul>
</li>
<li><strong>Don’t use DynamoDB for:</strong><ul>
<li>Prewritten application tied to a traditional relational database (<strong>RDS</strong>)</li>
<li>Joins and/or complex transactions (<strong>RDS</strong>)</li>
<li>Blob Data (<strong>S3</strong>)</li>
<li>Large data with low I/O rate (<strong>S3</strong>)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Domain 5<ul>
<li>**Domain 5.0: Data Storage for a complex large scale deployment<ul>
<li><strong>5.1</strong> Demonstrate ability to make architectural trade off decisions involving storage options</li>
<li><strong>5.2</strong> Demonstrate ability to make architectural trade off decisions involving database options</li>
<li><strong>5.3</strong> Demonstrate ability to implement the most appropriate data storage architecture</li>
<li><strong>5.4</strong> Determine use of synchronous versus asynchronous replication</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Read the <strong>White Paper</strong>!</li>
<li>AWS Storage Options</li>
<li><a href="https://d0.awsstatic.com/whitepapers/Storage/aws-storage-options.pdf" target="_blank" rel="external">https://d0.awsstatic.com/whitepapers/Storage/aws-storage-options.pdf</a></li>
<li>Take particular note of the Anti-Patterns for the different technologies.</li>
</ul>
</li>
<li>Exam Tips - S3 Performance<ul>
<li>Use <strong>parallelization</strong> to optimize both GETs &amp; PUTs.</li>
<li>S3 stores data in <strong>Lexicographical</strong> order.</li>
<li>Introduce <strong>randomness</strong> to maximize performance.</li>
</ul>
</li>
<li>Exam Tips - Securing S3<ul>
<li><strong>You can secure S3 by:</strong><ul>
<li>Using Bucket Policies</li>
<li>Using MFA Delete</li>
<li>Backing Your Bucket Up To Another S3 Bucket Owned By A Separate Account</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>Multi-AZ = <strong>Synchronous</strong> Replication</li>
<li>Read Replica = <strong>Asynchronous</strong> Replication</li>
<li><strong>RDS</strong> is optimal for new applications with structured data that requires more sophisticated querying and joining capabilities</li>
<li><strong>ACID</strong> = <strong>RDS</strong></li>
<li>Don’t use RDS for:<ul>
<li>Index and query-focused data (<strong>DynamoDB</strong>)</li>
<li>Numerous BLOBs (<strong>S3</strong>)</li>
<li>Automated scalability (<strong>DynamoDB</strong>)</li>
<li>Other database platforms such as IBM DB2, Informix, or Sybase (<strong>EC2</strong>)</li>
<li>Complete Control (<strong>EC2</strong>)</li>
</ul>
</li>
<li><strong>Don’t use DynamoDB for:</strong><ul>
<li>Prewritten application tied to a traditional relational database (<strong>RDS</strong>)</li>
<li>Joins and/or complex transactions (<strong>RDS</strong>)</li>
<li>Blob Data (<strong>S3</strong>)</li>
<li>Large data with low I/O rate (<strong>S3</strong>)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Domain-6-Security-20"><a href="#Domain-6-Security-20" class="headerlink" title="Domain 6 Security - 20%"></a>Domain 6 Security - 20%</h3><ul>
<li>AWS Directory Services<ul>
<li>AWS Directory Services<br>  <strong>AWS Directory Service</strong> is a managed service that allows you to connect your AWS resources with an existing on-premises Microsoft Active Directory or to set up a new, stand-alone directory in the AWS cloud.<ul>
<li><strong>Comes in 2 flavors:</strong><ul>
<li>AD Connector</li>
<li>Simple AD</li>
</ul>
</li>
</ul>
</li>
<li>AD Connector<br>  <strong>AD Connector</strong> enables you to easily connect your Microsoft Active Directory to the AWS cloud, without requiring complex directory synchronization technologies or the cost an complexity of hosting a federation infrastructure.<br>  Once set up, your end users and ID administrators can use their existing corporate credentials to log on to AWS applications such as Amazon Workspaces, Amazon WorkDocs or Amazon WorkMail and to manage AWS resources (e.g. Amazon EC2 instances or S3 buckets) via AWS Identity and Access Management (IAM) role-based access to the AWS Management Console.<br>  Your existing security policies, such as password expiration, password history and account lockouts can be <strong>enforced consistently</strong> – whether users to IT administrators are accessing resources in your on-premises infrastructure or the AWS cloud.<br>  You can also use <strong>AD Connector</strong> to enable multi-factor authentication by integrating with your existing <strong>RADIUS-based MFA infrastructure</strong> to provide an additional layer of security when users access AWS applications.</li>
<li>Simple AD<br>  <strong>Simple AD</strong> is a managed directory which is powered by <strong>Samba 4 Active Directory Compatible Server</strong>. It supports commonly used features such as user accounts, group memberships, domain-joining Amazon EC2 instances running Linux and Microsoft Windows, as well as Kerberos based single sign-on (<strong>SSO</strong>), and Group Policies.</li>
<li>Exam Tips<ul>
<li><strong>2 Types of Directory Services:</strong><ul>
<li>**AD Connector (Used for existing AD Deployments)<ul>
<li>Great way to connect in to existing AD environments.</li>
<li>Supports MFA</li>
</ul>
</li>
<li>**Simple AD (Used for new AD Deployments)<ul>
<li>MFA Not Supported</li>
<li>Cannot add additional AD servers</li>
<li>No Trust Relationships</li>
<li>Cannot Transfer FSMO Roles</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Security Token Service(STS)</p>
<ul>
<li>Security Token Service<ul>
<li>Grants users <strong>limited</strong> and <strong>temporary</strong> access to AWS resources. Users can come from three sources:</li>
<li><strong>Federation (typically Active Directory)</strong><ul>
<li>Uses Security Assertion Markup Language (SAML)</li>
<li>Grants temporary access based off the users Active Directory credentials. Does not need to be a user in IAM</li>
<li>Single sign on allows users to log in to AWS console without assigning IAM credentials</li>
<li>Federation with Mobile Apps<ul>
<li>Use Facebook/Amazon/Google or other OpenID providers to log in</li>
</ul>
</li>
<li>Cross Account Access<ul>
<li>Let’s users from one AWS account access resources in another</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Understanding The Key Terms<ul>
<li><strong>Federation</strong><ul>
<li>Combining or joining a list of users in one domain (such as IAM) with a list of users in another domain (such as Active Directory, Facebook etc.)</li>
</ul>
</li>
<li><strong>Identity Broker</strong><ul>
<li>A service that allows you to take an identity from point A and join it (federate it) to point B</li>
</ul>
</li>
<li><strong>Identity Store</strong><ul>
<li>Services like Active Directory, Facebook, Google etc.</li>
</ul>
</li>
<li><strong>Identities</strong><ul>
<li>A user of a service like Facebook etc.</li>
</ul>
</li>
</ul>
</li>
<li>Scenario<ul>
<li>AWS blog [<a href="https://aws.amazon.com/cn/blogs/aws/aws-identity-and-access-management-now-with-identity-federation/" target="_blank" rel="external">AWS Identity and Access Management – Now With Identity Federation</a>]</li>
<li>You are hosting a company website on some EC2 web servers in your VPC. Users of the website must log in to the site which then authenticates against the companies active directory servers which are based on site at the companies head quarters. Your VPC is connected to your company HQ via a secure IPSEC VPN. Once logged in the user can only have access to their own S3 bucket. <strong>How do you set this up?</strong><br>  <img src="/images/AWS/Sysops/sts_scenario.jpg" alt="sts_scenario"></li>
<li>Steps of scenario<ul>
<li>Employee enters their username and password</li>
<li>The application calls an Identity Broker. The broker captures the username and password.</li>
<li>The Identity Broker uses the organization’s LDAP directory to validate the employee’s identity.</li>
<li>The Identity Broker calls the new GetFederationToken function using IAM credentials. The call must include an IAM policy and a duration (1 to 36 hours), along with a policy that specifies the permissions to be granted to the temporary security credentials.</li>
<li>The Security Token Service confirms that the policy of the IAM user making the call to GetFederationToken gives permission to create new tokens and then returns four values to the application: An access key, a secret access key, a token, and a duration (the token’s lifetime).</li>
<li>The Identity Broker returns the temporary security credentials to the reporting application.</li>
<li>The data storage application uses the temporary security credentials (including the token) to make requests to Amazon S3.</li>
<li>Amazon S3 uses IAM to verify that the credentials allow the requested operation on the given S3 bucket and key</li>
<li>IAM provides S3 with the go-ahead to perform the requested operation.</li>
</ul>
</li>
<li>In the Exam<ul>
<li>Develop an <strong>Identity Broker</strong> to communicate with <strong>LDAP</strong> and <strong>AWS STS</strong></li>
<li><strong>Identity Broker</strong> always authenticates with <strong>LDAP</strong> first, <strong>Then</strong> with <strong>AWS STS</strong></li>
<li>Application then gets <strong>temporary access</strong> to AWS resources</li>
</ul>
</li>
</ul>
</li>
<li>Scenario 2<ul>
<li>steps of scenario<ul>
<li>Develop an Identity Broker to communicate with LDAP and AWS STS</li>
<li>Identity Broker always authenticates with LDAP first, gets an IAM Role associate with a user</li>
<li>Application then authenticates with STS and assumes that IAM Role</li>
<li>Application uses that IAM role to interact with S3</li>
</ul>
</li>
<li>In the Exam<ul>
<li>Develop an Identity Broker to communicate with LDAP and AWS STS</li>
<li>Identity Broker always authenticates with LDAP first, Then with AWS STS</li>
<li>Application then gets temporary access to AWS resources</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Monitoring In The Cloud</p>
<ul>
<li>Monitoring<ul>
<li><strong>It’s important to know the difference:</strong><ul>
<li>CloudWatch vs CloudTrail</li>
</ul>
</li>
<li><strong>Use cases as to where to store your logs</strong><ul>
<li>EBS volumes attached to EC2 instances?</li>
<li>S3?</li>
<li>CloudWatch?</li>
</ul>
</li>
</ul>
</li>
<li>CloudWatch vs CloudTrail<ul>
<li>CloudTrail<br>  <strong>CloudTrail</strong> enables you to retrieve a history of <strong>API calls</strong> and other events for all of the regions in your account. This includes calls and events made by the AWS Management Console and command line tools, by any of the AWS SDKs, or by other AWS services.<br>  CloudTrail is used for <strong>auditing</strong> and collecting a <strong>history of API calls</strong> made on your environment. It is not a logging service per se.</li>
<li>CloudWatch<ul>
<li><strong>Amazon CloudWatch</strong> is a <strong>monitoring service</strong> for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files and set alarms.</li>
<li>Amazon CloudWatch can monitor AWS resources such as Amazon EC2 instances, Amazon DynamoDB tables, and Amazon RDS DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate.</li>
<li>By <strong>default</strong>, CloudWatch Logs will store your log data <strong>indefinitely</strong>. You can change the retention for each Log Group at any time.</li>
<li>CloudWatch Alarm history is only stored for <strong>14 days</strong>.</li>
</ul>
</li>
<li>Billing<br>  Billing for CloudWatch Logging is:<br>  <strong>$0.50</strong> per GB ingested per month<br>  <strong>$0.03</strong> per GB archived per month<br>  <strong>$0.10</strong> per alarm per month<br>  S3 lowest tier is <strong>$0.03 per GB</strong>. Can be cheaper to store logs in S3 than CloudWatch, but depends on your environment.</li>
</ul>
</li>
<li>Using CloudWatch To Monitor CloudTrail<ul>
<li>One of the ways that you can work with CloudTrail logs is to monitor them in <strong>real time</strong> by sending them to CloudWatch Logs. For a trail that is enabled in all regions in your account, CloudTrail sends log files from all those regions to a CloudWatch Logs log group.</li>
<li>You define CloudWatch Logs metric filters that will evaluate your CloudTrail log events for matches in terms, phrases, or values. You assign CloudWatch metrics to the <strong>metric filters</strong>. You also create CloudWatch alarms that are triggered according to thresholds and time periods that you specify.</li>
</ul>
</li>
<li>Monitoring<ul>
<li><strong>You can monitor events and ship those logs to:</strong><ul>
<li>CloudWatch</li>
<li>A Centralized Logging System (AlertLogic, Splunk, SumoLogic)</li>
<li>S3 (via a script that exports the log files.)</li>
</ul>
</li>
<li><strong>Avoid</strong> storing your logs anywhere that’s not persistent:<ul>
<li>Root device volume on EC2 instance.</li>
<li>Ephemeral Storage</li>
</ul>
</li>
<li>Best answers are usually either <strong>S3</strong> or <strong>CloudWatch</strong></li>
<li>CloudTrail can be used across <strong>multiple</strong> accounts in a single S3 bucket.</li>
<li>CloudWatch now <strong>supports</strong> Cross Account Subscriptions (Very recent, August 10, 2015) <a href="https://aws.amazon.com/about-aws/whats-new/2015/08/amazon-cloudwatch-logs-cross-account-subscriptions/" target="_blank" rel="external">https://aws.amazon.com/about-aws/whats-new/2015/08/amazon-cloudwatch-logs-cross-account-subscriptions/</a></li>
</ul>
</li>
<li>Exam Tips<ul>
<li>CloudWatch for Logs</li>
<li>CloudTrail for Audits, tracing API calls</li>
<li>CloudWatch can monitor CloudTrail</li>
<li><strong>You can send your logs to:</strong><ul>
<li>CloudWatch</li>
<li>S3</li>
<li>Third Parties (ALertLogic, SumoLogic, Splunk etc.)</li>
</ul>
</li>
<li>CloudTrail can log events from multiple accounts to a single S3 bucket.</li>
<li>CloudWatch supports cross account subscriptions, however this a very new service.</li>
<li>Always look to store your logs in a <strong>persistent</strong> &amp; <strong>safe</strong> place. S3 or CloudWatch are the ideal places.</li>
<li>By <strong>default</strong>, CloudWatch Logs will store your log data <strong>indefinitely</strong>.</li>
<li>CloudWatch Alarm History is only <strong>14 days</strong>.</li>
</ul>
</li>
</ul>
</li>
<li><p>Cloud Hardware Security Modules (HSMs)</p>
<ul>
<li>What Is A HSM?<ul>
<li>HSM stands for <strong>Hardware Security Module</strong>, and is a physical device that safeguards and manages digital keys for strong authentication and provides cryptoprocessing. These modules traditionally come in the form of a plug-in card or an external device that attaches directly to a computer or network server.</li>
</ul>
</li>
<li>Cloud HSMs<ul>
<li>Prior to Cloud, HSM’s customers had to store these devices on premise, introducing a large amount of latency into their environment.</li>
<li>The <strong>AWS CloudHSM</strong> service allows you to <strong>protect</strong> your encryption keys within HSMs designed and validated to government standards for secure key management. You can securely <strong>generate, store and manage</strong> the cryptographic keys used for data encryption such that they are accessible only by you.</li>
</ul>
</li>
<li>Cloud HSM - Pricing<ul>
<li>You will be <strong>charged</strong> an upfront fee for each CloudHSM instance you launch, and an hourly fee for each hour thereafter until you terminate the instance. If you want to try the CloudHSM service for free, you can request a <strong>two week trial</strong>.<br>  <img src="/images/AWS/CSAP/CSAP_classic_CloudHSM_pricing.png" alt="CSAP_classic_CloudHSM_pricing"></li>
<li>Now. There are no upfront costs to use AWS CloudHSM. With CloudHSM, you pay an hourly fee for each HSM you launch until you terminate the HSM.  <a href="https://aws.amazon.com/cloudhsm/pricing/" target="_blank" rel="external">https://aws.amazon.com/cloudhsm/pricing/</a><br>  <img src="/images/AWS/CSAP/CSAP_current_CloudHSM_pricing.png" alt="CSAP_current_CloudHSM_pricing"></li>
</ul>
</li>
<li>Exam Tips - CloudHSM<ul>
<li>Single Tenanted (<strong>1 physical device</strong>, for you only)</li>
<li><strong>Must</strong> be used within a <strong>VPC</strong></li>
<li>You can use <strong>VPC peering</strong> to connect to a <strong>CloudHSM</strong></li>
<li>You can use <strong>EBS volume encryption</strong>, <strong>S3 object encryption</strong> and <strong>key management</strong> with CloudHSM, but this does require <strong>custom application scripting</strong>.</li>
<li>If you need fault tolerance, you need to build a <strong>cluster</strong>, so you will need <strong>2</strong>. If you have only one and it fails, you will <strong>lose</strong> your keys.</li>
<li>Can integrate with <strong>RDS</strong> (Oracle &amp; SQL) as well as <strong>Redshift</strong></li>
<li>Monitor via <strong>Syslog</strong></li>
</ul>
</li>
</ul>
</li>
<li>DDOS<ul>
<li>What Is A DDos Attach?<ul>
<li><strong>A Distributed Denial of Service (DDos)</strong> attack is an attack that attempts to make your website or application unavailable to your end users.</li>
<li>This can be achieved by <strong>multiple</strong> mechanisms, such as large packet floods, by using a combination of reflection and amplification techniques, or by using large botnets.</li>
</ul>
</li>
<li>Amplification/Reflection Attacks<ul>
<li><strong>Amplification/Reflection</strong> attacks can include things such as NTP, SSDP, DNS, Chargen, SNMP attacks etc. and is where an attacker may send a third party server (such as an NTP server) a request using a spoofed IP address. That server will then respond to that request with a greater payload than initial request (usually within the region of 28*54 times larger than the request) to the spoofed IP address.</li>
<li>This means that if the attacker sends a packet with a spoofed IP address of 64 bytes, the NTP server would respond with up to 3,456 bytes of traffic. Attackers can co-ordinate this and use multiple NTP servers a second to send legitimate NTP traffic to the target.</li>
</ul>
</li>
<li>Application Attacks (L7)<ul>
<li>Flood of Get request</li>
<li>Slowloris</li>
</ul>
</li>
<li>How To Mitigate DDoS?<ul>
<li>Minimize the Attack Surface Area</li>
<li>Be Ready to Scale to Absorb the Attack</li>
<li>Safeguard Exposed Resources</li>
<li>Learn Normal Behavior</li>
<li>Create a Plan for Attacks</li>
</ul>
</li>
<li>Minimize The Attack Surface Area<ul>
<li>Some production environments have <strong>multiple</strong> entry points in to them. Perhaps they allow direct SSH or RDP access to their web servers/application and DB servers for management.</li>
<li>This can be <strong>minimized</strong> by using a <strong>Bastion/Jump Box</strong> that only allows access to specific white list IP addresses to these bastion servers and move the web, application and DB servers to private subnets. By minimizing the attack surface area, you are <strong>limiting</strong> your exposure to just a few hardened entry points.<br>  <img src="/images/AWS/CSAP/CSAP_DDos_architecture.png" alt="CSAP_DDos_architecture"></li>
</ul>
</li>
<li>Be Ready To Scale To Absorb The Attack<ul>
<li>The key strategy behind a DDoS attack is to bring your infrastructure to <strong>breaking point</strong>. This strategy assumes one thing: that you <strong>can’t</strong> scale to meet the attack.</li>
<li>The easiest way to defeat this strategy is to design your infrastructure to scale as, and when, it is needed.</li>
<li>You can scale both <strong>Horizontally</strong> &amp; <strong>Vertically</strong>.</li>
</ul>
</li>
<li>Scaling Has The Following Benefits<ul>
<li>The attack is spread over a <strong>larger area</strong>.</li>
<li>Attackers then have to <strong>counter attack</strong>, taking up more of their resources.</li>
<li>Scaling buys you time to <strong>analyze</strong> the attack and to respond with the appropriate countermeasures.</li>
<li>Scaling has the added benefit of providing you with <strong>additional</strong> levels of redundancy.</li>
</ul>
</li>
<li>Safeguard Exposed Resources<ul>
<li>In situations where you <strong>cannot</strong> eliminate Internet entry points to your applications, you’ll need to take <strong>additional</strong> measures to restrict access and protect those entry points without interrupting legitimate en user traffic.</li>
<li><strong>Three resources</strong> that can provide this control and flexibility are <strong>Amazon CloudFront</strong>, <strong>Amazon Route 53</strong> and <strong>web application firewalls(WAFs)</strong>.<ul>
<li><strong>CloudFront:</strong><ul>
<li><strong>Geo Restriction/Blocking</strong> – Restrict access to users in specific countries (using whitelists or blacklists).</li>
<li><strong>Origin Access Identity</strong> – Restrict Access to your S3 bucket so that people can only access S3 using CloudFront URLs.</li>
</ul>
</li>
<li><strong>Route53:</strong><ul>
<li><strong>Alias Record Sets</strong> – You can use these to immediately redirect your traffic to an Amazon CloudFront distribution, or to a different ELB load balancer with higher capacity EC2 instances running WAFs or your own security tools. No DNS change, and no need to worry about propagation.</li>
<li><strong>Private DNS</strong> – Allow you to manage internal DNS names for your application resources (web servers, application servers, database etc.) without exposing this information to the public Internet.</li>
</ul>
</li>
<li><strong>WAFs:</strong><ul>
<li>DDos attacks that happen at the application layer commonly target web applications with lower volumes of traffic compared to infrastructure attacks. To mitigate these types of attacks, you’ll want to include a WAF as port of your infrastructure.<ul>
<li><strong>New WAF Service</strong> – You can use the new AWS WAF service.</li>
<li><strong>AWS Market Place</strong> – You can buy other WAF’s on the AWS Market Place.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Learn Normal Behavior<ul>
<li><strong>Be aware of normal and unusual behavior</strong><ul>
<li>Know the different types of traffic and what normal levels of this traffic should be.</li>
<li>Understand expected and unexpected resource spikes.</li>
</ul>
</li>
<li><strong>What are the benefits?</strong><ul>
<li>Allows you to spot abnormalities fast.</li>
<li>You can create alarms to alert you of abnormal behavior.</li>
<li>Helps you to collect forensic data to understand the attack.</li>
</ul>
</li>
</ul>
</li>
<li>Create A Plan For Attacks<ul>
<li><strong>Having a plan in place before an attack ensures that:</strong><ul>
<li>You’ve validated the design of your architecture.</li>
<li>You understand the costs for your increased resiliency and already know what techniques to employ when you come under attack.</li>
<li>You know who to contact when an attack happens.</li>
</ul>
</li>
</ul>
</li>
<li>DDoS Exam Tips<ul>
<li>Definitely worth a few points in the exam.</li>
<li>Worth reading the white paper: <a href="https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf" target="_blank" rel="external">https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf</a> and <a href="https://aws.amazon.com/blogs/security/updated-whitepaper-available-aws-best-practices-for-ddos-resiliency/" target="_blank" rel="external">https://aws.amazon.com/blogs/security/updated-whitepaper-available-aws-best-practices-for-ddos-resiliency/</a></li>
<li>Remember the technologies you can use to mitigate a DDoS attack:<ul>
<li>CloudFront</li>
<li>Route53</li>
<li>ELB’s</li>
<li>WAFs</li>
<li>Autoscaling (Use for both WAFs an Web Servers)</li>
<li>CloudWatch</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>IDS &amp; IPS<ul>
<li>What Is IDS?<ul>
<li>An <strong>Intrusion Detection System (IDS)</strong> inspects all inbound and outbound network activity and identifies suspicious patterns that may indicate a network or system attack from someone attempting to break into, or compromise, a system.</li>
</ul>
</li>
<li>What Is IPS?<ul>
<li>An <strong>Intrusion Prevention System (IPS)</strong> is a network security/threat prevention technology that examines network traffic flows to detect and prevent vulnerability exploits.<br><img src="/images/AWS/CSAP/CSAP_IDS_IPS.png" alt="CSAP_IDS_IPS"></li>
</ul>
</li>
<li>IDS/IPS Exam Tips<ul>
<li><strong>IDS</strong> is intrusion <strong>Detection</strong>.</li>
<li><strong>IPS</strong> is intrusion <strong>Prevention</strong>.</li>
<li>Generally, you have an appliance in a public subnet, and then agents installed on <strong>each</strong> EC2 instance.</li>
<li>Log files can be sent to a <strong>SOC (Security Operation Centre)</strong> or stored in <strong>S3</strong>.</li>
</ul>
</li>
<li>Great Youtube Video<ul>
<li><a href="https://www.youtube.com/watch?v=wZ8sB7hfyvw" target="_blank" rel="external">https://www.youtube.com/watch?v=wZ8sB7hfyvw</a></li>
<li>AltertLogic Threat Manager AWS Youtube.</li>
</ul>
</li>
</ul>
</li>
<li>Domain 6 - Summary<ul>
<li>Domain 6<ul>
<li><strong>Domain 6.0: Security</strong><ul>
<li>6.1 Design information security management systems and compliance controls</li>
<li>6.2 Design security controls with the AWS shared responsibility model and global infrastructure</li>
<li>6.3 Design identity and access management controls</li>
<li>6.4 Design protection of Data at Rest controls</li>
<li>6.5 Design protection of Data in Flight and Network Perimeter controls</li>
<li>Worth 20%</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips - CloudHSM<ul>
<li>Single Tenanted (<strong>1 physical device</strong>, for you only)</li>
<li><strong>Must</strong> be used within a <strong>VPC</strong></li>
<li>You can use <strong>VPC peering</strong> to connect to a <strong>CloudHSM</strong></li>
<li>You can use <strong>EBS volume encryption</strong>, <strong>S3 object encryption</strong> and <strong>key management</strong> with CloudHSM, but this does require <strong>custom application scripting</strong>.</li>
<li>If you need fault tolerance, you need to build a <strong>cluster</strong>, so you will need <strong>2</strong>. If you have only one and it fails, you will <strong>lose</strong> your keys.</li>
<li>Can integrate with <strong>RDS</strong> (Oracle &amp; SQL) as well as <strong>Redshift</strong></li>
<li>Monitor via <strong>Syslog</strong></li>
</ul>
</li>
<li>Directory Services - Exam Tips<ul>
<li><strong>2 Types of Directory Services:</strong><ul>
<li><strong>AD Connector (Used for existing AD Deployments)</strong><ul>
<li>Great way to connect in to existing AD environments.</li>
<li>Supports MFA</li>
</ul>
</li>
<li><strong>Simple AD (Used for new AD Deployments)</strong><ul>
<li>MFA Not Supported</li>
<li>Cannot add additional AD servers</li>
<li>No Trust Relationships</li>
<li>Cannot Transfer FSMO Roles</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>IDS/IPS Exam Tips<ul>
<li><strong>IDS</strong> is intrusion <strong>Detection</strong>.</li>
<li><strong>IPS</strong> is intrusion <strong>Prevention</strong>.</li>
<li>Generally, you have an appliance in a public subnet, and then agents installed on <strong>each</strong> EC2 instance.</li>
<li>Log files can be sent to a <strong>SOC (Security Operation Centre)</strong> or stored in <strong>S3</strong>.<br><img src="/images/AWS/CSAP/CSAP_IDS_IPS.png" alt="CSAP_IDS_IPS"></li>
</ul>
</li>
<li>Monitoring Exam Tips<ul>
<li>CloudWatch for Logs</li>
<li>CloudTrail for Audits, tracing API calls</li>
<li>CloudWatch can monitor CloudTrail</li>
<li><strong>You can send your logs to:</strong><ul>
<li>CloudWatch</li>
<li>S3</li>
<li>Third Parties (ALertLogic, SumoLogic, Splunk etc.)</li>
</ul>
</li>
<li>CloudTrail can log events from multiple accounts to a single S3 bucket.</li>
<li>CloudWatch supports cross account subscriptions, however this a very new service.</li>
<li>Always look to store your logs in a <strong>persistent</strong> &amp; <strong>safe</strong> place. S3 or CloudWatch are the ideal places.</li>
<li>By <strong>default</strong>, CloudWatch Logs will store your log data <strong>indefinitely</strong>.</li>
<li>CloudWatch Alarm History is only <strong>14 days</strong>.</li>
</ul>
</li>
<li>Identity Brokers<ul>
<li>Develop an <strong>Identity Broker</strong> to communicate with <strong>LDAP</strong> and <strong>AWS STS</strong></li>
<li><strong>Identity Broker</strong> always authenticates with <strong>LDAP</strong> first, <strong>Then</strong> with <strong>AWS STS</strong></li>
<li>Application then gets <strong>temporary access</strong> to AWS resources<br><img src="/images/AWS/Sysops/sts_scenario.jpg" alt="sts_scenario"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Domain-7-Scalability-amp-Elasticity-15"><a href="#Domain-7-Scalability-amp-Elasticity-15" class="headerlink" title="Domain 7 - Scalability &amp; Elasticity - 15%"></a>Domain 7 - Scalability &amp; Elasticity - 15%</h3><ul>
<li>CloudFront<ul>
<li>CloudFront<br>  <strong>Amazon CloudFront</strong> can be used to deliver your entire website, including dynamic, static, streaming, and interactive content using a global network of edge locations.<br>  Requests for your content are automatically routed to the nearest edge location, so content is delivered with the best possible performance. Amazon CloudFront is optimized to work with other Amazon Web Services, like S3, EC2, ELB and Route 53.</li>
<li>Key Concepts<ul>
<li>Distribution Types</li>
<li>Geo Restriction</li>
<li>Support for POST, PUT &amp; Other HTTP Methods</li>
<li>SSL Configurations</li>
<li>Wildcard CNAME support</li>
<li>Invalidation</li>
<li>Zone Apex Support</li>
<li>Edge Caching</li>
</ul>
</li>
<li>Distribution Types<ul>
<li><strong>Two types of Distributions:</strong><ul>
<li>Web Distributions</li>
<li>RTMP Distributions</li>
</ul>
</li>
</ul>
</li>
<li>Geo Restriction<ul>
<li><strong>Geo Restriction</strong> or <strong>Geoblocking</strong> lets you choose the countries in which you want to restrict access to your content</li>
<li><strong>Whitelists</strong> or <strong>Blacklists</strong> particular countries</li>
<li>Can be done either using the <strong>API</strong> or the <strong>console</strong></li>
<li>Viewer from blacklisted countries will see a <strong>HTTP 403</strong> error</li>
<li>You can also create <strong>custom</strong> error pages</li>
</ul>
</li>
<li>Support For<ul>
<li><strong>GET, HEAD, POST, PUT, PATCH, DELETE</strong> and <strong>OPTIONS</strong>.</li>
<li>Amazon CloudFront does <strong>not</strong> cache the responses to POST, PUT, DELETE and PATCH requests – these requests are proxied back to the origin server.</li>
</ul>
</li>
<li>SSL<br>  You can use either <strong>HTTP</strong> or <strong>HTTPS</strong> with <strong>CloudFront</strong>. With <strong>SSL</strong>, you can use the <strong>default</strong> CloudFront URL, or your own <strong>custom</strong> URL with your own <strong>SSL</strong> certificate.<ul>
<li><strong>Dedicated IP Custom SSL:</strong> Allocates dedicated IP addresses to serve your SSL content at each CloudFront edge location. VERY Expensive. $600 USD per certificate per month. However will support older browsers.</li>
<li><strong>SNI Custom SSL:</strong> Relies on the SNI extension of the Transport Layer Security protocol, which allows multiple domains to serve SSL traffic over the same IP address by including the hostname viewers are trying to connect to. Older browsers won’t support it.</li>
</ul>
</li>
<li>CNAME Support<ul>
<li><strong>CNAMEs are supported</strong><ul>
<li>You can have 10 CNAMES aliases to each distribution.</li>
<li>CloudFront also supports wildcard CNAMEs.</li>
</ul>
</li>
</ul>
</li>
<li>Invalidation Requests<br>  There are <strong>multiple</strong> options for removing a file from the edge locations. You can simply delete the file from your origin and, as content in the edge locations reaches the expiration period defined in each object’s HTTP header, it will be removed.<br>  In the event that offensive or potentially harmful material needs to be removed before the specified expiration time, you can use the <strong>Invalidation API</strong> to remove the object from all Amazon CloudFront edge locations.</li>
<li>Zone Apex Support<br>  Using <strong>Amazon Route S3</strong>, AWS’s authoritative DNS service, you can configure an ‘<strong>Alias</strong>‘ record that lets you map the apex or root (example.com) of your DNS name to your Amazon CloudFront distribution.<br>  <a href="http://example.com" target="_blank" rel="external">http://example.com</a> =&gt; <a href="http://d1111eft.cloufront.net" target="_blank" rel="external">http://d1111eft.cloufront.net</a><br>  Amazon Route 53 will then respond to each request for an Alias record with the right IP address(es) for your CloudFront distribution. Route 53 doesn’t charge for queries to Alias records that are mapped to a CloudFront distribution.</li>
<li>Dynamic Content Support<br>  <strong>Amazon CloudFront</strong> supports delivery of dynamic content that is customized or personalized using HTTP cookies. To use this feature, you specify whether you want Amazon CloudFront to forward some or all of your cookies to your custom origin server. Amazon CloudFront then considers the forwarded cookie values when identifying a unique object in its cache.<br>  This way, your end users get both the benefit of content that is personalized just for them with a cookie and the performance benefits of Amazon CloudFront. You can also optionally choose to log the cookie values in Amazon CloudFront access logs.</li>
<li>Exam Tips - CloudFront<ul>
<li><strong>2 distribution types:</strong> Web &amp; RTMP</li>
<li>You can use <strong>Geo Restriction</strong> to white list or black list a country.</li>
<li>You can use <strong>SSL</strong> with CloudFront. You can either use the <strong>default</strong> URL or your own <strong>custom</strong> URL. If you use custom URL you can use <strong>Dedicated IP Custom SSL</strong> or <strong>SNI Custom SSL</strong>.</li>
<li>CloudFront Supports <strong>GET, HEAD, POST, PUT, PATCH, DELETE</strong> and <strong>OPTIONS</strong>.</li>
<li>Amazon CloudFront does <strong>not</strong> cache the responses to POST, PUT, DELETE and PATCH requests – these requests are <strong>proxied</strong> back to the origin server.</li>
<li><strong>CNAMES</strong> are supported</li>
<li>In the event that offensive or potentially harmful material needs to be removed before the specified expiration time, you can use the <strong>Invalidation API</strong> to remove the object from all Amazon CloudFront edge locations. This is called an invalidation request.</li>
<li><strong>Zone Apex</strong> records are supported if you use <strong>Route53</strong>. This is achieved by using an <strong>Alias</strong> record.</li>
<li><strong>Dynamic Content</strong> is supported! Amazon CloudFront supports delivery of dynamic content that is <strong>customized</strong> or <strong>personalized</strong> using HTTP cookies.</li>
</ul>
</li>
</ul>
</li>
<li>Memcached vs Redis<ul>
<li>Memcached vs Redis<br>  In the SA Pro exam, you will be given a number of different scenarios. In some of these you will need to decide which <strong>ElastiCache Engine</strong> you need to use. ElastiCache has <strong>two</strong> different engines: <strong>Memcached</strong> &amp; <strong>Redis</strong>.<br>  Memcached and Redis look similar on the surface: both are <strong>in-memory key stores</strong>. However, in practice, there are significant differences.</li>
<li>Requirement in Memcached and Redis<br><img src="/images/AWS/CSAP/CSAP_Memcached_vs_Redis.png" alt="CSAP_Memcached_vs_Redis.png"></li>
<li>Exam Tips - Use Memcached If …<ul>
<li>You want the <strong>simplest</strong> model possible.</li>
<li>You need to run large <strong>nodes</strong> with <strong>multiple</strong> cores or threads.</li>
<li>You need the ability to <strong>scale out</strong>, <strong>adding</strong> and <strong>removing</strong> nodes as demand on your system increases and decreases.</li>
<li>You want to <strong>shard</strong> your data across <strong>multiple</strong> nodes.</li>
<li>You need to <strong>cache</strong> objects, such as a <strong>database</strong>.</li>
</ul>
</li>
<li>Exam Tips - Use Redis If…<ul>
<li>You need complex <strong>data types</strong>, such as <strong>strings, hashed, lists</strong> and <strong>sets</strong>.</li>
<li>You need to <strong>sort</strong> or <strong>rank</strong> in-memory data-sets.</li>
<li>You want <strong>persistence</strong> of your key store.</li>
<li>You want to <strong>replicate</strong> your data from the <strong>primary</strong> to one or more read replicas for availability.</li>
<li>You need <strong>automatic failover</strong> if any of your primary nodes fail.</li>
<li>You want <strong>publish</strong> and <strong>subscribe</strong> (pub/sub) capabilities – the client being informed of events on the server.</li>
<li>You want <strong>backup</strong> and <strong>restore</strong> capabilities.</li>
</ul>
</li>
</ul>
</li>
<li><p>Kinesis</p>
<ul>
<li>Kinesis<br>  <strong>Kinesis</strong> can be involved in many different scenario questions and you need to have a good understanding of what Kinesis is, what the different components are, and where you should use Kinesis.</li>
<li>What Is Kinesis?<br>  <strong>Amazon Kinesis Streams</strong> enable you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data, such as clickstreams, application logs, and social media to an Amazon Kinesis stream from hundreds of thousands of sources. Within seconds, the data will be available for your Amazon Kinesis Applications to read and process from the stream.<br>  <strong>Data in Kinesis is stored for 24 hours by default. You can increase this to 7 days if required.</strong><br>  <img src="/images/AWS/CSAP/CSAP_kinesis_flow.png" alt="CSAP_kinesis_flow.png"></li>
<li>Key Concepts<ul>
<li><strong>Data Producers</strong></li>
<li><strong>Shards</strong></li>
<li><strong>Records</strong><ul>
<li>Sequence Number</li>
<li>Partition Key</li>
<li>Data Itself (Blob)</li>
</ul>
</li>
<li><strong>Data Consumers</strong> (Kinesis Streams Applications)</li>
</ul>
</li>
<li>Data Producers<ul>
<li><strong>Amazon Kinesis Streams API</strong><ul>
<li>PutRecord (Single data record)</li>
<li>PutRecords (Multiple data records)</li>
</ul>
</li>
<li><strong>Amazon Kinesis Producer Library (KPL)</strong><ul>
<li>On Github, By using the KPL, customers do not need to develop the same logic every time they create a new application for data ingestion.</li>
</ul>
</li>
<li><strong>Amazon Kinesis Agent</strong><ul>
<li>Prebuilt Java Application you can install on your linux devices.</li>
</ul>
</li>
</ul>
</li>
<li>What Is A Shard?<br>  A <strong>Shard</strong> is simply the unit of measurement of data when referring to Kinesis.<br>  One shard provides a capacity of <strong>1MB/sec</strong> data input and <strong>2MB/sec</strong> data output. One shard can support up to <strong>1000 PUT</strong> records per second. You will specify the number of shards needed when you create a stream. For example, you can create a stream with two shards. This stream has a throughput of 2MB/sec data input and 4MB/sec data output, and allows up to 2000 PUT records per second. You can dynamically <strong>add</strong> or <strong>remove</strong> shards from your stream as your data throughput changes via <strong>resharding</strong>.</li>
<li>What Is A Partition Key?<br>  <strong>Kinesis Streams</strong> can consist of many different shards. You can group the data by shard by using a <strong>partition key</strong>.<br>  Essentially, the partition key tells you which shard the data belongs to. A partition key is specified by the applications putting the data into a stream.</li>
<li>What Is A Sequence Number?<br>  Each data record has a <strong>unique sequence number</strong>. Think of it as a unique key.<br>  The sequence number is assigned by Streams <strong>after</strong> you write to the stream with <strong>client.putRecord</strong> or <strong>client.putRecords</strong>.<br>  You can’t use sequence numbers to logically separate data in terms of what shards they have come from - you can <strong>only</strong> do this using partition keys.</li>
<li>Blobs<ul>
<li><strong>Data blobs</strong> are the data your data producer adds to a stream. The maximum size of a data blob (the data payload after Base64-decoding) is <strong>1 megabyte (MB)</strong>.</li>
</ul>
</li>
<li>Exam Tips - Kinesis<ul>
<li>Any kind of scenario where you are streaming large amounts of data that need to be processed quickly, think <strong>Kinesis</strong>.</li>
<li>Concepts<ul>
<li><strong>Data Producers</strong></li>
<li><strong>Shards</strong></li>
<li><strong>Records</strong><ul>
<li>Sequence Number</li>
<li>Partition Key</li>
<li>Data Itself (Blob)</li>
</ul>
</li>
<li><strong>Data Consumers</strong> (Kinesis Streams Applications)</li>
</ul>
</li>
<li>Data is stored for <strong>24 hours</strong> by default within streams and can be increased to <strong>7 days</strong>.</li>
<li>Use <strong>S3, Redshift</strong> etc. to store processed data for longer term. Kinesis streams <strong>IS NOT PERSISTENT</strong> storage.</li>
</ul>
</li>
</ul>
</li>
<li><p>SNS Mobile Push</p>
<ul>
<li>SNS Mobile Push<ul>
<li>A topic that can come up a fair bit in the various different scenarios. You send push notification messages to both mobile devices and desktops using one of the following supported push notification services:<ul>
<li>Amazon Device Messaging (ADM)</li>
<li>Apple Push Notification Service (APNS) for both iOS and Mac OS X</li>
<li>Baidu Cloud Push (Baidu)</li>
<li>Google Cloud Messaging for Android (GCM)</li>
<li>Microsoft Push Notification Service for Windows Phone (MPNS)</li>
<li>Windows Push Notification Services (WNS)<br><img src="/images/AWS/CSAP/CSAP_SNS_Mobile_Push.png" alt="CSAP_SNS_Mobile_Push.png"></li>
</ul>
</li>
</ul>
</li>
<li>Steps<ol>
<li>Request Credentials from Mobile Platforms</li>
<li>Request Token from Mobile Platforms</li>
<li>Create Platform Application Object</li>
<li>Create Platform Endpoint Object</li>
<li>Publish Message to Mobile Endpoint</li>
</ol>
<ul>
<li><strong>Further reading:</strong><ul>
<li><a href="https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html" target="_blank" rel="external">https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html</a></li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips - SNS Mobile Push<ul>
<li>You can extend SNS to mobile applications using <strong>SNS Mobile Push</strong>. The following push notifications are supported:<ul>
<li>Amazon Device Messaging (ADM)</li>
<li>Apple Push Notification Service (APNS) for both iOS and Mac OS X</li>
<li>Baidu Cloud Push (Baidu)</li>
<li>Google Cloud Messaging for Android (GCM)</li>
<li>Microsoft Push Notification Service for Windows Phone (MPNS)</li>
<li>Windows Push Notification Services (WNS)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Domain 7</p>
<ul>
<li>Domain 7.0 - Scalability &amp; Elasticity<ul>
<li>7.1 Demonstrate the ability to design a loosely coupled system</li>
<li>7.2 Demonstrate ability to implement the most appropriate front-end scaling architecture</li>
<li>7.3 Demonstrate ability to implement the most appropriate middle-tier scaling architecture</li>
<li>7.4 Demonstrate ability to implement the most appropriate data storage scaling architecture</li>
<li>7.5 Determine trade-offs between vertical and horizontal scaling</li>
</ul>
</li>
<li>Exam Tips - CloudFront<ul>
<li><strong>2 distribution types:</strong> Web &amp; RTMP</li>
<li>You can use <strong>Geo Restriction</strong> to white list or black list a country.</li>
<li>You can use <strong>SSL</strong> with CloudFront. You can either use the <strong>default</strong> URL or your own <strong>custom</strong> URL. If you use custom URL you can use <strong>Dedicated IP Custom SSL</strong> or <strong>SNI Custom SSL</strong>.</li>
<li>CloudFront Supports <strong>GET, HEAD, POST, PUT, PATCH, DELETE</strong> and <strong>OPTIONS</strong>.</li>
<li>Amazon CloudFront does <strong>not</strong> cache the responses to POST, PUT, DELETE and PATCH requests – these requests are <strong>proxied</strong> back to the origin server.</li>
<li><strong>CNAMES</strong> are supported</li>
<li>In the event that offensive or potentially harmful material needs to be removed before the specified expiration time, you can use the <strong>Invalidation API</strong> to remove the object from all Amazon CloudFront edge locations. This is called an invalidation request.</li>
<li><strong>Zone Apex</strong> records are supported if you use <strong>Route53</strong>. This is achieved by using an <strong>Alias</strong> record.</li>
<li><strong>Dynamic Content</strong> is supported! Amazon CloudFront supports delivery of dynamic content that is <strong>customized</strong> or <strong>personalized</strong> using HTTP cookies.</li>
</ul>
</li>
<li>Exam Tips - Use Memcached If …<ul>
<li>You want the <strong>simplest</strong> model possible.</li>
<li>You need to run large <strong>nodes</strong> with <strong>multiple</strong> cores or threads.</li>
<li>You need the ability to <strong>scale out</strong>, <strong>adding</strong> and <strong>removing</strong> nodes as demand on your system increases and decreases.</li>
<li>You want to <strong>shard</strong> your data across <strong>multiple</strong> nodes.</li>
<li>You need to <strong>cache</strong> objects, such as a <strong>database</strong>.</li>
</ul>
</li>
<li>Exam Tips - Use Redis If…<ul>
<li>You need complex <strong>data types</strong>, such as <strong>strings, hashed, lists</strong> and <strong>sets</strong>.</li>
<li>You need to <strong>sort</strong> or <strong>rank</strong> in-memory data-sets.</li>
<li>You want <strong>persistence</strong> of your key store.</li>
<li>You want to <strong>replicate</strong> your data from the <strong>primary</strong> to one or more read replicas for availability.</li>
<li>You need <strong>automatic failover</strong> if any of your primary nodes fail.</li>
<li>You want <strong>publish</strong> and <strong>subscribe</strong> (pub/sub) capabilities – the client being informed of events on the server.</li>
<li>You want <strong>backup</strong> and <strong>restore</strong> capabilities.</li>
</ul>
</li>
<li>Exam Tips - SNS Mobile Push<ul>
<li>You can extend SNS to mobile applications using <strong>SNS Mobile Push</strong>. The following push notifications are supported:<ul>
<li>Amazon Device Messaging (ADM)</li>
<li>Apple Push Notification Service (APNS) for both iOS and Mac OS X</li>
<li>Baidu Cloud Push (Baidu)</li>
<li>Google Cloud Messaging for Android (GCM)</li>
<li>Microsoft Push Notification Service for Windows Phone (MPNS)</li>
<li>Windows Push Notification Services (WNS)</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips - Kinesis<ul>
<li>Any kind of scenario where you are streaming large amounts of data that need to be processed quickly, think <strong>Kinesis</strong>.</li>
<li>Concepts<ul>
<li><strong>Data Producers</strong></li>
<li><strong>Shards</strong></li>
<li><strong>Records</strong><ul>
<li>Sequence Number</li>
<li>Partition Key</li>
<li>Data Itself (Blob)</li>
</ul>
</li>
<li><strong>Data Consumers</strong> (Kinesis Streams Applications)</li>
</ul>
</li>
<li>Data is stored for <strong>24 hours</strong> by default within streams and can be increased to <strong>7 days</strong>.</li>
<li>Use <strong>S3, Redshift</strong> etc. to store processed data for longer term. Kinesis streams <strong>IS NOT PERSISTENT</strong> storage.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Domain-8-Cloud-Migration-amp-Hybrid-Architecture-10"><a href="#Domain-8-Cloud-Migration-amp-Hybrid-Architecture-10" class="headerlink" title="Domain 8 - Cloud Migration &amp; Hybrid Architecture - 10%"></a>Domain 8 - Cloud Migration &amp; Hybrid Architecture - 10%</h3><ul>
<li>Vmware Migrations<ul>
<li>Migrations<ul>
<li><strong>AWS Management Portal</strong> for vCenter enables you to manage your AWS resources using VMware vCenter.</li>
<li>The portal installs as a vCenter plug-in within your existing vCenter environment. Once installed, it enables you to migrate VMware VMs to Amazon EC2 and manage AWS resources from within vCenter.</li>
</ul>
</li>
<li>Common Use Cases<ul>
<li>Migrate VMware VMs to Amazon EC2</li>
<li>Reach New Geographies from vCenter</li>
<li>Self-Service AWS Portal within vCenter</li>
<li>Leverage vCenter Experience While Getting Started with AWS</li>
</ul>
</li>
<li>Product Demo<ul>
<li><a href="https://aws.amazon.com/ec2/vcenter-portal/" target="_blank" rel="external">https://aws.amazon.com/ec2/vcenter-portal/</a></li>
</ul>
</li>
<li>Exam Tips - VMware<ul>
<li>vCenter has a plug-in that it enables you to migrate VMware VMs to Amazon EC2 and manage AWS resources from within vCenter.</li>
<li><strong>Use Cases Include:</strong><ul>
<li>Migrate VMware VMs to Amazon EC2</li>
<li>Reach New Geographies from vCenter</li>
<li>Self-Server AWS Portal within vCenter</li>
<li>Leverage vCenter Experience While Getting Started with AWS</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Storage Gateway<ul>
<li>Storage Gateway<ul>
<li><strong>Gateway-Cached Volumes</strong><ul>
<li>iSCSI based block storage</li>
</ul>
</li>
<li><strong>Gateway-Stored Volumes</strong><ul>
<li>iSCSI based block storage</li>
</ul>
</li>
<li><strong>Gateway-Virtual Tape Library</strong><ul>
<li>iSCSI based virtual tape solution</li>
</ul>
</li>
</ul>
</li>
<li>Storage Gateway Migrations<br><img src="/images/AWS/CSAP/CSAP_storage_gateway_migration.png" alt="CSAP_storage_gateway_migration.png"></li>
<li>Consistent Snapshots<ul>
<li>Snapshots provide a <strong>point-in-time view of data</strong> that has been written to your AWS Storage Gateway volumes. However, snapshots only capture data that has been written to your storage volumes, which can exclude data that has been buffered by either the client or OS.</li>
<li>Your application and operating system will eventually <strong>flush</strong> this buffered data to your storage volumes.</li>
<li>If you need to guarantee that your operating system and file system have flushed their buffered data to disk prior to taking a snapshot, you can do this by taking your storage volume <strong>offline</strong> before taking a snapshot.</li>
<li>Doing this forces your operating system to <strong>flush</strong> its data to disk. After the snapshot is complete. you can bring the volume back online.</li>
</ul>
</li>
<li>Exam Tips<ul>
<li>You can use <strong>Storage Gateway</strong> to migrate your existing VMs to AWS.</li>
<li>You need to ensure that the snapshots are <strong>consistent</strong>. The best way to do this is to your VM offline and then do the snapshot.</li>
</ul>
</li>
</ul>
</li>
<li>Data Pipeline<ul>
<li>Data Pipeline<ul>
<li><strong>AWS Data Pipeline</strong> is a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premise data sources, at specified intervals. </li>
</ul>
</li>
<li>Data Pipeline - Key Concepts<ul>
<li>Pipeline</li>
<li>Datanode</li>
<li>Activity</li>
<li>Procondition</li>
<li>Schedule</li>
</ul>
</li>
<li>Pipeline<ul>
<li><strong>Pipeline</strong><ul>
<li>A pipeline is simply the name of the container that contains the datanodes, activities, preconditions, and schedules required in order to move your data from one location to another.</li>
<li>A pipeline can run either on an EC2 instance or an EMR instance.</li>
<li>Data Pipeline will provision and terminate these resources for you automatically.</li>
</ul>
</li>
</ul>
</li>
<li>Data Pipeline - Use on Premise<ul>
<li><strong>Pipeline - On Premise</strong><ul>
<li>AWS Data Pipeline supplies a Task Runner package that can be installed on your on-premise hosts. This package continuously polls the AWS Data Pipeline service for work to perform. When it’s time to run a particular activity on your on-premise resources, for example, executing a DB stored procedure or a database dump, AWS Data Pipeline will issue the appropriate command to the Task Runner.</li>
</ul>
</li>
</ul>
</li>
<li>Data Pipeline - Data Nodes<ul>
<li><strong>DataNode</strong><ul>
<li>A data node is essentially the end destination for your data. For example, a data node can reference a specific Amazon S3 path. AWS Data Pipeline supports an expression language that makes it easy to reference data which is generated on a regular basis. For example, you could specify that your Amazon S3 data format is s3://example-bucket/my-logs/logdata-#{scheduledStartTime(‘YYYY-MM-dd-HH’)}.tgz.</li>
</ul>
</li>
</ul>
</li>
<li>Data Pipeline - Activity<ul>
<li><strong>Activity</strong><ul>
<li>An activity is an action that AWS Data Pipeline initiates on your behalf as part of a pipeline. Example activities are EMR or Hive jobs, copies, SQL queries, or command-line scripts.</li>
<li>You can specify your own custom activities using the ShellCommandActivity</li>
</ul>
</li>
</ul>
</li>
<li>Data Pipeline - Precondition<ul>
<li><strong>Precondition</strong><ul>
<li>A precondition is a readiness check that can be optionally associated with a data source or activity. If a data source has a precondition check, then that check must complete successfully before any activities consuming the data source are launched. If an activity has a precondition, then the precondition check must complete successfully before the activity is run. This can be useful if you are running an activity that is expensive to compute, and should not run until specific criteria are met.</li>
<li>You can specify custom preconditions.</li>
</ul>
</li>
<li><strong>Precondition</strong><ul>
<li>DynamoDBDataExists - Does data exist?</li>
<li>DynamoDBTableExists - Does Table exist?</li>
<li>S3KeyExists - Does S3 path Exist?</li>
<li>S3PrefixExists - Does a file exist within that S3 path?</li>
<li>ShellCommandPrecondition - Custom Preconditions</li>
</ul>
</li>
</ul>
</li>
<li>Data Pipeline - Schedule<ul>
<li><strong>Schedule</strong><ul>
<li>Schedules define when your pipeline activities run and the frequency with which the service expects your data to be available.</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips - Data Pipeline<ul>
<li><strong>AWS Data Pipeline</strong> is a web service that helps you reliably process and move data between different AWS compute and storage services.</li>
<li>Can be integrated with <strong>on-premise environments</strong>.</li>
<li>Can be <strong>scheduled</strong>.</li>
<li>Data Pipeline will <strong>provision</strong> and <strong>terminate</strong> resources as, and when, required.</li>
<li>A lot of its functionality has been replaced by <strong>Lambda</strong>.</li>
<li><strong>A Pipeline consists of:</strong><ul>
<li>Datanode</li>
<li>Activity</li>
<li>Precondition</li>
<li>Schedule</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Migrations &amp; Networking<ul>
<li>CIDR Reservations<ul>
<li>When you create a subnet, you specify the CIDR block for the subnet. The allowed block size is between a <strong>/28</strong> netmask and <strong>/16</strong> netmask.</li>
<li>If you create more than one subnet in a VPC, the CIDR blocks of the subnets must <strong>not</strong> overlap.</li>
<li>The first four IP addresses and the last IP address in each subnet CIDR block are not available for you to use, and cannot be assigned to an instance. For example, in a subnet with CIDR block 10.0.0.0/24, the following <strong>5 IP addresses are reserved</strong>:<ul>
<li>10.0.0.0: Network address.</li>
<li>10.0.0.1: Reserved by AWS for the VPC router.</li>
<li>10.0.0.2: Reserved by AWS for mapping to the Amazon-Provided DNS.</li>
<li>10.0.0.3: Reserved by AWS for future use.</li>
<li>10.0.0.255: Network broadcast address. AWS do not support broadcast in a VPC; therefore they reserve this address.</li>
</ul>
</li>
</ul>
</li>
<li>VPN To Direct Connect Migrations<ul>
<li>Most organizations will have a site-to-site VPN tunnel from their location to AWS. As traffic begins to become heavier, organizations will opt to use <strong>Direct Connect</strong>.</li>
<li>Once Direct Connect is installed, you should configure it so that your VPN connection &amp; your Direct Connect connection are within the same BGP community. You then <strong>configure</strong> BGP so that your VPN connection has a higher cost than the Direct Connect connection.</li>
</ul>
</li>
<li>Exam Tips - Network &amp; Migrations<ul>
<li><strong>CIDR blocks</strong> can be between /16 - /28</li>
<li>AWS reserve <strong>5</strong> IP addresses per CIDR block.</li>
<li>You can migrate from a VPN connection to a Direct Connect connection by using <strong>BGP</strong>. Simply ensure that the VPN connection has a higher BGP cost than the Direct Connect connection.</li>
</ul>
</li>
</ul>
</li>
<li>Domain 8 - Summary<ul>
<li>Domain 8 - Cloud Migration &amp; Hybrid Architecture<ul>
<li><strong>Domain 8.0: Cloud Migration and Hybrid Architecture</strong><ul>
<li>8.1 Plan and execute for applications migrations</li>
<li>8.2 Demonstrate ability to design hybrid cloud architectures</li>
<li>Worth 10%</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips - VMware<ul>
<li>vCenter has a plug-in that it enables you to migrate VMware VMs to Amazon EC2 and manage AWS resources from within vCenter.</li>
<li><strong>Use Cases Include:</strong><ul>
<li>Migrate VMware VMs to Amazon EC2</li>
<li>Reach New Geographies from vCenter</li>
<li>Self-Server AWS Portal within vCenter</li>
<li>Leverage vCenter Experience While Getting Started with AWS</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips - Storage Gateway Migrations<ul>
<li>You can use <strong>Storage Gateway</strong> to migrate your existing VMs to AWS.</li>
<li>You need to ensure that the snapshots are <strong>consistent</strong>. The best way to do this is to your VM offline and then do the snapshot.</li>
</ul>
</li>
<li>Exam Tips - Data Pipeline<ul>
<li><strong>AWS Data Pipeline</strong> is a web service that helps you reliably process and move data between different AWS compute and storage services.</li>
<li>Can be integrated with <strong>on-premise environments</strong>.</li>
<li>Can be <strong>scheduled</strong>.</li>
<li>Data Pipeline will <strong>provision</strong> and <strong>terminate</strong> resources as, and when, required.</li>
<li>A lot of its functionality has been replaced by <strong>Lambda</strong>.</li>
<li><strong>A Pipeline consists of:</strong><ul>
<li>Datanode</li>
<li>Activity</li>
<li>Precondition</li>
<li>Schedule</li>
</ul>
</li>
</ul>
</li>
<li>Exam Tips - Network &amp; Migrations<ul>
<li><strong>CIDR blocks</strong> can be between /16 - /28</li>
<li>AWS reserve <strong>5</strong> IP addresses per CIDR block.</li>
<li>You can migrate from a VPN connection to a Direct Connect connection by using <strong>BGP</strong>. Simply ensure that the VPN connection has a higher BGP cost than the Direct Connect connection.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Additional-Content"><a href="#Additional-Content" class="headerlink" title="Additional Content"></a>Additional Content</h2><ul>
<li><p>CloudFront</p>
<ul>
<li>CloudFront Origin Access Identity (OAI)<ul>
<li>used to restrict access to S3 content <a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html" target="_blank" rel="external">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></li>
<li>注意点: 做实验时，如果设置好后，发现不能通过cloudFront的节点来访问S3的内容。要注意看一下浏览器的url，当你通过cloudfront节点访问S3资源的时，有没有被redirect到S3的url去，如果有，就说明cloudfront设置还没有完全生效，隔一段时间后再试就会发现没有问题了<ul>
<li>官方论坛里面提及了，在CloudFront使用了新建立的bucket时会可能出现有这个问题，因为新Bucket的domian还没有完全扩散出去 <a href="https://forums.aws.amazon.com/message.jspa?messageID=677452" target="_blank" rel="external">Cloudfront domain redirects to S3 Origin URL</a> </li>
<li>详细的操作步骤 – <a href="https://stackoverflow.com/a/42285049" target="_blank" rel="external">AWS CloudFront access denied to S3 bucket</a></li>
<li><a href="https://acloud.guru/forums/aws-certified-developer-associate/discussion/-KcC5f1fw3l4-tclsXje/cloudfront_redirecting_to_orig" target="_blank" rel="external">CloudFront redirecting to origin?</a></li>
</ul>
</li>
</ul>
</li>
<li>Custom SSL Certificate<ul>
<li>use SSL Certificate stored in AWS Certificate Manager (ACM) in the US East(N. Virginia) Region</li>
<li>use a certificate stored in IAM.<ul>
<li><a href="https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-custom-certificate/" target="_blank" rel="external">How do I ensure that the certificate I upload for use with Amazon CloudFront is accessible from the CloudFront console?</a></li>
</ul>
</li>
</ul>
</li>
<li>Custom SSL Options for Amazon CloudFront - <a href="https://aws.amazon.com/cloudfront/custom-ssl-domains/" target="_blank" rel="external">https://aws.amazon.com/cloudfront/custom-ssl-domains/</a><ul>
<li>SNI Custom SSL - SNI (Server Name Indication)</li>
<li>Dedicated IP Custom SSL</li>
</ul>
</li>
<li>Why set TTL to 0<ul>
<li><a href="https://aws.amazon.com/blogs/aws/amazon-cloudfront-support-for-dynamic-content/" target="_blank" rel="external">https://aws.amazon.com/blogs/aws/amazon-cloudfront-support-for-dynamic-content/</a></li>
<li>In many cases, dynamic content is either not cacheable or cacheable for a very short period of time, perhaps just a few seconds. In the past, CloudFront’s minimum TTL was 60 minutes since all content was considered static. The new minimum TTL value is 0 seconds. If you set the TTL for a particular origin to 0, CloudFront will still cache the content from that origin. It will then make a GET request with an If-Modified-Since header, thereby giving the origin a chance to signal that CloudFront can continue to use the cached content if it hasn’t changed at the origin.</li>
</ul>
</li>
</ul>
</li>
<li><p>RedShift</p>
<ul>
<li>RedShift workload Management(WLM)<ul>
<li>enables users to manage priorities flexibly within workloads so that short, fast-running queries won’t get stuck in queues behind long-running queries.</li>
</ul>
</li>
<li>Copying Snapshots to Another Region – <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html" target="_blank" rel="external">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html</a><ul>
<li>You can configure Amazon Redshift to automatically copy snapshots (automated or manual) for a cluster to another region</li>
</ul>
</li>
</ul>
</li>
<li>NAT<ul>
<li>High Available NAT<ul>
<li>Good blog on how to do it here: <a href="https://aws.amazon.com/cn/articles/high-availability-for-amazon-vpc-nat-instances-an-example/" target="_blank" rel="external">https://aws.amazon.com/cn/articles/high-availability-for-amazon-vpc-nat-instances-an-example/</a></li>
</ul>
</li>
</ul>
</li>
<li><p>WAF</p>
<ul>
<li>WAF sandwich<ul>
<li><a href="https://www.cloudaxis.com/2016/11/21/waf-sandwich/" target="_blank" rel="external">WAF Sandwich</a><br><img src="/images/AWS/CSAP/CSAP_WAF_Sandwich.png" alt="CSAP_WAF_Sandwich.png"></li>
</ul>
</li>
</ul>
</li>
<li><p>Multicast</p>
<ul>
<li>Multicast is not available on AWS</li>
<li>You can create a virtual overlay network that runs on the OS level of the instance <a href="https://aws.amazon.com/articles/overlay-multicast-in-amazon-virtual-private-cloud/" target="_blank" rel="external">https://aws.amazon.com/articles/overlay-multicast-in-amazon-virtual-private-cloud/</a></li>
</ul>
</li>
<li>EC2<ul>
<li>RAID <a href="https://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/raid-config.html" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/raid-config.html</a><ul>
<li>RAID 0 - 增加吞吐量但是不提供冗余</li>
<li>RAID 1 - 增加冗余，但是不增加吞吐量</li>
</ul>
</li>
<li>ec2:RunInstances<ul>
<li>Policy中如果RunInstances的Resource只有“arn:aws:ec2:us-east-1:accountid:instance/*”的话，是不能获得新建Instance的权限的</li>
<li>因为新建Instance需要涉及到很多资源, 还需要”key-pair/<em>“,”security-group/</em>“,volume, ami,subnet, network-interface等等</li>
</ul>
</li>
<li>How to increase throughput of the instance bandwidth<ul>
<li>Enhanced networking uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types. SR-IOV is a method of device virtualization that provides higher I/O performance and lower CPU utilization when compared to traditional virtualized network interfaces. Enhanced networking provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies.</li>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html" target="_blank" rel="external">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html</a></li>
</ul>
</li>
</ul>
</li>
<li>Identity Broker<ul>
<li>Assume Role - AssumeRole<ul>
<li><a href="https://docs.aws.amazon.com/zh_cn/STS/latest/APIReference/API_AssumeRole.html" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/STS/latest/APIReference/API_AssumeRole.html</a></li>
</ul>
</li>
<li>GetFederationToken - <ul>
<li><a href="https://docs.aws.amazon.com/zh_cn/STS/latest/APIReference/API_GetFederationToken.html" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/STS/latest/APIReference/API_GetFederationToken.html</a></li>
</ul>
</li>
</ul>
</li>
<li>CloudWatch<ul>
<li>DashBoard<ul>
<li>You can monitor AWS resources in multiple regions using a single CloudWatch dashboard.</li>
<li>select the region and add the metric to dashboard, it can be shown in other region</li>
</ul>
</li>
</ul>
</li>
<li>IAM<ul>
<li>SAML<ul>
<li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html" target="_blank" rel="external">Enabling SAML 2.0 Federated Users to Access the AWS Management Console</a></li>
<li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html" target="_blank" rel="external">About SAML 2.0-based Federation</a></li>
</ul>
</li>
<li>Revoking IAM Role Temporary Security Credential<ul>
<li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_revoke-sessions.html" target="_blank" rel="external">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_revoke-sessions.html</a></li>
</ul>
</li>
</ul>
</li>
<li><p>IPSec</p>
<ul>
<li>advantage of an IPSec<ul>
<li>Data encryption across the internet</li>
<li>Protection of data in transit over the Internet</li>
<li>Peer identity authentication between source and destination (in AWS that is the VPN gateway and customer gateway)</li>
<li>Data integrity protection across the Internet</li>
</ul>
</li>
</ul>
</li>
<li><p>Kinesis</p>
<ul>
<li>What can I do with Amazon Kinesis Data Streams - <a href="https://aws.amazon.com/kinesis/data-streams/faqs/" target="_blank" rel="external">https://aws.amazon.com/kinesis/data-streams/faqs/</a><ul>
<li><strong>Accelerated log and data feed intake</strong>: Instead of waiting to batch up the data, you can have your data producers push data to an Amazon Kinesis data stream as soon as the data is produced, preventing data loss in case of data producer failures. For example, system and application logs can be continuously added to a data stream and be available for processing within seconds. </li>
<li><strong>Real-time metrics and reporting</strong>: You can extract metrics and generate reports from Amazon Kinesis data stream data in real-time. For example, your Amazon Kinesis Application can work on metrics and reporting for system and application logs as the data is streaming in, rather than wait to receive data batches.</li>
<li><strong>Real-time data analytics</strong>: With Amazon Kinesis Data Streams, you can run real-time streaming data analytics. For example, you can add clickstreams to your Amazon Kinesis data stream and have your Amazon Kinesis Application run analytics in real-time, enabling you to gain insights out of your data at a scale of minutes instead of hours or days. </li>
<li><strong>Complex stream processing</strong>: You can create Directed Acyclic Graphs (DAGs) of Amazon Kinesis Applications and data streams. In this scenario, one or more Amazon Kinesis Applications can add data to another Amazon Kinesis data stream for further processing, enabling successive stages of stream processing.</li>
</ul>
</li>
</ul>
</li>
<li><p>OpsWorks</p>
<ul>
<li>AWS OpsWorks Stacks Lifecycle Events<ul>
<li>Setup</li>
<li>Configure</li>
<li>Deploy</li>
<li>Undeploy</li>
<li>Shutdown</li>
</ul>
</li>
</ul>
</li>
<li><p>Direct Connect</p>
<ul>
<li>AWS re:Invent video about DX in 2016 and 2017<ul>
<li><a href="https://www.youtube.com/watch?v=eNxPhHTN8gY" target="_blank" rel="external">https://www.youtube.com/watch?v=eNxPhHTN8gY</a> (2017)</li>
<li><a href="https://www.youtube.com/watch?v=Qep11X1r1QA" target="_blank" rel="external">https://www.youtube.com/watch?v=Qep11X1r1QA</a> (2016)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Whitepaper"><a href="#Whitepaper" class="headerlink" title="Whitepaper"></a>Whitepaper</h2><ul>
<li><a href="https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf" target="_blank" rel="external">https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf</a><ul>
<li><a href="https://aws.amazon.com/blogs/security/how-to-configure-rate-based-blacklisting-with-aws-waf-and-aws-lambda/" target="_blank" rel="external">https://aws.amazon.com/blogs/security/how-to-configure-rate-based-blacklisting-with-aws-waf-and-aws-lambda/</a> </li>
<li><a href="https://aws.amazon.com/blogs/security/how-to-automatically-update-your-security-groups-for-amazon-cloudfront-and-aws-waf-by-using-aws-lambda/" target="_blank" rel="external">https://aws.amazon.com/blogs/security/how-to-automatically-update-your-security-groups-for-amazon-cloudfront-and-aws-waf-by-using-aws-lambda/</a></li>
<li><a href="https://aws.amazon.com/blogs/aws/subscribe-to-aws-public-ip-address-changes-via-amazon-sns/" target="_blank" rel="external">Subscribe to AWS Public IP Address Changes via Amazon SNS</a><ul>
<li>AmazonIpSpaceChanged的Topic是在us-ease-1中的，所以订阅这个Topic的时候，必须切换到N.Virginia才能订阅， 就像Billing Alarm只能在N.Virginia中设置一样</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="延伸"><a href="#延伸" class="headerlink" title="延伸"></a>延伸</h2><ul>
<li><a href="/2017/07/24/AWS-Certified-Developer/" title="AWS Certified Developer - Associate Road Map">AWS Certified Developer - Associate Road Map</a></li>
<li><a href="/2017/12/04/AWS-Certified-Sysops-Administrator-Associate/" title="AWS Certified Sysops Administrator - Associate Road Map">AWS Certified Sysops Administrator - Associate Road Map</a></li>
<li><a href="/2019/10/18/AWS-Advanced-Networking-Specialty/" title="AWS Advanced Networking Specialty 考试心得">AWS Advanced Networking Specialty 考试心得</a>
</li>
</ul>
	  
	</div>

	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
				
    	<li class="prev"><a href="/2018/02/09/non-greedy-match-in-grep-on-macos/" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i>Prev</a></li>
  		

        <li><a href="/archives"><i class="fa fa-archive"></i>Archive</a></li>

		
		   <li class="next"><a href="/2017/12/26/a-simple-elb-access-log-parser-by-ruby/" class="alignright next">Next<i class="fa fa-arrow-circle-o-right"></i></a></li>         
        
	
</ul>
</div>

    </center>
	</div>

    <!-- share -->
    
        
    <div class="bdsharebuttonbox">
        <a href="#" class="bds_more" data-cmd="more"></a>
        <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
        <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
        <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
        <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
        <a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a>
        <a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a>
        <a href="#" class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a>
        <a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a>
    </div>
    <script>
        window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{}};
        with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
    </script>


        

    
	
	<!-- comment -->
	
<section id="comment">
  <h2 class="title">Comments</h2>
  
  	 <div id="disqus_thread">
     <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  	 </div>
  
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta">

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2018-02-07
	</div>
	

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
			<ol class="toc-article"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Official-AWS-Certification-Page"><span class="toc-article-text">Official AWS Certification Page</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#考试指南"><span class="toc-article-text">考试指南</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#涉及到的AWS-Services"><span class="toc-article-text">涉及到的AWS Services</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#视频学习"><span class="toc-article-text">视频学习</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#要点摘录"><span class="toc-article-text">要点摘录</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Domain-1-High-Availability-and-Business-Continuity-15"><span class="toc-article-text">Domain 1 - High Availability and Business Continuity - 15%</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Domain-2-Costing-5"><span class="toc-article-text">Domain 2 -Costing - 5%</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Domain-3-Deployment-Management-10"><span class="toc-article-text">Domain 3 - Deployment Management - 10%</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Domain-4-Network-Design-10"><span class="toc-article-text">Domain 4 - Network Design - 10%</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Domain-5-Data-Storage-15"><span class="toc-article-text">Domain 5 Data Storage - 15%</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Domain-6-Security-20"><span class="toc-article-text">Domain 6 Security - 20%</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Domain-7-Scalability-amp-Elasticity-15"><span class="toc-article-text">Domain 7 - Scalability & Elasticity - 15%</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Domain-8-Cloud-Migration-amp-Hybrid-Architecture-10"><span class="toc-article-text">Domain 8 - Cloud Migration & Hybrid Architecture - 10%</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Additional-Content"><span class="toc-article-text">Additional Content</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Whitepaper"><span class="toc-article-text">Whitepaper</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#延伸"><span class="toc-article-text">延伸</span></a></li></ol>
		</div>
	
	</div>

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/AWS-Certified/">AWS Certified<span>4</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>
    <ul id="tags" class="tag_box list-unstyled collapse in">
	    
  <li><a href="/tags/Solutions-Architect/">Solutions Architect<span>1</span></a></li>
    </ul>
	</div>
	

    <hr>

</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->

<script type="text/javascript">
var disqus_shortname = 'jibing57-blog';
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2020 jibing57
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

</body>
   </html>
